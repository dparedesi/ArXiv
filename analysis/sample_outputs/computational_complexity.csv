paper_id,abstract
2501.00008,"We will study some important properties of Boolean functions based on newly introduced concepts called Special Decomposition of a Set and Special Covering of a Set. These concepts enable us to study important problems concerning Boolean functions represented in conjunctive normal form including the satisfiability problem. Studying the relationship between the Boolean satisfiability problem and the problem of existence of a special covering for set we show that these problems are polynomially equivalent. This means that the problem of existence of a special covering for a set is an NP complete problem. We prove an important theorem regarding the relationship between these problems. The Boolean function in conjunctive normal form is satisfiable if and only if there is a special covering for the set of clauses of this function. The purpose of the article is also to study some important properties of satisfiable Boolean functions using the concepts of special decomposition and special covering of a set. We introduce the concept of generation of satisfiable function by another satisfiable function by means of admissible changes in the clauses of the function. We will prove that if the generation of a function by another function is defined as a binary relation then the set of satisfiable functions of n variables represented in conjunctive normal form with m clauses is partitioned to equivalence classes In addition, extending the rules of admissible changes we prove that arbitrary two satisfiable Boolean functions of n variables represented in conjunctive normal form with m clauses can be generated from each other."
2501.00831,"Decision trees are one of the most fundamental computational models for computing Boolean functions $f : \{0, 1\}^n \mapsto \{0, 1\}$. It is well-known that the depth and size of decision trees are closely related to time and number of processors respectively for computing functions in the CREW-PRAM model. For a given $f$, a fundamental goal is to minimize the depth and/or the size of the decision tree computing it.In this paper, we extend the decision tree model to the world of hazard-free computation. We allow each query to produce three results: zero, one, or unknown. The output could also be: zero, one, or unknown, with the constraint that we should output ""unknown"" only when we cannot determine the answer from the input bits. This setting naturally gives rise to ternary decision trees computing functions, which we call hazard-free decision trees. We prove various lower and upper bounds on the depth and size of hazard-free decision trees and compare them to their Boolean counterparts. We prove optimal separations and relate hazard-free decision tree parameters to well-known Boolean function parameters. We show that the analogues of sensitivity, block sensitivity, and certificate complexity for hazard-free functions are all polynomially equivalent to each other and to hazard-free decision tree depth. i.e., we prove the sensitivity theorem in the hazard-free model. We then prove that hazard-free sensitivity satisfies an interesting structural property that is known to hold in the Boolean world. Hazard-free functions with small hazard-free sensitivity are completely determined by their values in any Hamming ball of small radius in $\{0, u, 1\}^n$."
2501.02347,"Let $A \in \mathbb{Z}^{m \times n}$ be an integer matrix with components bounded by $\Delta$ in absolute value. Cook et al.~(1986) have shown that there exists a universal matrix $B \in \mathbb{Z}^{m' \times n}$ with the following property: For each $b \in \mathbb{Z}^m$, there exists $t \in \mathbb{Z}^{m'}$ such that the integer hull of the polyhedron $P = \{ x \in \mathbb{R}^n \colon Ax \leq b\}$ is described by $P_I = \{ x \in \mathbb{R}^n \colon Bx \leq t\}$. Our \emph{main result} is that $t$ is an \emph{affine} function of $b$ as long as $b$ is from a fixed equivalence class of the lattice $D \cdot \mathbb{Z}^m$. Here $D \in \mathbb{N}$ is a number that depends on $n$ and $\Delta$ only. Furthermore, $D$ as well as the matrix $B$ can be computed in time depending on $\Delta$ and $n$ only. An application of this result is the solution of an open problem posed by Cslovjecsek et al.~(SODA 2024) concerning the complexity of \emph{2-stage-stochastic integer programming} problems. The main tool of our proof is the classical theory of \emph{Chvátal-Gomory cutting planes} and the \emph{elementary closure} of rational polyhedra."
2501.02653,"We establish new correlation bounds and pseudorandom generators for a collection of computation models. These models are all natural generalizations of structured low-degree $F_2$-polynomials that we did not have correlation bounds for before. In particular:1. We construct a PRG for width-2 $poly(n)$-length branching programs which read $d$ bits at a time with seed length $2^{O(\sqrt{\log n})}\cdot d^2\log^2(1/\epsilon)$. This comes quadratically close to optimal dependence in $d$ and $\log(1/\epsilon)$. The previous PRG by Bogdanov, Dvir, Verbin, and Yehudayoff had an exponentially worse dependence on $d$ with seed length of $O(d\log n + d2^d\log(1/\epsilon))$.2. We provide correlation bounds and PRGs against size-$n^{\Omega(\log n)}$ AC0 circuits with either $n^{.99}$ SYM gates (computing an arbitrary symmetric function) or $n^{.49}$ THR gates (computing an arbitrary linear threshold function). Previous work of Servedio and Tan only handled $n^{.49}$ SYM gates or $n^{.24}$ THR gates, and previous work of Lovett and Srinivasan only handled polysize circuits.3. We give exponentially small correlation bounds against degree-$n^{O(1)}$ $F_2$-polynomials set-multilinear over some partition of the input into $n^{.99}$ parts (noting that at $n$ parts, we recover all low-degree polynomials). This generalizes correlation bounds against degree-$(d-1)$ polynomials which are set-multilinear over a fixed partition into $d$ blocks, which were established by Bhrushundi, Harsha, Hatami, Kopparty and Kumar.The common technique behind all of these results is to fortify a hard function with the right type of extractor to obtain stronger correlation bounds. Although this technique has been used in previous work, it relies on the model shrinking to a very small class under random restrictions. Our results show such fortification can be done even for classes that do not enjoy such behavior."
2501.02886,"Gurumukhani et al. (CCC'24) proposed the local enumeration problem Enum(k, t) as an approach to break the Super Strong Exponential Time Hypothesis (SSETH): for a natural number $k$ and a parameter $t$, given an $n$-variate $k$-CNF with no satisfying assignment of Hamming weight less than $t(n)$, enumerate all satisfying assignments of Hamming weight exactly $t(n)$. Furthermore, they gave a randomized algorithm for Enum(k, t) and employed new ideas to analyze the first non-trivial case, namely $k = 3$. In particular, they solved Enum(3, n/2) in expected $1.598^n$ time. A simple construction shows a lower bound of $6^{\frac{n}{4}} \approx 1.565^n$.In this paper, we show that to break SSETH, it is sufficient to consider a simpler local enumeration problem NAE-Enum(k, t): for a natural number $k$ and a parameter $t$, given an $n$-variate $k$-CNF with no satisfying assignment of Hamming weight less than $t(n)$, enumerate all Not-All-Equal (NAE) solutions of Hamming weight exactly $t(n)$, i.e., those that satisfy and falsify some literal in every clause. We refine the algorithm of Gurumukhani et al. and show that it optimally solves NAE-Enum(3, n/2), namely, in expected time $poly(n) \cdot 6^{\frac{n}{4}}$."
2501.03281,Boolean Satisfiability (SAT) problems are expressed as mathematical formulas. This paper presents a matrix representation for these SAT problems.It shows how to use this matrix representation to get the full set of valid satisfying variable assignments. It proves that this is the set of answers for the given problem and is exponential in size relative to the matrix.It presents a simple algorithm that utilizes the inverse of each clause to find an intersection for the matrix. This gives a satisfying variable assignment.
2501.0371,"Decision DNNF (a.k.a. $\wedge_d$-FBDD) is an important special case of Decomposable Negation Normal Form (DNNF). Decision DNNF admits FPT sized representation of CNFs of bounded \emph{primal} treewidth. However, the complexity of representation for CNFs of bounded \emph{incidence} treewidth is wide open.In the main part of this paper we carry out an in-depth study of the $\wedge_d$-OBDD model. We formulate a generic methodology for proving lower bounds for the model. Using this methodology, we reestablish the XP lower bound provided in [arXiv:1708.07767]. We also provide exponential separations between FBDD and $\wedge_d$-OBDD and between $\wedge_d$-OBDD and an ordinary OBDD. The last separation is somewhat surprising since $\wedge_d$-FBDD can be quasipolynomially simulated by FBDD.In the remaining part of the paper, we introduce a relaxed version of Structured Decision DNNF that we name Structured $\wedge_d$-FBDD. We demonstrate that this model is quite powerful for CNFs of bounded incidence treewidth: it has an FPT representation for CNFs that can be turned into ones of bounded primal treewidth by removal of a constant number of clauses (while for both $\wedge_d$-OBDD and Structured Decision DNNF an XP lower bound is triggered by just two long clauses)."
2501.03871,"Segment Routing is a recent network technology that helps optimizing network throughput by providing finer control over the routing paths. Instead of routing directly from a source to a target, packets are routed via intermediate waypoints. Between consecutive waypoints, the packets are routed according to traditional shortest path routing protocols. Bottlenecks in the network can be avoided by such rerouting, preventing overloading parts of the network. The associated NP-hard computational problem is Segment Routing: Given a network on $n$ vertices, $d$ traffic demands (vertex pairs), and a (small) number $k$, the task is to find for each demand pair at most $k$ waypoints such that with shortest path routing along these waypoints, all demands are fulfilled without exceeding the capacities of the network. We investigate if special structures of real-world communication networks could be exploited algorithmically. Our results comprise NP-hardness on graphs with constant treewidth even if only one waypoint per demand is allowed. We further exclude (under standard complexity assumptions) algorithms with running time $f(d) n^{g(k)}$ for any functions $f$ and $g$. We complement these lower bounds with polynomial-time solvable special cases."
2501.04224,"The Constraint Satisfaction Problem (CSP) is ubiquitous in various areas of mathematics and computer science. Many of its variations have been studied including the Counting CSP, where the goal is to find the number of solutions to a CSP instance. The complexity of finding the exact number of solutions of a CSP is well understood (Bulatov, 2013, and Dyer and Richerby, 2013) and the focus has shifted to other variations of the Counting CSP such as counting the number of solutions modulo an integer. This problem has attracted considerable attention recently. In the case of CSPs based on undirected graphs Bulatov and Kazeminia (STOC 2022) obtained a complexity classification for the problem of counting solutions modulo p for arbitrary prime p. In this paper we report on the progress made towards a similar classification for the general CSP, not necessarily based on graphs.We identify several features that make the general case very different from the graph case such as a stronger form of rigidity and the structure of automorphisms of powers of relational structures. We provide a solution algorithm in the case p=2 that works under some additional conditions and prove the hardness of the problem under some assumptions about automorphisms of the powers of the relational structure. We also reduce the general CSP to the case that only uses binary relations satisfying strong additional conditions."
2501.05638,"We show that it is NP-hard to distinguish graphs of linear mim-width at most 1211 from graphs of sim-width at least 1216. This implies that Mim-Width, Sim-Width, One-Sided Mim-Width, and their linear counterparts are all paraNP-complete, i.e., NP-complete to compute even when upper bounded by a constant."
2501.06574,We demonstrate that Col is PSPACE-complete on triangular grid graphs via a reduction from Bounded Two-Player Constraint Logic. This is the most structured graph family that Col is known to be computationally hard for.
2501.07529,"The mutational heterogeneity of tumours can be described with a tree representing the evolutionary history of the tumour. With noisy sequencing data there may be uncertainty in the inferred tree structure, while we may also wish to study patterns in the evolution of cancers in different patients. In such situations, understanding tree similarities is a key challenge, and therefore we present an approach to determine distances between trees. Considering the bounded height of trees, we determine the distances associated with the swap operations over strings. While in general, by solving the {\sc Maximum Common Almost $v$-tree} problem between two trees, we describe an efficient approach to determine the minimum number of operations to transform one tree into another. The inherent noise in current statistical methods for constructing mutation evolution trees of cancer cells presents a significant challenge: handling such collections of trees to determine a consensus tree that accurately represents the set and evaluating the extent of their variability or dispersion. Given a set of mutation trees and the notion of distance, there are at least two natural ways to define the ``target'' tree, such as a min-sum (\emph{median tree}) or a min-max (\emph{closest tree}) of a set of trees. Thus, considering a set of trees as input and dealing with the {\sc median} and {\sc closest} problems, we prove that both problems are \NP-complete, even with only three input trees. In addition, we develop algorithms to obtain upper bounds on the median and closest solutions, which are analysed by the experiments presented on generated and on real databases. We show a fast way to find consensus trees with better results than any tree in the input set while still preserving all internal structure."
2501.07752,"Expander graphs are among the most useful combinatorial objects in theoretical computer science. A line of work studies random walks on expander graphs for their pseudorandomness against various classes of test functions, including symmetric functions, read-only branching programs, permutation branching programs, and $\mathrm{AC}^0$ circuits. The promising results of pseudorandomness of expander random walks against $\mathrm{AC}^0$ circuits indicate a robustness of expander random walks beyond symmetric functions, motivating the question of whether expander random walks can fool more robust \emph{asymmetric} complexity classes, such as $\mathrm{ACC}^0$. In this work, we make progress towards this question by considering certain two-layered circuit compositions of $\mathrm{MOD}[k]$ gates, where we show that these family of circuits are fooled by expander random walks with total variation distance error $O(\lambda)$, where $\lambda$ is the second largest eigenvalue of the underlying expander graph. For $k\geq 3$, these circuits can be highly asymmetric with complicated Fourier characters. In this context, our work takes a step in the direction of fooling more complex asymmetric circuits. Separately, drawing from the learning-theory literature, we construct an explicit threshold circuit in the circuit family $\mathrm{TC}^0$, and show that it is \emph{not} fooled by expander random walk, providing an upper bound on the set of functions fooled by expander random walks."
2501.09545,"We consider a problem of approximating the size of the largest clique in a graph, with a monotone circuit. Concretely, we focus on distinguishing a random Erdős-Renyi graph $\mathcal{G}_{n,p}$, with $p=n^{-\frac{2}{\alpha-1}}$ chosen st. with high probability it does not even have an $\alpha$-clique, from a random clique on $\beta$ vertices (where $\alpha \leq \beta$). Using the approximation method of Razborov, Alon and Boppana showed in 1987 that as long as $\sqrt{\alpha} \beta < n^{1-\delta}/\log n$, this problem requires a monotone circuit of size $n^{\Omega(\delta\sqrt{\alpha})}$, implying a lower bound of $2^{\tilde\Omega(n^{1/3})}$ for the exact version of the problem when $k\approx n^{2/3}$. Recently Cavalar, Kumar, and Rossman improved their result by showing the tight lower bound $n^{\Omega(k)}$, in a limited range $k \leq n^{1/3}$, implying a comparable $2^{\tilde{\Omega}(n^{1/3})}$ lower bound.We combine the ideas of Cavalar, Kumar and Rossman with the recent breakthrough results on the sunflower conjecture by Alweiss, Lovett, Wu and Zhang to show that as long as $\alpha \beta < n^{1-\delta}/\log n$, any monotone circuit rejecting $\mathcal{G}_{n,p}$ while accepting a $\beta$-clique needs to have size at least $n^{\Omega(\delta^2 \alpha)}$; this implies a stronger $2^{\tilde{\Omega}(\sqrt{n})}$ lower bound for the unrestricted version of the problem.We complement this result with a construction of an explicit monotone circuit of size $O(n^{\delta^2 \alpha/2})$ which rejects $\mathcal{G}_{n,p}$, and accepts any graph containing $\beta$-clique whenever $\beta > n^{1-\delta}$. Those two theorems explain the largest $\beta$-clique that can be distinguished from $\mathcal{G}_{n, 1/2}$: when $\beta > n / 2^{C \sqrt{\log n}}$, polynomial size circuit co do it, while for $\beta < n / 2^{\omega(\sqrt{\log n})}$ every circuit needs size $n^{\omega(1)}$."
2501.11192,"We prove new parameterized complexity results for the FO Model Checking problem on a well-known generalization of interval and circular-arc graphs: the class of $H$-graphs, for any fixed multigraph $H$. In particular, we research how the parameterized complexity differs between two subclasses of $H$-graphs: proper $H$-graphs and non-crossing $H$-graphs, each generalizing proper interval graphs and proper circular-arc graphs. We first generalize a known result of Bonnet et al. (IPEC 2022) from interval graphs to $H$-graphs, for any (simple) forest $H$, by showing that for such $H$, the class of $H$-graphs is delineated. This implies that for every hereditary subclass ${\cal D}$ of $H$-graphs, FO Model Checking is in FPT if ${\cal D}$ has bounded twin-width and AW[$*$]-hard otherwise. As proper claw-graphs have unbounded twin-width, this means that FO Model Checking is AW[$*$]-hard for proper $H$-graphs for certain forests $H$ like the claw. In contrast, we show that even for every multigraph $H$, non-crossing $H$-graphs have bounded proper mixed-thinness and hence bounded twin-width, and thus FO Model Checking is in FPT on non-crossing $H$-graphs when parameterized by $\Vert H \Vert+\ell$, where $\Vert H \Vert$ is the size of $H$ and $\ell$ is the size of a formula. It is known that a special case of FO Model Checking, Independent Set, is $\mathsf{W}[1]$-hard on $H$-graphs when parameterized by $\Vert H \Vert +k$, where $k$ is the size of a solution. We strengthen this $\mathsf{W}[1]$-hardness result to proper $H$-graphs. Hence, we solve, in two different ways, an open problem of Chaplick (Discrete Math. 2023), who asked about problems that can be solved faster for non-crossing $H$-graphs than for proper $H$-graphs."
2501.11683,"Flesh and Blood (FAB) is a trading card game that two players need to make a strategy to reduce the life points of their opponent to zero. The mechanics of the game present complex decision-making scenarios of resource management. Due the similarity of other card games, the strategy of the game have scenarios that can turn an NP-problem. This paper presents a model of an aggressive, single-turn strategy as a combinatorial optimization problem, termed the FAB problem. Using mathematical modeling, we demonstrate its equivalence to a 0-1 Knapsack problem, establishing the FAB problem as NP-hard. Additionally, an Integer Linear Programming (ILP) formulation is proposed to tackle real-world instances of the problem. By establishing the computational hardness of optimizing even relatively simple strategies, our work highlights the combinatorial complexity of the game."
2501.1226,"Nonuniform Deterministic Finite Automata (NUDFA) over monoids were invented by Barrington to study boundaries of nonuniform constant-memory computation. Later, results on these automata helped to indentify interesting classes of groups for which equation satisfiability problem is solvable in (probabilistic) polynomial-time. Based on these results, we present a full characterization of groups, for which the identity checking problem has a probabilistic polynomial-time algorithm. We also go beyond groups, and propose how to generalise the notion of NUDFA to arbitrary finite algebraic structures. We study satisfiability of these automata in this more general setting. As a consequence, we present full description of finite algebras from congruence modular varieties for which testing circuit equivalence can be solved by a probabilistic polynomial-time procedure. In our proofs we use two computational complexity assumptions: randomized Expotential Time Hypothesis and Constant Degree Hypothesis."
2501.12282,"This work shows new results on the complexity of games Jelly-No and Hanano with various constraints on the size of the board and number of colours. Hanano and Jelly-No are one-player, 2D side-view puzzle games with a dynamic board consisting of coloured, movable blocks disposed on platforms. These blocks can be moved by the player and are subject to gravity. Both games somehow vary in their gameplay, but the goal is always to move the coloured blocks in order to reach a specific configuration and make them interact with each other or with other elements of the game. In Jelly-No the goal is to merge all coloured blocks of a same colour, which also happens when they make contact. In Hanano the goal is to make all the coloured blocks bloom by making contact with flowers of the same colour. Jelly-No was proven by Chao Yang to be NP-Complete under the restriction that all movable blocks are the same colour and NP-Hard for more colours. Hanano was proven by Michael C. Chavrimootoo to be PSPACE-Complete under the restriction that all movable blocks are the same colour. However, the question whether Jelly-No for more than one colours is also PSPACE-complete or if it too stays in NP was left open. In this paper, we settle this question, proving that Jelly-No is PSPACE-Complete with an unbounded number of colours. We further show that, if we allow black jellies (that is, jellies that do not need to be merged), the game is PSPACE-complete even for one colour. We further show that one-colour Jelly-No and Hanano remain NP-Hard even if the width or the height of the board are small constants."
2501.12365,"Computing the Fourier transform of a $q$-ary function $f:\mathbb{Z}_{q}^n\rightarrow \mathbb{R}$, which maps $q$-ary sequences to real numbers, is an important problem in mathematics with wide-ranging applications in biology, signal processing, and machine learning. Previous studies have shown that, under the sparsity assumption, the Fourier transform can be computed efficiently using fast and sample-efficient algorithms. However, in most practical settings, the function is defined over a more general space -- the space of generalized $q$-ary sequences $\mathbb{Z}_{q_1} \times \mathbb{Z}_{q_2} \times \cdots \times \mathbb{Z}_{q_n}$ -- where each $\mathbb{Z}_{q_i}$ corresponds to integers modulo $q_i$. Herein, we develop GFast, a coding theoretic algorithm that computes the $S$-sparse Fourier transform of $f$ with a sample complexity of $O(Sn)$, computational complexity of $O(Sn \log N)$, and a failure probability that approaches zero as $N=\prod_{i=1}^n q_i \rightarrow \infty$ with $S = N^\delta$ for some $0 \leq \delta < 1$. We show that a noise-robust version of GFast computes the transform with a sample complexity of $O(Sn^2)$ and computational complexity of $O(Sn^2 \log N)$ under the same high probability guarantees. Additionally, we demonstrate that GFast computes the sparse Fourier transform of generalized $q$-ary functions $8\times$ faster using $16\times$ fewer samples on synthetic experiments, and enables explaining real-world heart disease diagnosis and protein fitness models using up to $13\times$ fewer samples compared to existing Fourier algorithms applied to the most efficient parameterization of the models as $q$-ary functions."
2501.14337,"We design an Interactive Oracle Proof of Proximity (IOPP) for codes on graphs inspired by the FRI protocol. The soundness is significantly improved compared to the FRI, the complexity parameters are comparable, and there are no restrictions on the field used, enabling to consider new codes to design code-based SNARKs."
2501.14569,"I present an analytic approach to establishing the presence of phase transitions in a large set of decision problems. This approach does not require extensive computational study of the problems considered. The set -- that of all paddable problems over even-sized alphabets satisfying a condition similar to not being sparse -- shown to exhibit phase transitions contains many ""practical"" decision problems, is very large, and also contains extremely intractable problems."
2502.00434,"Circuits in deterministic decomposable negation normal form (d-DNNF) are representations of Boolean functions that enable linear-time model counting. This paper strengthens our theoretical knowledge of what classes of functions can be efficiently transformed, or compiled, into d-DNNF. Our main contribution is the fixed-parameter tractable (FPT) compilation of conjunctions of specific constraints parameterized by incidence treewidth. This subsumes the known result for CNF. The constraints in question are all functions representable by constant-width ordered binary decision diagrams (OBDDs) for all variable orderings. For instance, this includes parity constraints and cardinality constraints with constant threshold. The running time of the FPT compilation is singly exponential in the incidence treewidth but hides large constants in the exponent. To balance that, we give a more efficient FPT algorithm for model counting that applies to a sub-family of the constraints and does not require compilation."
2502.0068,"The complexity class $\exists\mathbb R$, standing for the complexity of deciding the existential first order theory of the reals as real closed field in the Turing model, has raised considerable interest in recent years. It is well known that NP $ \subseteq \exists\mathbb R\subseteq$ PSPACE. In their compendium, Schaefer, Cardinal, and Miltzow give a comprehensive presentation of results together with a rich collection of open problems. Here, we answer some of them dealing with structural issues of $\exists\mathbb R$ as a complexity class. We show analogues of the classical results of Baker, Gill, and Solovay finding oracles which do and do not separate NP form $\exists\mathbb R$, of Ladner's theorem showing the existence of problems in $\exists\mathbb R \setminus$ NP not being complete for $\exists\mathbb R$ (in case the two classes are different), as well as a characterization of $\exists\mathbb R$ by means of descriptive complexity."
2502.019,"We study linearity testing over the $p$-biased hypercube $(\{0,1\}^n, \mu_p^{\otimes n})$ in the 1% regime. For a distribution $\nu$ supported over $\{x\in \{0,1\}^k:\sum_{i=1}^k x_i=0 \text{ (mod 2)} \}$, with marginal distribution $\mu_p$ in each coordinate, the corresponding $k$-query linearity test $\text{Lin}(\nu)$ proceeds as follows: Given query access to a function $f:\{0,1\}^n\to \{-1,1\}$, sample $(x_1,\dots,x_k)\sim \nu^{\otimes n}$, query $f$ on $x_1,\dots,x_k$, and accept if and only if $\prod_{i\in [k]}f(x_i)=1$.Building on the work of Bhangale, Khot, and Minzer (STOC '23), we show, for $0 < p \leq \frac{1}{2}$, that if $k \geq 1 + \frac{1}{p}$, then there exists a distribution $\nu$ such that the test $\text{Lin}(\nu)$ works in the 1% regime; that is, any function $f:\{0,1\}^n\to \{-1,1\}$ passing the test $\text{Lin}(\nu)$ with probability $\geq \frac{1}{2}+\epsilon$, for some constant $\epsilon > 0$, satisfies $\Pr_{x\sim \mu_p^{\otimes n}}[f(x)=g(x)] \geq \frac{1}{2}+\delta$, for some linear function $g$, and a constant $\delta = \delta(\epsilon)>0$.Conversely, we show that if $k < 1+\frac{1}{p}$, then no such test $\text{Lin}(\nu)$ works in the 1% regime. Our key observation is that the linearity test $\text{Lin}(\nu)$ works if and only if the distribution $\nu$ satisfies a certain pairwise independence property."
2502.02012,"The complexity classification of the Holant problem has remained unresolved for the past fifteen years. Counting complex-weighted Eulerian orientation problems, denoted as #EO, is regarded as one of the most significant challenges to the comprehensive complexity classification of the Holant problem. This article presents an $\text{FP}^\text{NP}$ vs. #P dichotomy for #EO, demonstrating that #EO defined by a signature set is either #P-hard or polynomial-time computable with a specific NP oracle. This result provides a comprehensive complexity classification for #EO, and potentially leads to a dichotomy for the Holant problem. Furthermore, we derive three additional dichotomies related to the Holant problem from the dichotomy for #EO."
2502.02022,"We study a decision tree model in which one is allowed to query subsets of variables. This model is a generalization of the standard decision tree model. For example, the $\lor-$decision (or $T_1$-decision) model has two queries, one is a bit-query and one is the $\lor$-query with arbitrary variables. We show that a monotone property graph, i.e. nontree graph is lower bounded by $n\log n$ in $T_1$-decision tree model. Also, in a different but stronger model, $T_2$-decision tree model, we show that the majority function and symmetric function can be queried in $\frac{3n}{4}$ and $n$, respectively."
2502.0209,"The path to the solution of Feder-Vardi dichotomy conjecture by Bulatov and Zhuk led through showing that more and more general algebraic conditions imply polynomial-time algorithms for the finite-domain Constraint Satisfaction Problems (CSPs) whose templates satisfy them. These investigations resulted in the discovery of the appropriate height 1 Maltsev conditions characterizing bounded strict width, bounded width, the applicability of the few-subpowers algorithm, and many others.For problems in the range of the similar Bodirsky-Pinsker conjecture on infinite-domain CSPs, one can only find such a characterization for the notion of bounded strict width, with a proof essentially the same as in the finite case. In this paper, we provide the first non-trivial results showing that certain height 1 Maltsev conditions imply bounded width, and in consequence tractability, for a natural subclass of templates within the Bodirsky-Pinsker conjecture which includes many templates in the literature as well as templates for which no complexity classification is known."
2502.02442,"It is a well-known fact that the permanent polynomial is complete for the complexity class VNP, and it is largely suspected that the determinant does not share this property, despite its similar expression. We study the question of why the VNP-completeness proof of the permanent fails for the determinant. We isolate three fundamental properties that are sufficient to prove a polynomial sequence is VNP-hard, of which two are shared by both the permanent and the determinant. We proceed to show that the permanent satisfies the third property, which we refer to as the ``cost of a boolean sum,"" while the determinant does not, showcasing the fundamental difference between the polynomial families. We further note that this differentiation also applies in the border complexity setting and that our results apply for counting complexity."
2502.02777,"The even online Kolmogorov complexity of a string $x = x_1 x_2 \cdots x_{n}$ is the minimal length of a program that for all $i\le n/2$, on input $x_1x_3 \cdots x_{2i-1}$ outputs $x_{2i}$. The odd complexity is defined similarly. The sum of the odd and even complexities is called the dialogue complexity.In [Bauwens, 2014] it is proven that for all $n$, there exist $n$-bit $x$ for which the dialogue complexity exceeds the Kolmogorov complexity by $n\log \frac 4 3 + O(\log n)$. Let $\mathrm C^s(x)$ denote the Kolmogorov complexity with space bound~$s$. Here, we prove that the space-bounded dialogue complexity with bound $s + 6n + O(1)$ is at most $\mathrm C^{s}(x) + O(\log (sn))$, where $n=|x|$."
2502.0315,"We prove that if a degree-$d$ homogeneous polynomial $f$ has border Waring rank $\underline{\mathrm{WR}}({f}) = r$, then its Waring rank is bounded by \[ {\mathrm{WR}}({f}) \leq d \cdot r^{O(\sqrt{r})}. \] This result significantly improves upon the recent bound ${\mathrm{WR}}({f}) \leq d \cdot 4^r$ established in [Dutta, Gesmundo, Ikenmeyer, Jindal, and Lysikov, STACS 2024], which itself was an improvement over the earlier bound ${\mathrm{WR}}({f}) \leq d^r$."
2502.04581,"In the last three decades, the $k$-SUM hypothesis has emerged as a satisfying explanation of long-standing time barriers for a variety of algorithmic problems. Yet to this day, the literature knows of only few proven consequences of a refutation of this hypothesis. Taking a descriptive complexity viewpoint, we ask: What is the largest logically defined class of problems \emph{captured} by the $k$-SUM problem?To this end, we introduce a class $\mathsf{FOP}_{\mathbb{Z}}$ of problems corresponding to deciding sentences in Presburger arithmetic/linear integer arithmetic over finite subsets of integers.We establish two large fragments for which the $k$-SUM problem is complete under fine-grained reductions:1. The $k$-SUM problem is complete for deciding the sentences with $k$ existential quantifiers.2. The $3$-SUM problem is complete for all $3$-quantifier sentences of $\mathsf{FOP}_{\mathbb{Z}}$ expressible using at most $3$ linear inequalities.Specifically, a faster-than-$n^{\lceil k/2 \rceil \pm o(1)}$ algorithm for $k$-SUM (or faster-than-$n^{2 \pm o(1)}$ algorithm for $3$-SUM, respectively) directly translate to polynomial speedups of a general algorithm for \emph{all} sentences in the respective fragment.Observing a barrier for proving completeness of $3$-SUM for the entire class $\mathsf{FOP}_{\mathbb{Z}}$, we turn to the question which other -- seemingly more general -- problems are complete for $\mathsf{FOP}_{\mathbb{Z}}$. In this direction, we establish $\mathsf{FOP}_{\mathbb{Z}}$-completeness of the \emph{problem pair} of Pareto Sum Verification and Hausdorff Distance under $n$ Translations under the $L_\infty$/$L_1$ norm in $\mathbb{Z}^d$. In particular, our results invite to investigate Pareto Sum Verification as a high-dimensional generalization of 3-SUM."
2502.05278,"The computational complexity of polynomial ideals and Gröbner bases has been studied since the 1980s. In recent years, the related notions of polynomial subalgebras and SAGBI bases have gained more and more attention in computational algebra, with a view towards effective algorithms. We investigate the computational complexity of the subalgebra membership problem and degree bounds. In particular, we show completeness for the complexity class EXPSPACE and prove PSPACE-completeness for homogeneous algebras. We highlight parallels and differences compared to the settings of ideals, and also look at important classes of polynomials such as monomial algebras."
2502.05289,"We exhibit a new obstacle to the nascent algorithmic theory for classes excluding an induced minor. We indeed show that on the class of string graphs -- which avoids the 1-subdivision of, say, $K_5$ as an induced minor -- Induced 2-Disjoint Paths is NP-complete. So, while $k$-Disjoint Paths, for a fixed $k$, is polynomial-time solvable in general graphs, the absence of a graph as an induced minor does not make its induced variant tractable, even for $k=2$. This answers a question of Korhonen and Lokshtanov [SODA '24], and complements a polynomial-time algorithm for Induced $k$-Disjoint Paths in classes of bounded genus by Kobayashi and Kawarabayashi [SODA '09]. In addition to being string graphs, our produced hard instances are subgraphs of a constant power of bounded-degree planar graphs, hence have bounded twin-width and bounded maximum degree.We also leverage our new result to show that there is a fixed subcubic graph $H$ such that deciding if an input graph contains $H$ as an induced subdivision is NP-complete. Until now, all the graphs $H$ for which such a statement was known had a vertex of degree at least 4. This answers a question by Chudnovsky, Seymour, and the fourth author [JCTB '13], and by Le [JGT '19]. Finally we resolve another question of Korhonen and Lokshtanov by exhibiting a subcubic graph $H$ without two adjacent degree-3 vertices and such that deciding if an input $n$-vertex graph contains $H$ as an induced minor is NP-complete, and unless the Exponential-Time Hypothesis fails, requires time $2^{\Omega(\sqrt n)}$. This complements an algorithm running in subexponential time $2^{O(n^{2/3} \log n)}$ by these authors [SODA '24] under the same technical condition."
2502.05348,"We consider the general problem of blocking all solutions of some given combinatorial problem with only few elements. For example, the problem of destroying all maximum cliques of a given graph by forbidding only few vertices. Problems of this kind are so fundamental that they have been studied under many different names in many different disjoint research communities already since the 90s. Depending on the context, they have been called the interdiction, most vital vertex, most vital edge, blocker, or vertex deletion problem. Despite their apparent popularity, surprisingly little is known about the computational complexity of interdiction problems in the case where the original problem is already NP-complete. In this paper, we fill that gap of knowledge by showing that a large amount of interdiction problems are even harder than NP-hard. Namely, they are complete for the second stage of Stockmeyer's polynomial hierarchy, the complexity class $\Sigma^p_2$. Such complexity insights are important because they imply that all these problems can not be modelled by a compact integer program (unless the unlikely conjecture NP $= \Sigma_2^p$ holds). Concretely, we prove $\Sigma^p_2$-completeness of the following interdiction problems: satisfiability, 3satisfiability, dominating set, set cover, hitting set, feedback vertex set, feedback arc set, uncapacitated facility location, $p$-center, $p$-median, independent set, clique, subset sum, knapsack, Hamiltonian path/cycle (directed/undirected), TSP, $k$ directed vertex disjoint path ($k \geq 2$), Steiner tree. We show that all of these problems share an abstract property which implies that their interdiction counterpart is $\Sigma_2^p$-complete. Thus, all of these problems are $\Sigma_2^p$-complete \enquote{for the same reason}. Our result extends a recent framework by Grüne and Wulf."
2502.05597,"\textsf{Holant} is an essential framework in the field of counting complexity. For over fifteen years, researchers have been clarifying the complexity classification for complex-valued \textsf{Holant} on the Boolean domain, a challenge that remains unresolved. In this article, we prove a complexity dichotomy for complex-valued \textsf{Holant} on Boolean domain when a non-trivial signature of odd arity exists. This dichotomy is based on the dichotomy for \textsf{\#EO}, and consequently is an $\text{FP}^\text{NP}$ vs. \#P dichotomy as well, stating that each problem is either in $\text{FP}^\text{NP}$ or \#P-hard.Furthermore, we establish a generalized version of the decomposition lemma for complex-valued \textsf{Holant} on Boolean domain. It asserts that each signature can be derived from its tensor product with other signatures, or conversely, the problem itself is in $\text{FP}^\text{NP}$. We believe that this result is a powerful method for building reductions in complex-valued \textsf{Holant}, as it is also employed as a pivotal technique in the proof of the aforementioned dichotomy in this article."
2502.05647,"Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to analyze gene expression at the cellular level. By providing data on gene expression for each individual cell, scRNA-seq generates large datasets with thousands of genes. However, handling such high-dimensional data poses computational challenges due to increased complexity. Dimensionality reduction becomes crucial for scRNA-seq analysis. Various dimensionality reduction algorithms, including Principal Component Analysis (PCA), Uniform Manifold Approximation and Projection (UMAP), and t-Distributed Stochastic Neighbor Embedding (t-SNE), are commonly used to address this challenge. These methods transform the original high-dimensional data into a lower-dimensional representation while preserving relevant information. In this paper we propose {\methodname}. Instead of applying dimensionality reduction directly to the entire dataset, we divide it into multiple subspaces. Within each subspace, we apply dimension reduction techniques, and then merge the reduced data. {\methodname} offers four variations for subspacing. Our experimental results demonstrate that clustering based on subspacing yields better accuracy than working with the full dataset. Across a variety of scRNA-seq datasets, {\methodname} consistently outperforms existing state-of-the-art clustering tools."
2502.06048,"Given an undirected graph $G$ and an integer $k$, the Secluded $\Pi$-Subgraph problem asks you to find a maximum size induced subgraph that satisfies a property $\Pi$ and has at most $k$ neighbors in the rest of the graph. This problem has been extensively studied; however, there is no prior study of the problem in directed graphs. This question has been mentioned by Jansen et al. [ISAAC'23].In this paper, we initiate the study of Secluded Subgraph problem in directed graphs by incorporating different notions of neighborhoods: in-neighborhood, out-neighborhood, and their union. Formally, we call these problems {\{In, Out, Total\}-Secluded $\Pi$-Subgraph, where given a directed graph $G$ and integers $k$, we want to find an induced subgraph satisfying $\Pi$ of maximum size that has at most $k$ in/out/total-neighbors in the rest of the graph, respectively. We investigate the parameterized complexity of these problems for different properties $\Pi$. In particular, we prove the following parameterized results: - We design an FPT algorithm for the Total-Secluded Strongly Connected Subgraph problem when parameterized by $k$. - We show that the In/Out-Secluded $\mathcal{F}$-Free Subgraph problem with parameter $k+w$ is W[1]-hard, where $\mathcal{F}$ is a family of directed graphs except any subgraph of a star graph whose edges are directed towards the center. This result also implies that In/Out-Secluded DAG is W[1]-hard, unlike the undirected variants of the two problems, which are FPT. - We design an FPT-algorithm for In/Out/Total-Secluded $\alpha$-Bounded Subgraph when parameterized by $k$, where $\alpha$-bounded graphs are a superclass of tournaments. - For undirected graphs, we improve the best-known FPT algorithm for Secluded Clique by providing a faster FPT algorithm that runs in time $1.6181^kn^{\mathcal{O}(1)}$."
2502.06464,"In their seminal work on the Stable Marriage Problem (SM), Gale and Shapley introduced a generalization of SM referred to as the Stable Roommates Problem (SR). An instance of SR consists of a set of $2n$ agents, and each agent has preferences in the form of a ranked list of all other agents. The goal is to find a one-to-one matching between the agents that is stable in the sense that no pair of agents have a mutual incentive to deviate from the matching. Unlike the (bipartite) stable marriage problem, in SR, stable matchings need not exist. Irving devised an algorithm that finds a stable matching or reports that none exists in $O(n^2)$ time. In their influential 1989 text, Gusfield and Irving posed the question of whether $\Omega(n^2)$ time is required for SR solvability -- the task of deciding if an SR instance admits a stable matching.In this paper we provide an affirmative answer to Gusfield and Irving's question. We show that any (randomized) algorithm that decides SR solvability requires $\Omega(n^2)$ adaptive Boolean queries to the agents' preferences (in expectation). Our argument follows from a reduction from the communication complexity of the set disjointness function. The query lower bound implies quadratic time lower bounds for Turing machines, and memory access lower bounds for random access machines. Thus, we establish that Irving's algorithm is optimal (up to a logarithmic factor) in a very strong sense."
2502.0674,"The central open question of algebraic complexity is whether VP is unequal to VNP, which is saying that the permanent cannot be represented by families of polynomial-size algebraic circuits. For symmetric algebraic circuits, this has been confirmed by Dawar and Wilsenach (2020) who showed exponential lower bounds on the size of symmetric circuits for the permanent. In this work, we set out to develop a more general symmetric algebraic complexity theory. Our main result is that a family of symmetric polynomials admits small symmetric circuits if and only if they can be written as a linear combination of homomorphism counting polynomials of graphs of bounded treewidth. We also establish a relationship between the symmetric complexity of subgraph counting polynomials and the vertex cover number of the pattern graph. As a concrete example, we examine the symmetric complexity of immanant families (a generalisation of the determinant and permanent) and show that a known conditional dichotomy due to Curticapean (2021) holds unconditionally in the symmetric setting."
2502.06762,"Larrauri and Živný [ICALP'25/ACM ToCL'24] recently established a complete complexity classification of the problem of solving a system of equations over a monoid $N$ assuming that a solution exists over a monoid $M$, where both monoids are finite and $M$ admits a homomorphism to $N$. Using the algebraic approach to promise constraint satisfaction problems, we extend their complexity classification in two directions: we obtain a complexity dichotomy in the case where arbitrary relations are added to the monoids, and we moreover allow the monoid $M$ to be finitely generated."
2502.07916,"In this paper we present two reductions between variants of the Code Equivalence problem. We give polynomial-time Karp reductions from Permutation Code Equivalence (PCE) to both Linear Code Equivalence (LCE) and Signed Permutation Code Equivalence (SPCE). Along with a Karp reduction from SPCE to the Lattice Isomorphism Problem (LIP) proved in a paper by Bennett and Win (2024), our second result implies a reduction from PCE to LIP."
2502.08272,"We study weighted pseudorandom generators (WPRGs) and derandomizations for read-once branching programs (ROBPs). Denote $n$ and $w$ as the length and the width of a ROBP. We have the following results.For standard ROBPs, we give an explicit $\varepsilon$-WPRG with seed length$$O\left(\frac{\log n\log (nw)}{\max\left\{1,\log\log w-\log\log n\right\}}+\log w \left(\log\log\log w-\log\log\max\left\{2,\frac{\log w}{\log \frac{n}{\varepsilon}}\right\}\right)+\log\frac{1}{\varepsilon}\right).$$For permutation ROBPs with unbounded widths and single accept nodes, we give an explicit $\varepsilon$-WPRG with seed length$$O\left( \log n\left( \log\log n + \sqrt{\log(1/\varepsilon)} \right)+\log(1/\varepsilon)\right). $$We also give a new Nisan-Zuckerman style derandomization for regular ROBPs with width $w$, length $n = 2^{O(\sqrt{\log w})}$, and multiple accept nodes. We attain optimal space complexity $O(\log w)$ for arbitrary approximation error $\varepsilon = 1/\text{poly} (w)$.All our results are based on iterative weighted pseudorandom reductions, which can iteratively reduce fooling long ROBPs to fooling short ones."
2502.09279,"While theoretical computer science primarily works with discrete models of computation, like the Turing machine and the wordRAM, there are many scenarios in which introducing real computation models is more adequate. We want to compare real models of computation with discrete models of computation. We do this by means of oracle separation results.We define the notion of a real Turing machine as an extension of the (binary) Turing machine by adding a real tape. Using those machines, we define and study the real polynomial hierarchy RPH. We are interested in RPH as the first level of the hierarchy corresponds to the well-known complexity class ER. It is known that $NP \subseteq ER \subseteq PSPACE$ and furthermore $PH \subseteq RPH \subseteq PSPACE$. We are interested to know if any of those inclusions are tight. In the absence of unconditional separations of complexity classes, we turn to oracle separation. We develop a technique that allows us to transform oracle separation results from the binary world to the real world. As applications, we show there are oracles such that:- $RPH^O$ proper subset of $PSPACE^O$,- $\Sigma_{k+1}^O$ not contained in $\Sigma_kR^O$, for all $k\geq 0$,- $\Sigma_kR^O$ proper subset of $\Sigma_{k+1}R^O$, for all $k\geq 0$,- $BQP^O$ not contained in $RPH^O$.Our results hint that ER is strictly contained in PSPACE and that there is a separation between the different levels of the real polynomial hierarchy. We also bound the power of real computations by showing that NP-hard problems are unlikely to be solvable using polynomial time on a realRAM. Furthermore, our oracle separations hint that polynomial-time quantum computing cannot be simulated on an efficient real Turing machine."
2502.09358,"Given a finite non-decreasing sequence $d=(d_1,\ldots,d_n)$ of natural numbers, the Graph Realization problem asks whether $d$ is a graphic sequence, i.e., there exists a labeled simple graph such that $(d_1,\ldots,d_n)$ is the degree sequence of this graph. Such a problem can be solved in polynomial time due to the Erdős and Gallai characterization of graphic sequences. Since vertex degree is the size of a trivial edge cut, we consider a natural generalization of Graph Realization, where we are given a finite sequence $d=(d_1,\ldots,d_n)$ of natural numbers (representing the trivial edge cut sizes) and a list of nontrivial cut constraints $\mathcal{L}$ composed of pairs $(S_j,\ell_j)$ where $S_j\subset \{v_1,\ldots,v_n\}$, and $\ell_j$ is a natural number. In such a problem, we are asked whether there is a simple graph with vertex set $V=\{v_1,\ldots,v_n\}$ such that $v_i$ has degree $d_i$ and $\partial(S_j)$ is an edge cut of size $\ell_j$, for each $(S_j,\ell_j)\in \mathcal{L}$. We show that such a problem is polynomial-time solvable whenever each $S_j$ has size at most three. Conversely, assuming P $\neq$ NP, we prove that it cannot be solved in polynomial time when $\mathcal{L}$ contains pairs with sets of size four, and our hardness result holds even assuming that each $d_i$ of $d$ equals $1$."
2502.09584,"We initiate the study of differentially private data-compression schemes motivated by the insecurity of the popular ""Compress-Then-Encrypt"" framework. Data compression is a useful tool which exploits redundancy in data to reduce storage/bandwidth when files are stored or transmitted. However, if the contents of a file are confidential then the length of a compressed file might leak confidential information about the content of the file itself. Encrypting a compressed file does not eliminate this leakage as data encryption schemes are only designed to hide the content of confidential message instead of the length of the message. In our proposed Differentially Private Compress-Then-Encrypt framework, we add a random positive amount of padding to the compressed file to ensure that any leakage satisfies the rigorous privacy guarantee of $(\epsilon,\delta)$-differential privacy. The amount of padding that needs to be added depends on the sensitivity of the compression scheme to small changes in the input, i.e., to what degree can changing a single character of the input message impact the length of the compressed file. While some popular compression schemes are highly sensitive to small changes in the input, we argue that effective data compression schemes do not necessarily have high sensitivity. Our primary technical contribution is analyzing the fine-grained sensitivity of the LZ77 compression scheme (IEEE Trans. Inf. Theory 1977) which is one of the most common compression schemes used in practice. We show that the global sensitivity of the LZ77 compression scheme has the upper bound $O(W^{2/3}\log n)$ where $W\leq n$ denotes the size of the sliding window. When $W=n$, we show the lower bound $\Omega(n^{2/3}\log^{1/3}n)$ for the global sensitivity of the LZ77 compression scheme which is tight up to a sublogarithmic factor."
2502.10103,"The membership problem for an algebraic structure asks whether a given element is contained in some substructure, which is usually given by generators. In this work we study the membership problem, as well as the conjugacy problem, for finite inverse semigroups. The closely related membership problem for finite semigroups has been shown to be PSPACE-complete in the transformation model by Kozen (1977) and NL-complete in the Cayley table model by Jones, Lien, and Laaser (1976). In the partial bijection model, the membership and the conjugacy problem for finite inverse semigroups were shown to be PSPACE-complete by Birget and Margolis (2008) and by Jack (2023).Here we present a more detailed analysis of the complexity of the membership and conjugacy problems parametrized by varieties of finite inverse semigroups. We establish dichotomy theorems for the partial bijection model and for the Cayley table model. In the partial bijection model these problems are in NC (resp. NP for conjugacy) for strict inverse semigroups and PSPACE-complete otherwise. In the Cayley table model we obtain general LOGSPACE-algorithms as well as NPOLYLOGTIME upper bounds for Clifford semigroups and LOGSPACE-completeness otherwise.Furthermore, by applying our findings, we show the following: the intersection non-emptiness problem for inverse automata is PSPACE-complete even for automata with only two states; the subpower membership problem is in NC for every strict inverse semi-group and PSPACE-complete otherwise; the minimum generating set and the equation satisfiability problems are in NP for varieties of finite strict inverse semigroups and PSPACE-complete otherwise."
2502.10449,"In this paper, we study the parameterized complexity of the MaxMin versions of two fundamental separation problems: Maximum Minimal $st$-Separator and Maximum Minimal Odd Cycle Transversal (OCT), both parameterized by the solution size. In the Maximum Minimal $st$-Separator problem, given a graph $G$, two distinct vertices $s$ and $t$ and a positive integer $k$, the goal is to determine whether there exists a minimal $st$-separator in $G$ of size at least $k$. Similarly, the Maximum Minimal OCT problem seeks to determine if there exists a minimal set of vertices whose deletion results in a bipartite graph, and whose size is at least $k$. We demonstrate that both problems are fixed-parameter tractable parameterized by $k$. Our FPT algorithm for Maximum Minimal $st$-Separator answers the open question by Hanaka, Bodlaender, van der Zanden and Ono (TCS 2019).One unique insight from this work is the following. We use the meta-result of Lokshtanov, Ramanujan, Saurabh and Zehavi (ICALP 2018) that enables us to reduce our problems to highly unbreakable graphs. This is interesting, as an explicit use of the recursive understanding and randomized contractions framework of Chitnis, Cygan, Hajiaghayi, Pilipczuk and Pilipczuk (SICOMP 2016) to reduce to the highly unbreakable graphs setting (which is the result that Lokshtanov et al. tries to abstract out in their meta-theorem) does not seem obvious because certain ``extension'' variants of our problems are W[1]-hard."
2502.1239,"We present an $O^*(|\mathbb{F}|^{\min\left\{R,\ \sum_{d\ge 2} n_d\right\} + (R-n_0)(\sum_{d\ne 0} n_d)})$-time algorithm for determining whether the rank of a concise tensor $T\in\mathbb{F}^{n_0\times\dots\times n_{D-1}}$ is $\le R$, assuming $n_0\ge\dots\ge n_{D-1}$ and $R\ge n_0$. For 3-dimensional tensors, we have a second algorithm running in $O^*(|\mathbb{F}|^{n_0+n_2 + (R-n_0+1-r_*)(n_1+n_2)+r_*^2})$ time, where $r_*:=\left\lfloor\frac{R}{n_0}\right\rfloor+1$. Both algorithms use polynomial space and improve on our previous work, which achieved running time $O^*(|\mathbb{F}|^{n_0+(R-n_0)(\sum_d n_d)})$."
2502.14353,"An opinion illusion refers to a phenomenon in social networks where agents may witness distributions of opinions among their neighbours that do not accurately reflect the true distribution of opinions in the population as a whole. A specific case of this occurs when there are only two possible choices, such as whether to receive the COVID-19 vaccine or vote on EU membership, which is commonly referred to as a majority illusion. In this work, we study the topological properties of social networks that lead to opinion illusions and focus on minimizing the number of agents that need to be influenced to eliminate these illusions. To do so, we propose an initial, but systematic study of the algorithmic behaviour of this problem.We show that the problem is NP-hard even for underlying topologies that are rather restrictive, being planar and of bounded diameter. We then look for exact algorithms that scale well as the input grows (FPT). We argue the in-existence of such algorithms even when the number of vertices that must be influenced is bounded, or when the social network is arranged in a ``path-like'' fashion (has bounded pathwidth). On the positive side, we present an FPT algorithm for networks with ``star-like'' structure (bounded vertex cover number). Finally, we construct an FPT algorithm for ``tree-like'' networks (bounded treewidth) when the number of vertices that must be influenced is bounded. This algorithm is then used to provide a PTAS for planar graphs."
2502.14358,"In the last year, there have been some remarkable improvements in the combinatorial list-size bounds of Folded Reed Solomon codes and multiplicity codes. Starting from the work on Kopparty, Ron-Zewi, Saraf and Wootters (SIAM J. Comput. 2023) (and subsequent simplifications due to Tamo (IEEE Trans. Inform. Theory 2024), we have had dramatic improvements in the list-size bounds of FRS codes due to Srivastava (SODA 2025) and Chen & Zhang (STOC 2025). In this note, we give a short exposition of these three results (Tamo, Srivastava and Chen-Zhang)."
2502.15024,"We investigate implications of the (extended) low-degree conjecture (recently formalized in [MW23]) in the context of the symmetric stochastic block model. Assuming the conjecture holds, we establish that no polynomial-time algorithm can weakly recover community labels below the Kesten-Stigum (KS) threshold. In particular, we rule out polynomial-time estimators that, with constant probability, achieve correlation with the true communities that is significantly better than random. Whereas, above the KS threshold, polynomial-time algorithms are known to achieve constant correlation with the true communities with high probability[Mas14,AS15].To our knowledge, we provide the first rigorous evidence for the sharp transition in recovery rate for polynomial-time algorithms at the KS threshold. Notably, under a stronger version of the low-degree conjecture, our lower bound remains valid even when the number of blocks diverges. Furthermore, our results provide evidence of a computational-to-statistical gap in learning the parameters of stochastic block models.In contrast to prior work, which either (i) rules out polynomial-time algorithms for hypothesis testing with 1-o(1) success probability [Hopkins18, BBK+21a] under the low-degree conjecture, or (ii) rules out low-degree polynomials for learning the edge connection probability matrix [LG23], our approach provides stronger lower bounds on the recovery and learning problem.Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with graph splitting and cross-validation techniques. In order to rule out general recovery algorithms, we employ the correlation preserving projection method developed in [HS17]."
2502.15194,"Fast shipping and efficient routing are key problems of modern logistics. Building on previous studies that address package delivery from a source node to a destination within a graph using multiple agents (such as vehicles, drones, and ships), we investigate the complexity of this problem in specialized graphs and with restricted agent types, both with and without predefined initial positions. Particularly, in this paper, we aim to minimize the delivery time for delivering a package. To achieve this, we utilize a set of collaborative agents, each capable of traversing a specific subset of the graph and operating at varying speeds. This challenge is encapsulated in the recently introduced Drone Delivery Problem with respect to delivery time (DDT).In this work, we show that the DDT with predefined initial positions on a line is NP-hard, even when considering only agents with two distinct speeds. This refines the results presented by Erlebach, et al.[10], who demonstrated the NP-hardness of DDT on a line with agents of arbitrary speeds. Additionally, we examine DDT in grid graphs without predefined initial positions, where each drone can freely choose its starting position. We show that the problem is NP-hard to approximate within a factor of $O(n^{1-\varepsilon}$), where $n$ is the size of the grid, even when all agents are restricted to two different speeds as well as rectangular movement areas. We conclude by providing an easy $O(n)$ approximation algorithm."
2502.1565,"Reed-Muller codes consist of evaluations of $n$-variate polynomials over a finite field $\mathbb{F}$ with degree at most $d$. Much like every linear code, Reed-Muller codes can be characterized by constraints, where a codeword is valid if and only if it satisfies all \emph{degree-$d$} constraints.For a subset $\tilde{X} \subseteq \mathbb{F}^n$, we introduce the notion of \emph{$\tilde{X}$-quotient} Reed-Muller code. A function $F : \tilde{X} \rightarrow \mathbb{F}$ is a valid codeword in the quotient code if it satisfies all the constraints of degree-$d$ polynomials \emph{lying in $\tilde{X}$}. This gives rise to a novel phenomenon: a quotient codeword may have \emph{many} extensions to original codewords. This weakens the connection between original codewords and quotient codewords which introduces a richer range of behaviors along with substantial new challenges.Our goal is to answer the following question: what properties of $\tilde{X}$ will imply that the quotient code inherits its distance and list-decoding radius from the original code? We address this question using techniques developed by Bhowmick and Lovett [BL14], identifying key properties of $\mathbb{F}^n$ used in their proof and extending them to general subsets $\tilde{X} \subseteq \mathbb{F}^n$. By introducing a new tool, we overcome the novel challenge in analyzing the quotient code that arises from the weak connection between original and quotient codewords. This enables us to apply known results from additive combinatorics and algebraic geometry [KZ18, KZ19, LZ21] to show that when $\tilde{X}$ is a \emph{high rank variety}, $\tilde{X}$-quotient Reed-Muller codes inherit the distance and list-decoding parameters from the original Reed-Muller codes."
2502.16398,"The diameter of a polytope is a fundamental geometric parameter that plays a crucial role in understanding the efficiency of the simplex method. Despite its central nature, the computational complexity of computing the diameter of a given polytope is poorly understood. Already in 1994, Frieze and Teng [Comp. Compl.] recognized the possibility that this task could potentially be harder than NP-hard, and asked whether the corresponding decision problem is complete for the second stage of the polynomial hierarchy, i.e. $\Pi^p_2$-complete. In the following years, partial results could be obtained. In a cornerstone result, Frieze and Teng themselves proved weak NP-hardness for a family of custom defined polytopes. Sanità [FOCS18] in a break-through result proved that already for the much simpler fractional matching polytope the problem is strongly NP-hard. Very recently, Steiner and Nöbel [SODA25] generalized this result to the even simpler bipartite perfect matching polytope and the circuit diameter. In this paper, we finally show that computing the diameter of the bipartite perfect matching polytope is $\Pi^p_2$-hard. Since the corresponding decision problem is also trivially contained in $\Pi^p_2$, this decidedly answers Frieze and Teng's 30 year old question. Our results also hold when the diameter is replaced by the circuit diameter. As our second main result, we prove that for some $\varepsilon > 0$ the (circuit) diameter of the bipartite perfect matching polytope cannot be approximated by a factor better than $(1 + \varepsilon)$. This answers a recent question by Nöbel and Steiner. It is the first known inapproximability result for the circuit diameter, and extends Sanità's inapproximability result of the diameter to the totally unimodular case."
2502.16679,"Tarski's theorem states that every monotone function from a complete lattice to itself has a fixed point. We analyze the query complexity of finding such a fixed point on the $k$-dimensional grid of side length $n$ under the $\leq$ relation. In this setting, there is an unknown monotone function $f: \{0,1,\ldots, n-1\}^k \to \{0,1,\ldots, n-1\}^k$ and an algorithm must query a vertex $v$ to learn $f(v)$. The goal is to find a fixed point of $f$ using as few oracle queries as possible.We show that the randomized query complexity of this problem is $\Omega\left( \frac{k \cdot \log^2{n}}{\log{k}} \right)$ for all $n,k \geq 2$. This unifies and improves upon two prior results: a lower bound of $\Omega(\log^2{n})$ from [EPRY 2019] and a lower bound of $\Omega\left( \frac{k \cdot \log{n}}{\log{k}}\right)$ from [BPR 2024], respectively."
2502.16912,"The weighted low-rank approximation problem is a fundamental numerical linear algebra problem and has many applications in machine learning. Given a $n \times n$ weight matrix $W$ and a $n \times n$ matrix $A$, the goal is to find two low-rank matrices $U, V \in \mathbb{R}^{n \times k}$ such that the cost of $\| W \circ (U V^\top - A) \|_F^2$ is minimized. Previous work has to pay $\Omega(n^2)$ time when matrices $A$ and $W$ are dense, e.g., having $\Omega(n^2)$ non-zero entries. In this work, we show that there is a certain regime, even if $A$ and $W$ are dense, we can still hope to solve the weighted low-rank approximation problem in almost linear $n^{1+o(1)}$ time."
2502.17596,"In recent years, much attention has been placed on the complexity of graph homomorphism problems when the input is restricted to ${\mathbb P}_k$-free and ${\mathbb P}_k$-subgraph-free graphs. We consider the directed version of this research line, by addressing the questions, is it true that digraph homomorphism problems CSP$({\mathbb H})$ have a P versus NP-complete dichotomy when the input is restricted to $\vec{\mathbb P}_k$-free (resp.\ $\vec{\mathbb P}_k$-subgraph-free) digraphs? Our main contribution in this direction shows that if CSP$({\mathbb H})$ is NP-complete, then there is a positive integer $N$ such that CSP$({\mathbb H})$ remains NP-hard even for $\vec{\mathbb P}_N$-subgraph-free digraphs. Moreover, it remains NP-hard for acyclic $\vec{\mathbb P}_N$-subgraph-free digraphs, and becomes polynomial-time solvable for $\vec{\mathbb P}_{N-1}$-subgraph-free acyclic digraphs. We then verify the questions above for digraphs on three vertices and a family of smooth tournaments. We prove these results by establishing a connection between $\mathbb F$-(subgraph)-free algorithmics and constraint satisfaction theory. On the way, we introduce restricted CSPs, i.e., problems of the form CSP$({\mathbb H})$ restricted to yes-instances of CSP$({\mathbb H}')$ -- these were called restricted homomorphism problems by Hell and Nešetřil. Another main result of this paper presents a P versus NP-complete dichotomy for these problems. Moreover, this complexity dichotomy is accompanied by an algebraic dichotomy in the spirit of the finite domain CSP dichotomy."
2502.17779,"We show that for all functions $t(n) \geq n$, every multitape Turing machine running in time $t$ can be simulated in space only $O(\sqrt{t \log t})$. This is a substantial improvement over Hopcroft, Paul, and Valiant's simulation of time $t$ in $O(t/\log t)$ space from 50 years ago [FOCS 1975, JACM 1977]. Among other results, our simulation implies that bounded fan-in circuits of size $s$ can be evaluated on any input in only $\sqrt{s} \cdot poly(\log s)$ space, and that there are explicit problems solvable in $O(n)$ space which require $n^{2-\varepsilon}$ time on a multitape Turing machine for all $\varepsilon > 0$, thereby making a little progress on the $P$ versus $PSPACE$ problem.Our simulation reduces the problem of simulating time-bounded multitape Turing machines to a series of implicitly-defined Tree Evaluation instances with nice parameters, leveraging the remarkable space-efficient algorithm for Tree Evaluation recently found by Cook and Mertz [STOC 2024]."
2502.18382,"We extend the bounded degree graph model for property testing introduced by Goldreich and Ron (Algorithmica, 2002) to hypergraphs. In this framework, we analyse the query complexity of three fundamental hypergraph properties: colorability, $k$-partiteness, and independence number. We present a randomized algorithm for testing $k$-partiteness within families of $k$-uniform $n$-vertex hypergraphs of bounded treewidth whose query complexity does not depend on $n$. In addition, we prove optimal lower bounds of $\Omega(n)$ on the query complexity of testing algorithms for $k$-colorability, $k$-partiteness, and independence number in $k$-uniform $n$-vertex hypergraphs of bounded degree. For each of these properties, we consider the problem of explicitly constructing $k$-uniform hypergraphs of bounded degree that differ in $\Theta(n)$ hyperedges from any hypergraph satisfying the property, but where violations of the latter cannot be detected in any neighborhood of $o(n)$ vertices."
2502.1958,"For an $N \times N$ matrix $A$, its rank-$r$ rigidity, denoted $\mathcal{R}_A(r)$, is the minimum number of entries of $A$ that one must change to make its rank become at most $r$. Determining the rigidity of interesting explicit families of matrices remains a major open problem, and is central to understanding the complexities of these matrices in many different models of computation and communication. We focus in this paper on the Walsh-Hadamard transform and on the `distance matrix', whose rows and columns correspond to binary vectors, and whose entries calculate whether the row and column are close in Hamming distance. Our results also generalize to other Kronecker powers and `Majority powers' of fixed matrices. We prove two new results about such matrices.First, we prove new rigidity lower bounds in the low-rank regime where $r < \log N$. For instance, we prove that over any finite field, there are constants $c_1, c_2 > 0$ such that the $N \times N$ Walsh-Hadamard matrix $H_n$ satisfies $$\mathcal{R}_{H_n}(c_1 \log N) \geq N^2 \left( \frac12 - N^{-c_2} \right),$$ and a similar lower bound for the other aforementioned matrices. This is tight, and is the new best rigidity lower bound for an explicit matrix family at this rank; the previous best was $\mathcal{R}(c_1 \log N) \geq c_3 N^2$ for a small constant $c_3>0$.Second, we give new hardness amplification results, showing that rigidity lower bounds for these matrices for slightly higher rank would imply breakthrough rigidity lower bounds for much higher rank. For instance, if one could prove $$\mathcal{R}_{H_n}(\log^{1 + \varepsilon} N) \geq N^2 \left( \frac12 - N^{-1/2^{(\log \log N)^{o(1)}}} \right)$$ over any finite field for some $\varepsilon>0$, this would imply that $H_n$ is Razborov rigid, giving a breakthrough lower bound in communication complexity."
2502.20253,"Littlewood-Richardson, Kronecker and plethysm coefficients are fundamental multiplicities of interest in Representation Theory and Algebraic Combinatorics. Determining a combinatorial interpretation for the Kronecker and plethysm coefficients is a major open problem, and prompts the consideration of their computational complexity. Recently it was shown that they behave relatively well with respect to quantum computation, and for some large families there are polynomial time quantum algorithms [Larocca,Havlicek,arXiv:2407.17649] (also [BCGHZ,arXiv:2302.11454]). In this paper we show that for many of those cases the Kronecker and plethysm coefficients can also be computed in polynomial time via classical algorithms, thereby refuting some of the conjectures in [LH24]. This vastly limits the cases in which the desired super-polynomial quantum speedup could be achieved."
2503.0118,"It is well known that the graph isomorphism problem is polynomial-time reducible to the graph automorphism problem (in fact these two problems are polynomial-time equivalent). We show that, analogously, the group isomorphism problem is polynomial-time reducible to the group automorphism problem. Reductions to other relevant problems like automorphism counting are also given."
2503.02694,"In directed graphs, a cycle can be seen as a structure that allows its vertices to loop back to themselves, or as a structure that allows pairs of vertices to reach each other through distinct paths. We extend these concepts to temporal graph theory, resulting in multiple interesting definitions of a ""temporal cycle"". For each of these, we consider the problems of Cycle Detection and Acyclic Temporization. For the former, we are given an input temporal digraph, and we want to decide whether it contains a temporal cycle. Regarding the latter, for a given input (static) digraph, we want to time the arcs such that no temporal cycle exists in the resulting temporal digraph. We're also interested in Acyclic Temporization where we bound the lifetime of the resulting temporal digraph. Multiple results are presented, including polynomial and fixed-parameter tractable search algorithms, polynomial-time reductions from 3-SAT and Not All Equal 3-SAT, and temporizations resulting from arbitrary vertex orderings which cover (almost) all cases."
2503.05548,"We study integer linear programs (ILP) of the form $\min\{c^\top x\ \vert\ Ax=b,l\le x\le u,x\in\mathbb Z^n\}$ and analyze their parameterized complexity with respect to their distance to the generalized matching problem, following the well-established approach of capturing the hardness of a problem by the distance to triviality. The generalized matching problem is an ILP where each column of the constraint matrix has a $1$-norm of at most $2$. It captures several well-known polynomial time solvable problems such as matching and flow problems. We parameterize by the size of variable and constraint backdoors, which measure the least number of columns or rows that must be deleted to obtain a generalized matching ILP.We present the following results: (i) a fixed-parameter tractable (FPT) algorithm for ILPs parameterized by the size $p$ of a minimum variable backdoor to generalized matching; (ii) a randomized slice-wise polynomial (XP) time algorithm for ILPs parameterized by the size $p+h$ of a mixed variable plus constraint backdoor to generalized matching as long as $c$ and $A$ are encoded in unary; (iii) we complement (ii) by proving that solving ILPs is W[1]-hard when parameterized by the size of a minimum constraint backdoor $h$ even when all coefficients are bounded. To obtain (i), we prove a variant of lattice-convexity of the degree sequences of weighted $b$-matchings, which we study in the light of SBO jump M-convex functions. This allows us to model the matching part as a polyhedral constraint on the integer backdoor variables. The resulting ILP is solved using an FPT integer programming algorithm. For (ii), the randomized XP time algorithm is obtained by pseudo-polynomially reducing the problem to the exact matching problem. To prevent an exponential blowup in terms of the encoding length of $b$, we bound the proximity of the ILP through a subdeterminant based circuit bound."
2503.05895,"A network $\mathcal{N}$ is formed by a (multi)digraph $D$ together with a \emph{capacity function} $u : A(D) \to R_+$, and it is denoted by $\mathcal{N} = (D,u)$. A flow on $\mathcal{N}$ is a function $x: A(D) \to R_+$ such that $x(a) \leq u(a)$ for all $a \in A(D)$, and it is said to be $k$-splittable if it can be decomposed into up to $k$ paths. We say that a flow is $\lambda$-uniform if its value on each arc of the network with positive flow value is exactly $\lambda$, for some $\lambda \in R_+^*$.Arc-coloured networks are used to model qualitative differences among different regions through which the flow will be sent. They have applications in several areas such as communication networks, multimodal transportation, molecular biology, packing etc.We consider the problem of decomposing a flow over an arc-coloured network with minimum cost, that is, with minimum sum of the cost of its paths, where the cost of each path is given by its number of colours. We show that this problem is NP-Hard for general flows. When we restrict the problem to $\lambda$-uniform flows, we show that it can be solved in polynomial time for networks with at most two colours, and it is NP-Hard for general networks with three colours and for acyclic networks with at least five colours."
2503.07285,"The complexity of solving equations over finite groups has been an active area of research over the last two decades, starting with Goldmann and Russell, \emph{The complexity of solving equations over finite groups} from 1999. One important case of a group with unknown complexity is the symmetric group $S_4.$ In 2023, Idziak, Kawałek, and Krzaczkowski published $\exp(\Omega(\log^2 n))$ lower bounds for the satisfiability and equivalence problems over $S_4$ under the Exponential Time Hypothesis. In the present note, we prove that the satisfiability problem $\textsc{PolSat}(S_4)$ can be reduced to the equivalence problem $\textsc{PolEqv}(S_4)$ and thus, the two problems have the same complexity. We provide several equivalent formulations of the problem. In particular, we prove that $\textsc{PolEqv}(S_4)$ is equivalent to the circuit equivalence problem for $\operatorname{CC}[2,3,2]$-circuits, which were introduced by Idziak, Kawełek and Krzaczkowski. Under their strong exponential size hypothesis, such circuits cannot compute $\operatorname{AND}_n$ in size $\exp(o(\sqrt{n})).$ Our results provide an upper bound on the complexity of $\textsc{PolEqv}(S_4)$ that is based on the minimal size of $\operatorname{AND}_n$ over $\operatorname{CC}[2,3,2]$-circuits."
2503.07611,"Evolomino is a pencil-and-paper logic puzzle popularized by the Japanese publisher Nikoli (like Sudoku, Kakuro, Slitherlink, Masyu, and Fillomino). The puzzle's name reflects its core mechanic: the shapes of polyomino-like blocks that players must draw gradually ""evolve"" in the directions indicated by pre-drawn arrows. We prove, by reduction from 3-SAT, that the question of whether there exists at least one solution to an Evolomino puzzle satisfying the rules is NP-complete. Since our reduction is parsimonious, i.e., it preserves the number of distinct solutions, we also prove that counting the number of solutions to an Evolomino puzzle is #P-complete."
2503.07665,"We study the classical and parameterized complexity of computing the positive non-clashing teaching dimension of a set of concepts, that is, the smallest number of examples per concept required to successfully teach an intelligent learner under the considered, previously established model. For any class of concepts, it is known that this problem can be effortlessly transferred to the setting of balls in a graph G. We establish (1) the NP-hardness of the problem even when restricted to instances with positive non-clashing teaching dimension k=2 and where all balls in the graph are present, (2) near-tight running time upper and lower bounds for the problem on general graphs, (3) fixed-parameter tractability when parameterized by the vertex integrity of G, and (4) a lower bound excluding fixed-parameter tractability when parameterized by the feedback vertex number and pathwidth of G, even when combined with k. Our results provide a nearly complete understanding of the complexity landscape of computing the positive non-clashing teaching dimension and answer open questions from the literature."
2503.10393,"Oredango puzzle, one of the pencil puzzles, was originally created by Kanaiboshi and published in the popular puzzle magazine Nikoli. In this paper, we show NP- and ASP-completeness of Oredango by constructing a reduction from the 1-in-3SAT problem. Next, we formulate Oredango as an 0-1 integer-programming problem, and present numerical results obtained by solving Oredango puzzles from Nikoli and PuzzleSquare JP using a 0-1 optimization solver."
2503.11131,"We give simple deterministic reductions demonstrating the NP-hardness of approximating the nearest codeword problem and minimum distance problem within arbitrary constant factors (and almost-polynomial factors assuming NP cannot be solved in quasipolynomial time). The starting point is a simple NP-hardness result without a gap, and is thus ""PCP-free."" Our approach is inspired by that of Bhattiprolu and Lee [BL24] who give a PCP-free randomized reduction for similar problems over the integers and the reals. We leverage the existence of $\varepsilon$-balanced codes to derandomize and further simplify their reduction for the case of finite fields."
2503.11238,"We study the problem of finding a subgroup of a given order in a finite group, where the group is represented by its Cayley table. We establish that this problem is NP-hard in the general case by providing a reduction from the Hamiltonian Cycle problem. Additionally, we analyze the complexity of the problem in the special case of abelian groups and present a linear-time algorithm for finding a subgroup of a given order when the input is given in the form of a Cayley table. To the best of our knowledge, no prior work has addressed the complexity of this problem under the Cayley table representation. Our results also provide insight into the computational difficulty of finding subgroup across different ways of groups representations."
2503.12996,"We introduce the {\em certification} of solutions to graph problems when access to the input is restricted. This topic has received a lot of attention in the distributed computing setting, and we introduce it here in the context of \emph{streaming} algorithms, where the input is too large to be stored in memory.Given a graph property $\mbox{P}$, a \emph{streaming certification scheme} for $\mbox{P}$ is a \emph{prover-verifier} pair where the prover is a computationally unlimited but non-trustable oracle, and the verifier is a streaming algorithm. For any input graph, the prover provides the verifier with a \emph{certificate}. The verifier then receives the input graph as a stream of edges in an adversarial order, and must check whether the certificate is indeed a \emph{proof} that the input graph satisfies $\mbox{P}$. The main complexity measure for a streaming certification scheme is its \emph{space complexity}, defined as the sum of the size of the certificate provided by the oracle, and of the memory space required by the verifier.We give streaming certification schemes for several graph properties, including maximum matching, diameter, degeneracy, and coloring, with space complexity matching the requirement of \emph{semi-streaming}, i.e., with space complexity $O(n\,\mbox{polylog}\, n)$ for $n$-node graphs. All these problems do {\em not} admit semi-streaming algorithms, showing that also in the (semi) streaming setting, certification is sometimes easier than calculation (like $NP$). For each of these properties, we provide upper and lower bounds on the space complexity of the corresponding certification schemes, many being tight up to logarithmic multiplicative factors. We also show that some graph properties are hard for streaming certification, in the sense that they cannot be certified in semi-streaming, as they require $\Omega(n^2)$-bit certificates."
2503.13398,"The \emph{interestingness score} of a directed path $\Pi = e_1, e_2, e_3, \dots, e_\ell$ in an edge-weighted directed graph $G$ is defined as $\texttt{score}(\Pi) := \sum_{i=1}^\ell w(e_i) \cdot \log{(i+1)}$, where $w(e_i)$ is the weight of the edge $e_i$. We consider two optimization problems that arise in the analysis of Mapper graphs, which is a powerful tool in topological data analysis. In the IP problem, the objective is to find a collection $\mathcal{P}$ of edge-disjoint paths in $G$ with the maximum total interestingness score. %; that is, two raised to the power of the sum of the weights of the paths in $\mathcal{P}$. For $k \in \mathbb{N}$, the $k$-IP problem is a variant of the IP problem with the extra constraint that each path in $\mathcal{P}$ must have exactly $k$ edges. Kalyanaraman, Kamruzzaman, and Krishnamoorthy (Journal of Computational Geometry, 2019) claim that both IP and $k$-IP (for $k \geq 3$) are NP-complete. We point out some inaccuracies in their proofs. Furthermore, we show that both problems are NP-hard in directed acyclic graphs."
2503.14117,"We reduce the problem of proving deterministic and nondeterministic Boolean circuit size lower bounds to the analysis of certain two-dimensional combinatorial cover problems. This is obtained by combining results of Razborov (1989), Karchmer (1993), and Wigderson (1993) in the context of the fusion method for circuit lower bounds with the graph complexity framework of Pudlák, Rödl, and Savický (1988). For convenience, we formalize these ideas in the more general setting of ""discrete complexity"", i.e., the natural set-theoretic formulation of circuit complexity, variants of communication complexity, graph complexity, and other measures.We show that random graphs have linear graph cover complexity, and that explicit super-logarithmic graph cover complexity lower bounds would have significant consequences in circuit complexity. We then use discrete complexity, the fusion method, and a result of Karchmer and Wigderson (1993) to introduce nondeterministic graph complexity. This allows us to establish a connection between graph complexity and nondeterministic circuit complexity.Finally, complementing these results, we describe an exact characterization of the power of the fusion method in discrete complexity. This is obtained via an adaptation of a result of Nakayama and Maruoka (1995) that connects the fusion method to the complexity of ""cyclic"" Boolean circuits, which generalize the computation of a circuit by allowing cycles in its specification."
2503.15771,"A temporal graph $\mathcal{G}=(G,\lambda)$ can be represented by an underlying graph $G=(V,E)$ together with a function $\lambda$ that assigns to each edge $e\in E$ the set of time steps during which $e$ is present. The reachability graph of $\mathcal{G}$ is the directed graph $D=(V,A)$ with $(u,v)\in A$ if only if there is a temporal path from $u$ to $v$. We study the Reachability Graph Realizability (RGR) problem that asks whether a given directed graph $D=(V,A)$ is the reachability graph of some temporal graph. The question can be asked for undirected or directed temporal graphs, for reachability defined via strict or non-strict temporal paths, and with or without restrictions on $\lambda$ (proper, simple, or happy). Answering an open question posed by Casteigts et al. (Theoretical Computer Science 991 (2024)), we show that all variants of the problem are NP-complete, except for two variants that become trivial in the directed case. For undirected temporal graphs, we consider the complexity of the problem with respect to the solid graph, that is, the graph containing all edges that could potentially receive a label in any realization. We show that the RGR problem is polynomial-time solvable if the solid graph is a tree and fixed-parameter tractable with respect to the feedback edge set number of the solid graph. As we show, the latter parameter can presumably not be replaced by smaller parameters like feedback vertex set or treedepth, since the problem is W[2]-hard with respect to these parameters."
2503.16089,"We prove that an $\epsilon$-approximate fixpoint of a map $f:[0,1]^d\rightarrow [0,1]^d$ can be found with $\mathcal{O}(d^2(\log\frac{1}{\epsilon} + \log\frac{1}{1-\lambda}))$ queries to $f$ if $f$ is $\lambda$-contracting with respect to an $\ell_p$-metric for some $p\in [1,\infty)\cup\{\infty\}$. This generalizes a recent result of Chen, Li, and Yannakakis [STOC'24] from the $\ell_\infty$-case to all $\ell_p$-metrics. Previously, all query upper bounds for $p\in [1,\infty) \setminus \{2\}$ were either exponential in $d$, $\log\frac{1}{\epsilon}$, or $\log\frac{1}{1-\lambda}$.Chen, Li, and Yannakakis also show how to ensure that all queries to $f$ lie on a discrete grid of limited granularity in the $\ell_\infty$-case. We provide such a rounding for the $\ell_1$-case, placing an appropriately defined version of the $\ell_1$-case in $\textsf{FP}^{dt}$.To prove our results, we introduce the notion of $\ell_p$-halfspaces and generalize the classical centerpoint theorem from discrete geometry: for any $p \in [1, \infty) \cup \{\infty\}$ and any mass distribution (or point set), we prove that there exists a centerpoint $c$ such that every $\ell_p$-halfspace defined by $c$ and a normal vector contains at least a $\frac{1}{d+1}$-fraction of the mass (or points)."
2503.17022,"We prove that polynomial calculus (and hence also Nullstellensatz) over any field requires linear degree to refute that sparse random regular graphs, as well as sparse Erdős-Rényi random graphs, are $3$-colourable. Using the known relation between size and degree for polynomial calculus proofs, this implies strongly exponential lower bounds on proof size."
2503.17114,"Given a circuit $C : \{0,1\}^n \to \{0,1\}^m$ from a circuit class $F$, with $m > n$, finding a $y \in \{0,1\}^m$ such that $\forall x \in \{0,1\}^n$, $C(x) \ne y$, is the range avoidance problem (denoted by $F$-$avoid$). Deterministic polynomial time algorithms (even with access to $NP$ oracles) solving this problem is known to imply explicit constructions of various pseudorandom objects like hard Boolean functions, linear codes, PRGs etc. Deterministic polynomial time algorithms are known for $NC^0_2$-$avoid$ when $m > n$, and for $NC^0_3$-$avoid$ when $m \ge \frac{n^2}{\log n}$, where $NC^0_k$ is the class of circuits with bounded fan-in which have constant depth and the output depends on at most $k$ of the input bits. On the other hand, it is also known that $NC^0_3$-$avoid$ when $m = n+O\left(n^{2/3}\right)$ is at least as hard as explicit construction of rigid matrices.In this paper, we propose a new approach to solving range avoidance problem via hypergraphs. We formulate the problem in terms of Turan-type problems in hypergraphs of the following kind - for a fixed $k$-uniform hypergraph $H'$, what is the maximum number of edges that can exist in a $k$-uniform hypergraph $H$ which does not have a sub-hypergraph isomorphic to $H'$? We use our approach to show (using known Turan-type bounds) that there is a constant $c$ such that $mon$-$NC^0_3$-$avoid$ can be solved in deterministic polynomial time when $m > cn^2$. To improve the stretch constraint to linear, we show a new Turan-type theorem for a hypergraph structure (which we call the the loose $chi$-cycles) and use it to show that $mon$-$NC^0_3$-$avoid$ can be solved in deterministic polynomial time when $m > n$, thus improving the known bounds of $NC^0_3$-avoid for the case of monotone circuits."
2503.17844,"We study the problem of approximating Hamming distance in sublinear time under property-preserving hashing (PPH), where only hashed representations of inputs are available. Building on the threshold evaluation framework of Fleischhacker, Larsen, and Simkin (EUROCRYPT 2022), we present a sequence of constructions with progressively improved complexity: a baseline binary search algorithm, a refined variant with constant repetition per query, and a novel hash design that enables constant-time approximation without oracle access. Our results demonstrate that approximate distance recovery is possible under strong cryptographic guarantees, bridging efficiency and security in similarity estimation."
2503.19188,"Korten and Pitassi (FOCS, 2024) defined a new complexity class $L_2P$ as the polynomial-time Turing closure of the Linear Ordering Principle. They put it between $MA$ (Merlin--Arthur protocols) and $S_2P$ (the second symmetric level of the polynomial hierarchy).In this paper we sandwich $L_2P$ between $P^{prMA}$ and $P^{prSBP}$. (The oracles here are promise problems, and $SBP$ is the only known class between $MA$ and $AM$.) The containment in $P^{prSBP}$ is proved via an iterative process that uses a $prSBP$ oracle to estimate the average order rank of a subset and find the minimum of a linear order.Another containment result of this paper is $P^{prO_2P} \subseteq O_2P$ (where $O_2P$ is the input-oblivious version of $S_2P$). These containment results altogether have several byproducts:We give an affirmative answer to an open question posed by of Chakaravarthy and Roy (Computational Complexity, 2011) whether $P^{prMA} \subseteq S_2P$, thereby settling the relative standing of the existing (non-oblivious) Karp-Lipton-style collapse results of Chakaravarthy and Roy (2011) and Cai (2007),We give an affirmative answer to an open question of Korten and Pitassi whether a Karp-Lipton-style collapse can be proven for $L_2P$,We show that the Karp-Lipton-style collapse to $P^{prOMA}$ is actually better than both known collapses to $P^{prMA}$ due to Chakaravarthy and Roy (Computational Complexity, 2011) and to $O_2P$ also due to Chakaravarthy and Roy (STACS, 2006). Thus we resolve the controversy between previously incomparable Karp-Lipton collapses stemming from these two lines of research."
2503.20071,"A central question in mathematics and computer science is the question of determining whether a given ideal $I$ is prime, which geometrically corresponds to the zero set of $I$, denoted $Z(I)$, being irreducible. The case of principal ideals (i.e., $m=1$) corresponds to the more familiar absolute irreducibility testing of polynomials, where the seminal work of (Kaltofen 1995) yields a randomized, polynomial time algorithm for this problem. However, when $m > 1$, the complexity of the primality testing problem seems much harder. The current best algorithms for this problem are only known to be in EXPSPACE.In this work, we significantly reduce the complexity-theoretic gap for the ideal primality testing problem for the important families of ideals $I$ (namely, radical ideals and equidimensional Cohen-Macaulay ideals). For these classes of ideals, assuming the Generalized Riemann Hypothesis, we show that primality testing lies in $\Sigma_3^p \cap \Pi_3^p$. This significantly improves the upper bound for these classes, approaching their lower bound, as the primality testing problem is coNP-hard for these classes of ideals. Another consequence of our results is that for equidimensional Cohen-Macaulay ideals, we get the first PSPACE algorithm for primality testing, exponentially improving the space and time complexity of prior known algorithms."
2503.214,"We explore the computational implications of a superposition of spacetimes, a phenomenon hypothesized in quantum gravity theories. This was initiated by Shmueli (2024) where the author introduced the complexity class $\mathbf{BQP^{OI}}$ consisting of promise problems decidable by quantum polynomial time algorithms with access to an oracle for computing order interference. In this work, it was shown that the Graph Isomorphism problem and the Gap Closest Vector Problem (with approximation factor $\mathcal{O}(n^{3/2})$) are in $\mathbf{BQP^{OI}}$. We extend this result by showing that the entire complexity class $\mathbf{SZK}$ (Statistical Zero Knowledge) is contained within $\mathbf{BQP^{OI}}$. This immediately implies that the security of numerous lattice based cryptography schemes will be compromised in a computational model based on superposition of spacetimes, since these often rely on the hardness of the Learning with Errors problem, which is in $\mathbf{SZK}$."
2503.21582,"We present new results on the landscape of problems that can be solved by quantum Turing machines (QTM's) employing severely limited amounts of memory. In this context, we demonstrate two infinite time hierarchies of complexity classes within the ``small space'' regime: For all $i\geq 0$, there is a language that can be recognized by a constant-space machine in $2^{O(n^{1/2^i})}$ time, but not by any sublogarithmic-space QTM in $2^{O(n^{1/2^{i+1}})}$ time. For quantum machines operating within $o(\log \log n)$ space, there exists another hierarchy, each level of which corresponds to an expected runtime of $2^{O((\log n)^i)}$ for a different positive integer $i$. We also improve a quantum advantage result, demonstrating a language that can be recognized by a polynomial-time constant-space QTM, but not by any classical machine using $o(\log \log n)$ space, regardless of the time budget. The implications of our findings for quantum space-time tradeoffs are discussed."
2503.21951,"This work establishes conditional lower bounds for average-case {\em parity}-counting versions of the problems $k$-XOR, $k$-SUM, and $k$-OV. The main contribution is a set of self-reductions for the problems, providing the first specific distributions, for which:$\mathsf{parity}\text{-}k\text{-}OV$ is $n^{\Omega(\sqrt{k})}$ average-case hard, under the $k$-OV hypothesis (and hence under SETH),$\mathsf{parity}\text{-}k\text{-}SUM$ is $n^{\Omega(\sqrt{k})}$ average-case hard, under the $k$-SUM hypothesis, and$\mathsf{parity}\text{-}k\text{-}XOR$ is $n^{\Omega(\sqrt{k})}$ average-case hard, under the $k$-XOR hypothesis.Under the very believable hypothesis that at least one of the $k$-OV, $k$-SUM, $k$-XOR or $k$-Clique hypotheses is true, we show that parity-$k$-XOR, parity-$k$-SUM, and parity-$k$-OV all require at least $n^{\Omega(k^{1/3})}$ (and sometimes even more) time on average (for specific distributions).To achieve these results, we present a novel and improved framework for worst-case to average-case fine-grained reductions, building on the work of Dalirooyfard, Lincoln, and Vassilevska Williams, FOCS 2020."
2503.22633,"Moment polytopes of tensors, the study of which is deeply rooted in invariant theory, representation theory and symplectic geometry, have found relevance in numerous places, from quantum information (entanglement polytopes) and algebraic complexity theory (GCT program and the complexity of matrix multiplication) to optimization (scaling algorithms). Towards an open problem in algebraic complexity theory, we prove separations between the moment polytopes of matrix multiplication tensors and unit tensors. As a consequence, we find that matrix multiplication moment polytopes are not maximal, i.e. are strictly contained in the corresponding Kronecker polytope. As another consequence, we obtain a no-go result for a natural operational characterization of moment polytope inclusion in terms of asymptotic restriction. We generalize the separation and non-maximality to moment polytopes of iterated matrix multiplication tensors. Our result implies that tensor networks where multipartite entanglement structures beyond two-party entanglement are allowed can go beyond projected entangled-pair states (PEPS) in terms of expressivity.Our proof characterizes membership of uniform points in moment polytopes of tensors, and establishes a connection to polynomial multiplication tensors via the minrank of matrix subspaces. As a result of independent interest, we extend these techniques to obtain a new proof of the optimal border subrank bound for matrix multiplication."
2503.22848,"Working in the multitape Turing model, we show how to reduce the problem of matrix transposition to the problem of integer multiplication. If transposing an $n \times n$ binary matrix requires $\Omega(n^2 \log n)$ steps on a Turing machine, then our reduction implies that multiplying $n$-bit integers requires $\Omega(n \log n)$ steps. In other words, if matrix transposition is as hard as expected, then integer multiplication is also as hard as expected."
2503.22997,"A recently introduced measure of Boolean functions complexity--disjunc\-tive complexity (DC)--is compared with other complexity measures: the space complexity of streaming algorithms and the complexity of nondeterministic branching programs (NBP). We show that DC is incomparable with NBP. Specifically, we present a function that has low NBP but has subexponential DC. Conversely, we provide arguments based on computational complexity conjectures to show that DC can superpolynomially exceed NBP in certain cases. Additionally, we prove that the monotone version of NBP complexity is strictly weaker than DC. We prove that the space complexity of one-pass streaming algorithms is strictly weaker than DC. Furthermore, we introduce a generalization of streaming algorithms that captures the full power of DC. This generalization can be expressed in terms of nondeterministic algorithms that irreversibly write 1s to entries of a Boolean vector (i.e., changes from 1 to 0 are not allowed).Finally, we discuss an unusual phenomenon in disjunctive complexity: the existence of uniformly hard functions. These functions exhibit the property that their disjunctive complexity is maximized, and this property extends to all functions dominated by them."
2503.23176,"We formulate low-level malware detection using algorithms based on feature matching as Order-based Malware Detection with Critical Instructions (General-OMDCI): given a pattern in the form of a sequence \(M\) of colored blocks, where each block contains a critical character (representing a unique sequence of critical instructions potentially associated with malware but without certainty), and a program \(A\), represented as a sequence of \(n\) colored blocks with critical characters, the goal is to find two subsequences, \(M'\) of \(M\) and \(A'\) of \(A\), with blocks matching in color and whose critical characters form a permutation of each other. When $M$ is a permutation in both colors and critical characters the problem is called OMDCI. If we additionally require $M'=M$, then the problem is called OMDCI+; if in this case $d=|M|$ is used as a parameter, then the OMDCI+ problem is easily shown to be FPT. Our main (negative) results are on the cases when $|M|$ is arbitrary and are summarized as follows: 
OMDCI+ is NP-complete, which implies OMDCI is also NP-complete.
For the special case of OMDCI, deciding if the optimal solution has length $0$ (i.e., deciding if no part of \(M\) appears in \(A\)) is co-NP-hard. As a result, the OMDCI problem does not admit an FPT algorithm unless P=co-NP. 
In summary, our results imply that using algorithms based on feature matching to identify malware or determine the absence of malware in a given low-level program are both hard."
2503.24061,"We introduce a technically and conceptually simple approach to magnification of circuit and formula lower bounds. Central to the method are so-called distinguishers, sparse matrices that retain some of the key properties of error-correcting codes. As applications, we generalize and strengthen known general (not problem specific) magnification results and in particular achieve magnification thresholds below known lower bounds. For example, we show that fixed-polynomial formula-size lower bounds for NP are implied by slightly superlinear formula-size lower bounds for approximating any sufficiently sparse problem in NP. We also show that the thresholds achieved are sharp. Additionally, our approach yields a uniform magnification result for the Minimum Circuit Size Problem (MCSP). This seems to sidestep the localization barrier."
2503.24144,"Local complementation of a graph $G$ on vertex $v$ is an operation that results in a new graph $G*v$, where the neighborhood of $v$ is complemented. Two graph are locally equivalent if on can be reached from the other one through local complementation.It was previously established that recognizing locally equivalent graphs can be done in $\mathcal{O}(n^4)$ time. We sharpen this result by proving it can be decided in $\mathcal{O}(\log^2(n))$ parallel time with $n^{\mathcal{O}(1)}$ processors.As a second contribution, we introduce the Local Complementation Problem, a decision problem that captures the complexity of applying a sequence of local complementations. Given a graph $G$, a sequence of vertices $s$, and a pair of vertices $u,v$, the problem asks whether the edge $(u,v)$ is present in the graph obtained after applying local complementations according to $s$. Regardless it simplicity, it is proven to be $\mathsf{P}$-complete, therefore it is unlikely to be efficiently parallelizable.Finally, it is conjectured that Local Complementation Problem remains $\mathsf{P}$-complete when restricted to circle graphs."
2503.24351,"We prove a sensitivity-to-communication lifting theorem for arbitrary gadgets. Given functions $f: \{0,1\}^n\to \{0,1\}$ and $g : \mathcal X\times \mathcal Y\to \{0,1\}$, denote $f\circ g(x,y) := f(g(x_1,y_1),\ldots,g(x_n,y_n))$. We show that for any $f$ with sensitivity $s$ and any $g$, \[D(f\circ g) \geq s\cdot \bigg(\frac{\Omega(D(g))}{\log\mathsf{rk}(g)} - \log\mathsf{rk}(g)\bigg),\] where $D(\cdot)$ denotes the deterministic communication complexity and $\mathsf{rk}(g)$ is the rank of the matrix associated with $g$. As a corollary, we get that if $D(g)$ is a sufficiently large constant, $D(f\circ g) = \Omega(\min\{s,d\}\cdot \sqrt{D(g)})$, where $s$ and $d$ denote the sensitivity and degree of $f$. In particular, computing the OR of $n$ copies of $g$ requires $\Omega(n\cdot\sqrt{D(g)})$ bits."
2504.00318,"This paper explores the Boolean Satisfiability Problem (SAT) in the context of Kolmogorov complexity theory. We present three versions of the distinguishability problem-Boolean formulas, Turing machines, and quantum systems-each focused on distinguishing between two Bernoulli distributions induced by these computational models. A reduction is provided that establishes the equivalence between the Boolean formula version of the program output statistical prediction problem and the #SAT problem. Furthermore, we apply Solomonoff's inductive reasoning theory, revealing its limitations: the only ""algorithm"" capable of determining the output of any shortest program is the program itself, and any other algorithms are computationally indistinguishable from a universal computer, based on the coding theorem. The quantum version of this problem introduces a unique algorithm based on statistical distance and distinguishability, reflecting a fundamental limit in quantum mechanics. Finally, the potential equivalence of Kolmogorov complexity between circuit models and Turing machines may have significant implications for the NP vs P problem. We also investigate the nature of short programs corresponding to exponentially long bit sequences that can be compressed, revealing that these programs inherently contain loops that grow exponentially."
2504.00346,"We give an IOPP (interactive oracle proof of proximity) for trivariate Reed-Muller codes that achieves the best known query complexity in some range of security parameters. Specifically, for degree $d$ and security parameter $\lambda\leq \frac{\log^2 d}{\log\log d}$ , our IOPP has $2^{-\lambda}$ round-by-round soundness, $O(\lambda)$ queries, $O(\log\log d)$ rounds and $O(d)$ length. This improves upon the FRI [Ben-Sasson, Bentov, Horesh, Riabzev, ICALP 2018] and the STIR [Arnon, Chiesa, Fenzi, Yogev, Crypto 2024] IOPPs for Reed-Solomon codes, that have larger query and round complexity standing at $O(\lambda \log d)$ and $O(\log d+\lambda\log\log d)$ respectively. We use our IOPP to give an IOP for the NP-complete language Rank-1-Constraint-Satisfaction with the same parameters. Our construction is based on the line versus point test in the low-soundness regime. Compared to the axis parallel test (which is used in all prior works), the general affine lines test has improved soundness, which is the main source of our improved soundness. Using this test involves several complications, most significantly that projection to affine lines does not preserve individual degrees, and we show how to overcome these difficulties. En route, we extend some existing machinery to more general settings. Specifically, we give proximity generators for Reed-Muller codes, show a more systematic way of handling ``side conditions'' in IOP constructions, and generalize the compiling procedure of [Arnon, Chiesa, Fenzi, Yogev, Crypto 2024] to general codes."
2504.01354,"We prove complexity dichotomies for \#CSP problems (not necessarily symmetric) with Boolean domain and complex range on several typical minor-closed graph classes. These dichotomies give a complete characterization of the complexity of \#CSP on graph classes that forbid a complete graph as a minor. In particular, we also demonstrate that, whether the maximum degree of vertices is bounded may influence the complexity on specific minor-closed graph classes, and this phenomenon has never been observed in the previous related studies. Furthermore, our proofs integrate the properties of each graph class with the techniques from counting complexity, and develop a systematic approach for analyzing the complexity of \#CSP on these graph classes."
2504.01856,"We study the tasks of collective coin flipping and leader election in the full-information model.We prove new lower bounds for coin flipping protocols, implying lower bounds for leader election protocols. We show that any $k$-round coin flipping protocol, where each of $\ell$ players sends 1 bit per round, can be biased by $O(\ell/\log^{(k)}(\ell))$ bad players. For all $k>1$ this strengthens previous lower bounds [RSZ, SICOMP 2002], which ruled out protocols resilient to adversaries controlling $O(\ell/\log^{(2k-1)}(\ell))$ players. Consequently, we establish that any protocol tolerating a linear fraction of corrupt players, with only 1 bit per round, must run for at least $\log^*\ell-O(1)$ rounds, improving on the prior best lower bound of $\frac12 \log^*\ell-\log^*\log^*\ell$. This lower bound matches the number of rounds, $\log^*\ell$, taken by the current best coin flipping protocols from [RZ, JCSS 2001], [F, FOCS 1999] that can handle a linear sized coalition of bad players, but with players sending unlimited bits per round. We also derive lower bounds for protocols allowing multi-bit messages per round. Our results show that the protocols from [RZ, JCSS 2001], [F, FOCS 1999] that handle a linear number of corrupt players are almost optimal in terms of round complexity and communication per player in a round.A key technical ingredient in proving our lower bounds is a new result regarding biasing most functions from a family of functions using a common set of bad players and a small specialized set of bad players specific to each function that is biased.We give improved constant-round coin flipping protocols in the setting that each player can send 1 bit per round. For two rounds, our protocol can handle $O(\ell/(\log\ell)(\log\log\ell)^2)$ sized coalition of bad players; better than the best one-round protocol by [AL, Combinatorica 1993] in this setting."
2504.01899,"We introduce and initiate the study of a new model of reductions called the random noise model. In this model, the truth table $T_f$ of the function $f$ is corrupted on a randomly chosen $\delta$-fraction of instances. A randomized algorithm $\mathcal{A}$ is a $\left(t, \delta, 1-\varepsilon\right)$-recovery reduction for $f$ if:1. With probability $1-\varepsilon$ over the choice of $\delta$-fraction corruptions, given access to the corrupted truth table, the algorithm $\mathcal{A}$ computes $f(\phi)$ correctly with probability at least $2/3$ on every input $\phi$.2. The algorithm $\mathcal{A}$ runs in time $O(t)$.This model, a natural relaxation of average-case complexity, has practical motivations and is mathematically interesting.Pointing towards this, we show the existence of robust deterministic polynomial-time recovery reductions with optimal parameters up to polynomial factors (that is, deterministic $\left( poly(n), 0.5 - 1/poly(n), 1-e^{-\Omega(poly(n))} \right)$-recovery reductions) for a large function class SLNP$^S$ containing many of the canonical NP-complete problems - SAT, $k$SAT, $k$CSP, CLIQUE and more. As a corollary, we obtain that the barrier of Bogdanov and Trevisan (2006) for non-adaptive worst-case to average-case reductions does not apply to our mild non-adaptive relaxation.Furthermore, we establish recovery reductions with optimal parameters for Orthogonal Vectors and Parity $k$-Clique problems. These problems exhibit structural similarities to NP-complete problems, with Orthogonal Vectors admitting a $2^{0.5n}$-time reduction from $k$SAT on $n$ variables; and Parity $k$-Clique a subexponential-time reduction from 3SAT."
2504.02387,"In his paper, we study the problems of abelian group isomorphism and basis construction in two models. In the {\it partially specified model} (PS-model), the algorithm does not know the group size but can access randomly chosen elements of the group along with the Cayley table of those elements, which provides the result of the binary operation for every pair of selected elements. In the stronger {\it fully specified model} (FS-model), the algorithm knows the size of the group and has access to its elements and Cayley table.Given two abelian groups, $G$, and $H$, we present an algorithm in the PS-model (and hence in the FS-model) that runs in time $\tilde O(\sqrt{|G|})$ and decides if they are isomorphic. This improves on Kavitha's linear-time algorithm and gives the first sublinear-time solution for this problem. We then prove the lower bound $\Omega(|G|^{1/4})$ for the FS-model and the tight bound $\Omega(\sqrt{|G|})$ for the PS-model. This is the first known lower bound for this problem. We obtain similar results for finding a basis for abelian groups. For deterministic algorithms, a simple $\Omega(|G|)$ lower bound is given."
2504.02695,"We prove new hardness results for fundamental lattice problems under the Exponential Time Hypothesis (ETH). Building on a recent breakthrough by Bitansky et al. [BHIRW24], who gave a polynomial-time reduction from $\mathsf{3SAT}$ to the (gap) $\mathsf{MAXLIN}$ problem-a class of CSPs with linear equations over finite fields-we derive ETH-hardness for several lattice problems.First, we show that for any $p \in [1, \infty)$, there exists an explicit constant $\gamma > 1$ such that $\mathsf{CVP}_{p,\gamma}$ (the $\ell_p$-norm approximate Closest Vector Problem) does not admit a $2^{o(n)}$-time algorithm unless ETH is false. Our reduction is deterministic and proceeds via a direct reduction from (gap) $\mathsf{MAXLIN}$ to $\mathsf{CVP}_{p,\gamma}$.Next, we prove a randomized ETH-hardness result for $\mathsf{SVP}_{p,\gamma}$ (the $\ell_p$-norm approximate Shortest Vector Problem) for all $p > 2$. This result relies on a novel property of the integer lattice $\mathbb{Z}^n$ in the $\ell_p$ norm and a randomized reduction from $\mathsf{CVP}_{p,\gamma}$ to $\mathsf{SVP}_{p,\gamma'}$.Finally, we improve over prior reductions from $\mathsf{3SAT}$ to $\mathsf{BDD}_{p, \alpha}$ (the Bounded Distance Decoding problem), yielding better ETH-hardness results for $\mathsf{BDD}_{p, \alpha}$ for any $p \in [1, \infty)$ and $\alpha > \alpha_p^{\ddagger}$, where $\alpha_p^{\ddagger}$ is an explicit threshold depending on $p$.We additionally observe that prior work implies ETH hardness for the gap minimum distance problem ($\gamma$-$\mathsf{MDP}$) in codes."
2504.03591,"Elementary Object Systems (EOS) are a form of Petri Net (PN) where tokens carry internal PN. This model has been recently proposed for analysis of robustness of Multi Agent Systems. While EOS reachability is known to be undecidable, the decidability of coverability of its conservative fragment (where the type of internal PN cannot be completely deleted and, thus, is conserved) was proved a decade ago, no study charted its complexity. Here, we take a first step in this direction, by showing how to encode $\nu$PNs, a well studied form of PN enriched with data, into conservative EOS (cEOS). This yields a non-Primitive Recursive, $F_{\omega2}$ lower-bound on cEOS coverability."
2504.04133,"QuickSort and the analysis of its expected run time was presented 1962 in a classical paper by C.A.R Hoare. There the run time analysis hinges on a by now well known recurrence equation for the expected run time, which in turn was justified by referring to ``the law of conditional expectations''. A probability space for the runs of the algorithms was not constructed. Subsequent textbooks treated the recurrence relation as self evident and present it until this day without proof. Here we give an inductive definition of the probability space for the runs of randomized QuickSort and subsequently derive the recurrence equation with a not completely trivial proof."
2504.04416,"We survey results on the formalization and independence of mathematical statements related to major open problems in computational complexity theory. Our primary focus is on recent findings concerning the (un)provability of complexity bounds within theories of bounded arithmetic. This includes the techniques employed and related open problems, such as the (non)existence of a feasible proof that P = NP."
2504.04639,"It is an open question whether the search and decision versions of promise CSPs are equivalent. Most known algorithms for PCSPs solve only their \emph{decision} variant, and it is unknown whether they can be adapted to solve \emph{search} as well. The main approaches, called BLP, AIP and BLP+AIP, handle a PCSP by finding a solution to a relaxation of some integer program. We prove that rounding those solutions to a proper search certificate can be as hard as any problem in the class TFNP. In other words, these algorithms are ineffective for search. Building on the algebraic approach to PCSPs, we find sufficient conditions that imply ineffectiveness for search. Our tools are tailored to algorithms that are characterized by minions in a suitable way, and can also be used to prove undecidability results for meta-problems. This way, we show that the families of templates solvable via BLP, AIP, and BLP+AIP are undecidable.Using the same techniques we also analyze several algebraic conditions that are known to guarantee the tractability of finite-template CSPs. We prove that several meta-problems related to cyclic polymorphims and WNUs are undecidable for PCSPs. In particular, there is no algorithm deciding whether a finite PCSP template (1) admits cyclic a polymorphism, (2) admits a WNU."
2504.04984,"We study the Max Partial $k$-Coloring problem, where we are given a vertex-weighted graph, and we ask for a maximum-weight induced subgraph that admits a proper $k$-coloring. For $k=1$ this problem coincides with Maximum Weight Independent Set, and for $k=2$ the problem is equivalent (by complementation) to Minimum Odd Cycle Transversal. Furthermore, it generalizes $k$-Coloring. We show that Max Partial $k$-Coloring on $n$-vertex instances with clique number $\omega$ can be solved in time* $n^{\mathcal{O}(k\omega)}$ if the input graph excludes the bull and the chair as an induced subgraph,* $n^{\mathcal{O}(k\omega \log n)}$ if the input graph excludes the bull and E as an induced subgraph.This implies that $k$-Coloring can be solved in polynomial time in the former class, and in quasipolynomial-time in the latter one."
2504.06044,"The celebrated result of Kabanets and Impagliazzo (Computational Complexity, 2004) showed that PIT algorithms imply circuit lower bounds, and vice versa. Since then it has been a major challenge to understand the precise connections between PIT and lower bounds. In particular, a main goal has been to understand which lower bounds suffice to obtain efficient PIT algorithms, and how close are they to lower bounds that are necessary for the conclusion.We construct polynomial-time PIT algorithms from lower bounds that are, up to relatively minor remaining gaps, necessary for the existence of such algorithms. That is, we prove that these lower bounds are, up to the mentioned minor gaps, both sufficient and necessary for polynomial-time PIT, over fields of characteristic zero. Over sufficiently large finite fields, we show a similar result wherein the PIT algorithm runs in time $n^{\log^{(c)}(n)}$, i.e. a power of $c$-iterated log for an arbitrarily large constant $c>1$.The key to these improvements is studying PIT versus lower bounds in the uniform setting, in which we focus on proving lower bounds for uniform arithmetic circuits and their variants (and on deducing algorithms from such lower bounds). Indeed, by working in this setting we obtain results that are significantly tighter than previously known results concerning polynomial-time PIT vs lower bounds, and are in fact also tighter than known hardness-vs-randomness connections in the Boolean setting.Our results are obtained by combining recent techniques from Boolean hardness vs randomness, and in particular the generator of Chen and Tell (FOCS 2021), with the algebraic hitting-set generator of Guo, Kumar, Saptharishi, and Solomon (SIAM J. Computing 2022) along with the bootstrapping ideas of Agrawal, Ghosh, and Saxena (STOC 2018) and of Kumar, Saptharishi, and Tengse (SODA 2019)."
2504.07592,"We study the complexity of a class of promise graph homomorphism problems. For a fixed graph H, the H-colouring problem is to decide whether a given graph has a homomorphism to H. By a result of Hell and Nešetřil, this problem is NP-hard for any non-bipartite loop-less graph H. Brakensiek and Guruswami [SODA 2018] conjectured the hardness extends to promise graph homomorphism problems as follows: fix a pair of non-bipartite loop-less graphs G, H such that there is a homomorphism from G to H, it is NP-hard to distinguish between graphs that are G-colourable and those that are not H-colourable. We confirm this conjecture in the cases when both G and H are 4-colourable. This is a common generalisation of previous results of Khanna, Linial, and Safra [Comb. 20(3): 393-415 (2000)] and of Krokhin and Opršal [FOCS 2019]. The result is obtained by combining the algebraic approach to promise constraint satisfaction with methods of topological combinatorics and equivariant obstruction theory."
2504.08063,"While efficient randomized algorithms for factorization of polynomials given by algebraic circuits have been known for decades, obtaining an even slightly non-trivial deterministic algorithm for this problem has remained an open question of great interest. This is true even when the input algebraic circuit has additional structure, for instance, when it is a constant-depth circuit. Indeed, no efficient deterministic algorithms are known even for the seemingly easier problem of factoring sparse polynomials or even the problem of testing the irreducibility of sparse polynomials.In this work, we make progress on these questions: we design a deterministic algorithm that runs in subexponential time, and when given as input a constant-depth algebraic circuit $C$ over the field of rational numbers, it outputs algebraic circuits (of potentially unbounded depth) for all the irreducible factors of $C$, together with their multiplicities. In particular, we give the first subexponential time deterministic algorithm for factoring sparse polynomials.For our proofs, we rely on a finer understanding of the structure of power series roots of constant-depth circuits and the analysis of the Kabanets-Impagliazzo generator. In particular, we show that the Kabanets-Impagliazzo generator constructed using low-degree hard polynomials (explicitly constructed in the work of Limaye, Srinivasan & Tavenas) preserves not only the non-zeroness of small constant-depth circuits (as shown by Chou, Kumar & Solomon), but also their irreducibility and the irreducibility of their factors."
2504.08444,"A catalytic machine is a space-bounded Turing machine with additional access to a second, much larger work tape, with the caveat that this tape is full, and its contents must be preserved by the computation. Catalytic machines were defined by Buhrman et al. (STOC 2014), who, alongside many follow-up works, exhibited the power of catalytic space ($CSPACE$) and in particular catalytic logspace machines ($CL$) beyond that of traditional space-bounded machines.Several variants of $CL$ have been proposed, including non-deterministic and co-non-deterministic catalytic computation by Buhrman et al. (STACS 2016) and randomized catalytic computation by Datta et al. (CSR 2020). These and other works proposed several questions, such as catalytic analogues of the theorems of Savitch and Immerman and Szelepcsényi. Catalytic computation was recently derandomized by Cook et al. (STOC 2025), but only in certain parameter regimes.We settle almost all questions regarding randomized and non-deterministic catalytic computation, by giving an optimal reduction from catalytic space with additional resources to the corresponding non-catalytic space classes. With regards to non-determinism, our main result is that \[CL=CNL\] and with regards to randomness we show \[CL=CPrL\] where $CPrL$ denotes randomized catalytic logspace where the accepting probability can be arbitrarily close to $1/2$. We also have a number of near-optimal partial results for non-deterministic and randomized catalytic computation with less catalytic space. We show catalytic versions of Savitch's theorem, Immerman-Szelepscényi, and the derandomization results of Nisan and Saks and Zhou, all of which are unconditional and hold for all parameter settings.Our results build on the compress-or-compute framework of Cook et al. (STOC 2025). Despite proving broader and stronger results, our framework is simpler and more modular."
2504.08987,"We study the relative-error property testing model for Boolean functions that was recently introduced in the work of Chen et al. (SODA 2025). In relative-error testing, the testing algorithm gets uniform random satisfying assignments as well as black-box queries to $f$, and it must accept $f$ with high probability whenever $f$ has the property that is being tested and reject any $f$ that is relative-error far from having the property. Here the relative-error distance from $f$ to a function $g$ is measured with respect to $|f^{-1}(1)|$ rather than with respect to the entire domain size $2^n$ as in the Hamming distance measure that is used in the standard model; thus, unlike the standard model, relative-error testing allows us to study the testability of sparse Boolean functions that have few satisfying assignments. It was shown in Chen et al. (SODA 2025) that relative-error testing is at least as difficult as standard-model property testing, but for many natural and important Boolean function classes the precise relationship between the two notions is unknown.In this paper we consider the well-studied and fundamental properties of being a conjunction and being a decision list. In the relative-error setting, we give an efficient one-sided error tester for conjunctions with running time and query complexity $O(1/\epsilon)$.Secondly, we give a two-sided relative-error $\tilde{O}$$(1/\epsilon)$ tester for decision lists, matching the query complexity of the state-of-the-art algorithm in the standard model Bshouty (RANDOM 2020) and Diakonikolas et al. (FOCS 2007)."
2504.09312,"This papers considers the junta testing problem in a recently introduced ``relative error'' variant of the standard Boolean function property testing model. In relative-error testing we measure the distance from $f$ to $g$, where $f,g: \{0,1\}^n \to \{0,1\}$, by the ratio of $|f^{-1}(1) \triangle g^{-1}(1)|$ (the number of inputs on which $f$ and $g$ disagree) to $|f^{-1}(1)|$ (the number of satisfying assignments of $f$), and we give the testing algorithm both black-box access to $f$ and also access to independent uniform samples from $f^{-1}(1)$.Chen et al. (SODA 2025) observed that the class of $k$-juntas is $\text{poly}(2^k,1/\epsilon)$-query testable in the relative-error model, and asked whether $\text{poly}(k,1/\epsilon)$ queries is achievable. We answer this question affirmatively by giving a $\tilde{O}(k/\epsilon)$-query algorithm, matching the optimal complexity achieved in the less challenging standard model. Moreover, as our main result, we show that any subclass of $k$-juntas that is closed under permuting variables is relative-error testable with a similar complexity. This gives highly efficient relative-error testing algorithms for a number of well-studied function classes, including size-$k$ decision trees, size-$k$ branching programs, and size-$k$ Boolean formulas."
2504.09991,"Matching is a central problem in theoretical computer science, with a large body of work spanning the last five decades. However, understanding matching in the time-space bounded setting remains a longstanding open question, even in the presence of additional resources such as randomness or non-determinism.In this work we study space-bounded machines with access to catalytic space, which is additional working memory that is full with arbitrary data that must be preserved at the end of its computation. Despite this heavy restriction, many recent works have shown the power of catalytic space, its utility in designing classical space-bounded algorithms, and surprising connections between catalytic computation and derandomization.Our main result is that bipartite maximum matching ($MATCH$) can be computed in catalytic logspace ($CL$) with a polynomial time bound ($CLP$). Moreover, we show that $MATCH$ can be reduced to the lossy coding problem for $NC$ circuits ($LOSSY[NC]$). This has consequences for matching, catalytic space, and derandomization:- Matching: this is the first well studied subclass of $P$ which is known to compute $MATCH$, as well as the first algorithm simultaneously using sublinear free space and polynomial time with any additional resources.- Catalytic space: this is the first new problem shown to be in $CL$ since the model was defined, and one which is extremely central and well-studied.- Derandomization: we give the first class $\mathcal{C}$ beyond $L$ for which we exhibit a natural problem in $LOSSY[\mathcal{C}]$ which is not known to be in $\mathcal{C}$, as well as a full derandomization of the isolation lemma in $CL$ in the context of $MATCH$.Our proof combines a number of strengthened ideas from isolation-based algorithms for matching alongside the compress-or-random framework in catalytic computation."
2504.10904,"Developing explicit pseudorandom generators (PRGs) for prominent categories of Boolean functions is a key focus in computational complexity theory. In this paper, we investigate the PRGs against the functions of degree-$d$ polynomial threshold functions (PTFs) over Gaussian space. Our main result is an explicit construction of PRG with seed length $\mathrm{poly}(k,d,1/\epsilon)\cdot\log n$ that can fool any function of $k$ degree-$d$ PTFs with probability at least $1-\varepsilon$. More specifically, we show that the summation of $L$ independent $R$-moment-matching Gaussian vectors $\epsilon$-fools functions of $k$ degree-$d$ PTFs, where $L=\mathrm{poly}( k, d, \frac{1}{\epsilon})$ and $R = O({\log \frac{kd}{\epsilon}})$. The PRG is then obtained by applying an appropriate discretization to Gaussian vectors with bounded independence."
2504.11348,"A new proof technique combining finite model theory and dynamical systems has recently been introduced to obtain general complexity lower bounds on any question one may formulate on the dynamics (seen as a graph) of a given automata network (AN). ANs are abstract finite dynamical systems of interacting entities whose evolution rules are encoded as circuits, hence the study also applies to succinct graph representations (SGRs). In this article, we detail the construction of circuits to obtain general complexity lower bounds (metareduction) and show that the reduction is feasible in logarithmic space."
2504.11859,"The Fewest Clues Problem (FCP) framework has been introduced to study the complexity of determining whether a solution to an \NP~problem can be uniquely identified by specifying a subset of the certificate. For a given problem $P \in \NP$, its FCP variant is denoted by FCP-$P$. While several \NP-complete problems have been shown to have $\Sigma_2^\p$-complete FCP variants, it remains open whether this holds for all \NP-complete problems.In this work, we propose a meta-theorem that establishes the $\Sigma_2^\p$-completeness of FCP-$P$ under the condition that the \NP-hardness of $P$ is proven via a polynomial-time reduction satisfying certain structural properties. Furthermore, we apply the meta-theorem to demonstrate the $\Sigma_2^\p$-completeness of the FCP variants of several \NP-complete problems."
2504.12302,"Reachability of vector addition systems with states (VASS) is Ackermann complete~\cite{leroux2021reachability,czerwinski2021reachability}. For $d$-dimensional VASS reachability it is known that the problem is NP-complete~\cite{HaaseKreutzerOuaknineWorrell2009} when $d=1$, PSPACE-complete~\cite{BlondinFinkelGoellerHaaseMcKenzie2015} when $d=2$, and in $\mathbf{F}_d$~\cite{FuYangZheng2024} when $d>2$. A geometrically $d$-dimensional VASS is a $D$-dimensional VASS for some $D\ge d$ such that the space spanned by the displacements of the circular paths admitted in the $D$-dimensional VASS is $d$-dimensional. It is proved that the $\mathbf{F}_d$ upper bounds remain valid for the reachability problem in the geometrically $d$-dimensional VASSes with $d>2$."
2504.13268,"The class $NP$ can be defined by the means of Monadic Second-Order logic going back to Fagin and Feder-Vardi, and also by forbidden expanded substructures (cf. lifts and shadows of Kun and Nešetřil). Consequently, for such problems there is no dichotomy, unlike for $CSP$'s. We prove that ordering problems for graphs defined by finitely many forbidden ordered subgraphs still capture the class $NP$. In particular, we refute a conjecture of Hell, Mohar and Rafiey that dichotomy holds for this class. On the positive side, we confirm the conjecture of Duffus, Ginn and Rödl that ordering problems defined by one single biconnected ordered graph are $NP$-complete but for the ordered complete graph. An interesting feature appeared and was noticed several times. For finite sets of biconnected patterns (which may be colored structures or ordered structures) complexity dichotomy holds. A principal tool for obtaining this result is known as the Sparse Incomparability Lemma, a classical result in the theory of homomorphisms of graphs and structures. We prove it here in the setting of ordered graphs as a Temporal Sparse Incomparability Lemma for orderings. Interestingly, our proof involves the Lovász Local Lemma."
2504.13536,"We study the computational complexity of fundamental problems over the $p$-adic numbers ${\mathbb Q}_p$ and the $p$-adic integers ${\mathbb Z}_p$. Guépin, Haase, and Worrell proved that checking satisfiability of systems of linear equations combined with valuation constraints of the form $v_p(x) = c$ for $p \geq 5$ is NP-complete (both over ${\mathbb Z}_p$ and over ${\mathbb Q}_p$), and left the cases $p=2$ and $p=3$ open. We solve their problem by showing that the problem is NP-complete for ${\mathbb Z}_3$ and for ${\mathbb Q}_3$, but that it is in P for ${\mathbb Z}_2$ and for ${\mathbb Q}_2$. We also present different polynomial-time algorithms for solvability of systems of linear equations in ${\mathbb Q}_p$ with either constraints of the form $v_p(x) \leq c$ or of the form $v_p(x)\geq c$ for $c \in {\mathbb Z}$. Finally, we show how our algorithms can be used to decide in polynomial time the satisfiability of systems of (strict and non-strict) linear inequalities over ${\mathbb Q}$ together with valuation constraints $v_p(x) \geq c$ for several different prime numbers $p$ simultaneously."
2504.13986,"Forgetting a specific belief revision episode may not erase information because the other revisions may provide or entail the same information. Whether it does was proved coNP-hard for sequences of two arbitrary lexicographic revisions or arbitrarily long lexicographic Horn revisions. A polynomial algorithm is presented for the case of two lexicographic Horn revision. Heterogeneous sequences, including revisions other than lexicographic, were proved to belong in Delta2. Their previously proved coNP-hardness is enhanced to Dp-hardness."
2504.14074,"Holant problems are a general framework to study the computational complexity of counting problems. It is a more expressive framework than counting constraint satisfaction problems (CSP) which are in turn more expressive than counting graph homomorphisms (GH). In this paper, we prove the first complexity dichotomy of $\mathrm{Holant}_3(\mathcal{F})$ where $\mathcal{F}$ is an arbitrary set of symmetric, real valued constraint functions on domain size $3$. We give an explicit tractability criterion and prove that, if $\mathcal{F}$ satisfies this criterion then $\mathrm{Holant}_3(\mathcal{F})$ is polynomial time computable, and otherwise it is \#P-hard, with no intermediate cases. We show that the geometry of the tensor decomposition of the constraint functions plays a central role in the formulation as well as the structural internal logic of the dichotomy."
2504.1439,"In a graph G, a k-attack A is any set of at most k vertices and l-defense D is a set of at most l vertices. We say that defense D counters attack A if each a in A can be matched to a distinct defender d in D with a equal to d or a adjacent to d in G. In the defensive domination problem, we are interested in deciding, for a graph G and positive integers k and l given on input, if there exists an l-defense that counters every possible k-attack on G. Defensive domination is a natural resource allocation problem and can be used to model network robustness and security, disaster response strategies, and redundancy designs.The defensive domination problem is naturally in the complexity class $\Sigma^P_2$. The problem was known to be NP-hard in general, and polynomial-time algorithms were found for some restricted graph classes. In this note we prove that the defensive domination problem is $\Sigma^P_2$-complete. We also introduce a natural variant of the defensive domination problem in which the defense is allowed to be a multiset of vertices. This variant is also $\Sigma^P_2$-complete, but we show that it admits a polynomial-time algorithm in the class of interval graphs. A similar result was known for the original setting in the class of proper interval graphs."
2504.14729,"We prove a non-linear Edelstein-Kelly theorem for polynomials of constant degree, fully settling a stronger form of Conjecture 30 in Gupta (2014), and generalizing the main result of Peleg and Shpilka (STOC 2021) from quadratic polynomials to polynomials of any constant degree.As a consequence of our result, we obtain constant rank bounds for depth-4 circuits with top fanin 3 and constant bottom fanin (denoted $\Sigma^{3}\Pi\Sigma\Pi^{d}$ circuits) which compute the zero polynomial. This settles a stronger form of Conjecture 1 in Gupta (2014) when $k=3$, for any constant degree bound; additionally this also makes progress on Conjecture 28 in Beecken, Mittmann, and Saxena (Information \& Computation, 2013). Our rank bounds, when combined with Theorem 2 in Beecken, Mittmann, and Saxena (Information \& Computation, 2013) yield the first deterministic, polynomial time PIT algorithm for $\Sigma^{3}\Pi\Sigma\Pi^{d}$ circuits."
2504.15143,"In this paper, we initiate the study of deterministic PIT for $\Sigma^{[k]}\Pi\Sigma\Pi^{[\delta]}$ circuits over fields of any characteristic, where $k$ and $\delta$ are bounded. Our main result is a deterministic polynomial-time black-box PIT algorithm for $\Sigma^{[3]}\Pi\Sigma\Pi^{[\delta]}$ circuits, under the additional condition that one of the summands at the top $\Sigma$ gate is squarefree.Our techniques are purely algebro-geometric: they do not rely on Sylvester--Gallai-type theorems, and our PIT result holds over arbitrary fields.The core of our proof is based on the normalization of algebraic varieties. Specifically, we carry out the analysis in the integral closure of a coordinate ring, which enjoys better algebraic properties than the original ring."
2504.16311,"Ball, Liu, Mazor and Pass proved that the existence of key-agreement protocols is equivalent to the hardness of a certain problem about interactive Kolmogorov complexity. We generalize the statement and give a short proof of the difficult implication."
2504.17412,"In a seminal work, Buhrman et al. (STOC 2014) defined the class $CSPACE(s,c)$ of problems solvable in space $s$ with an additional catalytic tape of size $c$, which is a tape whose initial content must be restored at the end of the computation. They showed that uniform $TC^1$ circuits are computable in catalytic logspace, i.e., $CL=CSPACE(O(\log{n}), 2^{O(\log{n})})$, thus giving strong evidence that catalytic space gives $L$ strict additional power. Their study focuses on an arithmetic model called register programs, which has been a focal point in development since then.Understanding $CL$ remains a major open problem, as $TC^1$ remains the most powerful containment to date. In this work, we study the power of catalytic space and register programs to compute circuits of larger depth. Using register programs, we show that for every $\epsilon > 0$,$SAC^2 \subseteq CSPACE\left(O\left(\frac{\log^2{n}}{\log\log{n}}\right), 2^{O(\log^{1+\epsilon} n)}\right)$This is an $O(\log \log n)$ factor improvement on the free space needed to compute $SAC^2$, which can be accomplished with near-polynomial catalytic space.We also exhibit non-trivial register programs for matrix powering, which is a further step towards showing $NC^2 \subseteq CL$."
2504.17756,"The Sum-of-Squares (SoS) hierarchy, also known as Lasserre hierarchy, has emerged as a promising tool in optimization. However, it remains unclear whether fixed-degree SoS proofs can be automated [O'Donnell (2017)]. Indeed, there are examples of polynomial systems with bounded coefficients that admit low-degree SoS proofs, but these proofs necessarily involve numbers with an exponential number of bits, implying that low-degree SoS proofs cannot always be found efficiently.A sufficient condition derived from the Nullstellensatz proof system [Raghavendra and Weitz (2017)] identifies cases where bit complexity issues can be circumvented. One of the main problems left open by Raghavendra and Weitz is proving any result for refutations, as their condition applies only to polynomial systems with a large set of solutions.In this work, we broaden the class of polynomial systems for which degree-$d$ SoS proofs can be automated. To achieve this, we develop a new criterion and we demonstrate how our criterion applies to polynomial systems beyond the scope of Raghavendra and Weitz's result. In particular, we establish a separation for instances arising from Constraint Satisfaction Problems (CSPs). Moreover, our result extends to refutations, establishing that polynomial-time refutation is possible for broad classes of polynomial time solvable constraint problems, highlighting a first advancement in this area."
2504.1864,"In this paper we present tight lower-bounds and new upper-bounds for hypergraph and database problems. We give tight lower-bounds for finding minimum hypercycles. We give tight lower-bounds for a substantial regime of unweighted hypercycle. We also give a new faster algorithm for longer unweighted hypercycles. We give a worst-case to average-case reduction from detecting a subgraph of a hypergraph in the worst-case to counting subgraphs of hypergraphs in the average-case. We demonstrate two applications of this worst-case to average-case reduction, which result in average-case lower bounds for counting hypercycles in random hypergraphs and queries in average-case databases. Our tight upper and lower bounds for hypercycle detection in the worst-case have immediate implications for the average-case via our worst-case to average-case reductions."
2504.18722,"Recent advances in large language models (LLMs) have led to their popularity across multiple use-cases. However, prompt engineering, the process for optimally utilizing such models, remains approximation-driven and subjective. Most of the current research on prompt engineering focuses on task-specific optimization, while neglecting the behavior of the LLM under consideration during prompt development. This paper introduces MODP -- Multi Objective Directional Prompting, a framework based on two key concepts: 1) multi-objectivity: the importance of considering an LLM's intrinsic behavior as an additional objective in prompt development, and 2) directional prompting: a metrics-driven method for prompt engineering to ensure development of robust and high-precision prompts. We demonstrate the effectiveness of our proposed ideas on a summarization task, using a synthetically created dataset, achieving a 26% performance gain over initial prompts. Finally, we apply MODP to develop prompts for Dell's Next Best Action support tool, which is now in production and is used by more than 10,000 internal support agents and serving millions of customers worldwide."
2504.19386,"A king in a directed graph is a vertex $v$ such that every other vertex is reachable from $v$ via a path of length at most $2$. It is well known that every tournament (a complete graph where each edge has a direction) has at least one king. Our contributions in this work are:- We show that the query complexity of determining existence of a king in arbitrary $n$-vertex digraphs is $\Theta(n^2)$. This is in stark contrast to the case where the input is a tournament, where Shen, Sheng, and Wu [SICOMP'03] showed that a king can be found in $O(n^{3/2})$ queries.- In an attempt to increase the ""fairness"" in the definition of tournament winners, Ho and Chang [IPL'03] defined a strong king to be a king $k$ such that, for every $v$ that dominates $k$, the number of length-$2$ paths from $k$ to $v$ is strictly larger than the number of length-$2$ paths from $v$ to $k$. We show that the query complexity of finding a strong king in a tournament is $\Theta(n^2)$. This answers a question of Biswas, Jayapaul, Raman, and Satti [DAM'22] in the negative.A key component in our proofs is the design of specific tournaments where every vertex is a king, and analyzing certain properties of these tournaments. We feel these constructions and properties are independently interesting and may lead to more interesting results about tournament solutions."
2504.19777,"In this paper, we exhibit an $\textsf{AC}^{3}$ isomorphism test for groups without Abelian normal subgroups (a.k.a. Fitting-free groups), a class for which isomorphism testing was previously known to be in $\mathsf{P}$ (Babai, Codenotti, and Qiao; ICALP '12). Here, we leverage the fact that $G/\text{PKer}(G)$ can be viewed as permutation group of degree $O(\log |G|)$. As $G$ is given by its multiplication table, we are able to implement the solution for the corresponding instance of Twisted Code Equivalence in $\textsf{AC}^{3}$.In sharp contrast, we show that when our groups are specified by a generating set of permutations, isomorphism testing of Fitting-free groups is at least as hard as Graph Isomorphism and Linear Code Equivalence (the latter being $\textsf{GI}$-hard and having no known subexponential-time algorithm).Lastly, we show that any Fitting-free group of order $n$ is identified by $\textsf{FO}$ formulas (without counting) using only $O(\log \log n)$ variables. This is in contrast to the fact that there are infinite families of Abelian groups that are not identified by $\textsf{FO}$ formulas with $o(\log n)$ variables (Grochow & Levet, FCT '23)."
2504.19944,"We study the complexity of satisfiability problems in probabilistic and causal reasoning. Given random variables $X_1, X_2,\ldots$ over finite domains, the basic terms are probabilities of propositional formulas over atomic events $X_i = x_i$, such as $P(X_1 = x_1)$ or $P(X_1 = x_1 \vee X_2 = x_2)$. The basic terms can be combined using addition (yielding linear terms) or multiplication (polynomial terms). The probabilistic satisfiability problem asks whether a joint probability distribution satisfies a Boolean combination of (in)equalities over such terms. Fagin et al. (1990) showed that for basic and linear terms, this problem is NP-complete, making it no harder than Boolean satisfiability, while Mossé et al. (2022) proved that for polynomial terms, it is complete for the existential theory of the reals.Pearl's Causal Hierarchy (PCH) extends the probabilistic setting with interventional and counterfactual reasoning, enriching the expressiveness of languages. However, Mossé et al. (2022) found that satisfiability complexity remains unchanged. Van der Zander et al. (2023) showed that introducing a marginalization operator to languages induces a significant increase in complexity.We extend this line of work by adding two new dimensions to the problem by constraining the models. First, we fix the graph structure of the underlying structural causal model, motivated by settings like Pearl's do-calculus, and give a nearly complete landscape across different arithmetics and PCH levels. Second, we study small models. While earlier work showed that satisfiable instances admit polynomial-size models, this is no longer guaranteed with compact marginalization. We characterize the complexities of satisfiability under small-model constraints across different settings."
2504.20262,"The class TotP consists of functions that count the number of all paths of a nondeterministic polynomial-time Turing machine. In this paper, we give a predicate based definition of TotP, analogous to a standard definition of #P. From a new characterization of TotP it follows that many well known #P problems belong to TotP, and TotP = #P if and only if P = NP. We show that TotP has several closure properties of #P and GapP, and also properties that are not known to hold for #P and GapP. We also prove that the closure of TotP under left composition with FP+ is equivalent to TotP = FP+ and P = PP, and give examples of FP+-functions such that if TotP is closed under composition with them, then it is closed under composition with FP+."
2504.2095,"Williams (STOC 2025) recently proved that time-$t$ multitape Turing machines can be simulated using $O(\sqrt{t \log t})$ space using the Cook-Mertz (STOC 2024) tree evaluation procedure. As Williams notes, applying this result to fast algorithms for the circuit value problem implies an $O(\sqrt{s} \cdot \mathrm{polylog}\; s)$ space algorithm for evaluating size $s$ circuits.In this work, we provide a direct reduction from circuit value to tree evaluation without passing through Turing machines, simultaneously improving the bound to $O(\sqrt{s \log s})$ space and providing a proof with fewer abstraction layers.This result can be thought of as a ""sibling"" result to Williams' for circuit complexity instead of time; in particular, using the fact that time-$t$ Turing machines have size $O(t \log t)$ circuits, we can recover a slightly weakened version of Williams' result, simulating time-$t$ machines in space $O(\sqrt{t} \log t)$."
2504.21624,"Given a graph $G$, a set $T$ of terminal vertices, and a demand graph $H$ on $T$, the \textsc{Multicut} problem asks for a set of edges of minimum weight that separates the pairs of terminals specified by the edges of $H$.The \textsc{Multicut} problem can be solved in polynomial time if the number of terminals and the genus of the graph is bounded (Colin de Verdière [Algorithmica, 2017]).Focke et al.~[SoCG 2024] characterized which special cases of Multicut are fixed-parameter tractable parameterized by the number of terminals on planar graphs. Moreover, they precisely determined how the parameter genus influences the complexity and presented partial results of this form for graphs that can be made planar by the deletion of $\pi$ edges. We complete the picture on how this parameter $\pi$ influences the complexity of different special cases and precisely determine the influence of the crossing number.Formally, let $\mathcal{H}$ be any class of graphs (satisfying a mild closure property) and let Multicut$(\mathcal{H})$ be the special case when the demand graph $H$ is in $\mathcal{H}$. Our first main result is showing that if $\mathcal{H}$ has the combinatorial property of having bounded distance to extended bicliques, then Multicut$(\mathcal{H})$ on unweighted graphs is FPT parameterized by the number $t$ of terminals and $\pi$. For the case when $\mathcal{H}$ does not have this combinatorial property,Focke et al.~[SoCG 2024] showed that $O(\sqrt{t})$ is essentially the best possible exponent of the running time; together with our result, this gives a complete understanding of how the parameter $\pi$ influences complexity on unweighted graphs.Our second main result is giving an algorithm whose existence shows that the parameter crossing number behaves analogously if we consider weighted graphs."
2505.00051,"This white paper demonstrates that reverse engineering Unidentified Aerial Phenomena (UAP) is NP-complete under classical computational paradigms. By modeling UAP reconstruction as an automaton identification problem with a state characterization matrix M(D, T, E) and examining the inherent challenges in data gathering as well as unknown physics, we show that inferring internal mechanisms (such as Isotopically-Engineered-Materials or unconventional propulsion systems) from finite observational data is computationally intractable. Data D, comprising both operational non-reproducible observations and reproducible analysis data from purported crash retrievals, remains inherently fragmentary. Even if UAP observables were reproducible, the absence of a comprehensive theoretical framework ensures that reverse engineering remains NP-complete, and may escalate to PSPACE-hard or to an Entscheidungsproblem. This intractability challenges current UAP reverse engineering efforts and has profound implications for transparency on UAP technology and related venture investments. Hence, UAP are as analogous to modern smartphones in the hands of Neanderthals."
2505.00164,"We study the communication complexity of the Minimum Vertex Cover (MVC) problem on general graphs within the \(k\)-party one-way communication model. Edges of an arbitrary \(n\)-vertex graph are distributed among \(k\) parties. The objective is for the parties to collectively find a small vertex cover of the graph while adhering to a communication protocol where each party sequentially sends a message to the next until the last party outputs a valid vertex cover of the whole graph. We are particularly interested in the trade-off between the size of the messages sent and the approximation ratio of the output solution.It is straightforward to see that any constant approximation protocol for MVC requires communicating \(\Omega(n)\) bits. Additionally, there exists a trivial 2-approximation protocol where the parties collectively find a maximal matching of the graph greedily and return the subset of vertices matched. This raises a natural question: \textit{What is the best approximation ratio achievable using optimal communication of \(O(n)\)?} We design a protocol with an approximation ratio of \((2-2^{-k+1}+\epsilon)\) and \(O(n)\) communication for any desirably small constant \(\epsilon>0\), which is strictly better than 2 for any constant number of parties. Moreover, we show that achieving an approximation ratio smaller than \(3/2\) for the two-party case requires \(n^{1 + \Omega(1/\lg\lg n)}\) communication, thereby establishing the tightness of our protocol for two parties."
2505.00181,"We study a discrete convolution streaming problem. An input arrives as a stream of numbers $z = (z_0,z_1,z_2,\ldots)$, and at time $t$ our goal is to output $(Tz)_t$ where $T$ is a lower-triangular Toeplitz matrix. We focus on space complexity; the algorithm can store a buffer of $\beta(t)$ numbers in order to achieve this goal.We characterize space complexity when algorithms perform continuous operations. The matrix $T$ corresponds to a generating function $G(x)$. If $G(x)$ is rational of degree $d$, then it is known that the space complexity is at most $O(d)$. We prove a corresponding lower bound; the space complexity is at least $\Omega(d)$. In addition, for irrational $G(x)$, we prove that the space complexity is infinite. We also provide finite-time guarantees. For example, for the generating function $\frac{1}{\sqrt{1-x}}$ that was studied in various previous works in the context of differentially private continual counting, we prove a sharp lower bound on the space complexity; at time $t$, it is at least $\Omega(t)$."
2505.00206,"In the $k$-Orthogonal Vectors ($k$-OV) problem we are given $k$ sets, each containing $n$ binary vectors of dimension $d=n^{o(1)}$, and our goal is to pick one vector from each set so that at each coordinate at least one vector has a zero. It is a central problem in fine-grained complexity, conjectured to require $n^{k-o(1)}$ time in the worst case.We propose a way to \emph{plant} a solution among vectors with i.i.d. $p$-biased entries, for appropriately chosen $p$, so that the planted solution is the unique one. Our conjecture is that the resulting $k$-OV instances still require time $n^{k-o(1)}$ to solve, \emph{on average}.Our planted distribution has the property that any subset of strictly less than $k$ vectors has the \emph{same} marginal distribution as in the model distribution, consisting of i.i.d. $p$-biased random vectors. We use this property to give average-case search-to-decision reductions for $k$-OV."
2505.00988,"A dominating set of a graph $G=(V,E)$ is a set of vertices $D \subseteq V$ whose closed neighborhood is $V$, i.e., $N[D]=V$. We view a dominating set as a collection of tokens placed on the vertices of $D$. In the token sliding variant of the Dominating Set Reconfiguration problem (TS-DSR), we seek to transform a source dominating set into a target dominating set in $G$ by sliding tokens along edges, and while maintaining a dominating set all along the transformation.TS-DSR is known to be PSPACE-complete even restricted to graphs of pathwidth $w$, for some non-explicit constant $w$ and to be XL-complete parameterized by the size $k$ of the solution. The first contribution of this article consists in using a novel approach to provide the first explicit constant for which the TS-DSR problem is PSPACE-complete, a question that was left open in the literature.From a parameterized complexity perspective, the token jumping variant of DSR, i.e., where tokens can jump to arbitrary vertices, is known to be FPT when parameterized by the size of the dominating sets on nowhere dense classes of graphs. But, in contrast, no non-trivial result was known about TS-DSR. We prove that DSR is actually much harder in the sliding model since it is XL-complete when restricted to bounded pathwidth graphs and even when parameterized by $k$ plus the feedback vertex set number of the graph. This gives, for the first time, a difference of behavior between the complexity under token sliding and token jumping for some problem on graphs of bounded treewidth. All our results are obtained using a brand new method, based on the hardness of the so-called Tape Reconfiguration problem, a problem we believe to be of independent interest."
2505.01587,"Building on the techniques behind the recent progress on the 3-term arithmetic progression problem [KM'23], Kelley, Lovett, and Meka [KLM'24] constructed the first explicit 3-player function $f:[N]^3 \rightarrow \{0,1\}$ that demonstrates a strong separation between randomized and (non-)deterministic NOF communication complexity. Specifically, their hard function can be solved by a randomized protocol sending $O(1)$ bits, but requires $\Omega(\log^{1/3}(N))$ bits of communication with a deterministic (or non-deterministic) protocol.We show a stronger $\Omega(\log^{1/2}(N))$ lower bound for their construction. To achieve this, the key technical advancement is an improvement to the sifting argument for grid norms of (somewhat dense) bipartite graphs. In addition to quantitative improvement, we qualitatively improve over [KLM'24] by relaxing the hardness condition: while [KLM'24] proved their lower bound for any function $f$ that satisfies a strong two-sided pseudorandom condition, we show that a weak one-sided condition suffices. This is achieved by a new structural result for cylinder intersections (or, in graph-theoretic language, the set of triangles induced from a tripartite graph), showing that any small cylinder intersection can be efficiently covered by a sum of simple ``slice'' functions."
2505.0199,"In a distinguishing problem, the input is a sample drawn from one of two distributions and the algorithm is tasked with identifying the source distribution. The performance of a distinguishing algorithm is measured by its advantage, i.e., its incremental probability of success over a random guess. A classic example of a distinguishing problem is the Planted Clique problem, where the input is a graph sampled from either $G(n,1/2)$ -- the standard Erdős-Rényi model, or $G(n,1/2,k)$ -- the Erdős-Rényi model with a clique planted on a random subset of $k$ vertices. The Planted Clique Hypothesis asserts that efficient algorithms cannot achieve advantage better than some absolute constant, say $1/4$, whenever $k=n^{1/2-\Omega(1)}$. In this work, we aim to precisely understand the optimal distinguishing advantage achievable by efficient algorithms on Planted Clique. We show the following results under the Planted Clique hypothesis:1. Optimality of low-degree polynomials: No efficient algorithm can beat the advantage the optimal low-degree polynomial. Concretely, this means that the advantage of any efficient algorithm is at most $(1+o(1))\cdot k^2/(\sqrt{\pi}n)$, which is optimal in light of a simple edge-counting algorithm achieving this bound.2. Harder planted distributions: There is an efficiently sampleable distribution $\mathcal{P}^*$ supported on graphs containing $k$-cliques such that no efficient algorithm can distinguish $\mathcal{P}^*$ from $G(n,1/2)$ with advantage $n^{-d}$ for an arbitrarily large constant $d$. In other words, there exist alternate planted distributions that are much harder than $G(n,1/2,k)$. Along the way, we prove a constructive hard-core lemma for a broad class of distributions with respect to low-degree polynomials. This result is applicable much more widely beyond Planted Clique and might be of independent interest."
2505.02622,"Bitstrings can be permuted via permutations and compared via the lexicographic order. In this paper we study the complexity of finding a minimum of a bitstring via given permutations. As a global optima is known to be NP-complete, we study the local optima via the class PLS and show hardness for PLS. Additionally, we show that even for one permutation the global optimization is NP-complete and give a formula that has these permutation as symmetries. This answers an open question inspired from Kolodziejczyk and Thapen and stated at the SAT and interactions seminar in Dagstuhl."
2505.05658,"In this paper, we examine Lin's ""On NP versus coNP and Frege Systems"" [Lin25]. Lin claims to prove that $\text{NP} \neq \text{coNP}$ by constructing a language $L_d$ such that $L_d \in \text{NP}$ but $L_d \notin \text{coNP}$. We present a flaw in Lin's construction of $D$ (a nondeterministic Turing machine that supposedly recognizes $L_d$ in polynomial time). We also provide a proof that $L_d \not\in \text{NP}$. In doing so, we demonstrate that Lin's claim that $\text{NP} \neq \text{coNP}$ is not established by his paper. In addition, we note that a number of further results that Lin claims are not validly established by his paper."
2505.06406,"We study the safety problem for the next-generation access control (NGAC) model. We show that under mild assumptions it is coNP-complete, and under further realistic assumptions we give an algorithm for the safety problem that significantly outperforms naive brute force search. We also show that real-world examples of mutually exclusive attributes lead to nearly worst-case behavior of our algorithm."
2505.06414,"Battle Sheep is a board game published by Blue Orange Games. With two players, it is a combinatorial game that uses normal play rules. We show that it is PSPACE-complete, even when each stack has only up to 3 tokens."
2505.06725,"We study a planted clique model introduced by Feige where a complete graph of size $c\cdot n$ is planted uniformly at random in an arbitrary $n$-vertex graph. We give a simple deterministic algorithm that, in almost linear time, recovers a clique of size $(c/3)^{O(1/c)} \cdot n$ as long as the original graph has maximum degree at most $(1-p)n$ for some fixed $p>0$. The proof hinges on showing that the degrees of the final graph are correlated with the planted clique, in a way similar to (but more intricate than) the classical $G(n,\frac{1}{2})+K_{\sqrt{n}}$ planted clique model. Our algorithm suggests a separation from the worst-case model, where, assuming the Unique Games Conjecture, no polynomial algorithm can find cliques of size $\Omega(n)$ for every fixed $c>0$, even if the input graph has maximum degree $(1-p)n$. Our techniques extend beyond the planted clique model. For example, when the planted graph is a balanced biclique, we recover a balanced biclique of size larger than the best guarantees known for the worst case."
2505.07443,"The problems Cluster Vertex Deletion (or Cluster-VD) and its generalization s-Club Cluster Vertex Deletion (or s-Club-VD, for any integer s>= 1), have been introduced with the aim of detecting highly-connected parts in complex systems. Their NP-completeness has been established for several classes of graphs, but remains open for smaller classes, including subcubic planar bipartite graphs and cubic graphs. In this paper, we show that Cluster-VD and more generally s-Club-VD are NP-complete for cubic planar bipartite graphs. We also deduce new results for the related k-Path Vertex Cover problem (or k-PVC), namely 3-PVC is NP-complete for cubic planar bipartite graphs, whereas k-PVC with k>= 4 is NP-complete for subcubic planar (and bipartite, when k is odd) graphs of arbitrarily large girth."
2505.08462,"Quantum Merlin-Arthur proof systems are believed to be stronger than both their classical counterparts and ``stand-alone'' quantum computers when Arthur is assumed to operate in $\Omega(\log n)$ space. No hint of such an advantage over classical computation had emerged from research on smaller space bounds, which had so far concentrated on constant-space verifiers. We initiate the study of quantum Merlin-Arthur systems with space bounds in $\omega(1) \cap o(\log n)$, and exhibit a problem family $\mathcal{F}$, whose yes-instances have proofs that are verifiable by polynomial-time quantum Turing machines operating in this regime. We show that no problem in $\mathcal{F}$ has proofs that can be verified classically or is solvable by a stand-alone quantum machine in polynomial time if standard complexity assumptions hold. Unlike previous examples of small-space verifiers, our protocols require only subpolynomial-length quantum proofs."
2505.09165,"This study examines the computational complexity of the decision problem modeled on the smartphone game Bus Out. The objective of the game is to load all the passengers in a queue onto appropriate buses using a limited number of bus parking spots by selecting and dispatching the buses on a map. We show that the problem is NP-complete, even for highly restricted instances. We also show that it is hard to approximate the minimum number of parking spots needed to solve a given instance."
2505.09282,"In [A. Jackson, Explaining the ubiquity of phase transitions in decision problems (2025),arXiv:2501.14569], I established that phase transitions are always present in a large subset of decision problems over even-sized alphabets, explaining -- in part -- why phase transitions are seen so often in decision problems. However, decision problems over odd-sized alphabets were not discussed. Here, I correct that oversight, showing that a similar subset of decision problems over odd-sized alphabets also always exhibit phase transitions."
2505.09824,"Canonical polyadic decomposition (CPD) is at the core of fast matrix multiplication, a computational problem with widespread implications across several seemingly unrelated problems in computer science. Much recent progress in this field has used randomized heuristic search to find new CPDs, often over a finite field. However, if these techniques fail to find a CPD with low enough rank, they cannot prove that no such CPD exists. Consequently, these methods fail to resolve certain long-standing questions, such as whether the tensor corresponding to $3\times 3$ matrix multiplication has rank less than 23. To make progress on these problems, we develop a novel algorithm that preserves exactness, i.e. they can provably verify whether or not a given tensor has a specified rank. Compared to brute force, when searching for a rank-$R$ CPD of a $n_0\times\dots\times n_{D-1}$-shaped tensor over a finite field $\mathbb{F}$, where $n_0\ge \dots\ge n_{D-1}$, our algorithm saves a multiplicative factor of roughly $|\mathbb{F}|^{R(n_0-1)+n_0(\sum_{d\ge 1} n_d)}$. Additionally, our algorithm runs in polynomial time. We also find a novel algorithm to search border CPDs, a variant of CPDs that is also important in fast matrix multiplication. Finally, we study the maximum rank problem and give new upper and lower bounds, both for families of tensor shapes and specific shapes. Although our CPD search algorithms are still too slow to resolve the rank of $3\times 3$ matrix multiplication, we are able to utilize them in this problem by adding extra search pruners that do not affect exactness or increase asymptotic running time."
2505.10201,"The Boolean satisfiability problem (SAT) is a well-known example of monotonic reasoning, of intense practical interest due to fast solvers, complemented by rigorous fine-grained complexity results. However, for non-monotonic reasoning, e.g., abductive reasoning, comparably little is known outside classic complexity theory. In this paper we take a first step of bridging the gap between monotonic and non-monotonic reasoning by analyzing the complexity of intractable abduction problems under the seemingly overlooked but natural parameter n: the number of variables in the knowledge base. We obtain several positive results for $\Sigma^P_2$- as well as NP- and coNP-complete fragments, which implies the first example of beating exhaustive search for a $\Sigma^P_2$-complete problem (to the best of our knowledge). We complement this with lower bounds and for many fragments rule out improvements under the (strong) exponential-time hypothesis."
2505.10675,"We study the arithmetic complexity of hitting set generators, which are pseudorandom objects used for derandomization of the polynomial identity testing problem. We give new explicit constructions of hitting set generators whose outputs are computable in $VNC^0$, i.e., can be computed by arithmetic formulas of constant size. Unconditionally, we construct a $VNC^0$-computable generator that hits arithmetic circuits of constant depth and polynomial size. We also give conditional constructions, under strong but plausible hardness assumptions, of $VNC^0$-computable generators that hit arithmetic formulas and arithmetic branching programs of polynomial size, respectively. As a corollary of our constructions, we derive lower bounds for subsystems of the Geometric Ideal Proof System of Grochow and Pitassi.Constructions of such generators are implicit in prior work of Kayal on lower bounds for the degree of annihilating polynomials. Our main contribution is a construction whose correctness relies on circuit complexity lower bounds rather than degree lower bounds."
2505.11082,"We consider a pursuit-evasion game that describes the process of extinguishing a fire burning on the nodes of an undirected graph. We denote the minimum number of firefighters required by $\text{ffn}(G)$ and provide a characterization for the graphs with $\text{ffn}(G)=1$ and $\text{ffn}(G)=2$ as well as almost sharp bounds for complete binary trees. We show that deciding whether $\text{ffn}(G) \leq m$ for given $G$ and $m$ is NP-hard. Furthermore, we show that shortest strategies can have superpolynomial length, leaving open whether the problem is in NP. Based on some plausible conjectures, we also prove that this decision problem is neither NP-hard for graphs with bounded treewidth nor for constant $m$."
2505.13134,"In this paper, we give a quadratic Goldreich-Levin algorithm that is close to optimal in the following ways. Given a bounded function $f$ on the Boolean hypercube $\mathbb{F}_2^n$ and any $\varepsilon>0$, the algorithm returns a quadratic polynomial $q: \mathbb{F}_2^n \to \mathbb{F}_2$ so that the correlation of $f$ with the function $(-1)^q$ is within an additive $\varepsilon$ of the maximum possible correlation with a quadratic phase function. The algorithm runs in $O_\varepsilon(n^3)$ time and makes $O_\varepsilon(n^2\log n)$ queries to $f$, which matches the information-theoretic lower bound of $\Omega(n^2)$ queries up to a logarithmic factor.As a result, we obtain a number of corollaries:- A near-optimal self-corrector of quadratic Reed-Muller codes, which makes $O_\varepsilon(n^2\log n)$ queries to a Boolean function $f$ and returns a quadratic polynomial $q$ whose relative Hamming distance to $f$ is within $\varepsilon$ of the minimum distance.- An algorithmic polynomial inverse theorem for the order-3 Gowers uniformity norm.- An algorithm that makes a polynomial number of queries to a bounded function $f$ and decomposes $f$ as a sum of poly$(1/\varepsilon)$ quadratic phase functions and error terms of order $\varepsilon$.Our algorithm is obtained using ideas from recent work on quantum learning theory. Its construction deviates from previous approaches based on algorithmic proofs of the inverse theorem for the order-3 uniformity norm (and in particular does not rely on the recent resolution of the polynomial Fre\uıman-Ruzsa conjecture)."
2505.16012,"Decomposable Negation Normal Forms \textsc{dnnf} [Darwiche, 'Decomposable Negation Normal Form', JACM, 2001] is a landmark Knowledge Compilation (\textsc{kc}) model, highly important both in \textsc{ai} and Theoretical Computer Science. Numerous restrictions of the model have been studied. In this paper we consider the restriction where all the gates are $\alpha$-imbalanced that is, at most one input of each gate depends on more than $n^{\alpha}$ variables (where $n$ is the number if variables of the function being represented).The concept of imbalanced gates has been first considered in [Lai, Liu, Yin 'New canonical representations by augmenting OBDDs with conjunctive decomposition', JAIR, 2017]. We consider the idea in the context of representation of \textsc{cnf}s of bounded primal treewidth. We pose an open question as to whether \textsc{cnf}s of bounded primal treewidth can be represented as \textsc{fpt}-sized \textsc{dnnf} with $\alpha$-imbalanced gates.We answer the question negatively for Decision \textsc{dnnf} with $\alpha$-imbalanced conjunction gates. In particular, we establish a lower bound of $n^{\Omega((1-\alpha) \cdot k)}$ for the representation size (where $k$ is the primal treewidth of the input \textsc{cnf}). The main engine for the above lower bound is a combinatorial result that may be of an independent interest in the area of parameterized complexity as it introduces a novel concept of bidimensionality."
2505.16716,"An established measure of the expressive power of a given ReLU neural network is the number of linear regions into which it partitions the input space. There exist many different, non-equivalent definitions of what a linear region actually is. We systematically assess which papers use which definitions and discuss how they relate to each other. We then analyze the computational complexity of counting the number of such regions for the various definitions. Generally, this turns out to be an intractable problem. We prove NP- and #P-hardness results already for networks with one hidden layer and strong hardness of approximation results for two or more hidden layers. Finally, on the algorithmic side, we demonstrate that counting linear regions can at least be achieved in polynomial space for some common definitions."
2505.17314,"Is detecting a $k$-clique in $k$-partite regular (hyper-)graphs as hard as in the general case? Intuition suggests yes, but proving this -- especially for hypergraphs -- poses notable challenges. Concretely, we consider a strong notion of regularity in $h$-uniform hypergraphs, where we essentially require that any subset of at most $h-1$ is incident to a uniform number of hyperedges. Such notions are studied intensively in the combinatorial block design literature. We show that any $f(k)n^{g(k)}$-time algorithm for detecting $k$-cliques in such graphs transfers to an $f'(k)n^{g(k)}$-time algorithm for the general case, establishing a fine-grained equivalence between the $h$-uniform hyperclique hypothesis and its natural regular analogue.Equipped with this regularization result, we then fully resolve the fine-grained complexity of optimizing Boolean constraint satisfaction problems over assignments with $k$ non-zeros. Our characterization depends on the maximum degree $d$ of a constraint function. Specifically, if $d\le 1$, we obtain a linear-time solvable problem, if $d=2$, the time complexity is essentially equivalent to $k$-clique detection, and if $d\ge 3$ the problem requires exhaustive-search time under the 3-uniform hyperclique hypothesis. To obtain our hardness results, the regularization result plays a crucial role, enabling a very convenient approach when applied carefully. We believe that our regularization result will find further applications in the future."
2505.1736,"There is a growing body of work on proving hardness results for average-case estimation problems by bounding the low-degree advantage (LDA) - a quantitative estimate of the closeness of low-degree moments - between a null distribution and a related planted distribution. Such hardness results are now ubiquitous not only for foundational average-case problems but also central questions in statistics and cryptography. This line of work is supported by the low-degree conjecture of Hopkins, which postulates that a vanishing degree-$D$ LDA implies the absence of any noise-tolerant distinguishing algorithm with runtime $n^{\widetilde{O}(D)}$ whenever 1) the null distribution is product on $\{0,1\}^{\binom{n}{k}}$, and 2) the planted distribution is permutation invariant, that is, invariant under any relabeling $[n] \rightarrow [n]$.In this paper, we disprove this conjecture. Specifically, we show that for any fixed $\varepsilon>0$ and $k\geq 2$, there is a permutation-invariant planted distribution on $\{0,1\}^{\binom{n}{k}}$ that has a vanishing degree-$n^{1-O(\varepsilon)}$ LDA with respect to the uniform distribution on $\{0,1\}^{\binom{n}{k}}$, yet the corresponding $\varepsilon$-noisy distinguishing problem can be solved in $n^{O(\log^{1/(k-1)}(n))}$ time. Our construction relies on algorithms for list-decoding for noisy polynomial interpolation in the high-error regime.We also give another construction of a pair of planted and (non-product) null distributions on $\mathbb{R}^{n \times n}$ with a vanishing $n^{\Omega(1)}$-degree LDA while the largest eigenvalue serves as an efficient noise-tolerant distinguisher.Our results suggest that while a vanishing LDA may still be interpreted as evidence of hardness, developing a theory of average-case complexity based on such heuristics requires a more careful approach."
2505.18885,"The \emph{linear vertex arboricity} of a graph is the smallest number of sets into which the vertices of a graph can be partitioned so that each of these sets induces a linear forest. Chaplick et al. [JoCG 2020] showed that, somewhat surprisingly, the linear vertex arboricity of a graph is the same as the \emph{3D weak line cover number} of the graph, that is, the minimum number of straight lines necessary to cover the vertices of a crossing-free straight-line drawing of the graph in $\mathbb{R}^3$. Chaplick et al. [JGAA 2023] showed that deciding whether a given graph has linear vertex arboricity 2 is NP-hard.In this paper, we investigate the parameterized complexity of computing the linear vertex arboricity. We show that the problem is para-NP-hard with respect to the parameter maximum degree. Our result is tight in the following sense. All graphs of maximum degree 4 (except for $K_4$) have linear vertex arboricity at most 2, whereas we show that it is NP-hard to decide, given a graph of maximum degree 5, whether its linear vertex arboricity is 2. Moreover, we show that, for planar graphs, the same question is NP-hard for graphs of maximum degree 6, leaving open the maximum-degree-5 case. Finally, we prove that, for any $k \ge 1$, deciding whether the linear vertex arboricity of a graph is at most $k$ is fixed-parameter tractable with respect to the treewidth of the given graph."
2505.19137,"In this paper, we present algorithms to solve matrix multiplication problems in the MPC model. In particular, we consider the problem under various processor/memory constraints in the MPC model and prove the following results.1. Multiplication of two rectangular matrices of size $d \times n$ and $n \times d$ ( where $d \leq n$) respectively can be done in,i) $O(\sqrt{d} + \log_d n)$ rounds with $n$ processors and $\Theta(d)$ memory per processorii) $O(\frac{d}{\sqrt{n}})$ rounds with $d$ processors and $\Theta(n)$ memory per processor.2. Multiplication of two rectangular matrices of size $n \times d$ and $d \times n$ (where $d \leq n$) respectively, with $n$ processors of $\Theta(n)$ memory per processor, can be done in $O(\frac{d}{\sqrt{n}})$ rounds.this http URLmultiplication of two $d$-sparse matrices (matrices that contain at most $d$-nonzero elements in each row and in each column) with $n$ processors and $\Theta(d)$ memory per processor can be done in $O(d^{0.9})$ rounds."
2505.223,"We consider the parameterized problem $\#$IndSub$(\Phi)$ for fixed graph properties $\Phi$: Given a graph $G$ and an integer $k$, this problem asks to count the number of induced $k$-vertex subgraphs satisfying $\Phi$. Dörfler et al. [Algorithmica 2022] and Roth et al. [SICOMP 2024] conjectured that $\#$IndSub$(\Phi)$ is $\#$W[1]-hard for all non-meager properties $\Phi$, i.e., properties that are nontrivial for infinitely many $k$. This conjecture has been confirmed for several restricted types of properties, including all hereditary properties [STOC 2022] and all edge-monotone properties [STOC 2024].In this work, we refute this conjecture by showing that scorpion graphs, certain $k$-vertex graphs which were introduced more than 50 years ago in the context of the evasiveness conjecture, can be counted in time $O(n^4)$ for all $k$. A simple variant of this construction results in graph properties that achieve arbitrary intermediate complexity assuming ETH.We formulate an updated conjecture on the complexity of $\#$IndSub$(\Phi)$ that correctly captures the complexity status of scorpions and related constructions."
2505.22894,"For every fixed graph $H$, it is known that homomorphism counts from $H$ and colorful $H$-subgraph counts can be determined in $O(n^{t+1})$ time on $n$-vertex input graphs $G$, where $t$ is the treewidth of $H$. On the other hand, a running time of $n^{o(t / \log t)}$ would refute the exponential-time hypothesis. Komarath, Pandey and Rahul (Algorithmica, 2023) studied algebraic variants of these counting problems, i.e., homomorphism and subgraph $\textit{polynomials}$ for fixed graphs $H$. These polynomials are weighted sums over the objects counted above, where each object is weighted by the product of variables corresponding to edges contained in the object. As shown by Komarath et al., the $\textit{monotone}$ circuit complexity of the homomorphism polynomial for $H$ is $\Theta(n^{\mathrm{tw}(H)+1})$.In this paper, we characterize the power of monotone $\textit{bounded-depth}$ circuits for homomorphism and colorful subgraph polynomials. This leads us to discover a natural hierarchy of graph parameters $\mathrm{tw}_\Delta(H)$, for fixed $\Delta \in \mathbb N$, which capture the width of tree-decompositions for $H$ when the underlying tree is required to have depth at most $\Delta$. We prove that monotone circuits of product-depth $\Delta$ computing the homomorphism polynomial for $H$ require size $\Theta(n^{\mathrm{tw}_\Delta(H^{\dagger})+1})$, where $H^{\dagger}$ is the graph obtained from $H$ by removing all degree-$1$ vertices. This allows us to derive an optimal depth hierarchy theorem for monotone bounded-depth circuits through graph-theoretic arguments."
2505.23718,"In this work, we present the \emph{twiddless fast Fourier transform (TFFT)}, a novel algorithm for computing the $N$-point discrete Fourier transform (DFT). The TFFT's divide strategy builds on recent results that decimate an $N$-point signal (by a factor of $p$) into an $N/p$-point compressed signal whose DFT readily yields $N/p$ coefficients of the original signal. However, existing compression-domain DFT analyses have been limited to computing only the even-indexed DFT coefficients. With TFFT, we overcome this limitation by efficiently computing both \emph{even- and odd-indexed} DFT coefficients in the compressed domain with $O(N \log N)$ complexity. TFFT introduces a new recursive decomposition of the DFT problem, wherein $N/2^i$ coefficients of the original input are computed at recursion level $i$, with no need for twiddle factor multiplications or butterfly structures. Additionally, TFFT generalizes the input length to $N = c \cdot 2^k$ (for $k \geq 0$ and non-power-of-two $c > 0$), reducing the need for zero-padding and potentially improving efficiency and stability over classical FFTs. We believe TFFT represents a \emph{novel paradigm} for DFT computation, opening new directions for research in optimized implementations, hardware design, parallel computation, and sparse transforms."
2506.01832,"We obtain new explicit pseudorandom generators for several computational models involving groups. Our main results are as follows:1. We consider read-once group-products over a finite group $G$, i.e., tests of the form $\prod_{i=1}^n g_i^{x_i}$ where $g_i\in G$, a special case of read-once permutation branching programs. We give generators with optimal seed length $c_G \log(n/\varepsilon)$ over any $p$-group. The proof uses the small-bias plus noise paradigm, but derandomizes the noise to avoid the recursion in previous work. Our generator works when the bits are read in any order. Previously for any non-commutative group the best seed length was $\ge\log n\log(1/\varepsilon)$, even for a fixed order.2. We give a reduction that ""lifts"" suitable generators for group products over $G$ to a generator that fools width-$w$ block products, i.e., tests of the form $\prod g_i^{f_i}$ where the $f_i$ are arbitrary functions on disjoint blocks of $w$ bits. Block products generalize several previously studied classes. The reduction applies to groups that are mixing in a representation-theoretic sense that we identify.3. Combining (2) with (1) and other works we obtain new generators for block products over the quaternions or over any commutative group, with nearly optimal seed length. In particular, we obtain generators for read-once polynomials modulo any fixed $m$ with nearly optimal seed length. Previously this was known only for $m=2$.4. We give a new generator for products over ""mixing groups."" The construction departs from previous work and uses representation theory. For constant error, we obtain optimal seed length, improving on previous work (which applied to any group).This paper identifies a challenge in the area that is reminiscent of a roadblock in circuit complexity -- handling composite moduli -- and points to several classes of groups to be attacked next."
2506.03492,"Hive is an abstract strategy game played on a table with hexagonal pieces. First published in 2001, it was and continues to be highly popular among both casual and competitive players. In this paper, we show that for a suitably generalized version of the game, the computational problem of determining whether a given player in an arbitrary position has a winning strategy is PSPACE-hard. We do this by reduction from a variant of Generalized Geography we call Formula Game Geography."
2506.04127,"In this work, we consider extensions of both the Line Traveling Salesman and Line Traveling Repairman Problem, in which a single server must service a set of clients located along a line segment under the assumption that not only the server, but also the clients can move along the line and seek to collaborate with the server to speed up service times. We analyze the structure of different problem versions and identify hard and easy subproblems by building up on prior results from the literature. Specifically, we investigate problem versions with zero or general processing times, clients that are either slower or faster than the server, as well as different time window restrictions. Collectively, these results map out the complexity landscape of the Line Traveling Salesman and Repairman Problem with collaboration."
2506.04529,"Motivated by practical applications in the design of optimization compilers for neural networks, we initiated the study of identity testing problems for arithmetic circuits augmented with \emph{exponentiation gates} that compute the real function $x\mapsto e^x$. These circuits compute real functions of form $P(\vec x)/P'(\vec x)$, where both $P(\vec x)$ and $P'(\vec x)$ are exponential polynomials\[\sum_{i=1}^k f_i(\vec x)\cdot \exp\left(\frac{g_i(\vec x)}{h_i(\vec x)}\right),\]for polynomials $f_i(\vec x),g_i(\vec x)$, and $h_i(\vec x)$.We formalize a black-box query model over finite fields for this class of circuits, which is mathematical simple and reflects constraints faced by real-world neural network compilers. We proved that a simple and efficient randomized identity testing algorithm achieves perfect completeness and non-trivial soundness. Concurrent with our work, the algorithm has been implemented in the optimization compiler Mirage by Wu et al.~(OSDI 2025), demonstrating promising empirical performance in both efficiency and soundness error. Finally, we propose a number-theoretic conjecture under which our algorithm is sound with high probability."
2506.05351,"This work establishes a rigorous theoretical foundation for analyzing deep learning systems by leveraging Infinite Time Turing Machines (ITTMs), which extend classical computation into transfinite ordinal steps. Using ITTMs, we reinterpret modern architectures like Transformers, revealing fundamental limitations in scalability, efficiency, and interpretability. Building on these insights, we propose the Universal State Machine (USM), a novel computational paradigm designed from first principles. The USM employs a dynamic, queryable computation graph that evolves in real time, enabling modular, interpretable, and resource-efficient computation. This framework not only overcomes the inefficiencies and rigidity of current models but also lays the groundwork for scalable, generalizable artificial intelligence systems."
2506.05518,"Inspired by a common technique for shuffling a deck of cards on a table without riffling, we continue the study of a prequel paper on the pile shuffle and its capabilities as a sorting device. We study two sort feasibility problems of general interest concerning pile shuffle, first introduced in the prequel. These problems are characterized by: (1) bounds on the number of sequential rounds of shuffle, and piles created in each round; (2) the use of a heterogeneous mixture of queue-like and stack-like piles, as when each round of shuffle may have a combination of face-up and face-down piles; and (3) the ability of the dealer to choose the types of piles used during each round of shuffle. We prove by a sequence of reductions from the Boolean satisfiability problem (SAT) that the more general problem is NP-Hard. We leave as an open question the complexity of its arguably more natural companion, but discuss avenues for further investigation. Our analysis leverages a novel framework, introduced herein, which equates instances of shuffle to members of a particular class of deterministic finite automata."
2506.06044,"Vertex splitting consists of taking a vertex $v$ in a graph and replacing it with two non-adjacent vertices whose combined neighborhoods is the neighborhood of $v$. The split is said to be exclusive when these neighborhoods are disjoint. In the Claw-Free (Exclusive) Vertex Splitting problem, we are given a graph $G$ and an integer $k$, and we are asked if we can perform at most $k$ (exclusive) vertex splits to obtain a claw-free graph. We consider the complexity of Claw-Free Exclusive Vertex Splitting and prove it to be NP-complete in general, while admitting a polynomial-time algorithm when the input graph has maximum degree 4. This result settles an open problem posed in [Firbas \& Sorge, ISAAC 2024]. We also show that our results can be generalized to $K_{1,c}$-Free Vertex Splitting for all $c \geq 3$."
2506.06138,"Dembo-Hammer's Reduction Algorithm (DHR) is one of the classical algorithms for the 0-1 Knapsack Problem (0-1 KP) and its variants, which reduces an instance of the 0-1 KP to a sub-instance of smaller size with reduction time complexity $O(n)$. We present an extension of DHR (abbreviated as EDHR), which reduces an instance of 0-1 KP to at most $n^i$ sub-instances for any positive integer $i$. In practice, $i$ can be set as needed. In particular, if we choose $i=1$ then EDHR is exactly DHR. Finally, computational experiments on randomly generated data instances demonstrate that EDHR substantially reduces the search tree size compared to CPLEX."
2506.0659,"We initiate the study of rate-constant-independent computation of Boolean predicates and numerical functions in the continuous model of chemical reaction networks (CRNs), which model the amount of a chemical species as a nonnegative, real-valued *concentration*. Real-valued numerical functions have previously been studied, finding that exactly the continuous, piecewise rational linear functions $f: \mathbb{R}_{> 0}^k \to \mathbb{R}_{> 0}$ can be computed *stably*, a.k.a., *rate-independently*, meaning that the CRN gets the answer correct no matter the rate at which reactions occur.We show that, contrary to functions, continuous CRNs are severely limited in the Boolean predicates they can stably decide, reporting an answer based only on which inputs are 0 or positive.This limitation motivates a slightly relaxed notion of rate-independent computation in CRNs that we call *robust computation*. The standard mass-action rate model is used, in which each reaction is assigned a rate equal to the product of its reactant concentrations and its rate constant. The computation is correct in this model if it converges to the correct output for any positive choice of rate constants. This adversary is weaker than the stable computation adversary, the latter being able to run reactions at non-mass-action rates.We show that CRNs can robustly decide every finite Boolean combination of *threshold predicates*: those predicates defined by taking a rational weighted sum of the inputs $\mathbf{x} \in \mathbb{R}^k_{\ge 0}$ and comparing to a constant, answering the question ``Is $\sum_{i=1}^k w_i \cdot \mathbf{x}(i) > h$?'', for rational weights $w_i$ and real threshold $h$. Turning to function computation, we show that CRNs can robustly compute any piecewise affine function with rational coefficients, where threshold predicates determine which affine piece to evaluate for a given input."
2506.06716,"The canonical class in the realm of counting complexity is #P. It is well known that the problem of counting the models of a propositional formula in disjunctive normal form (#DNF) is complete for #P under Turing reductions. On the other hand, #DNF $\in$ spanL and spanL $\not\subseteq$ #P unless NL = NP. Hence, the class of functions logspace-reducible to #DNF is a strict subset of #P under plausible complexity-theoretic assumptions. By contrast, we show that two calls to a (restricted) #2DNF oracle suffice to capture gapP, namely, that the logspace many-one closure of the subtraction between the results of two #2DNF calls is gapP. Because #P $\not\subseteq$ gapP, #P is strictly contained between one and two #2DNF oracle calls.Surprisingly, the propositional formulas needed in both calls are linear-time computable, and the reduction preserves interesting structural as well as symmetry properties, leading to algorithmic applications. We show that a single subtraction suffices to compensate for the absence of negation while still capturing gapP, i.e., our results carry over to the monotone fragments of #2SAT and #2DNF. Since our reduction is linear-time, it preserves sparsity and, as a consequence we obtain a sparsification lemma for both #2SAT and #2DNF. This has only been known for kSAT with k $\geq$ 3 and respective counting versions. We further show that both #2DNF calls can be combined into a single call if we allow a little postprocessing (computable by AC0- or TC0-circuits). Consequently, we derive refined versions of Toda's Theorem: PH $\subseteq$ [#MON2SAT]$^{log}_{TC0}$ = [#MON2DNF]$^{log}_{TC0}$ and PH $\subseteq$ [#IMPL2SAT]$^{log}_{AC0}$. Our route to these results is via structure-aware reductions that preserve parameters like treewidth up to an additive overhead. The absence of multiplicative overhead indeed yields parameterized SETH-tight lower bounds."
2506.1121,"We develop a unified second-order parameterized complexity theory for spaces of integrable functions. This generalizes the well-established case of second-order parameterized complexity theory for spaces of continuous functions. Specifically we prove the mutual linear equivalence of three natural parameterizations of the space $\Lrm{p}$ of $p$-integrable complex functions on the real unit interval: (binary) $\Lrm{p}$-modulus, rate of convergence of Fourier series, and rate of approximation by step functions."
2506.12019,"This paper presents a deterministic algorithmic approach of exploring the solution space of the Subset Sum Problem. The algorithm presented is input-robust and structurally adaptive. Exploration is guided and narrows into areas in the solution space where solutions are possible, referred to as in-bound solution space, skipping all areas where solutions are impossible. Unfortunately, this can lead to false positives: paths that are hinted to potential have solutions but ultimately realized to not lead to solutions. The in-bound solution space navigated can therefore be filled with only false positives, only true solutions or a mix of the two, affecting the algorithm's performance in different ways. We then detail the challenges of exploring the in-bound solution space for different instances. Further, we show how this algorithm may practically generalize to other NP/NP-complete problems with appropriate adaptation. An introductory discussion is done on this generalization to k-SAT and general CNF-SAT, deferring extensive detail to a follow-up paper. This paper does not satisfy P vs NP proof requirements and does not claim to resolve the problem. However, it has implications for the P vs NP and offers a practical lens through the algorithm of what is feasible with it. The feasibility bounds of the algorithm reveal a nontrivial relationship between decision and counting complexity. To facilitate easy reproducibility, we include in the paper a full C++ implementation of the algorithm."
2506.1202,"Marginalization -- summing a function over all assignments to a subset of its inputs -- is a fundamental computational problem with applications from probabilistic inference to formal verification. Despite its computational hardness in general, there exist many classes of functions (e.g., probabilistic models) for which marginalization remains tractable, and they can be commonly expressed by polynomial size arithmetic circuits computing multilinear polynomials. This raises the question, can all functions with polynomial time marginalization algorithms be succinctly expressed by such circuits? We give a negative answer, exhibiting simple functions with tractable marginalization yet no efficient representation by known models, assuming $\textsf{FP}\neq\#\textsf{P}$ (an assumption implied by $\textsf{P} \neq \textsf{NP}$). To this end, we identify a hierarchy of complexity classes corresponding to stronger forms of marginalization, all of which are efficiently computable on the known circuit models. We conclude with a completeness result, showing that whenever there is an efficient real RAM performing virtual evidence marginalization for a function, then there are small circuits for that function's multilinear representation."
2506.12021,"Edge-Geodetic Sets play a crucial role in network monitoring and optimization, wherein the goal is to strategically place monitoring stations on vertices of a network, represented as a graph, to ensure complete coverage of edges and mitigate faults by monitoring lines of communication. This paper illustrates and explores the Monitoring Edge-Geodetic Set (MEG-set) problem, which involves determining the minimum set of vertices that need to be monitored to achieve geodetic coverage for a given network. The significance of this problem lies in its potential to facilitate efficient network monitoring, enhancing the overall reliability and performance of various applications. In this work, we prove the $\mathcal{NP}$-completeness of the MEG-set optimization problem by showing a reduction from the well-known Vertex Cover problem. Furthermore, we present inapproximability results, proving that the MEG-set optimization problem is $\mathcal{APX}$-Hard and that, if the unique games conjecture holds, the problem is not approximable within a factor of $2-\epsilon$ for any constant $\epsilon > 0$. Despite its $\mathcal{NP}$-hardness, we propose an efficient approximation algorithm achieving an approximation ratio of $O(\sqrt{|V(G)| \cdot \ln{|V(G)|})}$ for the MEG-set optimization problem, based on the well-known Set Cover approximation algorithm, where $|V(G)|$ is the number of nodes of the MEG-set instance."
2506.12022,"We prove that the sign-rank of the $k$-Hamming Distance matrix on $n$ bits is $2^{O(k)}$, independent of the number of bits $n$. This strongly refutes the conjecture of Hatami, Hatami, Pires, Tao, and Zhao (RANDOM 2022), and Hatami, Hosseini, and Meng (STOC 2023), repeated in several other papers, that the sign-rank should depend on $n$. This conjecture would have qualitatively separated margin from sign-rank (or, equivalently, bounded-error from unbounded-error randomized communication). In fact, our technique gives constant sign-rank upper bounds for all matrices which reduce to $k$-Hamming Distance, as well as large-margin matrices recently shown to be irreducible to $k$-Hamming Distance."
2506.12027,"We prove that any Turing machine running on inputs of arbitrary length can be simulated by a constant bit-size transformer, as long as the context window is sufficiently long. This improves previous works, which require scaling up either the model's precision or the number of parameters on longer inputs. Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly characterizes the expressive power of a constant bit-size transformer with a context window of length $s(n)$. Our approach relies on simulating Post machines, a Turing-complete computational model. Post machines can be modeled as automata equipped with a queue, exhibiting computational behaviors naturally aligned with those of transformers. The behavioral similarity between transformers and Post machines may offer new insights into the mechanisms underlying the reasoning abilities of transformers."
2506.12246,"We study the computational complexity of the membership problem for arithmetic circuits over natural numbers with division. We consider different subsets of the operations {intersection,union,complement,+,x,/}, where / is the element-wise integer division (without remainder and without rounding). Results for the subsets without division have been studied before, in particular by McKenzie and Wagner and Yang. The division is expressive because it makes it possible to describe the set of factors of a given number as a circuit. Surprisingly, the cases {intersection,union,complement,+,/} and {intersection,union,complement,x,/} are PSPACE-complete and therefore equivalent to the corresponding cases without division. The case {union,/} is NP-hard in contrast to the case {union} which is NL-complete. Further upper bounds, lower bounds and completeness results are given."
2506.12255,"This thesis centers around the concept of Subset Search Problems (SSP), a type of computational problem introduced by Grüne and Wulf to analyze the complexity of more intricate optimization problems. These problems are given an input set, a so-called universe, and their solution lies within their own universe, e.g. the shortest path between two point is a subset of all possible paths. Due to this, reductions upholding the SSP property require an injective embedding from the universe of the first problem into that of the second. This, however, appears inherently similar to the concept of a Parsimonious reduction, a reduction type requiring a bijective function between the solution spaces of the two problems. Parsimonious reductions are mainly used within the complexity class #P, as this class of problems concerns itself with the number of possible solutions in a given problem. These two concepts, SSP and Parsimonious reductions, are inherently similar but, crucially, not equivalent. We therefore explore the interplay between reductions upholding the SSP and Parsimonious properties, highlighting both the similarities and differences by providing a comprehensive theorem delineating the properties required for reductions to uphold both attributes. We also compile and evaluate 46 reductions between 30 subset search variants of computational problems, including those of classic NP-complete problems such as Satisfiability, Vertex Cover, Hamiltonian Cycle, the Traveling Salesman Problem and Subset Sum, providing reduction proofs, illustrative examples and insights as to where the SSP and Parsimonious properties coexist or diverge. With this compendium we contribute to the understanding of the computational complexity of bilevel and robust optimization problems, by contributing a vast collection of proven SSP- and #P-complete problems."
2506.1242,"Lifting theorems are one of the most powerful tools for proving communication lower bounds, with numerous downstream applications in proof complexity, monotone circuit lower bounds, data structures, and combinatorial optimization. However, to the best of our knowledge, prior lifting theorems have primarily focused on the two-party communication.In this paper, we propose a new lifting theorem that establishes connections between two-party communication and the Number-on-Forehead (NOF) communication model. Specifically, we present a deterministic lifting theorem that translates one-way two-party communication lower bounds into one-way NOF lower bounds.Our lifting theorem yields two applications. First, we obtain an optimal explicit separation between randomized and deterministic one-way NOF communication, even in the multi-player setting. This improves the prior square-root vs. constant separation for three players established by Kelley and Lyu (arXiv 2025). Second, we achieve optimal separations between one-round and two-round deterministic NOF communication, improving upon the previous separation of $\Omega(\frac{n^{1/(k-1)}}{k^k})$ vs. $O(\log n)$ for $k$ players, as shown by Viola and Wigderson (FOCS 2007).Beyond the lifting theorems, we also apply our techniques to the disjointness problem. In particular, we provide a new proof that the deterministic one-way three-party NOF communication complexity of set disjointness is $\Omega(n)$, further demonstrating the broader applicability of our methods."
2506.12562,"It is well-known that every quantified boolean formula (QBF) can be transformed into a prenex QBF whose only boolean operators are negation, conjunction, and disjunction. It is also well-known that the transformation is polynomial if the boolean operators of the original QBF are restricted to negation, conjunction, and disjunction. In contrast, up to now no polynomial transformation has been found when the original QBF contains other boolean operators such as biconditionals or exclusive disjunction. We define such a transformation and show that it is polynomial and preserves quantifier depth."
2506.12595,"Given a sequence of $N$ independent sources $\mathbf{X}_1,\mathbf{X}_2,\dots,\mathbf{X}_N\sim\{0,1\}^n$, how many of them must be good (i.e., contain some min-entropy) in order to extract a uniformly random string? This question was first raised by Chattopadhyay, Goodman, Goyal and Li (STOC '20), motivated by applications in cryptography, distributed computing, and the unreliable nature of real-world sources of randomness. In their paper, they showed how to construct explicit low-error extractors for just $K \geq N^{1/2}$ good sources of polylogarithmic min-entropy. In a follow-up, Chattopadhyay and Goodman improved the number of good sources required to just $K \geq N^{0.01}$ (FOCS '21). In this paper, we finally achieve $K=3$.Our key ingredient is a near-optimal explicit construction of a new pseudorandom primitive, called a leakage-resilient extractor (LRE) against number-on-forehead (NOF) protocols. Our LRE can be viewed as a significantly more robust version of Li's low-error three-source extractor (FOCS '15), and resolves an open question put forth by Kumar, Meka, and Sahai (FOCS '19) and Chattopadhyay, Goodman, Goyal, Kumar, Li, Meka, and Zuckerman (FOCS '20). Our LRE construction is based on a simple new connection we discover between multiparty communication complexity and non-malleable extractors, which shows that such extractors exhibit strong average-case lower bounds against NOF protocols."
2506.13655,"The word problem for products of symmetric groups (WPPSG) is a well-known NP-complete problem. An input instance of this problem consists of ``specification sets'' $X_1,\ldots,X_m \seq \{1,\ldots,n\}$ and a permutation $\tau$ on $\{1,\ldots,n\}$. The sets $X_1,\ldots,X_m$ specify a subset of the symmetric group $\cS_n$ and the question is whether the given permutation $\tau$ is a member of this subset. We discuss three subproblems of WPPSG and show that they can be solved efficiently. The subproblem WPPSG$_0$ is the restriction of WPPSG to specification sets all of which are sets of consecutive integers. The subproblem WPPSG$_1$ is the restriction of WPPSG to specification sets which have the Consecutive Ones Property. The subproblem WPPSG$_2$ is the restriction of WPPSG to specification sets which have what we call the Weak Consecutive Ones Property. WPPSG$_1$ is more general than WPPSG$_0$ and WPPSG$_2$ is more general than WPPSG$_1$. But the efficient algorithms that we use for solving WPPSG$_1$ and WPPSG$_2$ have, as a sub-routine, the efficient algorithm for solving WPPSG$_0$."
2506.14081,"Subgraph counting is a fundamental and well-studied problem whose computational complexity is well understood. Quite surprisingly, the hypergraph version of subgraph counting has been almost ignored. In this work, we address this gap by investigating the most basic sub-hypergraph counting problem: given a (small) hypergraph $H$ and a (large) hypergraph $G$, compute the number of sub-hypergraphs of $G$ isomorphic to $H$. Formally, for a family $\mathcal{H}$ of hypergraphs, let #Sub($\mathcal{H}$) be the restriction of the problem to $H \in \mathcal{H}$; the induced variant #IndSub($\mathcal{H}$) is defined analogously. Our main contribution is a complete classification of the complexity of these problems. Assuming the Exponential Time Hypothesis, we prove that #Sub($\mathcal{H}$) is fixed-parameter tractable if and only if $\mathcal{H}$ has bounded fractional co-independent edge-cover number, a novel graph parameter we introduce. Moreover, #IndSub($\mathcal{H}$) is fixed-parameter tractable if and only if $\mathcal{H}$ has bounded fractional edge-cover number. Both results subsume pre-existing results for graphs as special cases. We also show that the fixed-parameter tractable cases of #Sub($\mathcal{H}$) and #IndSub($\mathcal{H}$) are unlikely to be in polynomial time, unless respectively #P = P and Graph Isomorphism $\in$ P. This shows a separation with the special case of graphs, where the fixed-parameter tractable cases are known to actually be in polynomial time."
2506.14713,"Several fragments of the satisfiability problem have been studied in the literature. Among these, Linear 3-SAT is a satisfaction problem in which each clause (viewed as a set of literals) intersects with at most one other clause; moreover, any pair of clauses have at most one literal in common. Planar 3-SAT is a fragment which requires that the so-called variable-clause graph is planar. Both fragments are NP-complete and have applications in encoding NP-hard planning problems. In this paper, we investigate the complexity and applications of the fragment obtained combining both features. We define Linear Planar 3-SAT and prove its NP-completeness. We also study the reconfiguration problem of Linear Planar 3-SAT and show that it is PSPACE-complete. As an application, we use these new results to prove the NP-completeness of Bounded Connected Multi-Agent Pathfinding and the PSPACE-completeness of Connected Multi-Agent Pathfinding in two-dimensional grids."
2506.14725,"A \emph{linear extension} of a partial order \(\preceq\) over items \(A = \{ 1, 2, \ldots, n \}\) is a permutation \(\sigma\) such that for all \(i < j\) in \(A\), it holds that \(\neg(\sigma(j) \preceq \sigma(i))\). Consider the problem of generating uniformly from the set of linear extensions of a partial order. The best method currently known uses \(O(n^3 \ln(n))\) operations and \(O(n^3 \ln(n)^2)\) iid fair random bits to generate such a permutation. This paper presents a method that generates a uniform linear extension using only \(2.75 n^3 \ln(n)\) operations and \( 1.83 n^3 \ln(n) \) iid fair bits on average."
2506.16171,"We aim to find orientations of mixed graphs optimizing the total reachability, a problem that has applications in causality and biology. For given a digraph $D$, we use $P(D)$ for the set of ordered pairs of distinct vertices in $V(D)$ and we define $\kappa_D:P(D)\rightarrow \{0,1\}$ by $\kappa_D(u,v)=1$ if $v$ is reachable from $u$ in $D$, and $\kappa_D(u,v)=0$, otherwise. We use $R(D)=\sum_{(u,v)\in P(D)}\kappa_D(u,v)$.Now, given a mixed graph $G$, we aim to find an orientation $\vec{G}$ of $G$ that maximizes $R(\vec{G})$. Hakimi, Schmeichel, and Young proved that the problem can be solved in polynomial time when restricted to undirected inputs. They inquired about the complexity in mixed graphs.We answer this question by showing that this problem is NP-hard, and, moreover, APX-hard.We then develop a finer understanding of how quickly the problem becomes difficult when going from undirected to mixed graphs. To this end, we consider the parameterized complexity of the problem with respect to the number $k$ of preoriented arcs of $G$, a poorly understood form of parameterization.We show that the problem can be solved in time $n^{O(k)}$ and that a $(1-\epsilon)$-approximation can be computed in time $f(k,\epsilon)n^{O(1)}$ for any $\epsilon > 0$."
2506.16397,"In this work, we prove upper and lower bounds over fields of positive characteristics for several fragments of the Ideal Proof System (IPS), an algebraic proof system introduced by Grochow and Pitassi (J. ACM 2018). Our results extend the works of Forbes, Shpilka, Tzameret, and Wigderson (Theory of Computing 2021) and also of Govindasamy, Hakoniemi, and Tzameret (FOCS 2022). These works primarily focused on proof systems over fields of characteristic $0$, and we are able to extend these results to positive characteristic.The question of proving general IPS lower bounds over positive characteristic is motivated by the important question of proving $AC^{0}[p]$-Frege lower bounds. This connection was observed by Grochow and Pitassi (J. ACM 2018). Additional motivation comes from recent developments in algebraic complexity theory due to Forbes (CCC 2024) who showed how to extend previous lower bounds over characteristic $0$ to positive characteristic.In our work, we adapt the functional lower bound method of Forbes et al. (Theory of Computing 2021) to prove exponential-size lower bounds for various subsystems of IPS. Additionally, we derive upper bounds for the instances presented above. We show that they have efficient constant-depth IPS refutations. We also show that constant-depth IPS can efficiently refute a general class of instances, namely all symmetric instances, thereby further uncovering the strength of these algebraic proofs in positive characteristic.Notably, our lower bounds hold for fields of arbitrary characteristic but require the field size to be $n^{\omega(1)}$. In a concurrent work, Elbaz, Govindasamy, Lu, and Tzameret have shown lower bounds against restricted classes of IPS over finite fields of any size by considering different hard instances."
2506.16662,"The current paper investigates the bounded distance decoding (BDD) problem for ensembles of lattices whose generator matrices have sub-Gaussian entries. We first prove that, for these ensembles the BDD problem is NP-hard in the worst case. Then, we introduce a polynomial-time algorithm based on singular value decomposition (SVD) and establish, both theoretically and through extensive experiments, that, for a random selected lattice from the same ensemble, the algorithm solves the BDD problem with high probability. To the best of our knowledge, this work provides the first example of a lattice problem that is NP-hard in the worst case yet admits a polynomial time algorithm on the average case."
2506.16956,"Atserias and Müller (JACM, 2020) proved that for every unsatisfiable CNF formula $\varphi$, the formula $\operatorname{Ref}(\varphi)$, stating ""$\varphi$ has small Resolution refutations"", does not have subexponential-size Resolution refutations. Conversely, when $\varphi$ is satisfiable, Pudlák (TCS, 2003) showed how to construct a polynomial-size Resolution refutation of $\operatorname{Ref}(\varphi)$ given a satisfying assignment of $\varphi$. A question that remained open is: do all short Resolution refutations of $\operatorname{Ref}(\varphi)$ explicitly leak a satisfying assignment of $\varphi$?We answer this question affirmatively by giving a polynomial-time algorithm that extracts a satisfying assignment for $\varphi$ given any short Resolution refutation of $\operatorname{Ref}(\varphi)$. The algorithm follows from a new feasibly constructive proof of the Atserias-Müller lower bound, formalizable in Cook's theory $\mathsf{PV_1}$ of bounded arithmetic.Motivated by this, we introduce a computational problem concerning Resolution lower bounds: the Proof Analysis Problem (PAP). For a proof system $Q$, the Proof Analysis Problem for $Q$ asks, given a CNF formula $\varphi$ and a $Q$-proof of a Resolution lower bound for $\varphi$, encoded as $\neg \operatorname{Ref}(\varphi)$, whether $\varphi$ is satisfiable. In contrast to PAP for Resolution, we prove that PAP for Extended Frege (EF) is NP-complete.Our results yield new insights into proof complexity: (i) every proof system simulating EF is (weakly) automatable if and only if it is (weakly) automatable on formulas stating Resolution lower bounds; (ii) we provide Ref formulas exponentially hard for bounded-depth Frege systems; and (iii) for every strong enough theory of arithmetic $T$ we construct unsatisfiable CNF formulas exponentially hard for Resolution but for which $T$ cannot prove even a quadratic lower bound."
2506.17066,"The Quantum k-SAT problem is the quantum generalization of the k-SAT problem. It is the problem whether a given local Hamiltonian is frustration-free. Frustration-free means that the ground state of the k-local Hamiltonian minimizes the energy of every local interaction term simultaneously. This is a central question in quantum physics and a canonical QMA_1-complete problem. The Quantum k-SAT problem is not as well studied as the classical k-SAT problem in terms of special tractable cases, approximation algorithms and parameterized complexity. In this paper, we will give a graph-theoretic study of the Quantum k-SAT problem with the structures core and radius. These hypergraph structures are important to solve the Quantum k-SAT problem. We can solve a Quantum k-SAT instance in polynomial time if the derived hypergraph has a core of size n-m+a, where a is a constant, and the radius is at most logarithmic. If it exists, we can find a core of size n-m+a with the best possible radius in polynomial time, whereas finding a general minimum core with minimal radius is NP-hard."
2506.1721,"Lower bounds against strong algebraic proof systems and specifically fragments of the Ideal Proof System (IPS), have been obtained in an ongoing line of work. All of these bounds, however, are proved only over large (or characteristic $0$) fields, yet finite fields are the more natural setting for propositional proof complexity, especially for progress toward lower bounds for Frege systems such as $AC^0[p]$-Frege. This work establishes lower bounds against fragments of IPS over fixed finite fields. Specifically, we show that a variant of the knapsack instance studied by Govindasamy, Hakoniemi, and Tzameret (FOCS'22) has no polynomial-size IPS refutation over finite fields when the refutation is multilinear and written as a constant-depth circuit. The key ingredient of our argument is the recent set-multilinearization result of Forbes (CCC'24), which extends the earlier result of Limaye, Srinivasan, and Tavenas (FOCS'21) to all fields, and an extension of the techniques of Govindasamy, Hakoniemi, and Tzameret to finite fields. We also separate this proof system from the one studied by Govindasamy, Hakoniemi, and Tzameret.In addition, we present new lower bounds for read-once algebraic branching program refutations, roABP-IPS, in finite fields, extending results of Forbes, Shpilka, Tzameret, and Wigderson (Theor. of Comput.'21) and Hakoniemi, Limaye, and Tzameret (STOC'24).Finally, we show that any lower bound against any proof system at least as strong as (non-multilinear) constant-depth IPS over finite fields for any instance, even a purely algebraic instance (i.e., not a translation of a Boolean formula or CNF), implies a hard CNF formula for the respective IPS fragment, and hence an $AC^0[p]$-Frege lower bound by known simulations over finite fields (Grochow and Pitassi (J. ACM'18))."
2506.17485,"For a given graph $G = (V, E)$, a subset of the vertices $D\subseteq V$ is called a semitotal dominating set, if $D$ is a dominating set and every vertex $v \in D$ is within distance two to another witness $v' \in D$. We want to find a semitotal dominating set of minimum cardinality. We show that the problem is $\mathrm{W}[2]$-hard on bipartite and split graphs when parameterized by the solution size $k$. On the positive side, we extend the kernelization technique of Alber, Fellows, and Niedermeier [JACM 2004] to obtain a linear kernel of size $358k$ on planar graphs. This result complements known linear kernels already known for several variants, including Total, Connected, Red-Blue, Efficient, Edge, and Independent Dominating Set."
2506.1844,"The low-rank matrix completion problem asks whether a given real matrix with missing values can be completed so that the resulting matrix has low rank or is close to a low-rank matrix. The completed matrix is often required to satisfy additional structural constraints, such as positive semi-definiteness or a bounded infinity norm. The problem arises in various research fields, including machine learning, statistics, and theoretical computer science, and has broad real-world applications.This paper presents new $\mathsf{NP} $-hardness results for low-rank matrix completion problems. We show that for every sufficiently large integer $d$ and any real number $\varepsilon \in [ 2^{-O(d)},\frac{1}{7}]$, given a partial matrix $A$ with exposed values of magnitude at most $1$ that admits a positive semi-definite completion of rank $d$, it is $\mathsf{NP}$-hard to find a positive semi-definite matrix that agrees with each given value of $A$ up to an additive error of at most $\varepsilon$, even when the rank is allowed to exceed $d$ by a multiplicative factor of $O (\frac{1}{\varepsilon ^2 \cdot \log(1/\varepsilon)} )$. This strengthens a result of Hardt, Meka, Raghavendra, and Weitz (COLT, 2014), which applies to multiplicative factors smaller than $2$ and to $\varepsilon $ that decays polynomially in $d$. We establish similar $\mathsf{NP}$-hardness results for the case where the completed matrix is constrained to have a bounded infinity norm (rather than be positive semi-definite), for which all previous hardness results rely on complexity assumptions related to the Unique Games Conjecture. Our proofs involve a novel notion of nearly orthonormal representations of graphs, the concept of line digraphs, and bounds on the rank of perturbed identity matrices."
2506.18755,"We study the Universal Solvability of Robot Motion Planning on Graphs (USolR) problem: given an undirected graph G = (V, E) and p robots, determine whether any arbitrary configuration of the robots can be transformed into any other arbitrary configuration via a sequence of valid, collision-free moves. We design a canonical accumulation procedure that maps arbitrary configurations to configurations that occupy a fixed subset of vertices, enabling us to analyze configuration reachability in terms of equivalence classes. We prove that in instances that are not universally solvable, at least half of all configurations are unreachable from a given one, and leverage this to design an efficient randomized algorithm with one-sided error, which can be derandomized with a blow-up in the running time by a factor of p. Further, we optimize our deterministic algorithm by using the structure of the input graph G = (V, E), achieving a running time of O(p * (|V| + |E|)) in sparse graphs and O(|V| + |E|) in dense graphs. Finally, we consider the Graph Edge Augmentation for Universal Solvability (EAUS) problem, where given a connected graph G that is not universally solvable for p robots, the question is to check if for a given budget b, at most b edges can be added to G to make it universally solvable for p robots. We provide an upper bound of p - 2 on b for general graphs. On the other hand, we also provide examples of graphs that require Theta(p) edges to be added. We further study the Graph Vertex and Edge Augmentation for Universal Solvability (VEAUS) problem, where a vertices and b edges can be added, and we provide lower bounds on a and b."
2506.18921,"We propose the Transcendental Encoding Conjecture for decision problems, which asserts that every language in complexity class P encodes to an algebraic real (possibly rational or algebraic irrational) under its binary characteristic encoding or other relevant encodings, whereas every NP-complete language encodes to a transcendental real. In particular, we exhibit languages whose encodings are provably rational (hence algebraic), discuss the status of encodings for other ""natural"" languages such as PRIMES (its encoding is irrational but not known to be algebraic)."
2506.19604,"Polynomial factorization is a fundamental problem in computational algebra. Over the past half century, a variety of algorithmic techniques have been developed to tackle different variants of this problem. In parallel, algebraic complexity theory classifies polynomials into complexity classes based on their perceived `hardness'. This raises a natural question: Do these classes afford efficient factorization?In this survey, we revisit two pivotal techniques in polynomial factorization: Hensel lifting and Newton iteration. Though they are variants of the same theme, their distinct applications across the literature warrant separate treatment. These techniques have played an important role in resolving key factoring questions in algebraic complexity theory. We examine and organise the known results through the lens of these techniques to highlight their impact. We also discuss their equivalence while reflecting on how their use varies with the context of the problem.We focus on four prominent complexity classes: circuits of polynomial size ($\text{VP}_{\text{nb}}$), circuits with both polynomial size and degree (VP and its border $\overline{\text{VP}}$), verifier circuits of polynomial size and degree (VNP), and polynomial-size algebraic branching programs (VBP). We also examine more restricted models, such as formulas and bounded-depth circuits. Along the way, we list several open problems that remain unresolved."
2506.19756,"To understand and engineer biological and artificial nucleic acid systems, algorithms are employed for prediction of secondary structures at thermodynamic equilibrium. Dynamic programming algorithms are used to compute the most favoured, or Minimum Free Energy (MFE), structure, and the Partition Function (PF), a tool for assigning a probability to any structure. However, in some situations, such as when there are large numbers of strands, or pseudoknoted systems, NP-hardness results show that such algorithms are unlikely, but only for MFE. Curiously, algorithmic hardness results were not shown for PF, leaving two open questions on the complexity of PF for multiple strands and single strands with pseudoknots. The challenge is that while the MFE problem cares only about one, or a few structures, PF is a summation over the entire secondary structure space, giving theorists the vibe that computing PF should not only be as hard as MFE, but should be even harder.We answer both questions. First, we show that computing PF is #P-hard for systems with an unbounded number of strands, answering a question of Condon Hajiaghayi, and Thachuk [DNA27]. Second, for even a single strand, but allowing pseudoknots, we find that PF is #P-hard. Our proof relies on a novel magnification trick that leads to a tightly-woven set of reductions between five key thermodynamic problems: MFE, PF, their decision versions, and #SSEL that counts structures of a given energy. Our reductions show these five problems are fundamentally related for any energy model amenable to magnification. That general classification clarifies the mathematical landscape of nucleic acid energy models and yields several open questions."
2506.20221,"Motivated by the theory of proof complexity generators we consider the following $\Sigma^p_2$ search problem $\mbox{DD}_P$ determined by a propositional proof system $P$: given a $P$-proof $\pi$ of a disjunction $\bigvee_i \alpha_i$, no two $\alpha_i$ having an atom in common, find $i$ such that $\alpha_i \in \mbox{TAUT}$. We formulate a hypothesis (ST) that for some strong proof system $P$ the problem $\mbox{DD}_P$ is not solvable in the student-teacher model with a p-time student and a constant number of rounds. The hypothesis follows from the existence of hard one-way permutations. We prove, using a model-theoretic assumption, that (ST) implies $NP \neq coNP$. The assumption concerns the existence of extensions of models of a bounded arithmetic theory and it is open at present if it holds."
2506.21084,"We investigate the computational complexity of the timed prediction problem in two-dimensional sandpile models. This question refines the classical prediction problem, which asks whether a cell q will eventually become unstable after adding a grain at cell p from a given configuration. The prediction problem has been shown to be P-complete in several settings, including for subsets of the Moore neighborhood, but its complexity for the von Neumann neighborhood remains open. In a previous work, we provided a complete characterization of crossover gates (a key to the implementation of non-planar monotone circuits) for these small neighborhoods, leading to P-completeness proofs with only 4 and 5 neighbors among the eight adjancent cells. In this paper, we introduce the timed setting, where the goal is to determine whether cell q becomes unstable exactly at time t. We distinguish several cases: some neighborhoods support complete timed toolkits (including timed crossover gates) and exhibit P-completeness; others admit timed crossovers but suffer from synchronization issues; planar neighborhoods provably do not admit any timed crossover; and finally, for some remaining neighborhoods, we conjecture that no timed crossover is possible."
2506.22344,"Elementary Object Systems (EOSs) are a model in the nets-within-nets (NWNs) paradigm, where tokens in turn can host standard Petri nets. We study the complexity of the reachability problem of EOSs when subjected to non-deterministic token losses. It is known that this problem is equivalent to the coverability problem with no lossiness of conservative EOSs (cEOSs). We precisely characterize cEOS coverability into the framework of data nets, whose tokens carry data from an infinite domain. Specifically, we show that cEOS coverability is equivalent to the coverability of an interesting fragment of data nets that extends beyond $\nu$PNs (featuring globally fresh name creation), yet remains less expressive than Unordered Data Nets (featuring lossy name creation as well as powerful forms of whole-place operations and broadcasts). This insight bridges two apparently orthogonal approaches to PN extensions, namely data nets and NWNs. At the same time, it enables us to analyze cEOS coverability taking advantage of known results on data nets. As a byproduct, we immediately get that the complexity of cEOS coverability lies between $\mathbf{F}_{\omega 2}$ and $\mathbf{F}_{\omega^\omega}$, two classes beyond Primitive Recursive."
2506.23214,"We show that algebraic formulas and constant-depth circuits are closed under taking factors. In other words, we show that if a multivariate polynomial over a field of characteristic zero has a small constant-depth circuit or formula, then all its factors can be computed by small constant-depth circuits or formulas respectively.Our result turns out to be an elementary consequence of a fundamental and surprising result of Furstenberg from the 1960s, which gives a non-iterative description of the power series roots of a bivariate polynomial. Combined with standard structural ideas in algebraic complexity, we observe that this theorem yields the desired closure results.As applications, we get alternative (and perhaps simpler) proofs of various known results and strengthen the quantitative bounds in some of them. This includes a unified proof of known closure results for algebraic models (circuits, branching programs and VNP), an extension of the analysis of the Kabanets-Impagliazzo hitting set generator to formulas and constant-depth circuits, and a (significantly) simpler proof of correctness as well as stronger guarantees on the output in the subexponential time deterministic algorithm for factorization of constant-depth circuits from a recent work of Bhattacharjee, Kumar, Ramanathan, Saptharishi & Saraf."
2506.2322,"We show that the GCD of two univariate polynomials can be computed by (piece-wise) algebraic circuits of constant depth and polynomial size over any sufficiently large field, regardless of the characteristic. This extends a recent result of Andrews & Wigderson who showed such an upper bound over fields of zero or large characteristic.Our proofs are based on a recent work of Bhattacharjee, Kumar, Rai, Ramanathan, Saptharishi \& Saraf that shows closure of constant depth algebraic circuits under factorization. On our way to the proof, we show that any $n$-variate symmetric polynomial $P$ that has a small constant depth algebraic circuit can be written as the composition of a small constant depth algebraic circuit with elementary symmetric polynomials. This statement is a constant depth version of a result of Bläser & Jindal, who showed this for algebraic circuits of unbounded depth. As an application of our techniques, we also strengthen the closure results for factors of constant-depth circuits in the work of Bhattacharjee et al. over fields for small characteristic."
2506.23404,"In this paper, we provide a uniform framework for investigating small circuit classes and bounds through the lens of ordinary differential equations (ODEs). Following an approach recently introduced to capture the class of polynomial-time computable functions via ODE-based recursion schemas and later applied to the context of functions computed by unbounded fan-in circuits of constant depth (FAC^0), we study multiple relevant small circuit classes. In particular, we show that natural restrictions on linearity and derivation along functions with specific growth rate correspond to kinds of functions that can be proved to be in various classes, ranging from FAC^0 to FAC^1. This reveals an intriguing link between constraints over linear-length ODEs and circuit computation, providing new tools to tackle the complex challenge of establishing bounds for classes in the circuit hierarchies and possibly enhancing our understanding of the role of counters in this setting. Additionally, we establish several completeness results, in particular obtaining the first ODE-based characterizations for the classes of functions computable in constant depth with unbounded fan-in and Mod 2 gates (FACC[2]) and in logarithmic depth with bounded fan-in Boolean gates (FNC1)."
2507.00148,"In this paper, we study the query complexity of Boolean functions in the presence of uncertainty, motivated by parallel computation with an unlimited number of processors where inputs are allowed to be unknown. We allow each query to produce three results: zero, one, or unknown. The output could also be: zero, one, or unknown, with the constraint that we should output ''unknown'' only when we cannot determine the answer from the revealed input bits. Such an extension of a Boolean function is called its hazard-free extension.- We prove an analogue of Huang's celebrated sensitivity theorem [Annals of Mathematics, 2019] in our model of query complexity with uncertainty.- We show that the deterministic query complexity of the hazard-free extension of a Boolean function is at most quadratic in its randomized query complexity and quartic in its quantum query complexity, improving upon the best-known bounds in the Boolean world.- We exhibit an exponential gap between the smallest depth (size) of decision trees computing a Boolean function, and those computing its hazard-free extension.- We present general methods to convert decision trees for Boolean functions to those for their hazard-free counterparts, and show optimality of this construction. We also parameterize this result by the maximum number of unknown values in the input.- We show lower bounds on size complexity of decision trees for hazard-free extensions of Boolean functions in terms of the number of prime implicants and prime implicates of the underlying Boolean function."
2507.00612,"We prove that Hamiltonian Path and Hamiltonian Cycle are NP-hard on graphs of linear mim-width 26, even when a linear order of the input graph with mim-width 26 is provided together with input. This fills a gap left by a broken proof of the para-NP-hardness of Hamiltonicity problems parameterized by mim-width."
2507.01192,"The Reconfiguration Inapproximability Hypothesis (RIH), recently established by Hirahara-Ohsaka (STOC'24) and Karthik-Manurangsi (ECCC'24), studies the hardness of reconfiguring one solution into another in constraint satisfaction problems (CSP) when restricted to approximate intermediate solutions. In this work, we make a tighter connection between RIH's soundness gap and that of probabilistically checkable proofs of proximity (PCPP). Consequently, we achieve an improved trade-off between soundness and query complexity in Gap CSP Reconfiguration. Our approach leverages a parallelization framework, which also appears in some recent parameterized inapproximability results."
2507.01657,"Membrane systems represent a computational model that operates in a distributed and parallel manner, inspired by the behavior of biological cells. These systems feature objects that transform within a nested membrane structure. This research concentrates on a specific type of these systems, based on cellular symport/antiport communication of chemicals.Results in the literature show that systems of this type that also allow cell division can solve PSPACE problems. In our study, we investigate systems that use membrane separation instead of cell division, for which only limited results are available. Notably, it has been shown that any problem solvable by such systems in polynomial time falls within the complexity class P^(#P).By implementing a system solving MIDSAT, a P^(#P)-complete problem, we demonstrate that the reverse inclusion is true as well, thus providing an exact characterization of the problem class solvable by P systems with symport/antiport and membrane separation.Moreover, our implementation uses rules of length at most three. With this limit, systems were known to be able to solve NP-complete problems, whereas limiting the rules by length two, they characterize P."
2507.03151,"The problem of learning or reconstructing an unknown graph from a known family via partial-information queries arises as a mathematical model in various contexts. The most basic type of access to the graph is via \emph{edge queries}, where an algorithm may query the presence/absence of an edge between a pair of vertices of its choosing, at unit cost.While more powerful query models have been extensively studied in the context of graph reconstruction, the basic model of edge queries seems to have not attracted as much attention. In this paper we study the edge query complexity of learning a hidden bipartite graph, or equivalently its bipartite adjacency matrix, in the classical as well as quantum settings. We focus on learning matchings and half graphs, which are graphs whose bipartite adjacency matrices are a row/column permutation of the identity matrix and the lower triangular matrix with all entries on and below the principal diagonal being 1, respectively.\begin{itemize}\item For matchings of size $n$, we show a tight deterministic bound of $n(n-1)/2$ and an asymptotically tight randomized bound of $\Theta(n^2)$. A quantum bound of $\Theta(n^{1.5})$ was shown in a recent work of van Apeldoorn et al.~[ICALP'21].\item For half graphs whose bipartite adjacency matrix is a column-permutation of the $n \times n$ lower triangular matrix,we give tight $\Theta(n \log n)$ bounds in both deterministic and randomized settings, and an $\Omega(n)$ quantum lower bound. \item For general half graphs,we observe that the problem is equivalent to a natural generalization of the famous nuts-and-bolts problem, leading to a tight $\Theta(n \log n)$ randomized bound.We also present a simple quicksort-style method that instantiates to a $O(n \log^2 n)$ randomized algorithm and a tight $O(n \log n)$ quantum algorithm.\end{itemize}"
2507.03193,"The celebrated Ore-DeMillo-Lipton-Schwartz-Zippel (ODLSZ) lemma asserts that n-variate non-zero polynomial functions of degree d over a field $\mathbb{F}$ are non-zero over any ""grid"" $S^n$ for finite subset $S \subseteq \mathbb{F}$, with probability at least $\max\{|S|^{-d/(|S|-1)},1-d/|S|\}$ over the choice of random point from the grid. In particular, over the Boolean cube ($S = \{0,1\} \subseteq \mathbb{F}$), the lemma asserts non-zero polynomials are non-zero with probability at least $2^{-d}$. In this work we extend the ODLSZ lemma optimally (up to lower-order terms) to ""Boolean slices"" i.e., points of Hamming weight exactly $k$. We show that non-zero polynomials on the slice are non-zero with probability $(t/n)^{d}(1 - o_{n}(1))$ where $t = \min\{k,n-k\}$ for every $d\leq k\leq (n-d)$. As with the ODLSZ lemma, our results extend to polynomials over Abelian groups. This bound is tight (upto the error term) as evidenced by degree d multilinear monomials. A particularly interesting case is the ""balanced slice"" ($k=n/2$) where our lemma asserts that non-zero polynomials are non-zero with roughly the same probability on the slice as on the whole cube.The behaviour of low-degree polynomials over Boolean slices has received much attention in recent years. However, the problem of proving a tight version of the ODLSZ lemma does not seem to have been considered before, except for a recent work of Amireddy, Behera, Paraashar, Srinivasan and Sudan (SODA 2025) who established a sub-optimal bound of approximately $((k/n)\cdot(1-(k/n)))^d$ using a proof similar to that of the standard ODLSZ lemma.While the statement of our result mimics that of the ODLSZ lemma, our proof is significantly more intricate and involves spectral reasoning which is employed to show that a natural way of embedding a copy of the Boolean cube inside a balanced Boolean slice is a good sampler."
2507.03593,"We study the basic regular expression intersection testing problem, which asks to determine whether the intersection of the languages of two regular expressions is nonempty. A textbook solution to this problem is to construct the nondeterministic finite automaton that accepts the language of both expressions. This procedure results in a $\Theta(mn)$ running time, where $m$ and $n$ are the sizes of the two expressions, respectively. Following the approach of Backurs and Indyk [FOCS'16] and Bringmann, Grønlund, and Larsen [FOCS'17] on regular expression matching and membership testing, we study the complexity of intersection testing for homogeneous regular expressions of bounded depth involving concatenation, OR, Kleene star, and Kleene plus. Specifically, we consider all combinations of types of depth-2 regular expressions and classify the time complexity of intersection testing as either linear or quadratic, assuming SETH. The most interesting result is a quadratic conditional lower bound for testing the intersection of a ''concatenation of +s'' expression with a ''concatenation of ORs'' expression: this is the only hard case that does not involve the Kleene star operator and is not implied by existing lower bounds for the simpler membership testing problem."
2507.03692,"In 2004, Aaronson introduced the complexity class $\mathsf{PostBQP}$ ($\mathsf{BQP}$ with postselection) and showed that it is equal to $\mathsf{PP}$. In this paper, we define a new complexity class, $\mathsf{CorrBQP}$, a modification of $\mathsf{BQP}$ which has the power to perform correlated measurements, i.e. measurements that output the same value across a partition of registers. We show that $\mathsf{CorrBQP}$ is exactly equal to $\mathsf{BPP}^{\mathsf{PP}}$, placing it ""just above"" $\mathsf{PP}$. In fact, we show that other metaphysical modifications of $\mathsf{BQP}$, such as $\mathsf{CBQP}$ (i.e. $\mathsf{BQP}$ with the ability to clone arbitrary quantum states), are also equal to $\mathsf{BPP}^{\mathsf{PP}}$. Furthermore, we show that $\mathsf{CorrBQP}$ is self-low with respect to classical queries. In contrast, if it were self-low under quantum queries, the counting hierarchy ($\mathsf{CH}$) would collapse to $\mathsf{BPP}^{\mathsf{PP}}$. Finally, we introduce a variant of rational degree that lower-bounds the query complexity of $\mathsf{BPP}^{\mathsf{PP}}$."
2507.0411,"A language L is low for a relativizable complexity class C, if it doesn't increase the computational power of a class when it is used as an oracle: C$^L$ = C. For the classes #P, GapP, and SpanP low classes of languages are known: Low(#P) = UP $\cap$ coUP, Low(GapP) = SPP, and Low(SpanP) = NP $\cap$ coNP. In this paper, we prove that Low(TotP) = P, and give characterizations of low function classes for #P, GapP, TotP, and SpanP. We establish the relations between NPSVt, UPSVt, and the counting function classes. For each of the inclusions between these classes we give an equivalent inclusion between language classes. We also prove that SpanP $\subseteq$ GapP if and only if NP $\subseteq$ SPP, and the inclusion GapP+ $\subseteq$ SpanP implies PH = $\Sigma_{2}^{P}$. For the classes #P, GapP, TotP, and SpanP we summarize the known results and prove that each of these classes is closed under left composition with FP+ if and only if it collapses to its low class of functions."
2507.05475,"We prove that in every direction in the Euclidean plane, there exists a line containing no double exponential time random (ee-random) points. This means each point on these lines has an algorithmically predictable location, to the extent that a gambler in an environment with fair payouts can, using double exponential time computing resources, amass unbounded capital placing bets on increasingly precise estimates of the point's location. Our proof relies on effectivizing the construction of the lineal extension of a Kakeya set. This resolves an open question of Lutz and Lutz (2015)."
2507.0576,"Recently, Bumpus and Meeks introduced a purely temporal parameter, called vertex-interval-membership-width, which is promising for the design of fixed-parameter tractable (FPT) algorithms for vertex reachability problems in temporal graphs. We study this newly introduced parameter for the problem of restless temporal paths, in which the waiting time at each node is restricted. In this article, we prove that, in the interval model, where arcs are present for entire time intervals, finding a restless temporal path is NP-hard even if the vertex-interval-membership-width is equal to three. We exhibit FPT algorithms for the point model, where arcs are present at specific points in time, both with uniform delay one and arbitrary positive delays. In the latter case, this comes with a slight additional computational cost."
2507.05809,"The so-called blockchain trilemma asserts the impossibility of simultaneously achieving scalability, security, and decentralisation within a single blockchain protocol. In this paper, we formally refute that proposition. Employing predicate logic, formal automata theory, computational complexity analysis, and graph-theoretic measures of relay topology--specifically Baran's model of network path redundancy--we demonstrate that the trilemma constitutes a category error, conflates distinct analytical domains, and relies upon unproven causal assumptions. We further expose its reliance on composition fallacies drawn from flawed system implementations. A constructive counterexample is presented: a blockchain protocol exhibiting unbounded transaction throughput, cryptographic security under adversarial load, and multipath decentralised propagation. This example is not hypothetical but grounded in protocol design enabled by compact block relay, SPV verification, and IPv6 multicast. The trilemma is revealed not as a law of protocol architecture, but as a heuristic fallacy sustained by imprecision and design defeatism."
2507.0586,"We study the complexity of graph problems on graphs defined on groups, especially power graphs. We observe that an isomorphism invariant problem, such as Hamiltonian Path, Partition into Cliques, Feedback Vertex Set, Subgraph Isomorphism, cannot be NP-complete for power graphs, commuting graphs, enhanced power graphs, directed power graphs, and bounded-degree Cayley graphs, assuming the Exponential Time Hypothesis (ETH). An analogous result holds for isomorphism invariant group problems: no such problem can be NP-complete unless ETH is false. We show that the Weighted Max-Cut problem is NP-complete in power graphs. We also show that, unless ETH is false, the Graph Motif problem cannot be solved in quasipolynomial time on power graphs, even for power graphs of cyclic groups. We study the recognition problem of power graphs when the adjacency matrix or list is given as input and show that for abelian groups and some classes of nilpotent groups, it is solvable in polynomial time."
2507.05951,We prove that persuasion is an NP-complete problem.
2507.05972,"Pseudoentropy characterizations provide a quantitatively precise demonstration of the close relationship between computational hardness and computational randomness. We prove a unified pseudoentropy characterization that generalizes and strengthens previous results for both uniform and non-uniform models of computation. Our characterization holds for a general family of entropy notions that encompasses the common notions of Shannon entropy and min entropy as special cases. Moreover, we show that the characterizations for different entropy notions can be simultaneously achieved by a single, universal function that simultaneously witnesses computational hardness and computational randomness. A key technical insight of our work is that the notion of weight-restricted calibration from the recent literature on algorithm fairness, along with standard computational indistinguishability (known as multiaccuracy in the fairness literature), suffices for proving pseudoentropy characterizations for general entropy notions. This demonstrates the power of weight-restricted calibration to enhance the classic Complexity-Theoretic Regularity Lemma (Trevisan, Tulsiani, and Vadhan, 2009) and Leakage Simulation Lemma (Jetchev and Pietrzak, 2014) and allows us to achieve an exponential improvement in the complexity dependency on the alphabet size compared to the pseudoentropy characterizations by Casacuberta, Dwork, and Vadhan (2024) based on the much stronger notion of multicalibration. We show that the exponential dependency on the alphabet size is inevitable for multicalibration as well as for the weaker notion of calibrated multiaccuracy."
2507.07283,"Nonogram is a popular combinatorial puzzle (similar in nature to Sudoku or Minesweeper) in which a puzzle solver must determine if there exists a setting of the puzzle parameters that satisfy a given set of constraints. It has long been known that the problem of deciding if a solution exists is a computationally difficult problem. Despite this fact, humans still seem to enjoy playing it. This work aims to reconcile these seemingly contradictory facts by (1) analyzing the complexity of the inference problem for Nonogram (the problem of determining if there exists a puzzle parameter that can be inferred from the constraints without guessing) and (2) experimentally establishing the existence of a phase transition behavior for this inference problem. Our results show that the difficulty of the inference problem is largely determined by the density of filled cells (positive parameters) in a given puzzle. Along the way we implement an efficient encoding of a Nonogram board as a Boolean formula in Conjunctive Normal Form (CNF) through the use of regular expressions in order to make our experiments feasible."
2507.07654,"Let $f$ and $g$ be Boolean functions over a finite Abelian group $\mathcal{G}$, where $g$ is fully known, and we have {\em query access} to $f$, that is, given any $x \in \mathcal{G}$ we can get the value $f(x)$. We study the tolerant isomorphism testing problem: given $\epsilon \geq 0$ and $\tau > 0$, we seek to determine, with minimal queries, whether there exists an automorphism $\sigma$ of $\mathcal{G}$ such that the fractional Hamming distance between $f \circ \sigma$ and $g$ is at most $\epsilon$, or whether for all automorphisms $\sigma$, the distance is at least $\epsilon + \tau$.We design an efficient tolerant testing algorithm for this problem, with query complexity $\mathrm{poly}\left( s, 1/\tau \right)$, where $s$ bounds the spectral norm of $g$. Additionally, we present an improved algorithm when $g$ is Fourier sparse.Our approach uses key concepts from Abelian group theory and Fourier analysis, including the annihilator of a subgroup, Pontryagin duality, and a pseudo inner-product for finite Abelian groups. We believe these techniques will find further applications in property testing."
2507.08919,I give a simple proof of a tight communication lower bound for pointer chasing.
2507.09018,"In this paper, we critically examine Deng's ""P=NP"" [Den24]. The paper claims that there is a polynomial-time algorithm that decides 3-coloring for graphs with vertices of degree at most 4, which is known to be an NP-complete problem. Deng presents a semidefinite program with an objective function that is unboundedly negative if the graph is not 3-colorable, and a minimum of 0 if the graph is 3-colorable. Through detailed analysis, we find that Deng conflates subgraphs with induced subgraphs, leading to a critical error which thereby invalidates Deng's proof that $\text{P}=\text{NP}$."
2507.09515,"We give new lower bounds for the fragments of the Ideal Proof System (IPS) introduced by Grochow and Pitassi (JACM 2018). The Ideal Proof System is a central topic in algebraic proof complexity developed in the context of Nullstellensatz refutation (Beame, Impagliazzo, Krajicek, Pitassi, Pudlak, FOCS 1994) and simulates Extended Frege efficiently. Our main results are as follows.1. mult-IPS_{Lin'}: We prove nearly quadratic-size formula lower bound for multilinear refutation (over the Boolean hypercube) of a variant of the subset-sum axiom polynomial. Extending this, we obtain a nearly matching qualitative statement for a constant degree target polynomial.2. IPS_{Lin'}: Over the fields of characteristic zero, we prove exponential-size sum-of-ROABPs lower bound for the refutation of a variant of the subset-sum axiom polynomial. The result also extends over the fields of positive characteristics when the target polynomial is suitably modified. The modification is inspired by the recent results (Hakoniemi, Limaye, Tzameret, STOC 2024 and Behera, Limaye, Ramanathan, Srinivasan, ICALP 2025).The mult-IPS_{Lin'} lower bound result is obtained by combining the quadratic-size formula lower bound technique of Kalorkoti (SICOMP 1985) with some additional ideas. The proof technique of IPS_{Lin'} lower bound result is inspired by the recent lower bound result of Chatterjee, Kush, Saraf and Shpilka (CCC 2024)."
2507.09868,"In the Vertex Disjoint Paths with Congestion problem, the input consists of a digraph $D$, an integer $c$ and $k$ pairs of vertices $(s_i, t_i)$, and the task is to find a set of paths connecting each $s_i$ to its corresponding $t_i$, whereas each vertex of $D$ appears in at most $c$ many paths. The case where $c = 1$ is known to be NP-complete even if $k = 2$ [Fortune, Hopcroft and Wyllie, 1980] on general digraphs and is W[1]-hard with respect to $k$ (excluding the possibility of an $f(k)n^{O(1)}$-time algorithm under standard assumptions) on acyclic digraphs [Slivkins, 2010]. The proof of [Slivkins, 2010] can also be adapted to show W[1]-hardness with respect to $k$ for every congestion $c \geq 1$.We strengthen the existing hardness result by showing that the problem remains W[1]-hard for every congestion $c \geq 1$ even if:- the input digraph $D$ is acyclic,- $D$ does not contain an acyclic $(5, 5)$-grid as a butterfly minor,- $D$ does not contain an acyclic tournament on 9 vertices as a butterfly minor, and- $D$ has ear-anonymity at most 5.Further, we also show that the edge-congestion variant of the problem remains W[1]-hard for every congestion $c \geq 1$ even if:- the input digraph $D$ is acyclic,- $D$ has maximum undirected degree 3,- $D$ does not contain an acyclic $(7, 7)$-wall as a weak immersion and- $D$ has ear-anonymity at most 5."
2507.10426,"In the paper where he first defined Communication Complexity, Yao asks: \emph{Is computing $CC(f)$ (the 2-way communication complexity of a given function $f$) NP-complete?} The problem of deciding whether $CC(f) \le k$, when given the communication matrix for $f$ and a number $k$, is easily seen to be in NP. Kushilevitz and Weinreb have shown that this problem is cryptographically hard. Here we show it is NP-hard."
2507.10731,"We consider random walks on ``balanced multislices'' of any ``grid'' that respects the ``symmetries'' of the grid, and show that a broad class of such walks are good spectral expanders. (A grid is a set of points of the form $\mathcal{S}^n$ for finite $\mathcal{S}$, and a balanced multi-slice is the subset that contains an equal number of coordinates taking every value in $\mathcal{S}$. A walk respects symmetries if the probability of going from $u = (u_1,\ldots,u_n)$ to $v = (v_1,\ldots,v_n)$ is invariant under simultaneous permutations of the coordinates of $u$ and $v$.) Our main theorem shows that, under some technical conditions, every such walk where a single step leads to an almost $\mathcal{O}(1)$-wise independent distribution on the next state, conditioned on the previous state, satisfies a non-trivially small singular value bound.We give two applications of our theorem to error-correcting codes: (1) We give an analog of the Ore-DeMillo-Lipton-Schwartz-Zippel lemma for polynomials, and junta-sums, over balanced multislices. (2) We also give a local list-correction algorithm for $d$-junta-sums mapping an arbitrary grid $\mathcal{S}^n$ to an Abelian group, correcting from a near-optimal $(\frac{1}{|\mathcal{S}|^{d}} - \varepsilon)$ fraction of errors for every $\varepsilon > 0$, where a $d$-junta-sum is a sum of (arbitrarily many) $d$-juntas (and a $d$-junta is a function that depends on only $d$ of the $n$ variables).Our proofs are obtained by exploring the representation theory of the symmetric group and merging it with some careful spectral analysis."
2507.11162,"We exhibit an $n$-bit communication problem with a constant-cost randomized protocol but which requires $n^{\Omega(1)}$ deterministic (or even non-deterministic) queries to an Equality oracle. Therefore, even constant-cost randomized protocols cannot be efficiently ""derandomized"" using Equality oracles. This improves on several recent results and answers a question from the survey of Hatami and Hatami (SIGACT News 2024). It also gives a significantly simpler and quantitatively superior proof of the main result of Fang, Göös, Harms, and Hatami ( STOC 2025), that constant-cost communication does not reduce to the $k$-Hamming Distance hierarchy."
2507.11234,"The Skolem Problem asks to determine whether a given linear recurrence sequence (LRS) $\langle u_n \rangle_{n=0}^\infty$ over the integers has a zero term, that is, whether there exists $n$ such that $u_n = 0$. Decidability of the problem is open in general, with the most notable positive result being a decision procedure for LRS of order at most 4.In this paper we consider a bounded version of the Skolem Problem, in which the input consists of an LRS $\langle u_n \rangle_{n=0}^\infty$ and a bound $N \in \mathbb N$ (with all integers written in binary), and the task is to determine whether there exists $n\in\{0,\ldots,N\}$ such that $u_n=0$. We give a randomised algorithm for this problem that, for all $d\in \mathbb N$, runs in polynomial time on the class of LRS of order at most $d$. As a corollary we show that the (unrestricted) Skolem Problem for LRS of order at most 4 lies in $\mathsf{coRP}$, improving the best previous upper bound of $\mathsf{NP}^{\mathsf{RP}}$.The running time of our algorithm is exponential in the order of the LRS -- a dependence that appears necessary in view of the $\mathsf{NP}$-hardness of the Bounded Skolem Problem. However, even for LRS of a fixed order, the problem involves detecting zeros within an exponentially large range. For this, our algorithm relies on results from $p$-adic analysis to isolate polynomially many candidate zeros and then test in randomised polynomial time whether each candidate is an actual zero by reduction to arithmetic-circuit identity testing."
2507.12124,We show that for a randomly sampled unsatisfiable $O(\log n)$-CNF over $n$ variables the randomized two-party communication cost of finding a clause falsified by the given variable assignment is linear in $n$.
2507.12244,"For a fixed graph H, the function #IndSub(H,*) maps graphs G to the count of induced H-copies in G; this function obviously ""counts something"" in that it has a combinatorial interpretation. Linear combinations of such functions are called graph motif parameters and have recently received significant attention in counting complexity after a seminal paper by Curticapean, Dell and Marx (STOC'17). We show that, among linear combinations of functions #IndSub(H,*) involving only graphs H without isolated vertices, precisely those with positive integer coefficients maintain a combinatorial interpretation. It is important to note that graph motif parameters can be nonnegative for all inputs G, even when some coefficients are negative.Formally, we show that evaluating any graph motif parameter with a negative coefficient is impossible in an oracle variant of #P, where an implicit graph is accessed by oracle queries. Our proof follows the classification of the relativizing closure properties of #P by Hertrampf, Vollmer, and Wagner (SCT'95) and the framework developed by Ikenmeyer and Pak (STOC'22), but our application of the required Ramsey theorem turns out to be more subtle, as graphs do not have the required Ramsey property.Our techniques generalize from graphs to relational structures, including colored graphs. Vastly generalizing this, we introduce motif parameters over categories that count occurrences of sub-objects in the category. We then prove a general dichotomy theorem that characterizes which such parameters have a combinatorial interpretation. Using known results in Ramsey theory for categories, we obtain a dichotomy for motif parameters of finite vector spaces as well as parameter sets."
2507.12469,"This paper explores the computational complexity of diffusion-based language modeling. We prove a dichotomy based on the quality of the score-matching network in a diffusion model. In one direction, a network that exactly computes the score function of some initial distribution can only perform language modeling within the $\mathsf{TC}^0$ complexity class, reflecting limitations tied to rapid convergence. In the other direction, we show that if there is no requirement for the network to match any score function, then diffusion modeling can simulate any Turing machine in a certain sense. This dichotomy provides a theoretical lens on the capabilities and limitations of diffusion models, particularly concerning tasks requiring sequential computation. We conjecture extensions of our theoretical results, including for the case where the diffusion model is not perfect, but merely good. We also discuss the wider context and practical implications, and hypothesize that a machine learning architecture that can interpolate between sequential and parallel modes of operation would be superior to both Transformers and diffusion models."
2507.13222,"A central question in computer science and statistics is whether efficient algorithms can achieve the information-theoretic limits of statistical problems. Many computational-statistical tradeoffs have been shown under average-case assumptions, but since statistical problems are average-case in nature, it has been a challenge to base them on standard worst-case assumptions.In PAC learning where such tradeoffs were first studied, the question is whether computational efficiency can come at the cost of using more samples than information-theoretically necessary. We base such tradeoffs on $\mathsf{NP}$-hardness and obtain:$\circ$ Sharp computational-statistical tradeoffs assuming $\mathsf{NP}$ requires exponential time: For every polynomial $p(n)$, there is an $n$-variate class $C$ with VC dimension $1$ such that the sample complexity of time-efficiently learning $C$ is $\Theta(p(n))$.$\circ$ A characterization of $\mathsf{RP}$ vs. $\mathsf{NP}$ in terms of learning: $\mathsf{RP} = \mathsf{NP}$ iff every $\mathsf{NP}$-enumerable class is learnable with $O(\mathrm{VCdim}(C))$ samples in polynomial time. The forward implication has been known since (Pitt and Valiant, 1988); we prove the reverse implication.Notably, all our lower bounds hold against improper learners. These are the first $\mathsf{NP}$-hardness results for improperly learning a subclass of polynomial-size circuits, circumventing formal barriers of Applebaum, Barak, and Xiao (2008)."
2507.13576,"This paper proposes a characterization of when one axiomatic theory, as a proof system for tautologies, $p$-simulates another, by showing: (i)~if c.e. theory $\mathcal{S}$ efficiently interprets $\mathcal{S}{+}\phi$, then $\mathcal{S}$ $p$-simulates $\mathcal{S}{+}\phi$ (Jeřábek in Pudlák17 proved simulation), since the interpretation maps an $\mathcal{S}{+}\phi$-proof whose lines are all theorems into an $\mathcal{S}$-proof; (ii)~$\mathcal{S}$ proves ``$\mathcal{S}$ efficiently interprets $\mathcal{S}{+}\phi$'' iff $\mathcal{S}$ proves ``$\mathcal{S}$ $p$-simulates $\mathcal{S}{+}\phi$'' (if so, $\mathcal{S}$ already proves the $\Pi_1$ theorems of $\mathcal{S}{+}\phi$). To explore whether this framework conceivably resolves other open questions, the paper formulates conjectures stronger than ``no optimal proof system exists'' that imply Feige's Hypothesis, the existence of one-way functions, and circuit lower bounds."
2507.13818,"Treedepth is a central parameter to algorithmic graph theory. The current state-of-the-art in computing and approximating treedepth consists of a $2^{O(k^2)} n$-time exact algorithm and a polynomial-time $O(\text{OPT} \log^{3/2} \text{OPT})$-approximation algorithm, where the former algorithm returns an elimination forest of height $k$ (witnessing that treedepth is at most $k$) for the $n$-vertex input graph $G$, or correctly reports that $G$ has treedepth larger than $k$, and $\text{OPT}$ is the actual value of the treedepth. On the complexity side, exactly computing treedepth is NP-complete, but the known reductions do not rule out a polynomial-time approximation scheme (PTAS), and under the Exponential Time Hypothesis (ETH) only exclude a running time of $2^{o(\sqrt n)}$ for exact algorithms.We show that 1.0003-approximating treedepth is NP-hard, and that exactly computing the treedepth of an $n$-vertex graph requires time $2^{\Omega(n)}$, unless the ETH fails. We further derive that there exist absolute constants $\delta, c > 0$ such that any $(1+\delta)$-approximation algorithm requires time $2^{\Omega(n / \log^c n)}$. We do so via a simple direct reduction from Satisfiability to Treedepth, inspired by a reduction recently designed for Treewidth [STOC '25]."
2507.13963,"A seminal result of Nisan and Szegedy (STOC, 1992) shows that for any total Boolean function, the degree of the real polynomial that computes the function, and the minimal degree of a real polynomial that point-wise approximates the function, are at most polynomially separated. Extending this result from degree to other complexity measures like sparsity of the polynomial representation, or total weight of the coefficients, remains poorly understood.In this work, we consider this problem in the De Morgan basis, and prove an analogous result for the sparsity of the polynomials at a logarithmic scale. Our result further implies that the exact $\ell_1$ norm and its approximate variant are also similarly related to each other at a log scale. This is in contrast to the Fourier basis, where the analog of our results are known to be false.Our proof is based on a novel random restriction method. Unlike most existing random restriction methods used in complexity theory, our random restriction process is adaptive and is based on how various complexity measures simplify during the restriction process."
2507.14445,"One approach to study the pseudorandomness properties of walks on expander graphs is to label the vertices of an expander with elements from an alphabet $\Sigma$, and study the mean of functions over $\Sigma^n$. We say expander walks $\varepsilon$-fool a function if, for any unbiased labeling of the vertices, the expander walk mean is $\varepsilon$-close to the true mean. We show that:- The class of symmetric functions is $O(|\Sigma|\cdot\lambda)$-fooled by expander walks over any generic $\lambda$-expander, and any alphabet $\Sigma$ . This generalizes the result of Cohen, Peri, Ta-Shma [STOC'21] which analyzes it for $|\Sigma| =2$, and exponentially improves the previous bound of $O(|\Sigma|^{O(|\Sigma|)}\cdot \lambda)$, by Golowich and Vadhan [CCC'22]. Additionally, if the expander is a Cayley graph over $\mathbb{Z}_{|\Sigma|}$, we get a further improved bound of $O(\sqrt{|\Sigma|}\cdot\lambda)$.Morever, when $\Sigma$ is a finite group $G$, we show the following for functions over $G^n$:- The class of symmetric class functions is $O\Big({\frac{\sqrt{|G|}}{D}\cdot\lambda}\Big)$-fooled by expander walks over ""structured"" $\lambda$-expanders, if $G$ is $D$-quasirandom.- We show a lower bound of $\Omega(\lambda)$ for symmetric functions for any finite group $G$ (even for ""structured"" $\lambda$-expanders).- We study the Fourier spectrum of a class of non-symmetric functions arising from word maps, and show that they are exponentially fooled by expander walks.Our proof employs Fourier analysis over general groups, which contrasts with earlier works that have studied either the case of $\mathbb{Z}_2$ or $\mathbb{Z}$. This enables us to get quantitatively better bounds even for unstructured sets."
2507.15511,"The Subset Sum problem is a classical NP-complete problem with a long-standing $O^*(2^{n/2})$ deterministic bound due to Horowitz and Sahni. We present results at two distinct levels of generality.First (instance-sensitive bound), we introduce, to our knowledge, the first deterministic algorithm whose runtime provably scales with the certificate size $U = |\Sigma(S)|$, the number of distinct subset sums. Our enumerator constructs all such sums in time $O(U \cdot n^2)$, with a randomized variant achieving expected time $O(U \cdot n)$. This provides a constructive link to Instance Complexity by tying runtime to the size of an information-theoretically minimal certificate.Second (unconditional worst-case bound), by combining this enumerator with a double meet-in-the-middle strategy and a Controlled Aliasing technique that enforces a simple canonical-normal-form (CNF) expansion policy on aliased states, we obtain a deterministic solver running in $O^*(2^{n/2-\varepsilon})$ time with $\varepsilon=\log_2(\frac{4}{3})$ - the first unconditional deterministic improvement over the classical $O^*(2^{n/2})$ bound for all sufficiently large $n$.Finally, we refine fine-grained hardness for Subset Sum by making explicit the structural regime (high collision entropy / near collision-free) implicitly assumed by SETH-based reductions, i.e., instances with near-maximal $U$."
2507.16105,We show that the perfect matching function on $n$-vertex graphs requires monotone circuits of size $\smash{2^{n^{\Omega(1)}}}$. This improves on the $n^{\Omega(\log n)}$ lower bound of Razborov (1985). Our proof uses the standard approximation method together with a new sunflower lemma for matchings.
2507.18375,"In recent years, quantitative complexity over semirings has been intensively investigated. An important problem in this context is to connect computational complexity with logical expressiveness. In this paper we improve on the model of \emph{Semiring Turing Machines} (distinct from so called weighted Turing machines) introduced by Eiter \& Kiesel (Semiring Reasoning Frameworks in AI and Their Computational Complexity, \emph{J. Artif. Intell. Res.}, 2023). Our central result is a Fagin-style theorem for a new quantitative complexity class using a suitable weighted logical formalism. We show that the quantitative complexity class that we call \NPnewinf{$\mathcal{R}$}, where $\mathcal{R}$ is a commutative semiring, can be captured using a version of weighted existential second-order logic that allows for predicates interpreted as semiring-annotated relations. This result provides a precise logical characterization of the power series that form the class \NPnewinf{$\mathcal{R}$}. We also give the exact relation between Eiter \& Kiesel's version of NP, called \NPoldinf{$\mathcal{R}$}, and the class \NPnewinf{$\mathcal{R}$}. Incidentally, we are able to recapture all the complexity results by Eiter \& Kiesel (2023) in our new model, connecting a quantitative version of NP to various counting complexity classes."
2507.18666,"The theory of evolvability, introduced by Valiant (2009), formalizes evolution as a constrained learning algorithm operating without labeled examples or structural knowledge. While theoretical work has established the evolvability of specific function classes under idealized conditions, the framework remains largely untested empirically. In this paper, we implement a genetic algorithm that faithfully simulates Valiant's model and conduct extensive experiments across six Boolean function classes: monotone conjunctions, monotone disjunctions, parity, majority, general conjunctions, and general disjunctions. Our study examines evolvability under uniform and non-uniform distributions, investigates the effects of fixed initial hypotheses and the removal of neutral mutations, and highlights how these constraints alter convergence behavior. We validate known results (e.g., evolvability of monotone conjunctions, non-evolvability of parity) and offer the first empirical evidence on the evolvability of majority and general Boolean classes. Our findings reveal sharp performance drops at intermediate dimensions and expose the essential role of neutral mutations in escaping fitness plateaus. We also demonstrate that evolvability can depend strongly on the input distribution. These insights clarify practical limits of evolutionary search and suggest new directions for theoretical work, including potential refinements to evolvability definitions and bounds. Our implementation provides a rigorous, extensible framework for empirical analysis and serves as a testbed for future explorations of learning through evolution."
2507.19,"Edge-coloring problems with forbidden patterns refer to decision problems whose input is a graph $\mathbb G$ and where the goal is to determine whether the edges of $\mathbb G$ can be colored (with a fixed finite set of colors) in a way that in the resulting colored graph $(\mathbb G, \xi)$, none of a fixed set of edge-colored obstructions admits a homomorphism to $(\mathbb G, \xi)$. In the coloring extension setting, some of the edges of $\mathbb G$ are already colored and the goal is to find an extension of this coloring omitting the obstructions. We show that for certain sets of obstructions, there is a polynomial-time equivalence between the coloring problem and the extension problem. We also show that for natural sets of obstructions, such coloring problems exhibit a P vs. NP-complete complexity dichotomy."
2507.19108,"A problem $\mathcal{P}$ is considered downward self-reducible, if there exists an efficient algorithm for $\mathcal{P}$ that is allowed to make queries to only strictly smaller instances of $\mathcal{P}$. Downward self-reducibility has been well studied in the case of decision problems, and it is well known that any downward self-reducible problem must lie in $\mathsf{PSPACE}$. Harsha, Mitropolsky and Rosen [ITCS, 2023] initiated the study of downward self reductions in the case of search problems. They showed the following interesting collapse: if a problem is in $\mathsf{TFNP}$ and also downward self-reducible, then it must be in $\mathsf{PLS}$. Moreover, if the problem admits a unique solution then it must be in $\mathsf{UEOPL}$.We demonstrate that this represents just the tip of a much more general phenomenon, which holds for even harder search problems that lie higher up in the total function polynomial hierarchy ($\mathsf{TF\Sigma_i^P}$). In fact, even if we allow our downward self-reduction to be much more powerful, such a collapse will still occur.We show that any problem in $\mathsf{TF\Sigma_i^P}$ which admits a randomized downward self-reduction with access to a $\mathsf{\Sigma_{i-1}^P}$ oracle must be in $\mathsf{PLS}^{\mathsf{\Sigma_{i-1}^P}}$. If the problem has \textit{essentially unique solutions} then it lies in $\mathsf{UEOPL}^{\mathsf{\Sigma_{i-1}^P}}$.As one (out of many) application of our framework, we get new upper bounds for the problems $\mathrm{Range Avoidance}$ and $\mathrm{Linear Ordering Principle}$ and show that they are both in $\mathsf{UEOPL}^{\mathsf{NP}}$."
2507.20321,"In this paper, we extend Busy Beaver function to a class of higher order Busy Beaver functions based on Turing oracle machine. We prove some results about the relation between decidability of number theoretical formula and higher order Busy Beaver functions, and the relation between computability of max-min partial recursive functions and higher order Busy Beaver functions. We also present some conjectures on higher order Busy Beaver functions."
2507.2155,"We initiate the study of complexity classes $\mathsf{A^B}$ where $\mathsf{A}$ and $\mathsf{B}$ are both $\mathsf{TFNP}$ subclasses. For example, we consider complexity classes of the form $\mathsf{PPP^{PPP}}$, $\mathsf{PPAD^{PPA}}$, and $\mathsf{PPA^{PLS}}$. We define complete problems for such classes, and show that they belong in $\mathsf{TFNP}$. These definitions require some care, since unlike a class like $\mathsf{PPA^{NP}}$, where the $\mathsf{NP}$ oracle defines a function, in $\mathsf{PPA^{PPP}}$, the oracle is for a search problem with many possible solutions. Intuitively, the definitions we introduce quantify over all possible instantiations of the $\mathsf{PPP}$ oracle.With these notions in place, we then show that several $\mathsf{TFNP}$ subclasses are self-low. In particular, $\mathsf{PPA^{PPA}} = \mathsf{PPA}$, $\mathsf{PLS^{PLS}} = \mathsf{PLS}$, and $\mathsf{LOSSY^{LOSSY}} = \mathsf{LOSSY}$. These ideas introduce a novel approach for classifying computational problems within $\mathsf{TFNP}$, such as the problem of deterministically generating large primes."
2507.22265,"A recent work (Korten, Pitassi, and Impagliazzo, FOCS 2025) established an insightful connection between static data structure lower bounds, range avoidance of $\text{NC}^0$ circuits, and the refutation of pseudorandom CSP instances, leading to improvements to some longstanding lower bounds in the cell-probe/bit-probe models. Here, we improve these lower bounds in certain cases via a more streamlined reduction to XOR refutation, coupled with handling the odd-arity case. Our result can be viewed as a complete derandomization of the state-of-the-art semi-random $k$-XOR refutation analysis (Guruswami, Kothari and Manohar, STOC 2022, Hsieh, Kothari and Mohanty, SODA 2023), which complements the derandomization of the even-arity case obtained by Korten et al.As our main technical statement, we show that for any multi-output constant-depth circuit that substantially stretches its input, its output is very likely far from strings sampled from distributions with sufficient independence, and further this can be efficiently certified. Via suitable shifts in perspectives, this gives applications to cell-probe lower bounds and range avoidance algorithms for $\mathsf{NC}^0$ circuits."
2507.22444,"We generalize Håstad's long-code test for projection games and show that it remains complete and sound against entangled provers.Combined with a result of Dong et al. \cite{Dong25}, which establishes that $\MIP^*=\RE$ with constant-length answers, we derive that $\LIN^*_{1-\epsilon,s}=\RE$, for some $1/2< s<1$ and for every sufficiently small $\epsilon>0$, where LIN refers to linearity (over $\mathbb{F}_2$) of the verifier predicate. Achieving the same result with $\epsilon=0$ would imply the existence of a non-hyperlinear group."
2507.23008,"Itsykson and Sokolov identified resolution over parities, denoted by $\text{Res}(\oplus)$, as a natural and simple fragment of $\text{AC}^0[2]$-Frege for which no super-polynomial lower bounds on size of proofs are known. Building on a recent line of work, Efremenko and Itsykson proved lower bounds of the form $\text{exp}(N^{\Omega(1)})$, on the size of $\text{Res}(\oplus)$ proofs whose depth is upper bounded by $O(N\log N)$, where $N$ is the number of variables of the unsatisfiable CNF formula. The hard formula they used was Tseitin on an appropriately expanding graph, lifted by a $2$-stifling gadget. They posed the natural problem of proving super-polynomial lower bounds on the size of proofs that are $\Omega(N^{1+\epsilon})$ deep, for any constant $\epsilon > 0$.We provide a significant improvement by proving a lower bound on size of the form $\text{exp}(\tilde{\Omega}(N^{\epsilon}))$, as long as the depth of the $\text{Res}(\oplus)$ proofs are $O(N^{2-\epsilon})$, for every $\epsilon > 0$. Our hard formula is again Tseitin on an expander graph, albeit lifted with a different type of gadget. Our gadget needs to have small correlation with all parities.An important ingredient in our work is to show that arbitrary distributions \emph{lifted} with such gadgets fool \emph{safe} affine spaces, an idea which originates in the earlier work of Bhattacharya, Chattopadhyay and Dvorak."
2507.23345,"Ward and Szabó [WS94] have shown that a complete graph with $N^2$ nodes whose edges are colored by $N$ colors and that has at least two colors contains a bichromatic triangle. This fact leads us to a total search problem: Given an edge-coloring on a complete graph with $N^2$ nodes using at least two colors and at most $N$ colors, find a bichromatic triangle. Bourneuf, Folwarczný, Hubácek, Rosen, and Schwartzbach [Bou+23] have proven that such a total search problem, called Ward-Szabó, is PWPP-hard and belongs to the class TFNP, a class for total search problems in which the correctness of every candidate solution is efficiently verifiable. However, it is open which TFNP subclass contains Ward-Szabó. This paper will improve the computational complexity of Ward-Szabó. We prove that Ward-Szabó is a complete problem for the complexity class PPP, a TFNP subclass of problems in which the existence of solutions is guaranteed by the pigeonhole principle."
2507.23563,"In this monograph, we study complexity classes that are defined using $O(\log n)$-space bounded non-deterministic Turing machines. We prove salient results of Computational Complexity in this topic such as the Immerman-Szelepcs$\rm\acute{e}$nyi Theorem, the Isolating Lemma, theorems of Mahajan-Vinay on the determinant and many consequences of these very important results. The manuscript is intended to be a comprehensive textbook on the topic of The Complexity of Logarithmic Space Bounded Counting Classes."
2508.00276,"In the Maxmin E$k$-SAT Reconfiguration problem, we are given a satisfiable $k$-CNF formula $\varphi$ where each clause contains exactly $k$ literals, along with a pair of its satisfying assignments. The objective is transform one satisfying assignment into the other by repeatedly flipping the value of a single variable, while maximizing the minimum fraction of satisfied clauses of $\varphi$ throughout the transformation. In this paper, we demonstrate that the optimal approximation factor for Maxmin E$k$-SAT Reconfiguration is $1 - \Theta\left(\frac{1}{k}\right)$. On the algorithmic side, we develop a deterministic $\left(1-\frac{1}{k-1}-\frac{1}{k}\right)$-factor approximation algorithm for every $k \geq 3$. On the hardness side, we show that it is $\mathsf{PSPACE}$-hard to approximate this problem within a factor of $1-\frac{1}{10k}$ for every sufficiently large $k$. Note that an ``$\mathsf{NP}$ analogue'' of Maxmin E$k$-SAT Reconfiguration is Max E$k$-SAT, whose approximation threshold is $1-\frac{1}{2^k}$ shown by Håstad (JACM 2001). To the best of our knowledge, this is the first reconfiguration problem whose approximation threshold is (asymptotically) worse than that of its $\mathsf{NP}$ analogue. To prove the hardness result, we introduce a new ``non-monotone'' test, which is specially tailored to reconfiguration problems, despite not being helpful in the PCP regime."
2508.01649,"Consider graphs of n nodes, and use a Bloom filter of length 2 log3 n bits. An edge between nodes i and j, with i < j, turns on a certain bit of the Bloom filter according to a hash function on i and j. Pick a set of log n nodes and turn on all the bits of the Bloom filter required for these log n nodes to form a clique. As a result, the Bloom filter implies the existence of certain other edges, those edges (x, y), with x < y, such that all the bits selected by applying the hash functions to x and y happen to have been turned on due to hashing the clique edges into the Bloom filter.Constructing the graph consisting of the clique-selected edges and those edges mapped to the turned-on bits of the Bloom filter can be performed in polynomial time in n. Choosing a large enough polylogarithmic in n Bloom filter yields that the graph has only one clique of size log n, the planted clique. When the hash function is black-boxed, finding that clique is intractable and, therefore, inverting the function that maps log n nodes to a graph is not (likely to be) possible in polynomial time."
2508.04133,"A distribution over instances of a sampling problem is said to exhibit transport disorder chaos if perturbing the instance by a small amount of random noise dramatically changes the stationary distribution (in Wasserstein distance). Seeking to provide evidence that some sampling tasks are hard on average, a recent line of work has demonstrated that disorder chaos is sufficient to rule out ""stable"" sampling algorithms, such as gradient methods and some diffusion processes.We demonstrate that disorder chaos does not preclude polynomial-time sampling by canonical algorithms in canonical models. We show that with high probability over a random graph $\boldsymbol{G} \sim G(n,1/2)$: (1) the hardcore model (at fugacity $\lambda = 1$) on $\boldsymbol{G}$ exhibits disorder chaos, and (2) Glauber dynamics run for $O(n)$ time can approximately sample from the hardcore model on $\boldsymbol{G}$ (in Wasserstein distance)."
2508.05597,"We prove that computing the deterministic communication complexity D(f) of a Boolean function is NP-hard in the standard protocol-tree model, answering, independently and concurrently with Hirahara-Llango-Loff (arXiv:2507.10426), a question first posed by Yao (1979). Our reduction builds and expands on a suite of structural ""interlacing"" lemmas introduced by Mackenzie and Saffidine (arXiv:2411.19003); these lemmas can be reused as black boxes in future lower-bound constructions.The instances produced by our reduction admit optimal protocols for self-similar constructions with strong structural properties, giving a flexible framework for the design of reductions showing NP-hardness of deciding the communication complexity of a Boolean matrix. This complements the work by Hirahara, Ilango, and Loff, which establishes NP-hardness in the same model via a different route; our analysis additionally yields reusable structural guarantees and underpins further consequences concerning inapproximability.Because the gadgets in our construction are self-similar, they can be recursively embedded. We sketch how this yields, under the Exponential-Time Hypothesis, an additive inapproximability gap that grows without bound. Furthermore we outline a route toward NP-hardness of approximating D(f) within a fixed constant additive error. Full details of the ETH-based inapproximability results will appear in a future version.Beyond settling the complexity of deterministic communication complexity itself, the modular framework we develop opens the door to a wider class of reductions and, we believe, will prove useful in tackling other long-standing questions in communication complexity."
2508.07619,"This paper makes two primary contributions. First, we introduce the concept of counting martingales and use it to define counting measures, counting dimensions, and counting strong dimensions. Second, we apply these new tools to strengthen previous circuit lower bounds.Resource-bounded measure and dimension have traditionally focused on deterministic time and space bounds. We use counting complexity classes to develop resource-bounded counting measures and dimensions. Counting martingales are constructed using functions from the #P, SpanP, and GapP complexity classes. We show that counting martingales capture many martingale constructions in complexity theory. The resulting counting measures and dimensions are intermediate in power between the standard time-bounded and space-bounded notions, enabling finer-grained analysis where space-bounded measures are known, but time-bounded measures remain open. For example, we show that BPP has #P-dimension 0 and BQP has GapP-dimension 0.As our main application, we improve circuit-size lower bounds. Lutz (1992) strengthened Shannon's classic $(1-\epsilon)\frac{2^n}{n}$ lower bound (1949) to PSPACE-measure, showing that almost all problems require circuits of size $\frac{2^n}{n}\left(1+\frac{\alpha \log n}{n}\right)$, for any $\alpha < 1$. We extend this result to SpanP-measure, with a proof that uses a connection through the Minimum Circuit Size Problem (MCSP) to construct a counting martingale. Our results imply that the stronger lower bound holds within the third level of the exponential-time hierarchy, whereas previously, it was only known in ESPACE. We study the #P-dimension of classical circuit complexity classes and the GapP-dimension of quantum circuit complexity classes. We also show that if one-way functions exist, then #P-dimension is strictly more powerful than P-dimension."
2508.1055,"Kernelization is the standard framework to analyze preprocessing routines mathematically. Here, in terms of efficiency, we demand the preprocessing routine to run in time polynomial in the input size. However, today, various NP-complete problems are already solved very fast in practice; in particular, SAT-solvers and ILP-solvers have become extremely powerful and used frequently. Still, this fails to capture the wide variety of computational problems that lie at higher levels of the polynomial hierarchy. Thus, for such problems, it is natural to relax the definition of kernelization to permit the preprocessing routine to make polynomially many calls to a SAT-solver, rather than run, entirely, in polynomial time.Our conceptual contribution is the introduction of a new notion of a kernel that harnesses the power of SAT-solvers for preprocessing purposes, and which we term a P^NP-Kernel. Technically, we investigate various facets of this notion, by proving both positive and negative results, including a lower-bounds framework to reason about the negative results. Here, we consider both satisfiability and graph problems. Additionally, we present a meta-theorem for so-called ""discovery problems"". This work falls into a long line of research on extensions of the concept of kernelization, including lossy kernels [Lokshtanov et al., STOC '17], dynamic kernels [Alman et al., ACM TALG '20], counting kernels [Lokshtanov et al., ICTS '24], and streaming kernels [Fafianie and Kratsch, MFCS '14]."
2508.1154,"Constraint Satisfaction Problems (CSPs, for short) make up a class of problems with applications in many areas of computer science. The first classification of these problems was given by Schaeffer who showed that every CSP over the domain {0,1} is either in P or is NP-complete. More recently this was shown to hold for all CSPs over finite relational structures independently by Bulatov and Zhuk. Furthermore, they characterized the complexity based solely on the polymorphism algebra of the associated relational structure, building upon the deep connections between universal algebra and complexity theory. In this article we extend this and consider what happens if the instance forms a special type of relational core called a multisorted core. Our main result is that in this case the problem is reducible to computing the determinant of an integer valued matrix which places it in the complexity class DET, which is likely a strict subset of P."
2508.1157,"Pencil puzzles are puzzles that can be solved by writing down solutions on a paper, using only logical reasoning. In this paper, we utilize the ""T-metacell"" framework developed by Tang and the MIT Hardness Group to prove the ASP-completeness of four new pencil puzzles: Grand Tour, Entry Exit, Yagit, and Zahlenschlange. The results demonstrate how versatile the framework is, offering new insights into the computational complexity of problems with various constraints."
2508.12241,"In this work, the classical problem of multi-cast beamforming in wireless communication is reconsidered. An existing work has shown that the multi-cast beamforming problem is NP-hard. In this project, we show that the corresponding decision problem for real channel matrices and beamformers is NP-complete. Finally, we carry out simulations to reveal the computational complexity of solving the NP-complete beamforming problem in various setting using SAT/SMT solvers."
2508.12747,"A storyplan visualizes a graph $G=(V,E)$ as a sequence of $\ell$ frames $\Gamma_1, \dots, \Gamma_\ell$, each of which is a drawing of the induced subgraph $G[V_i]$ of a vertex subset $V_i \subseteq V$. Moreover, each vertex $v \in V$ is contained in a single consecutive sequence of frames $\Gamma_i, \dots, \Gamma_j$, all vertices and edges contained in consecutive frames are drawn identically, and the union of all frames is a drawing of $G$. In GD 2022, the concept of planar storyplans was introduced, in which each frame must be a planar (topological) drawing. Several (parameterized) complexity results for recognizing graphs that admit a planar storyplan were provided, including NP-hardness. In this paper, we investigate an open question posed in the GD paper and show that the geometric and topological settings of the planar storyplan problem differ: We provide an instance of a graph that admits a planar storyplan, but no planar geometric storyplan, in which each frame is a planar straight-line drawing. Still, by adapting the reduction proof from the topological to the geometric setting, we show that recognizing the graphs that admit planar geometric storyplans remains NP-hard."
2508.13032,"Coordinating the motion of multiple agents in constrained environments is a fundamental challenge in robotics, motion planning, and scheduling. A motivating example involves $n$ robotic arms, each represented as a line segment. The objective is to rotate each arm to its vertical orientation, one at a time (clockwise or counterclockwise), without collisions nor rotating any arm more than once. This scenario is an example of the more general $k$-Compatible Ordering problem, where $n$ agents, each capable of $k$ state-changing actions, must transition to specific target states under constraints encoded as a set $\mathcal{G}$ of $k$ pairs of directed graphs.We show that $k$-Compatible Ordering is $\mathsf{NP}$-complete, even when $\mathcal{G}$ is planar, degenerate, or acyclic. On the positive side, we provide polynomial-time algorithms for cases such as when $k = 1$ or $\mathcal{G}$ has bounded treewidth. We also introduce generalized variants supporting multiple state-changing actions per agent, broadening the applicability of our framework. These results extend to a wide range of scheduling, reconfiguration, and motion planning applications in constrained environments."
2508.13166,"The P = NP problem asks whether every problem whose solution can be verified in polynomial time (NP) can also be solved in polynomial time (P). In this paper, we present a proof that P = NP, demonstrating that every NP problem can be solved deterministically in polynomial time. We introduce a new Computation Model that enables the simulation of a Turing machine, and show that NP problems can be simulated efficiently within this framework. By introducing the concept of a Feasible Graph, we ensure that the simulation can be performed in polynomial time, providing a direct path to resolving the P = NP question. Our result has significant implications for fields such as cryptography, optimization, and artificial intelligence, where NP-complete problems play a central role."
2508.132,"We present a topological barrier to efficient computation, revealed by comparing the geometry of 2 SAT and 3 SAT solution spaces. Viewing the set of satisfying assignments as a cubical complex within the Boolean hypercube, we prove that every 2 SAT instance has a contractible solution space, topologically flat, with all higher Betti numbers bk equals 0 for k greater than or equal 1, while both random and explicit 3 SAT families can exhibit exponential second Betti numbers, corresponding to exponentially many independent voids. These voids are preserved under standard SAT reductions and cannot be collapsed without solving NP-hard subproblems, making them resistant to the three major complexity theoretic barriers, relativization, natural proofs, and algebrization. We establish exponential time lower bounds in restricted query models and extend these to broader algorithmic paradigms under mild information-theoretic or encoding assumptions. This topological contrast flat, connected landscapes in 2 SAT versus tangled, high-dimensional void-rich landscapes in 3 SAT, provides structural evidence toward P does not equal NP, identifying b2 as a paradigm-independent invariant of computational hardness."
2508.14017,"Transcriptional networks represent one of the most extensively studied types of systems in synthetic biology. Although the completeness of transcriptional networks for digital logic is well-established, *analog* computation plays a crucial role in biological systems and offers significant potential for synthetic biology applications. While transcriptional circuits typically rely on cooperativity and highly non-linear behavior of transcription factors to regulate *production* of proteins, they are often modeled with simple linear *degradation* terms. In contrast, general analog dynamics require both non-linear positive as well as negative terms, seemingly necessitating control over not just transcriptional (i.e., production) regulation but also the degradation rates of transcription factors.Surprisingly, we prove that controlling transcription factor production (i.e., transcription rate) without explicitly controlling degradation is mathematically complete for analog computation, achieving equivalent capabilities to systems where both production and degradation are programmable. We demonstrate our approach on several examples including oscillatory and chaotic dynamics, analog sorting, memory, PID controller, and analog extremum seeking. Our result provides a systematic methodology for engineering novel analog dynamics using synthetic transcriptional networks without the added complexity of degradation control and informs our understanding of the capabilities of natural transcriptional circuits.We provide a compiler, in the form of a Python package that can take any system of polynomial ODEs and convert it to an equivalent transcriptional network implementing the system *exactly*, under appropriate conditions."
2508.14606,"Given a satisfiable instance of 1-in-3 SAT, it is NP-hard to find a satisfying assignment for it, but it may be possible to efficiently find a solution subject to a weaker (not necessarily Boolean) predicate than `1-in-3'. There is a folklore conjecture predicting which choices of weaker predicates lead to tractability and for which the task remains \NP-hard. One specific predicate, corresponding to the problem of linearly ordered $3$-colouring of 3-uniform hypergraphs, has been mentioned in several recent papers as an obstacle to further progress in proving this conjecture. We prove that the problem for this predicate is NP-hard, as predicted by the conjecture.We use the Promise CSP framework, where the complexity analysis is performed via the algebraic approach, by studying the structure of polymorphisms, which are multidimensional invariants of the problem at hand. The analysis of polymorphisms is in general a highly non-trivial task, and topological combinatorics was recently discovered to provide a useful tool for this. There are two distinct ways in which it was used: one is based on variations of the Borsuk-Ulam theorem, and the other aims to classify polymorphisms up to certain reconfigurations (homotopy). Our proof, whilst combinatorial in nature, shows that our problem is the first example where the features behind the two uses of topology appear together. Thus, it is likely to be useful in guiding further development of the topological method aimed at classifying Promise CSPs. An easy consequence of our result is the hardness of another specific Promise CSP, which was recently proved by Filakovský et al. by employing a deep topological analysis of polymorphisms."
2508.14831,"We prove a square-root space simulation for deterministic multitape Turing machines, showing $\mathrm{TIME}[t]\subseteq \mathrm{SPACE}[O(\sqrt{t})]$ \emph{measured in tape cells over a fixed finite alphabet}. The key step is a Height Compression Theorem that uniformly (and in logspace) reshapes the canonical left-deep succinct computation tree for a block-respecting run into a binary tree whose evaluation-stack depth along any DFS path is $O(\log T)$ for $T=\lceil t/b\rceil$, while preserving $O(b)$ workspace at leaves and $O(1)$ at internal nodes. Edges have \emph{addressing/topology} checkable in $O(\log t)$ space, and \emph{semantic} correctness across merges is witnessed by an exact $O(b)$ bounded-window replay at the unique interface. Algorithmically, an Algebraic Replay Engine with constant-degree maps over a constant-size field, together with pointerless DFS and index-free streaming, ensures constant-size per-level tokens and eliminates wide counters, yielding the additive tradeoff $S(b)=O(b+t/b)$. Choosing $b=\Theta(\sqrt{t})$ gives $O(\sqrt{t})$ space with no residual multiplicative polylog factors. The construction is uniform, relativizes, and is robust to standard model choices. Consequences include branching-program upper bounds $2^{O(\sqrt{s})}$ for size-$s$ bounded-fan-in circuits, tightened quadratic-time lower bounds for $\mathrm{SPACE}[n]$-complete problems via the standard hierarchy argument, and $O(\sqrt{t})$-space certifying interpreters; under explicit locality assumptions, the framework extends to geometric $d$-dimensional models. Conceptually, the work isolates path bookkeeping as the chief obstruction to $O(\sqrt{t})$ and removes it via structural height compression with per-path analysis rather than barrier-prone techniques."
2508.16014,"We introduce Pudlak-Buss style Prover-Adversary games to characterise proof systems reasoning over deterministic branching programs (BPs) and non-deterministic branching programs (NBPs). Our starting points are the proof systems eLDT and eLNDT, for BPs and NBPs respectively, previously introduced by Buss, Das and Knop. We prove polynomial equivalences between these proof systems and the corresponding games we introduce. This crucially requires access to a form of negation of branching programs which, for NBPs, requires us to formalise a non-uniform version of the Immerman-Szelepcsenyi theorem that coNL = NL. Thanks to the techniques developed, we further obtain a proof complexity theoretic version of Immerman-Szelepcsenyi, showing that eLNDT is polynomially equivalent to systems over boundedly alternating branching programs."
2508.17602,"Push-1 is one of the simplest abstract frameworks for motion planning; however, the complexity of deciding if a Push-1 problem can be solved was a several-decade-old open question. We resolve the complexity of the motion planning problem Push-1 by showing that it is PSPACE-complete, and we formally verify the correctness of our constructions. Our results build upon a recent work which demonstrated that Push-1F (a variant of Push-1 with fixed blocks) and Push-k for $k \geq 2$ (a variant of Push-1 where the agent can push $k$ blocks at once) are PSPACE-complete and more generally on the motion-planning-though-gadgets framework.In the process of resolving this open problem, we make two general contributions to the motion planning complexity theory. First, our proof technique extends the standard motion planning framework by assigning the agent a state. This state is preserved when traversing between gadgets but can change when taking transitions in gadgets. Second, we designed and implemented a system, GADGETEER, for computationally verifying the behavior of systems of gadgets. This system is agnostic to the underlying motion planning problem, and allows for formally verifying the correspondence between a low-level construction and a high-level system of gadgets as well as automatically synthesizing gadgets from low-level constructions. In the case of Push-1, we use this system to formally prove that our constructions match our high-level specifications of their behavior. This culminates in the construction and verification of a self-closing door, as deciding reachability in a system of self-closing doors is PSPACE-complete."
2508.18104,"We consider two different problem families that deal with domination in graphs. On the one hand, we focus on dominating sequences. In such a sequence, every vertex dominates some vertex of the graph that was not dominated by any earlier vertex in the sequence. The problem of finding the longest dominating sequence is known as $\mathsf{Grundy~Domination}$. Depending on whether the closed or the open neighborhoods are used for domination, there are three other versions of this problem. We show that all four problem variants are $\mathsf{W[1]}$-complete when parameterized by the solution size. On the other hand, we consider the family of zero forcing problems which form the parameterized duals of the Grundy domination problems. In these problems, one looks for the smallest set of vertices initially colored blue such that certain color change rules are able to color all other vertices blue. Bhyravarapu et al. [IWOCA 2025] showed that one of these problems, known as $\mathsf{Zero~Forcing~Set}$, is in $\mathsf{FPT}$ when parameterized by the treewidth or the solution size. We extend their treewidth result to the other three variants of zero forcing and their respective Grundy domination problems. Our algorithm also implies an $\mathsf{FPT}$ algorithm for $\mathsf{Grundy~Domination}$ when parameterized by the number of vertices that are not in the dominating sequence."
2509.02423,"The $k$-Coloring problem on hereditary graph classes has been a deeply researched problem over the last decade. A hereditary graph class is characterized by a (possibly infinite) list of minimal forbidden induced subgraphs. We say that a graph is $(H_1,H_2,\ldots)$-free if it does not contain any of $H_1,H_2,\ldots$ as induced subgraphs. The complexity landscape of the problem remains unclear even when restricting to the case $k=4$ and classes defined by a few forbidden induced subgraphs. While the case of only one forbidden induced subgraph has been completely resolved lately, the complexity when considering two forbidden induced subgraphs still has a couple of unknown cases. In particular, $4$-Coloring on $(P_6,C_3)$-free graphs is polynomial while it is NP-hard on $(P_{22},C_3)$-free graphs.We provide a reduction showing NP-completeness of $4$-Coloring on $(P_t,C_3)$-free graphs for $19\leq t\leq 21$, thus constricting the gap of cases whose complexity remains unknown. Our proof includes a computer search ensuring that the graph family obtained through the reduction is indeed $P_{19}$-free."
2509.0273,"We consider a static data structure problem of computing a linear operator under cell-probe model. Given a linear operator $M \in \mathbb{F}_2^{m \times n}$, the goal is to pre-process a vector $X \in \mathbb{F}_2^n$ into a data structure of size $s$ to answer any query $\langle M_i , X \rangle$ in time $t$. We prove that for a random operator $M$, any such data structure requires:$$ t \geq \Omega ( \min \{ \log (m/s) , n / \log s \} ).$$ This result overcomes the well-known logarithmic barrier in static data structures [MNSW98, Sie04, PD06, PTW08, Pat11, DGW19] by using a random linear operator. Furthermore, it provides the first significant progress toward confirming a decades-old folklore conjecture: that non-linear pre-processing does not substantially help in computing most linear operators.A straightforward modification of our proof also yields a wire lower bound of $\Omega(n \cdot \log^{1/d}(n))$ for depth-$d$ circuits with arbitrary gates that compute a specific linear operator $M \in \mathbb{F}_2^{O(n) \times n}$, even against some small constant advantage over random guessing. This bound holds even for circuits with only a small constant advantage over random guessing, improving upon longstanding results [RS03, Che08a, Che08b, GHK+13] for a random operator.Finally, our work partially resolves the communication form of the Multiphase Conjecture [Pat10] and makes progress on Jukna-Schnitger's Conjecture [JS11, Juk12]. We address the former by considering the Inner Product (mod 2) problem (instead of Set Disjointness) when the number of queries $m$ is super-polynomial (e.g., $2^{n^{1/3}}$), and the total update time is $m^{0.99}$. Our result for the latter also applies to cases with super-polynomial $m$."
2509.05009,"We first extend the results of Chatterjee, Kumar, Shi, Volk (Computational Complexity 2022) by showing that the degree $d$ elementary symmetric polynomials in $n$ variables have formula lower bounds of $\Omega(d(n-d))$ over fields of positive characteristic. Then, we show that the results of the universality of the symmetric model from Shpilka (Journal of Computer and System Sciences 2002) and the results about border fan-in two $\Sigma\Pi\Sigma$ circuits from Kumar (ACM Trans. Comput. Theory 2020) over zero characteristic fields do not extend to fields of positive characteristic. In particular, we show that * There are polynomials that cannot be represented as linear projections of the elementary symmetric polynomials (in fact, we show that they cannot be represented as the sum of $k$ such projections for a fixed $k$) and * There are polynomials that cannot be computed by border depth-$3$ circuits of top fan-in $k$, called $\overline{\Sigma^{[k]}\Pi\Sigma}$, for $k = o(n)$.To prove the first result, we consider a geometric property of the elementary symmetric polynomials, namely, the set of all points in which the polynomial and all of its first-order partial derivatives vanish. It was shown in Meckler, Zaimi and Limaye, Mittal, Pareek that the dimension of this space was exactly $d-2$ for fields of zero characteristic. We extend this to fields of positive characteristic by showing that this dimension must be between $d-2$ and $d-1$. In fact, we show this bound is tight.Then, to consider the border top fan-in of the symmetric model and depth-$3$ circuits (sometimes called border affine Chow rank), we show that it is sufficient to consider the border top fan-in of the sum of linear projections of the elementary symmetric polynomials. This is done by constructing an explicit metapolynomial to check the condition, meaning that this result also applies in the border setting."
2509.05122,"The $H$-Coloring problem is a well-known generalization of the classical NP-complete problem $k$-Coloring where the task is to determine whether an input graph admits a homomorphism to the template graph $H$. This problem has been the subject of intense theoretical research and in this article we study the complexity of $H$-Coloring with respect to the parameters clique-width and the more recent component twin-width, which describe desirable computational properties of graphs. We give two surprising linear bounds between these parameters, thus improving the previously known exponential and double exponential bounds. Our constructive proof naturally extends to related parameters and as a showcase we prove that total twin-width and linear clique-width can be related via a tight quadratic bound. These bounds naturally lead to algorithmic applications. The linear bounds between component twin-width and clique-width entail natural approximations of component twin-width, by making use of the results known for clique-width. As for computational aspects of graph coloring, we target the richer problem of counting the number of homomorphisms to $H$ (#$H$-Coloring). The first algorithm that we propose uses a contraction sequence of the input graph $G$ parameterized by the component twin-width of $G$. This leads to a positive FPT result for the counting version. The second uses a contraction sequence of the template graph $H$ and here we instead measure the complexity with respect to the number of vertices in the input graph. Using our linear bounds we show that our algorithms are always at least as fast as the previously best #$H$-Coloring algorithms (based on clique-width) and for several interesting classes of graphs (e.g., cographs, cycles of length $\ge 7$, or distance-hereditary graphs) are in fact strictly faster."
2509.05211,"We develop quantitative algorithmic information bounds for orthogonal projections and distances in the plane. Under mild independence conditions, the distance $|x-y|$ and a projection coordinate $p_e x$ each retain at least half the algorithmic information content of $x$ in the sense of finite-precision Kolmogorov complexity, up to lower-order terms. Our bounds support conditioning on coarser approximations, enabling case analyses across precision scales. The proofs introduce a surrogate point selection step. Via the point-to-set principle we derive a new bound on the Hausdorff dimension of pinned distance sets, showing that every analytic set $E\subseteq\mathbb{R}^2$ with $\dim_H(E)\leq 1$ satisfies\[\sup_{x\in E}\dim_H(\Delta_x E)\geq \frac{3}{4}\dim_H(E).\]We also extend Bourgain's theorem on exceptional sets for orthogonal projections to all sets that admit optimal Hausdorff oracles."
2509.05871,"We introduce a general framework to design and analyze algorithms for the problem of testing homomorphisms between finite groups in the low-soundness regime.In this regime, we give the first constant-query tests for various families of groups. These include tests for: (i) homomorphisms between arbitrary cyclic groups, (ii) homomorphisms between any finite group and $\mathbb{Z}_p$, (iii) automorphisms of dihedral and symmetric groups, (iv) inner automorphisms of non-abelian finite simple groups and extraspecial groups, and (v) testing linear characters of $\mathrm{GL}_n(\mathbb{F}_q)$, and finite-dimensional Lie algebras over $\mathbb{F}_q$. We also recover the result of Kiwi [TCS'03] for testing homomorphisms between $\mathbb{F}_q^n$ and $\mathbb{F}_q$.Prior to this work, such tests were only known for abelian groups with a constant maximal order (such as $\mathbb{F}_q^n$). No tests were known for non-abelian groups.As an additional corollary, our framework gives combinatorial list decoding bounds for cyclic groups with list size dependence of $O(\varepsilon^{-2})$ (for agreement parameter $\varepsilon$). This improves upon the currently best-known bound of $O(\varepsilon^{-105})$ due to Dinur, Grigorescu, Kopparty, and Sudan [STOC'08], and Guo and Sudan [RANDOM'14]."
2509.06435,"Linear matroid intersection is an important problem in combinatorial optimization. Given two linear matroids over the same ground set, the linear matroid intersection problem asks you to find a common independent set of maximum size. The deep interest in linear matroid intersection is due to the fact that it generalises many classical problems in theoretical computer science, such as bipartite matching, edge disjoint spanning trees, rainbow spanning tree, and many more.We study this problem in the model of catalytic computation: space-bounded machines are granted access to \textit{catalytic space}, which is additional working memory that is full with arbitrary data that must be preserved at the end of its computation.Although linear matroid intersection has had a polynomial time algorithm for over 50 years, it remains an important open problem to show that linear matroid intersection belongs to any well studied subclass of $\mathsf{P}$. We address this problem for the class catalytic logspace ($\mathsf{CL}$) with a polynomial time bound ($\mathsf{CLP}$).Recently, Agarwala and Mertz (2025) showed that bipartite maximum matching can be computed in the class $\mathsf{CLP}\subseteq \mathsf{P}$. This was the first subclass of $\mathsf{P}$ shown to contain bipartite matching, and additionally the first problem outside $\mathsf{TC}^1$ shown to be contained in $\mathsf{CL}$. We significantly improve the result of Agarwala and Mertz by showing that linear matroid intersection can be computed in $\mathsf{CLP}$."
2509.0688,"A strength of parameterized algorithmics is that each problem can be parameterized by an essentially inexhaustible set of parameters. Usually, the choice of the considered parameter is informed by the theoretical relations between parameters with the general goal of achieving FPT-algorithms for smaller and smaller parameters. However, the FPT-algorithms for smaller parameters usually have higher running times and it is unclear whether the decrease in the parameter value or the increase in the running time bound dominates in real-world data. This question cannot be answered from purely theoretical considerations and any answer requires knowledge on typical parameter values.To provide a data-driven guideline for parameterized complexity studies of graph problems, we present the first comprehensive comparison of parameter values for a set of benchmark graphs originating from real-world applications. Our study covers degree-related parameters, such as maximum degree or degeneracy, neighborhood-based parameters such as neighborhood diversity and modular-width, modulator-based parameters such as vertex cover number and feedback vertex set number, and the treewidth of the graphs.Our results may help assess the significance of FPT-running time bounds on the solvability of real-world instances. For example, the vertex cover number $vc$ of $n$-vertex graphs is often only slightly below $n/2$. Thus, a running time bound of $O(2^{vc})$ is only slightly better than a running time bound of $O(1.4^{n})$. In contrast, the treewidth $tw$ is almost always below $n/3$ and often close to $n/9$, making a running time of $O(2^{tw})$ much more practical on real-world instances.We make our implementation and full experimental data openly available. In particular, this provides the first implementations for several graph parameters such as 4-path vertex cover number and vertex integrity."
2509.06928,"The Sum-of-Squares (SoS) hierarchy is a powerful framework for polynomial optimization and proof complexity, offering tight semidefinite relaxations that capture many classical algorithms. Despite its broad applicability, several works have revealed fundamental limitations to SoS automatability. (i) While low-degree SoS proofs are often desirable for tractability, recent works have revealed they may require coefficients of prohibitively large bit size, rendering them computationally infeasible. (ii) Prior works have shown that SoS proofs for seemingly easy problems require high-degree. In particular, this phenomenon also arises in highly symmetric problems. Instances of symmetric problems-particularly those with a small number of constraints-have repeatedly served as benchmarks for establishing high-degree lower bounds in the SoS hierarchy. It has remained unclear whether symmetry can also lead to large bit sizes in SoS proofs, potentially making low-degree proofs computationally infeasible even in symmetric settings.In this work, we resolve this question by proving that symmetry alone does not lead to large bit size SoS proofs. Focusing on symmetric Archimedean instances, we show that low-degree SoS proofs for such systems admit compact, low bit size representations. Together, these results provide a conceptual separation between two sources of SoS hardness-degree and bit size-by showing they do not necessarily align, even in highly symmetric instances. This insight guides future work on automatability and lower bounds: symmetry may necessitate high-degree proofs, but it does not by itself force large coefficients."
2509.08798,"Different variations of alliances in graphs have been introduced into the graph-theoretic literature about twenty years ago. More broadly speaking, they can be interpreted as groups that collaborate to achieve a common goal, for instance, defending themselves against possible attacks from outside. In this paper, we initiate the study of reconfiguring alliances. This means that, with the understanding of having an interconnection map given by a graph, we look at two alliances of the same size~$k$ and investigate if there is a reconfiguration sequence (of length at most~$\ell$) formed by alliances of size (at most)~$k$ that transfers one alliance into the other one. Here, we consider different (now classical) movements of tokens: sliding, jumping, addition/removal. We link the latter two regimes by introducing the concept of reconfiguration monotonicity. Concerning classical complexity, most of these reconfiguration problems are \textsf{PSPACE}-complete, although some are solvable in \textsf{Log\-SPACE}. We also consider these reconfiguration questions through the lense of parameterized algorithms and prove various \textsf{FPT}-results, in particular concerning the combined parameter $k+\ell$ or neighborhood diversity together with $k$ or neighborhood diversity together with $k$."
2509.09657,"We study uniformity conditions for parameterized Boolean circuit families. Uniformity conditions require that the infinitely many circuits in a circuit family are in some sense easy to construct from one shared description. For shallow circuit families, logtime-uniformity is often desired but quite technical to prove. Despite that, proving it is often left as an exercise for the reader -- even for recently introduced classes in parameterized circuit complexity, where uniformity conditions have not yet been explicitly studied. We formally define parameterized versions of linear-uniformity, logtime-uniformity, and FO-uniformity, and prove that these result in equivalent complexity classes when imposed on $\text{para-}\textsf{AC}^0$ and $\text{para-}\textsf{AC}^{0\uparrow}$. Overall, we provide a convenient way to verify uniformity for shallow parameterized circuit classes, and thereby substantiate claims of uniformity in the literature."
2509.10361,"The Vehicle Routing Problem (VRP) is a popular generalization of the Traveling Salesperson Problem. Instead of one salesperson traversing the entire weighted, undirected graph $G$, there are $k$ vehicles available to jointly cover the set of clients $C \subseteq V(G)$. Every vehicle must start at one of the depot vertices $D \subseteq V(G)$ and return to its start. Capacitated Vehicle Routing (CVRP) additionally restricts the route of each vehicle by limiting the number of clients it can cover, the distance it can travel, or both.In this work, we study the complexity of VRP and the three variants of CVRP for several parameterizations, in particular focusing on the treewidth of $G$. We present an FPT algorithm for VRP parameterized by treewidth. For CVRP, we prove paraNP- and $W[\cdot]$-hardness for various parameterizations, including treewidth, thereby rendering the existence of FPT algorithms unlikely. In turn, we provide an XP algorithm for CVRP when parameterized by both treewidth and the vehicle capacity."
2509.10725,"We investigate the closure properties of read-once oblivious Algebraic Branching Programs (roABPs) under various natural algebraic operations and prove the following.- Non-closure under factoring: There is a sequence of explicit polynomials $(f_n(x_1,\ldots, x_n))_n$ that have $\mathsf{poly}(n)$-sized roABPs such that some irreducible factor of $f_n$ does not have roABPs of superpolynomial size in any order.- Non-closure under powering: There is a sequence of polynomials $(f_n(x_1,\ldots, x_n))_n$ with $\mathsf{poly}(n)$-sized roABPs such that any super-constant power of $f_n$ does not have roABPs of polynomial size in any order (and $f_n^n$ requires exponential size in any order).- Non-closure under symmetric compositions: There are symmetric polynomials $(f_n(e_1,\ldots, e_n))_n$ that have roABPs of polynomial size such that $f_n(x_1,\ldots, x_n)$ do not have roABPs of subexponential size. (Here, $e_1,\ldots, e_n$ denote the elementary symmetric polynomials in $n$ variables.)These results should be viewed in light of known results on models such as algebraic circuits, (general) algebraic branching programs, formulas and constant-depth circuits, all of which are known to be closed under these operations.To prove non-closure under factoring, we construct hard polynomials based on expander graphs using gadgets that lift their hardness from sparse polynomials to roABPs. For symmetric compositions, we show that the circulant polynomial requires roABPs of exponential size in every variable order."
2509.10846,"The New York Times (NYT) games have found widespread popularity in recent years and reportedly account for an increasing fraction of the newspaper's readership. In this paper, we bring the computational lens to the study of New York Times games and consider four of them not previously studied: Letter Boxed, Pips, Strands and Tiles. We show that these games can be just as hard as they are fun. In particular, we characterize the hardness of several variants of computational problems related to these popular puzzle games. For Letter Boxed, we show that deciding whether an instance is solvable is in general NP-Complete, while in some parameter settings it can be done in polynomial time. Similarly, for Pips we prove that deciding whether a puzzle has a solution is NP-Complete even in some restricted classes of instances. We then show that one natural computational problem arising from Strands is NP-Complete in most parameter settings. Finally, we demonstrate that deciding whether a Tiles puzzle is solvable with a single, uninterrupted combo requires polynomial time."
2509.11322,"Arithmetic circuits are a natural well-studied model for computing multivariate polynomials over a field. In this paper, we study planar arithmetic circuits. These are circuits whose underlying graph is planar. In particular, we prove an $\Omega(n\log n)$ lower bound on the size of planar arithmetic circuits computing explicit bilinear forms on $2n$ variables. As a consequence, we get an $\Omega(n\log n)$ lower bound on the size of arithmetic formulas and planar algebraic branching programs computing explicit bilinear forms on $2n$ variables. This is the first such lower bound on the formula complexity of an explicit bilinear form. In the case of read-once planar circuits, we show $\Omega(n^2)$ size lower bounds for computing explicit bilinear forms on $2n$ variables. Furthermore, we prove fine separations between the various planar models of computations mentioned above.In addition to this, we look at multi-output planar circuits and show $\Omega(n^{4/3})$ size lower bound for computing an explicit linear transformation on $n$-variables. For a suitable definition of multi-output formulas, we extend the above result to get an $\Omega(n^2/\log n)$ size lower bound. As a consequence, we demonstrate that there exists an $n$-variate polynomial computable by an $n^{1 + o(1)}$-sized formula such that any multi-output planar circuit (resp., multi-output formula) simultaneously computing all its first-order partial derivatives requires size $\Omega(n^{4/3})$ (resp., $\Omega(n^2/\log n)$). This shows that a statement analogous to that of Baur, Strassen (1983) does not hold in the case of planar circuits and formulas."
2509.11349,"We design the first efficient polynomial identity testing algorithms over the nonassociative polynomial algebra. In particular, multiplication among the formal variables is commutative but it is not associative. This complements the strong lower bound results obtained over this algebra by Hrubeš, Yehudayoff, and Wigderson (2010) and Fijalkow, Lagarde, Ohlmann, and Serre (2021) from the identity testing perspective. Our main results are the following:(1) We construct nonassociative algebras (both commutative and noncommutative) which have no low degree identities. As a result, we obtain the first Amitsur-Levitzki type theorems over nonassociative polynomial algebras. As a direct consequence, we obtain randomized polynomial-time black-box PIT algorithms for nonassociative polynomials which allow evaluation over such algebras.(2) On the derandomization side, we give a deterministic polynomial-time identity testing algorithm for nonassociative polynomials given by arithmetic circuits in the white-box setting. Previously, such an algorithm was known with the additional restriction of noncommutativity.(3) In the black-box setting, we construct a hitting set of quasipolynomial-size for nonassociative polynomials computed by arithmetic circuits of small depth. Understanding the black-box complexity of identity testing, even in the randomized setting, was open prior to our work."
2509.11399,"We show a dichotomy result for $p$-pass streaming algorithms for all CSPs and for up to polynomially many passes. More precisely, we prove that for any arity parameter $k$, finite alphabet $\Sigma$, collection $\mathcal{F}$ of $k$-ary predicates over $\Sigma$ and any $c\in (0,1)$, there exists $0<s\leq c$ such that:1. For any $\varepsilon>0$ there is a constant pass, $O_{\varepsilon}(\log n)$-space randomized streaming algorithm solving the promise problem $\text{MaxCSP}(\mathcal{F})[c,s-\varepsilon]$. That is, the algorithm accepts inputs with value at least $c$ with probability at least $2/3$, and rejects inputs with value at most $s-\varepsilon$ with probability at least $2/3$.2. For all $\varepsilon>0$, any $p$-pass (even randomized) streaming algorithm that solves the promise problem $\text{MaxCSP}(\mathcal{F})[c,s+\varepsilon]$ must use $\Omega_{\varepsilon}(n^{1/3}/p)$ space.Our approximation algorithm is based on a certain linear-programming relaxation of the CSP and on a distributed algorithm that approximates its value. This part builds on the works [Yoshida, STOC 2011] and [Saxena, Singer, Sudan, Velusamy, SODA 2025]. For our hardness result we show how to translate an integrality gap of the linear program into a family of hard instances, which we then analyze via studying a related communication complexity problem. That analysis is based on discrete Fourier analysis and builds on a prior work of the authors and on the work [Chou, Golovnev, Sudan, Velingker, Velusamy,this http URL2024]."
2509.1312,"We give a new, elementary proof of what we believe is the simplest known example of a ``natural'' problem in computational 3-dimensional topology that is $\mathsf{NP}$-hard -- namely, the \emph{Trivial Sublink Problem}: given a diagram $L$ of a link in $S^3$ and a positive integer $k$, decide if $L$ contains a $k$ component sublink that is trivial. This problem was previously shown to be $\mathsf{NP}$-hard in independent works of Koenig-Tsvietkova and de Mesmay-Rieck-Sedgwick-Tancer, both of which used reductions from $\mathsf{3SAT}$. The reduction we describe instead starts with the Independent Set Problem, and allows us to avoid the use of Brunnian links such as the Borromean rings. On the technical level, this entails a new conceptual insight: the Trivial Sublink Problem is hard entirely due to mod 2 pairwise linking, with no need for integral or higher order linking. On the pedagogical level, the reduction we describe is entirely elementary, and thus suitable for introducing undergraduates and non-experts to complexity-theoretic low-dimensional topology. To drive this point home, in this work we assume no familiarity with low-dimensional topology, and -- other than Reidemeister's Theorem and Karp's result that the Clique Problem is $\mathsf{NP}$-hard -- we provide more-or-less complete definitions and proofs. We have also constructed a web app that accompanies this work and allows a user to visualize the new reduction interactively."
2509.13238,"The complexity of representing a polynomial by a Read-Once Oblivious Algebraic Branching Program (ROABP) is highly dependent on the chosen variable ordering. Bhargava et al. prove that finding the optimal ordering is NP-hard, and provide some evidence (based on the Small Set Expansion hypothesis) that it is also hard to approximate the optimal ROABP width. In another work, Baraskar et al. show that it is NP-hard to test whether a polynomial is in the $\mathrm{GL}_n$ orbit of a polynomial of sparsity at most $s$. Building upon these works, we show the following results: first, we prove that approximating the minimum ROABP width up to any constant factor is NP-hard, when the input is presented as a circuit. This removes the reliance on stronger conjectures in the previous work. Second, we show that testing if an input polynomial given in the sparse representation is in the affine $\mathrm{GL}_n$ orbit of a width-$w$ ROABP is NP-hard. Furthermore, we show that over fields of characteristic $0$, the problem is NP-hard even when the input polynomial is homogeneous. This provides the first NP-hardness results for membership testing for a dense subclass of polynomial sized algebraic branching programs (VBP). Finally, we locate the source of hardness for the order finding problem at the lowest possible non-trivial degree, proving that the problem is NP-hard even for quadratic forms."
2509.13966,"Bit addition arises virtually everywhere in digital circuits: arithmetic operations, increment/decrement operators, computing addresses and table indices, and so on. Since bit addition is such a basic task in Boolean circuit synthesis, a lot of research has been done on constructing efficient circuits for various special cases of it. A vast majority of these results are devoted to optimizing the circuit depth (also known as delay).In this paper, we investigate the circuit size (also known as area) over the full binary basis of bit addition. Though most of the known circuits are built from Half Adders and Full Adders, we show that, in many interesting scenarios, these circuits have suboptimal size. Namely, we improve an upper bound $5n-3m$ to $4.5n-2m$, where $n$ is the number of input bits and $m$ is the number of output bits. In the regimes where $m$ is small compared to $n$ (for example, for computing the sum of $n$ bits or multiplying two $n$-bit integers), this leads to $10\%$ improvement.We complement our theoretical result by an open-source implementation of generators producing circuits for bit addition and multiplication. The generators allow one to produce the corresponding circuits in two lines of code and to compare them to existing designs."
2509.14305,"We introduce a model-agnostic MDL-style cost functional $K_C$ for resource-bounded classifiers and prove a Total-Variation stable reduction lemma ($A2^d$) for distribution-preserving many-to-one reductions. On a balanced distribution of random 3XOR instances (with co-rank $t'=\Theta(n)$) we obtain a size-aware lower bound against P-uniform AC^0+log models: $\Pr[M=\chi] \le \frac{1}{2} + s(N)\exp(-\alpha_d m^{c/d})$ with an absolute $c \in (0,1)$ (e.g., $c=1/3$ gives $\beta_d=1/(3d)$). A deterministic, injective 3XOR->3SAT translation (four 3-clauses per XOR, no auxiliaries) is $\delta=0$ measure-preserving on its image window; by $A2^d$ the bound transfers to 3SAT. This yields, to our knowledge, the first explicit $K_C$-reading of such size-aware bounds under a $\delta=0$ measure-preserving reduction in small-depth circuit lower bounds. We provide artifacts (generator -> DIMACS -> verification) with match-rate 1.0."
2509.14807,"We study the complexity of counting and finding small tournament patterns inside large tournaments. Given a fixed tournament $T$ of order $k$, we write ${\#}\text{IndSub}_{\text{To}}(\{T\})$ for the problem whose input is a tournament $G$ and the task is to compute the number of subtournaments of $G$ that are isomorphic to $T$. Previously, Yuster [Yus25] obtained that ${\#}\text{IndSub}_{\text{To}}(\{T\})$ is hard to compute for random tournaments $T$. We consider a new approach that uses linear combinations of subgraph-counts [CDM17] to obtain a finer analysis of the complexity of ${\#}\text{IndSub}_{\text{To}}(\{T\})$.We show that for all tournaments $T$ of order $k$ the problem ${\#}\text{IndSub}_{\text{To}}(\{T\})$ is always at least as hard as counting $\lfloor 3k/4 \rfloor$-cliques. This immediately yields tight bounds under ETH. Further, we consider the parameterized version of ${\#}\text{IndSub}_{\text{To}}(\mathcal{T})$ where we only consider patterns $T \in \mathcal{T}$ and that is parameterized by the pattern size $|V(T)|$. We show that ${\#}\text{IndSub}_{\text{To}}(\mathcal{T})$ is ${\#}W[1]$-hard as long as $\mathcal{T}$ contains infinitely many tournaments."
2509.16065,"In this article we investigate the computational complexity of predicting two dimensional freezing majority cellular automata with states $\{-1,+1\}$, where the local interactions are based on an L-shaped neighborhood structure. In these automata, once a cell reaches state $+1$, it remains fixed in that state forever, while cells in state $-1$ update to the most represented state among their neighborhoods. We consider L-shaped neighborhoods, which mean that the vicinity of a given cell $c$ consists in a subset of cells in the north and east of $c$.We focus on the prediction problem, a decision problem that involves determining the state of a given cell after a given number of time-steps. We prove that when restricted to the simplest L-shaped neighborhood, consisting of the central cell and its nearest north and east neighbors, the prediction problem belongs to $\mathsf{NC}$, meaning it can be solved efficiently in parallel. We generalize this result for any L-shaped neighborhood of size two. On the other hand, for other L-shaped neighborhoods, the problem becomes $\mathsf{P}$-complete, indicating that the problem might be inherently sequential."
2509.16824,"We study whether lower bounds against constant-depth algebraic circuits computing the Permanent over finite fields (Limaye-Srinivasan-Tavenas, J. ACM 2025; Forbes, CCC 2024) are hard to prove in certain proof systems. We focus on a DNF formula that expresses that such lower bounds are hard for constant-depth algebraic proofs. Using an adaptation of the diagonalization framework of Santhanam and Tzameret (SIAM J. Comput. 2025), we show unconditionally that this family of DNF formulas does not admit polynomial-size propositional AC0[p]-Frege proofs infinitely often. This rules out the possibility that the DNF family is easy, and establishes that its status is either that of a hard tautology for AC0[p]-Frege or else unprovable (not a tautology). While it remains open whether the DNFs in question are tautologies, we provide evidence in this direction. In particular, under the plausible assumption that certain weak properties of multilinear algebra, specifically those involving tensor rank, do not admit short constant-depth algebraic proofs, the DNFs are tautologies. We also observe that several weaker variants of the DNF formula are provably tautologies, and we show that the question of whether the DNFs are tautologies connects to conjectures of Razborov (ICALP 1996) and Krajicek (J. Symb. Log. 2004). Our result has two additional features. (i) Existential depth amplification: the DNF formula is parameterised by a constant depth d bounding the depth of the algebraic proofs. We show that there exists some fixed depth d such that if there are no small depth-d algebraic proofs of certain circuit lower bounds for the Permanent, then there are no such small algebraic proofs in any constant depth. (ii) Necessity: we show that our result is a necessary step towards establishing lower bounds against constant-depth algebraic proofs, and more generally against any sufficiently strong proof system."
2509.17926,"We identify a connection between the approximability of CSPs in two models: (i) sublinear space streaming algorithms, and (ii) the basic LP relaxation. We show that whenever the basic LP admits an integrality gap, there is an $\Omega(\sqrt{n})$-space sketching lower bound. We also show that all existing linear space streaming lower bounds for Max-CSPs can be lifted to integrality gap instances for basic LPs. For bounded-degree graphs, by combining the distributed algorithm of Yoshida (STOC 2011) for approximately solving the basic LP with techniques described in Saxena, Singer, Sudan, and Velusamy (SODA 2025) for simulating a distributed algorithm by a sublinear space streaming algorithm on bounded-degree instances of Max-DICUT, it appears that there are sublinear space streaming algorithms implementing the basic LP, for every CSP.Based on our results, we conjecture the following dichotomy theorem: Whenever the basic LP admits an integrality gap, there is a linear space single-pass streaming lower bound, and when the LP is roundable, there is a sublinear space streaming algorithm."
2509.17994,"We prove that every randomized Boolean function admits a supersimulator: a randomized polynomial-size circuit whose output on random inputs cannot be efficiently distinguished from reality with constant advantage, even by polynomially larger distinguishers. Our result builds on the landmark complexity-theoretic regularity lemma of Trevisan, Tulsiani and Vadhan (2009), which, in contrast, provides a simulator that fools smaller distinguishers. We circumvent lower bounds for the simulator size by letting the distinguisher size bound vary with the target function, while remaining below an absolute upper bound independent of the target function. This dependence on the target function arises naturally from our use of an iteration technique originating in the graph regularity literature.The simulators provided by the regularity lemma and recent refinements thereof, known as multiaccurate and multicalibrated predictors, respectively, as per Hebert-Johnson et al. (2018), have previously been shown to have myriad applications in complexity theory, cryptography, learning theory, and beyond. We first show that a recent multicalibration-based characterization of the computational indistinguishability of product distributions actually requires only (calibrated) multiaccuracy. We then show that supersimulators yield an even tighter result in this application domain, closing a complexity gap present in prior versions of the characterization."
2509.19161,"Classical circuit complexity characterizes parallel computation in purely combinatorial terms, ignoring the physical constraints that govern real hardware. The standard classes $\mathbf{NC}$, $\mathbf{AC}$, and $\mathbf{TC}$ treat unlimited fan-in, free interconnection, and polynomial gate counts as feasible -- assumptions that conflict with geometric, energetic, and thermodynamic realities. We introduce the family of \textit{realizable circuit classes} $\mathbf{RC}_d$, which model computation embedded in physical $d$-dimensional space. Each circuit in $\mathbf{RC}_d$ obeys conservative realizability laws: volume scales as $\mathcal{O}(t^d)$, cross-boundary information flux is bounded by $\mathcal{O}(t^{d-1})$ per unit time, and growth occurs through local, physically constructible edits. These bounds apply to all causal systems, classical or quantum. Within this framework, we show that algorithms with runtime $\omega(n^{d/(d-1)})$ cannot scale to inputs of maximal entropy, and that any $d$-dimensional parallel implementation offers at most a polynomial speed-up of degree $(d-1)$ over its optimal sequential counterpart. In the limit $d\to\infty$, $\mathbf{RC}_\infty(\mathrm{polylog})=\mathbf{NC}$, recovering classical parallelism as a non-physical idealization. By unifying geometry, causality, and information flow, $\mathbf{RC}_d$ extends circuit complexity into the physical domain, revealing universal scaling laws for computation."
2509.22004,"Similarly to the Chomsky hierarchy, we offer a classification of communication complexity measures such that these measures are organized into equivalence classes. Different from previous attempts of this endeavor, we consider two communication complexity measures as equivalent, if, when one is constant, then the other is constant as well, and vice versa. Most previous considerations of similar topics have been using polylogarithmic input length as a defining characteristic of equivalence. In this paper, two measures ${\cal C}_1, {\cal C}_2$ are constant-equivalent, if and only if for all total Boolean (families of) functions $f:\{0, 1\}^n\times\{0, 1\}^n\rightarrow \{0, 1\}$ we have ${\cal C}_1(f)=O(1)$ if and only if ${\cal C}_2(f)=O(1)$. We identify five equivalence classes according to the above equivalence relation. Interestingly, the classification is counter-intuitive in that powerful models of communication are grouped with weak ones, and seemingly weaker models end up on the top of the hierarchy."
2509.22849,"Neural networks with ReLU activations are a widely used model in machine learning. It is thus important to have a profound understanding of the properties of the functions computed by such networks. Recently, there has been increasing interest in the (parameterized) computational complexity of determining these properties. In this work, we close several gaps and resolve an open problem posted by Froese et al. [COLT '25] regarding the parameterized complexity of various problems related to network verification. In particular, we prove that deciding positivity (and thus surjectivity) of a function $f\colon\mathbb{R}^d\to\mathbb{R}$ computed by a 2-layer ReLU network is W[1]-hard when parameterized by $d$. This result also implies that zonotope (non-)containment is W[1]-hard with respect to $d$, a problem that is of independent interest in computational geometry, control theory, and robotics. Moreover, we show that approximating the maximum within any multiplicative factor in 2-layer ReLU networks, computing the $L_p$-Lipschitz constant for $p\in(0,\infty]$ in 2-layer networks, and approximating the $L_p$-Lipschitz constant in 3-layer networks are NP-hard and W[1]-hard with respect to $d$. Notably, our hardness results are the strongest known so far and imply that the naive enumeration-based methods for solving these fundamental problems are all essentially optimal under the Exponential Time Hypothesis."
2509.23615,"A Roman $\{3\}$-dominating function on a graph $G = (V, E)$ is a function $f: V \rightarrow \{0, 1, 2, 3\}$ such that for each vertex $u \in V$, if $f(u) = 0$ then $\sum_{v \in N(u)} f(v) \geq 3$ and if $f(u) = 1$ then $\sum_{v \in N(u)} f(v) \geq 2$. The weight of a Roman $\{3\}$-dominating function $f$ is $\sum_{u \in V} f(u)$. The objective of \rtd{} is to compute a Roman $\{3\}$-dominating function of minimum weight. The problem is known to be NP-complete on chordal graphs, star-convex bipartite graphs and comb-convex bipartite graphs. In this paper, we study the complexity of \rtd{} and show that the problem is NP-complete on split graphs. In addition, we prove that the problem is W[2]-hard parameterized by weight. On the positive front, we present a polynomial-time algorithm for block graphs, thereby resolving an open question posed by Chaudhary and Pradhan [Discrete Applied Mathematics, 2024]."
2509.2428,"We prove an entropy versus degree dichotomy for low-degree tests and the Sum-of-Squares (SoS) hierarchy on a calibrated window after a gadget layer. For a target distribution \(\mu\) and a product-like proxy \(u\), we study the low-degree discrepancy \(\Delta_k(\mu,u)\), defined as the optimal distinguishing advantage of degree \(\le k\) polynomial tests. Using a bias-orthonormal Walsh basis and a test-moment equivalence on the window, we relate \(\Delta_k\) (up to constants) to the squared \(\ell_2\) mass of signed low-degree moments. Calibrated pseudoexpectations match \(u\) on all moments of degree \(\le k\), hence test discrepancy equals SoS pseudoexpectation deviation. Under bias, product, and width assumptions along a switching path, a windowed Bonami--Beckner inequality yields hypercontractive tail bounds. Combining these with moment matching, we obtain a discrepancy-to-degree theorem: if \(\Delta_k(\mu,u) \ge n^{-\beta}\), then any polynomial-calculus or SoS refutation separating \(\mu\) from \(u\) requires degree \(\Omega(k)\). Instantiating \(k = c \log n\) gives an explicit \(\Omega(\log n)\) SoS degree lower bound whenever \(\Delta_k \ge n^{-\eta}\). All constants are explicit and depend only on calibrated window parameters. This work provides the SoS/low-degree core and complements a prior calibration blueprint; a companion paper lifts the windowed statements to full distribution families."
2509.26248,"In pursuit of a deeper understanding of Boolean Promise Constraint Satisfaction Problems (PCSPs), we identify a class of problems with restricted structural complexity, which could serve as a promising candidate for complete characterization. Specifically, we investigate the class of PCSPs whose polymorphisms are Polynomial Threshold Functions (PTFs) of bounded degree. We obtain two complexity characterization results: (1) with a hardness condition introduced in [ACMTCT'21], we establish a complete complexity dichotomy in the case where coefficients of PTF representations are non-negative; (2) dropping the non-negativity assumption, we show a hardness result for PTFs admitting coordinates with significant influence, conditioned on the Rich 2-to-1 Conjecture proposed in [ITCS'21]. In order to prove the latter, we show that a random 2-to-1 minor map retains significant coordinate influence over the $p$-biased hypercube with constant probability."
2510.01701,"We study certificates of positivity for univariate polynomials with rational coefficients that are positive over (an interval of) $\mathbb{R}$, given as weighted sums of squares (SOS) of rational polynomials. We build on the algorithm of Chevillard, Harrison, Joldes, and Lauter~\cite{chml-usos-alg-11}, which we call \usos. For a polynomial of degree~$d$ and coefficient bitsize~$\tau$, we show that a rational weighted SOS representation can be computed in $\widetilde{\mathcal{O}}_B(d^3 + d^2 \tau)$ bit operations, and the certificate has bitsize $\widetilde{\mathcal{O}}(d^2 \tau)$. This improves the best-known bounds by a factor~$d$ and completes previous analyses. We also extend the method to positivity over arbitrary rational intervals, again saving a factor~$d$. For univariate rational polynomials we further introduce \emph{perturbed SOS certificates}. These consist of a sum of two rational squares approximating the input polynomial so that nonnegativity of the approximation implies that of the original. Their computation has the same bit complexity and certificate size as in the weighted SOS case. We also investigate structural properties of these SOS decompositions. Using the classical fact that any nonnegative univariate real polynomial is a sum of two real squares, we prove that the summands form an interlacing pair. Their real roots correspond to the Karlin points of the original polynomial, linking our construction to the T-systems of Karlin~\cite{Karlin-repr-pos-63}. This enables explicit computation of such decompositions, whereas only existential results were previously known. We obtain analogous results for positivity over $(0,\infty)$ and thus over arbitrary real intervals. Finally, we present an open-source Maple implementation of \usos and report experiments on diverse inputs that demonstrate its efficiency."
2510.0256,"We introduce and investigate the computational complexity of a novel physical problem known as the Pinball Wizard problem. It involves an idealized pinball moving through a maze composed of one-way gates (outswing doors), plane walls, parabolic walls, moving plane walls, and bumpers that cause acceleration or deceleration. Given the initial position and velocity of the pinball, the task is to decide whether it will hit a specified target point.By simulating a two-stack pushdown automaton, we show that the problem is Turing-complete -- even in two-dimensional space. In our construction, each step of the automaton corresponds to a constant number of reflections. Thus, deciding the Pinball Wizard problem is at least as hard as the Halting problem. Furthermore, our construction allows bumpers to be replaced with moving walls. In this case, even a ball moving at constant speed -- a so-called ray particle -- can be used, demonstrating that the Ray Particle Tracing problem is also Turing-complete."
2510.02583,"The log-rank conjecture is a longstanding open problem with multiple equivalent formulations in complexity theory and mathematics. In its linear-algebraic form, it asserts that the rank and partitioning number of a Boolean matrix are quasi-polynomially related.We propose a relaxed but still equivalent version of the conjecture based on a new matrix parameter, signed rectangle rank: the minimum number of all-1 rectangles needed to express the Boolean matrix as a $\pm 1$-sum. Signed rectangle rank lies between rank and partition number, and our main result shows that it is in fact equivalent to rank up to a logarithmic factor. Additionally, we extend the main result to tensors. This reframes the log-rank conjecture as: can every signed decomposition of a Boolean matrix be made positive with only quasi-polynomial blowup?As an application, we prove an equivalence between the log-rank conjecture and a conjecture of Lovett and Singer-Sudan on cross-intersecting set systems."
2510.03143,"We investigate the complexity of stable (or perturbation-resilient) instances of $\mathrm{k-M\small{EANS}}$ and $\mathrm{k-M\small{EDIAN}}$ clustering problems in metrics with small doubling dimension. While these problems have been extensively studied under multiplicative perturbation resilience in low-dimensional Euclidean spaces (e.g., (Friggstad et al., 2019; Cohen-Addad and Schwiegelshohn, 2017)), we adopt a more general notion of stability, termed ``almost stable'', which is closer to the notion of $(\alpha, \varepsilon)$-perturbation resilience introduced by Balcan and Liang (2016). Additionally, we extend our results to $\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ with penalties, where each data point is either assigned to a cluster centre or incurs a penalty.We show that certain special cases of almost stable $\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ (with penalties) are solvable in polynomial time. To complement this, we also examine the hardness of almost stable instances and $(1 + \frac{1}{poly(n)})$-stable instances of $\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ (with penalties), proving super-polynomial lower bounds on the runtime of any exact algorithm under the widely believed Exponential Time Hypothesis (ETH)."
2510.04418,"A homeomorphically irreducible spanning tree (HIST) is a spanning tree with no degree-2 vertices, serving as a structurally minimal backbone of a graph. While the existence of HISTs has been widely studied from a structural perspective, the algorithmic complexity of finding them remains less understood. In this paper, we provide a comprehensive investigation of the HIST problem from both structural and algorithmic viewpoints. We present a simple characterization that precisely describes which chordal graphs of diameter at most~3 admit a HIST, leading to a polynomial-time decision procedure for this class. In contrast, we show that the problem is NP-complete for strongly chordal graphs of diameter~4. From the perspective of parameterized complexity, we establish that the HIST problem is W[1]-hard when parameterized by clique-width, indicating that the problem is unlikely to be efficiently solvable in general dense graphs. On the other hand, we present fixed-parameter tractable (FPT) algorithms when parameterized by treewidth, modular-width, or cluster vertex deletion number. Specifically, we develop an $O^*(4^{k})$-time algorithm parameterized by modular-width~$k$, and an FPT algorithm parameterized by the cluster vertex deletion number based on kernelization techniques that bound clique sizes while preserving the existence of a HIST. These results together provide a clearer understanding of the structural and computational boundaries of the HIST problem."
2510.0487,"A fixed set of vertices in the plane may have multiple planar straight-line triangulations in which the degree of each vertex is the same. As such, the degree information does not completely determine the triangulation. We show that even if we know, for each vertex, the number of neighbors in each of the four cardinal directions, the triangulation is not completely determined. In fact, we show that counting such triangulations is #P-hard via a reduction from #3-regular bipartite planar vertex cover."
2510.05927,"We initiate a systematic study of the computational complexity of property testing, focusing on the relationship between query and time complexity. While traditional work in property testing has emphasized query complexity, relatively little is known about the computational hardness of property testers. Our goal is to chart the landscape of time-query interplay and develop tools for proving time complexity lower bounds. Our first contribution is a pair of time-query hierarchy theorems for property testing. For all suitable nondecreasing functions $q(n)$ and $t(n)$ with $t(n)\geq q(n)$, we construct properties with query complexity $\tilde{\Theta}(q(n))$ and time complexity $\tilde\Omega(t(n))$. Our weak hierarchy holds unconditionally, whereas the strong version-assuming the Strong Exponential Time Hypothesis-provides better control over the time complexity of the constructed properties.We then turn to halfspaces in $\mathbb{R}^d$, a fundamental class in property testing and learning theory. We study the problem of approximating the distance from the input function to the nearest halfspace within additive error $\epsilon$. For the distribution-free distance approximation problem, known algorithms achieve query complexity $O(d/\epsilon^2)$, but take time $\tilde{\Theta}(1/\epsilon^d)$. We provide a fine-grained justification for this gap: assuming the $k$-SUM conjecture, any algorithm must have running time ${\Omega}(1/\epsilon^{d/2})$. This fine-grained lower bound yields a provable separation between query and time complexity for a natural and well-studied (tolerant) testing problem. We also prove that any Statistical Query (SQ) algorithm under the standard Gaussian distribution requires $(1/\epsilon)^{\Omega(d)}$ queries if the queries are answered with additive error up to $\epsilon^{\Omega(d)}$, revealing a fundamental barrier even in the distribution-specific setting."
2510.07808,"We construct a family of distributions $\{\mathcal{D}_n\}_n$ with $\mathcal{D}_n$ over $\{0, 1\}^n$ and a family of depth-$7$ quantum circuits $\{C_n\}_n$ such that $\mathcal{D}_n$ is produced exactly by $C_n$ with the all zeros state as input, yet any constant-depth classical circuit with bounded fan-in gates evaluated on any binary product distribution has total variation distance $1 - e^{-\Omega(n)}$ from $\mathcal{D}_n$. Moreover, the quantum circuits we construct are geometrically local and use a relatively standard gate set: Hadamard, controlled-phase, CNOT, and Toffoli gates. All previous separations of this type suffer from some undesirable constraint on the classical circuit model or the quantum circuits witnessing the separation.Our family of distributions is inspired by the Parity Halving Problem of Watts, Kothari, Schaeffer, and Tal (STOC, 2019), which built on the work of Bravyi, Gosset, and König (Science, 2018) to separate shallow quantum and classical circuits for relational problems."
2510.08185,"We show that if k-SUM is hard, in the sense that the standard algorithm is essentially optimal, then a variant of the SETH called the Primal Treewidth SETH is true. Formally: if there is an $\varepsilon>0$ and an algorithm which solves SAT in time $(2-\varepsilon)^{tw}|\phi|^{O(1)}$, where $tw$ is the width of a given tree decomposition of the primal graph of the input, then there exists a randomized algorithm which solves k-SUM in time $n^{(1-\delta)\frac{k}{2}}$ for some $\delta>0$ and all sufficiently large $k$. We also establish an analogous result for the k-XOR problem, where integer addition is replaced by component-wise addition modulo $2$.As an application of our reduction we are able to revisit tight lower bounds on the complexity of several fundamental problems parameterized by treewidth (Independent Set, Max Cut, $k$-Coloring). Our results imply that these bounds, which were initially shown under the SETH, also hold if one assumes the k-SUM or k-XOR Hypotheses, arguably increasing our confidence in their validity."
2510.08577,"We introduce Psi-Turing Machines (Psi-TM): classical Turing machines equipped with a constant-depth introspection interface $ \iota $ and an explicit per-step information budget $ B(d,n)=c\,d\log_2 n $. With the interface frozen, we develop an information-theoretic lower-bound toolkit: Budget counting, $ \Psi $-Fooling, and $ \Psi $-Fano, with worked examples $ L_k $ and $ L_k^{\mathrm{phase}} $. We prove an oracle-relative separation $ P^{\Psi} \neq NP^{\Psi} $ and a strict depth hierarchy, reinforced by an Anti-Simulation Hook that rules out polynomial emulation of $ \iota_k $ using many calls to $ \iota_{k-1} $ under the budget regime. We also present two independent platforms (Psi-decision trees and interface-constrained circuits IC-AC$^{0}$/IC-NC$^{1}$) and bridges that transfer bounds among machine, tree, and circuit with explicit poly/log losses. The model preserves classical computational power outside $ \iota $ yet enables precise oracle-aware statements about barriers (relativization; partial/conditional progress on natural proofs and proof complexity). The aim is a standardized minimal introspection interface with clearly accounted information budgets."
2510.08814,"We give a compositional, information-theoretic framework that turns short programs into locality on many independent blocks, and combine it with symmetry and sparsity of masked random Unique-SAT to obtain distributional lower bounds that contradict the self-reduction upper bound under $\mathsf{P}=\mathsf{NP}$. We work in the weakness quantale $w_Q=K_{\mathrm{poly}}(\cdot\mid\cdot)$. For an efficiently samplable ensemble $D_m$ made by masking random $3$-CNFs with fresh $S_m\ltimes(\mathbb{Z}_2)^m$ symmetries and a small-seed Valiant--Vazirani isolation layer, we prove a Switching-by-Weakness normal form: for any polytime decoder $P$ of description length $\le \delta t$ (with $t=\Theta(m)$ blocks), a short wrapper $W$ makes $(P\circ W)$ per-bit local on a $\gamma$-fraction of blocks. Two ingredients then force near-randomness on $\Omega(t)$ blocks for every short decoder: (a) a sign-invariant neutrality lemma giving $\Pr[X_i=1\mid \mathcal{I}]=1/2$ for any sign-invariant view $\mathcal{I}$; and (b) a template sparsification theorem at logarithmic radius showing that any fixed local rule appears with probability $m^{-\Omega(1)}$. Combined with single-block bounds for tiny $\mathrm{ACC}^0$/streaming decoders, this yields a success bound $2^{-\Omega(t)}$ and, by Compression-from-Success, $K_{\mathrm{poly}}\big((X_1,\ldots,X_t)\mid(\Phi_1,\ldots,\Phi_t)\big)\ge \eta t$. If $\mathsf{P}=\mathsf{NP}$, a uniform constant-length program maps any on-promise instance to its unique witness in polytime (bit fixing via a $\mathrm{USAT}$ decider), so $K_{\mathrm{poly}}(X\mid\Phi)\le O(1)$ and the tuple complexity is $O(1)$, contradicting the linear bound. The proof is non-relativizing and non-natural; symmetry, sparsification, and switching yield a quantale upper-lower clash, hence $\mathsf{P}\ne\mathsf{NP}$."
2510.09808,"This paper develops a compact, size-aware blueprint for transferring structure through gadget lifts. Two low-order invariants -- cumulative mod-$q$ Fourier mass up to degree $k$ and noise stability $\mathrm{Stab}_\rho$ -- are treated as a reusable ""profile"" tied to the gadget's affine interface. Under coordinate permutations ($\Delta=1$) the profile is preserved exactly; under bounded fan-in the degree budget relaxes by at most $+\Delta k$ (i.e., $k \mapsto k + \Delta k$), with all overheads tracked explicitly. In a balanced window $m=(1+\gamma)n$ the framework yields a distributional lower bound for a monotone cost (Erasure Complexity, EC) and an ""echo"" to correlation against size-aware $\mathrm{AC}^0{+}\log$ and to logarithmic degree in the polynomial-calculus setting. The accounting keeps total-variation non-expansion and a single $O(\log N)$ prefix-free header visible end to end, avoiding hidden slack."
2510.103,"The regulator theorem states that, under certain conditions, any optimal controller must embody a model of the system it regulates, grounding the idea that controllers embed, explicitly or implicitly, internal models of the controlled. This principle underpins neuroscience and predictive brain theories like the Free-Energy Principle or Kolmogorov/Algorithmic Agent theory. However, the theorem is only proven in limited settings. Here, we treat the deterministic, closed, coupled world-regulator system $(W,R)$ as a single self-delimiting program $p$ via a constant-size wrapper that produces the world output string~$x$ fed to the regulator. We analyze regulation from the viewpoint of the algorithmic complexity of the output, $K(x)$. We define $R$ to be a \emph{good algorithmic regulator} if it \emph{reduces} the algorithmic complexity of the readout relative to a null (unregulated) baseline $\varnothing$, i.e., \[ \Delta = K\big(O_{W,\varnothing}\big) - K\big(O_{W,R}\big) > 0. \] We then prove that the larger $\Delta$ is, the more world-regulator pairs with high mutual algorithmic information are favored. More precisely, a complexity gap $\Delta > 0$ yields \[ \Pr\big((W,R)\mid x\big) \le C\,2^{\,M(W{:}R)}\,2^{-\Delta}, \] making low $M(W{:}R)$ exponentially unlikely as $\Delta$ grows. This is an AIT version of the idea that ``the regulator contains a model of the world.'' The framework is distribution-free, applies to individual sequences, and complements the Internal Model Principle. Beyond this necessity claim, the same coding-theorem calculus singles out a \emph{canonical scalar objective} and implicates a \emph{planner}. On the realized episode, a regulator behaves \emph{as if} it minimized the conditional description length of the readout."
2510.10714,"In this column, we overview recent progress by many authors on understanding the approximability of constraint satisfaction problems (CSPs) in low-space streaming models. Inspired by this recent progress, we collate nine conjectural lower bounds against streaming algorithms for CSPs, some of which appear here for the first time."
2510.12005,"In the standard model of computing multi-output functions in logspace ($\mathsf{FL}$), we are given a read-only tape holding $x$ and a logarithmic length worktape, and must print $f(x)$ to a dedicated write-only tape. However, there has been extensive work (both in theory and in practice) on algorithms that transform $x$ into $f(x)$ in-place on a single read-write tape with limited (in our case $O(\log n)$) additional workspace. We say $f\in \mathsf{inplaceFL}$ if $f$ can be computed in this model. We initiate the study of in-place computation from a structural complexity perspective, proving upper and lower bounds on the power of $\mathsf{inplaceFL}$. We show the following:i) Unconditionally, $\mathsf{FL}\not\subseteq \mathsf{inplaceFL}$.ii) The problems of integer multiplication and evaluating $\mathsf{NC}^0_4$ circuits lie outside $\mathsf{inplaceFL}$ under cryptographic assumptions. However, evaluating $\mathsf{NC}^0_2$ circuits can be done in $\mathsf{inplaceFL}$.iii) We have $\mathsf{FL} \subseteq \mathsf{inplaceFL}^{\mathsf{STP}}.$ Consequently, proving $\mathsf{inplaceFL} \not\subseteq \mathsf{FL}$ would imply $\mathsf{SAT} \not\in \mathsf{L}$.We also consider the analogous catalytic class ($\mathsf{inplaceFCL}$), where the in-place algorithm has a large additional worktape tape that it must reset at the end of the computation. We give $\mathsf{inplaceFCL}$ algorithms for matrix multiplication and inversion over polynomial-sized finite fields. We furthermore use our results and techniques to show two novel barriers to proving $\mathsf{CL} \subseteq \mathsf{P}$. First, we show that any proof of $\mathsf{CL}\subseteq \mathsf{P}$ must be non-relativizing, by giving an oracle relative to which $\mathsf{CL}^O=\mathsf{EXP}^O$. Second, we identify a search problem in $\mathsf{searchCL}$ but not known to be in $\mathsf{P}$."
2510.12112,"In permutation inversion, we are given a permutation $\pi : [N] \rightarrow [N]$, and want to prepare some advice of size $S$, such that we can efficiently invert any image in time $T$. This is a fundamental cryptographic problem with profound connections to communication complexity and circuit lower bounds.In the classical setting, a tight $ST = \tilde{\Theta}(N)$ bound has been established since the seminal work of Hellman (1980) and Yao (1990). In the quantum setting, a lower bound of $ST^2 = \tilde{\Omega}(N)$ is proved by Nayebi, Aaronson, Belovs, and Trevisan (2015) against classical advice, and by Hhan, Xagawa and Yamakawa (2019) against quantum advice. It left open an intriguing possibility that Grover's search can be sped up to time $\tilde{O}(\sqrt{N / S})$.In this work, we prove an $ST + T^2 = \Omega(N)$ lower bound for permutation inversion with even quantum advice. This bound matches the best known attacks and shows that Grover's search and the classical Hellman's algorithm cannot be further sped up.Our proof combines recent techniques by Liu (2023) and by Rosmanis (2022). Specifically, we first reduce the permutation inversion problem against quantum advice to a variant by Liu's technique, then we analyze this variant via representation theory inspired by Rosmanis (2022)."
2510.13049,"Border complexity captures functions that can be approximated by low-complexity ones. Debordering is the task of proving an upper bound on some non-border complexity measure in terms of a border complexity measure, thus getting rid of limits. Debordering lies at the heart of foundational complexity theory questions relating Valiant's determinant versus permanent conjecture (1979) and its geometric complexity theory (GCT) variant proposed by Mulmuley and Sohoni (2001). The debordering of matrix multiplication tensors by Bini (1980) played a pivotal role in the development of efficient matrix multiplication algorithms. Consequently, debordering finds applications in both establishing computational complexity lower bounds and facilitating algorithm design. Recent years have seen notable progress in debordering various restricted border complexity measures. In this survey, we highlight these advances and discuss techniques underlying them."
2510.13268,"Growing demand for sustainable logistics and higher space utilization, driven by e-commerce and urbanization, increases the need for storage systems that are both energy- and space-efficient. Compact storage systems aim to maximize space utilization in limited storage areas and are therefore particularly suited in densely-populated urban areas where space is scarce. In this paper, we examine a recently introduced compact storage system in which uniformly shaped bins are stacked directly on top of each other, eliminating the need for aisles used to handle materials. Target bins are retrieved in a fully automated process by first lifting all other bins that block access and then accessing the target bin from the side of the system by a dedicated robot. Consequently, retrieving a bin can require substantial lifting effort, and thus energy. However, this energy can be reduced through smart retrieval strategies. From an operational perspective, we investigate how retrievals can be optimized with respect to energy consumption.We model the retrieval problem within a mathematical framework. We show that the problem is strongly NP-complete and derive structural insights. Building on these insights, we propose two exact methods: a mixed-integer programming (MIP) formulation and a dynamic programming algorithm, along with a simple, practitioner-oriented greedy algorithm that yields near-instant solutions. Numerical experiments reveal that dynamic programming consistently outperforms state-of-the-art MIP solvers in small to medium sized instances, while the greedy algorithm delivers satisfactory performance, especially when exact methods become computationally impractical."
2510.14347,"Prange's information set algorithm is a decoding algorithm for arbitrary linear codes. It decodes corrupted codewords of any $\mathbb{F}_2$-linear code $C$ of message length $n$ up to relative error rate $O(\log n / n)$ in $\mathsf{poly}(n)$ time. We show that the error rate can be improved to $O((\log n)^2 / n)$, provided: (1) the decoder has access to a polynomial-length advice string that depends on $C$ only, and (2) $C$ is $n^{-\Omega(1)}$-balanced.As a consequence we improve the error tolerance in decoding random linear codes if inefficient preprocessing of the code is allowed. This reveals potential vulnerabilities in cryptographic applications of Learning Noisy Parities with low noise rate.Our main technical result is that the Hamming weight of $Hw$, where $H$ is a random sample of *short dual* codewords, measures the proximity of a word $w$ to the code in the regime of interest. Given such $H$ as advice, our algorithm corrects errors by locally minimizing this measure. We show that for most codes, the error rate tolerated by our decoder is asymptotically optimal among all algorithms whose decision is based on thresholding $Hw$ for an arbitrary polynomial-size advice matrix $H$."
2510.15002,"The problem of determining whether a graph G can be realized as a unit-distance graph in $\mathbb{R}^2$ with integer-valued coordinates is NP-complete. We implement Eades and Whitesides' logic engine in this setting, and construct a graph that is realizable if and only if an arbitrary NA3SAT formula is satisfiable."
2510.15168,"We present the first uniform XP exact algorithm for unconstrained binary optimization of quadratic, polynomial, fractional, and other objectives under a single parameter, the differentially affine (DA) rank $r$. An objective $f: \{0,1\}^n \to \mathbb{R}$ has DA rank $r$ if there is a feature map $\psi: \{0,1\}^n \to \mathbb{R}^r$ such that each coordinate flip has finite gain $\Delta_{\pm e_i}f(x)=\langle v_{\pm e_i},\psi(x)\rangle+\beta_{\pm e_i}$. Our algorithm enumerates the $O((2n)^r)$ chambers of the induced hyperplane arrangement and applies a two-sided local-optimality test: a solution exists on a chamber and is unique iff $\operatorname{sign}\Delta_{+e_i}=-\operatorname{sign}\Delta_{-e_i}$ for all $i$, in which case $x_i^\star=1$ iff $\Delta_{+e_i}>0$. This yields $n^{O(r)}$ time with $O(n)$ decoding per chamber. The framework uniformly covers a wide range of nonlinear functions, including all rank-$r$ quadratics, low-Waring-rank pseudo-Boolean polynomials, finite products/ratios on positive domains, finite-basis separable sums via explicit lifts, Taylor-series approximations of analytic functions, and compositions of all the foregoing. Applications include Ising spin models, optimal experimental design, portfolio optimization, and robust statistics. Prior to our work, only specialized subcases involving sparsity, convexity, submodularity, etc. were known to be tractable. Analogous in spirit to Courcelle's theorem (MSO on bounded treewidth graphs) and Grohe's meta-theorems for constraint satisfaction, our result replaces logical width with analytic rank for nonlinear pseudo-Boolean optimization."
2510.15268,"In this work we study oblivious complexity classes. These classes capture the power of interactive proofs where the prover(s) are only given the input size rather than the actual input. In particular, we study the connections between the symmetric polynomial time $\mathsf{S_2P}$ and its oblivious counterpart $\mathsf{O_2P}$. Among our results, we construct an explicit language in $\mathsf{O_2P}$ that cannot be computed by circuits of size $n^k$, and thus prove a hierarchy theorem for $\mathsf{O_2TIME}$. Along the way we also make partial progress towards the resolution of an open question posed by Goldreich and Meir (TOCT 2015) that relates the complexity of $\mathsf{NP}$ to its oblivious counterpart $\mathsf{ONP}$. To the best of our knowledge, these results constitute the first explicit fixed-polynomial lower bound and hierarchy theorem for $\mathsf{O_2P}$. The smallest uniform complexity class for which such lower bounds were previously known was $\mathsf{S_2P}$, due to Cai (JCSS 2007). In addition, this is the first uniform hierarchy theorem for a semantic class. All previous results required some non-uniformity."
2510.15712,"How hard is it to find a local optimum? If we are given a graph and want to find a locally maximal cut--meaning that the number of edges in the cut cannot be improved by moving a single vertex from one side to the other--then just iterating improving steps finds a local maximum since the size of the cut can increase at most $|E|$ times. If, on the other hand, the edges are weighted, this problem becomes hard for the class PLS (Polynomial Local Search)[16].We are interested in optimization problems with lexicographic costs. For Max-Cut this would mean that the edges $e_1,\dots, e_m$ have costs $c(e_i) = 2^{m-i}$. For such a cost function, it is easy to see that finding a global Max-Cut is easy. In contrast, we show that it is PLS-complete to find an assignment for a 4-CNF formula that is locally maximal (when the clauses have lexicographic weights); and also for a 3-CNF when we relax the notion of local by allowing to switch two variables at a time.We use these results to answer a question in Scheder and Tantow[15], who showed that finding a lexicographic local minimum of a string $s \in \{0,1\}^n$ under the action of a list of given permutations $\pi_1, \dots, \pi_k \in S_{n}$ is PLS-complete. They ask whether the problem stays PLS-complete when the $\pi_1,\dots,\pi_k$ commute, i.e., generate an Abelian subgroup $G$ of $S_n$. In this work, we show that it does, and in fact stays PLS-complete even (1) when every element in $G$ has order two and also (2) when $G$ is cyclic, i.e., all $\pi_1,\dots,\pi_k$ are powers of a single permutations $\pi$."
2510.16991,"We establish deterministic hardness of approximation results for the Shortest Vector Problem in $\ell_p$ norm ($\mathsf{SVP}_p$) and for Unique-SVP ($\mathsf{uSVP}_p$) for all $p > 2$. Previously, no deterministic hardness results were known, except for $\ell_\infty$.For every $p > 2$, we prove constant-ratio hardness: no polynomial-time algorithm approximates $\mathsf{SVP}_p$ or $\mathsf{uSVP}_p$ within a ratio of $\sqrt{2} - o(1)$, assuming $\textsf{3SAT} \notin \text{DTIME}(2^{O(n^{2/3}\log n)})$, and, $\textsf{Unambiguous-3SAT} \notin \text{DTIME}(2^{O(n^{2/3}\log n)})$.We also show that for any $\varepsilon > 0$ there exists $p_\varepsilon > 2$ such that for every $p \ge p_\varepsilon$: no polynomial-time algorithm approximates $\mathsf{SVP}_p$ within a ratio of $2^{(\log n)^{1- \varepsilon}}$, assuming $\text{NP} \nsubseteq \text{DTIME}(n^{(\log n)^\varepsilon})$; and within a ratio of $n^{1/(\log\log(n))^\varepsilon}$, assuming $\text{NP} \nsubseteq \text{SUBEXP}$. This improves upon [Haviv, Regev, Theory of Computing 2012], which obtained similar inapproximation ratios under randomized reductions. We obtain analogous results for $\mathsf{uSVP}_p$ under the assumptions $\textsf{Unambiguous-3SAT} \not\subseteq \text{DTIME}(n^{(\log n)^\varepsilon})$ and $\textsf{Unambiguous-3SAT} \not\subseteq \text{SUBEXP}$, improving the previously known $1+o(1)$ [Stephens-Davidowitz, Approx 2016].Strengthening the hardness of $\textsf{uSVP}$ has direct cryptographic impact. By the reduction of Lyubashevsky and Micciancio [Lyubashevsky, Micciancio, CRYPTO 2009], hardness for $\gamma$-$\mathsf{uSVP}_p$ carries over to ${\frac{1}{\gamma}}$-$\mathsf{BDD}_p$ (Bounded Distance Decoding). Thus, understanding the hardness of $\textsf{uSVP}$ improves worst-case guarantees for two core problems that underpin security in lattice-based cryptography."
2510.17451,"The VC-dimension is a well-studied and fundamental complexity measure of a set system (or hypergraph) that is central to many areas of machine learning. We establish several new results on the complexity of computing the VC-dimension. In particular, given a hypergraph $\mathcal{H}=(\mathcal{V},\mathcal{E})$, we prove that the naive $2^{\mathcal{O}(|\mathcal{V}|)}$-time algorithm is asymptotically tight under the Exponential Time Hypothesis (ETH). We then prove that the problem admits a $1$-additive fixed-parameter approximation algorithm when parameterized by the maximum degree of $\mathcal{H}$ and a fixed-parameter algorithm when parameterized by its dimension, and that these are essentially the only such exploitable structural parameters. Lastly, we consider a generalization of the problem, formulated using graphs, which captures the VC-dimension of both set systems and graphs. We design a $2^{\mathcal{O}(\rm{tw}\cdot \log \rm{tw})}\cdot |V|$-time algorithm for any graph $G=(V,E)$ of treewidth $\rm{tw}$ (which, for a set system, applies to the treewidth of its incidence graph). This is in contrast with closely related problems that require a double-exponential dependency on the treewidth (assuming the ETH)."
2510.17717,"We prove a general translation theorem for converting one-way communication lower bounds over a product distribution to dynamic cell-probe lower bounds.Specifically, we consider a class of problems considered in [Pat10] where:1. $S_1, \ldots, S_m \in \{0, 1\}^n$ are given and publicly known.2. $T \in \{0, 1\}^n$ is a sequence of updates, each taking $t_u$ time.3. For a given $Q \in [m]$, we must output $f(S_Q, T)$ in $t_q$ time. Our main result shows that for a ""hard"" function $f$, for which it is difficult to obtain a non-trivial advantage over random guessing with one-way communication under some product distribution over $S_Q$ and $T$ (for example, a uniform distribution), then the above explicit dynamic cell-probe problem must have $\max \{ t_u, t_q \} \geq \tilde{\Omega}(\log^{3/2}(n))$ if $m = \Omega(n^{0.99})$. This result extends and unifies the super-logarithmic dynamic data structure lower bounds from [LWY20] and [LY25] into a more general framework.From a technical perspective, our approach merges the cell-sampling and chronogram techniques developed in [LWY20] and [LY25] with the new static data structure lower bound methods from [KW20] and [Ko25], thereby merging all known state-of-the-art cell-probe lower-bound techniques into one.As a direct consequence of our method, we establish a super-logarithmic lower bound against the Multiphase Problem [Pat10] for the case where the data structure outputs the Inner Product (mod 2) of $S_Q$ and $T$. We suspect further applications of this general method towards showing super-logarithmic dynamic cell-probe lower bounds. We list some example applications of our general method, including a novel technique for a one-way communication lower bound against small-advantage protocols for a product distribution using average min-entropy, which could be of independent interest."
2510.17829,"This paper establishes the separation of complexity classes $\mathbf{P}$ and $\mathbf{NP}$ through a novel homological algebraic approach grounded in category theory. We construct the computational category $\mathbf{Comp}$, embedding computational problems and reductions into a unified categorical framework. By developing computational homology theory, we associate to each problem $L$ a chain complex $C_{\bullet}(L)$ whose homology groups $H_n(L)$ capture topological invariants of computational processes. Our main result demonstrates that problems in $\mathbf{P}$ exhibit trivial computational homology ($H_n(L) = 0$ for all $n > 0$), while $\mathbf{NP}$-complete problems such as SAT possess non-trivial homology ($H_1(\mathrm{SAT}) \neq 0$). This homological distinction provides the first rigorous proof of $\mathbf{P} \neq \mathbf{NP}$ using topological methods. The proof is formally verified in Lean 4, ensuring absolute mathematical rigor. Our work inaugurates computational topology as a new paradigm for complexity analysis, offering finer distinctions than traditional combinatorial approaches and establishing connections between structural complexity theory and homological invariants."
2511.00058,"We present a unifying representation of computation as a two-player game between an \emph{Algorithm} and \emph{Nature}, grounded in domain theory and game theory. The Algorithm produces progressively refined approximations within a Scott domain, while Nature assigns penalties proportional to their distance from the true value. Correctness corresponds to equilibrium in the limit of refinement. This framework allows us to define complexity classes game-theoretically, characterizing $\mathbf{P}$, $\mathbf{NP}$, and related classes as sets of problems admitting particular equilibria. The open question $\mathbf{P} \stackrel{?}{=} \mathbf{NP}$ becomes a problem about the equivalence of Nash equilibria under differing informational and temporal constraints."
2511.00589,"This paper presents a refined complexity calculus model: r-Complexity, a new asymptotic notation that offers better complexity feedback for similar programs than the traditional Bachmann-Landau notation, providing subtle insights even for algorithms that are part of the same conventional complexity class. The architecture-dependent metric represents an enhancement that provides better sensitivity with respect to discrete analysis."
2511.02262,"This article concerns the computational complexity of a fundamental problem in number theory: counting points on curves and surfaces over finite fields. There is no subexponential-time algorithm known and it is unclear if it can be $\mathrm{NP}$-hard.Given a curve, we present the first efficient Arthur-Merlin protocol to certify its point-count, its Jacobian group structure, and its Hasse-Weil zeta function. We extend this result to a smooth projective surface to certify the factor $P_{1}(T)$, corresponding to the first Betti number, of the zeta function; by using the counting oracle. We give the first algorithm to compute $P_{1}(T)$ that is poly($\log q$)-time if the degree $D$ of the input surface is fixed; and in quantum poly($D\log q$)-time in general.Our technique in the curve case, is to sample hash functions using the Weil and Riemann-Roch bounds, to certify the group order of its Jacobian. For higher dimension varieties, we first reduce to the case of a surface, which is fibred as a Lefschetz pencil of hyperplane sections over $\mathbb{P}^{1}$. The formalism of vanishing cycles, and the inherent big monodromy, enable us to prove an effective version of Deligne's `theoreme du pgcd' using the hard-Lefschetz theorem and an equidistribution result due to Katz. These reduce our investigations to that of computing the zeta function of a curve, defined over a finite field extension $\mathbb{F}_{Q}/\mathbb{F}_{q}$ of poly-bounded degree. This explicitization of the theory yields the first nontrivial upper bounds on the computational complexity."
2511.02264,"The $k$-$\mathsf{XOR}$ problem is one of the most well-studied problems in classical complexity. We study a natural quantum analogue of $k$-$\mathsf{XOR}$, the problem of computing the ground energy of a certain subclass of structured local Hamiltonians, signed sums of $k$-local Pauli operators, which we refer to as $k$-$\mathsf{XOR}$ Hamiltonians. As an exhibition of the connection between this model and classical $k$-$\mathsf{XOR}$, we extend results on refuting $k$-$\mathsf{XOR}$ instances to the Hamiltonian setting by crafting a quantum variant of the Kikuchi matrix for CSP refutation, instead capturing ground energy optimization. As our main result, we show an $n^{O(\ell)}$-time classical spectral algorithm certifying ground energy at most $\frac{1}{2} + \varepsilon$ in (1) semirandom Hamiltonian $k$-$\mathsf{XOR}$ instances or (2) sums of Gaussian-signed $k$-local Paulis both with $O(n) \cdot \left(\frac{n}{\ell}\right)^{k/2-1} \log n /\varepsilon^4$ local terms, a tradeoff known as the refutation threshold. Additionally, we give evidence this tradeoff is tight in the semirandom regime via non-commutative Sum-of-Squares lower bounds embedding classical $k$-$\mathsf{XOR}$ instances as entirely classical Hamiltonians."
2511.02633,"A locally decodable code (LDC) $C \colon \{0,1\}^k \to \{0,1\}^n$ is an error-correcting code that allows one to recover any bit of the original message with good probability while only reading a small number of bits from a corrupted codeword. A relaxed locally decodable code (RLDC) is a weaker notion where the decoder is additionally allowed to abort and output a special symbol $\bot$ if it detects an error. For a large constant number of queries $q$, there is a large gap between the blocklength $n$ of the best $q$-query LDC and the best $q$-query RLDC. Existing constructions of RLDCs achieve polynomial length $n = k^{1 + O(1/q)}$, while the best-known $q$-LDCs only achieve subexponential length $n = 2^{k^{o(1)}}$. On the other hand, for $q = 2$, it is known that RLDCs and LDCs are equivalent. We thus ask the question: what is the smallest $q$ such that there exists a $q$-RLDC that is not a $q$-LDC?In this work, we show that any linear $3$-query RLDC is in fact a $3$-LDC, i.e., linear RLDCs and LDCs are equivalent at $3$ queries. More generally, we show for any constant $q$, there is a soundness error threshold $s(q)$ such that any linear $q$-RLDC with soundness error below this threshold must be a $q$-LDC. This implies that linear RLDCs cannot have ""strong soundness"" -- a stricter condition satisfied by linear LDCs that says the soundness error is proportional to the fraction of errors in the corrupted codeword -- unless they are simply LDCs.In addition, we give simple constructions of linear $15$-query RLDCs that are not $q$-LDCs for any constant $q$, showing that for $q = 15$, linear RLDCs and LDCs are not equivalent.We also prove nearly identical results for locally correctable codes and their corresponding relaxed counterpart."
2511.03083,"Let $\mathcal{G}$ be a $k$-player game with value $<1$, whose query distribution is such that no marginal on $k-1$ players admits a non-trivial Abelian embedding. We show that for every $n\geq N$, the value of the $n$-fold parallel repetition of $\mathcal{G}$ is $$ \text{val}(\mathcal{G}^{\otimes n}) \leq \frac{1}{\underbrace{\log\log\cdots\log}_{C\text{ times}} n}, $$ where $N=N(\mathcal{G})$ and $1\leq C\leq k^{O(k)}$ are constants. As a consequence, we obtain a parallel repetition theorem for all $3$-player games whose query distribution is pairwise-connected. Prior to our work, only inverse Ackermann decay bounds were known for such games [Ver96].As additional special cases, we obtain a unified proof for all known parallel repetition theorems, albeit with weaker bounds: (1) A new analytic proof of parallel repetition for all 2-player games [Raz98, Hol09, DS14]. (2) A new proof of parallel repetition for all $k$-player playerwise connected games [DHVY17, GHMRZ22]. (3) Parallel repetition for all $3$-player games (in particular $3$-XOR games) whose query distribution has no non-trivial Abelian embedding into $(\mathbb{Z}, +)$ [BKM23c, BBKLM25]. (4) Parallel repetition for all 3-player games with binary inputs [HR20, GHMRZ21, GHMRZ22, GMRZ22]."
2511.03388,"We characterize the monotone bounded depth formula complexity for graph homomorphism and colored isomorphism polynomials using a graph parameter called the cost of bounded product depth baggy elimination tree. Using this characterization, we show an almost optimal separation between monotone circuits and monotone formulas using constant-degree polynomials for all fixed product depths, and an almost optimal separation between monotone formulas of product depths $\Delta$ and $\Delta$ + 1 for all $\Delta$ $\ge$ 1."
2511.03653,"Given a small random sample of $n$-bit strings labeled by an unknown Boolean function, which properties of this function can be tested computationally efficiently? We show an equivalence between properties that are efficiently testable from few samples and properties with structured symmetry, which depend only on the function's average values on parts of a low-complexity partition of the domain. Without the efficiency constraint, a similar characterization in terms of unstructured symmetry was obtained by Blais and Yoshida (2019). Our main technical tool is supersimulation, which builds on methods from the algorithmic fairness literature to approximate arbitrarily complex functions by small-circuit simulators that fool significantly larger distinguishers.We extend the characterization along other axes as well. We show that allowing parts to overlap exponentially reduces their required number, broadening the scope of the construction from properties testable with $O(\log n)$ samples to properties testable with $O(n)$ samples. For larger sample sizes, we show that any efficient tester is essentially checking for indistinguishability from a bounded collection of small circuits, in the spirit of a characterization of testable graph properties. Finally, we show that our results for Boolean function testing generalize to high-entropy distribution testing on arbitrary domains."
2511.03703,"All known proofs of the PCP theorem rely on multiple ""composition"" steps, where PCPs over large alphabets are turned into PCPs over much smaller alphabets at a (relatively) small price in the soundness error of the PCP. Algebraic proofs, starting with the work of Arora, Lund, Motwani, Sudan, and Szegedy use at least 2 such composition steps, whereas the ""Gap amplification"" proof of Dinur uses $\Theta(\log n)$ such composition steps. In this work, we present the first PCP construction using just one composition step. The key ingredient, missing in previous work and finally supplied in this paper, is a basic PCP (of Proximity) of size $2^{n^\epsilon}$, for any $\epsilon > 0$, that makes $O_\epsilon(1)$ queries.At the core of our new construction is a new class of alternatives to ""sum-check"" protocols. As used in past PCPs, these provide a method by which to verify that an $m$-variate degree $d$ polynomial $P$ evaluates to zero at every point of some set $S \subseteq \mathbb{F}_q^m$. Previous works had shown how to check this condition for sets of the form $S = H^m$ using $O(m)$ queries with alphabet $\mathbb{F}_q^d$ assuming $d \geq |H|$. Our work improves this basic protocol in two ways: First we extend it to broader classes of sets $S$ (ones closer to Hamming balls rather than cubes). Second, it reduces the number of queries from $O(m)$ to an absolute constant for the settings of $S$ we consider. Specifically when $S = (\{0,1\}^{m/c}_{\leq 1})^c$, we give such an alternate to the sum-check protocol with $O(1)$ queries with alphabet $\mathbb{F}_q^{O(c+d)}$, using proofs of size $q^{O(m^2/c)}$. Our new protocols use insights from the powerful theory of Gröbner bases to extend previously known protocols to these new settings with surprising ease. In doing so, they highlight why these theories from algebra may be of further use in complexity theory."
2511.04125,"We prove that SVP$_p$ is NP-hard to approximate within a factor of $2^{\log^{1 - \varepsilon} n}$, for all constants $\varepsilon > 0$ and $p > 2$, under standard deterministic Karp reductions. This result is also the first proof that \emph{exact} SVP$_p$ is NP-hard in a finite $\ell_p$ norm. Hardness for SVP$_p$ with $p$ finite was previously only known if NP $\not \subseteq$ RP, and under that assumption, hardness of approximation was only known for all constant factors. As a corollary to our main theorem, we show that under the Sliding Scale Conjecture, SVP$_p$ is NP-hard to approximate within a small polynomial factor, for all constants $p > 2$.Our proof techniques are surprisingly elementary; we reduce from a \emph{regularized} PCP instance directly to the shortest vector problem by using simple gadgets related to Vandermonde matrices and Hadamard matrices."
2511.04308,"The websitethis http URLserves as a comprehensive database for exploring problems and reductions between them. It presents several complexity classes in the form of an interconnected graph where problems are represented as vertices, while edges represent reductions between them. This graphical perspective allows for identifying problem clusters and simplifying finding problem candidates to reduce from. Moreover, users can easily search for existing problems via a dedicated search bar, and various filters allow them to focus on specific subgraphs of interest. The design of the website enables users to contribute by adding new problems and reductions to the database. Furthermore, the software architecture allows for the integration of additional graphs corresponding to new complexity classes. In the current state, the following networks with their respective complexity classes are included:- classical complexity including the classes NP, #P, and SSP-NP- parameterized complexity including the classes W[1], W[2]- gap-preserving reductions under the PCP-Theorem and the Unique Games Conjecture."
2511.04558,"We show that for any constant $c>0$, any (two-sided error) adaptive algorithm for testing monotonicity of Boolean functions must have query complexity $\Omega(n^{1/2-c})$. This improves the $\tilde\Omega(n^{1/3})$ lower bound of [CWX17] and almost matches the $\tilde{O}(\sqrt{n})$ upper bound of [KMS18]."
2511.04794,"We exhibit an $n$-bit partial function with randomized communication complexity $O(\log n)$ but such that any completion of this function into a total one requires randomized communication complexity $n^{\Omega(1)}$. In particular, this shows an exponential separation between randomized and \emph{pseudodeterministic} communication protocols. Previously, Gavinsky (2025) showed an analogous separation in the weaker model of parity decision trees. We use lifting techniques to extend his proof idea to communication complexity."
2511.04954,"We prove the bivariate Cayley-Hamilton theorem, a powerful generalization of the classical Cayley-Hamilton theorem. The bivariate Cayley-Hamilton theorem has three direct corollaries that are usually proved independently: The classical Cayley-Hamilton theorem, the Girard-Newton identities, and the fact that the determinant and every coefficient of the characteristic polynomial has polynomially sized algebraic branching programs (ABPs) over arbitrary commutative rings. This last fact could so far only be obtained from separate constructions, and now we get it as a direct consequence of this much more general statement.The statement of the bivariate Cayley-Hamilton theorem involves the gradient of the coefficient of the characteristic polynomial, which is a generalization of the adjugate matrix. Analyzing this gradient, we obtain another new ABP for the determinant and every coefficient of the characteristic polynomial. This ABP has one third the size and half the width compared to the current record-holder ABP constructed by Mahajan-Vinay in 1997. This is the first improvement on this problem for 28 years.Our ABP is built around algebraic identities involving the first order partial derivatives of the coefficients of the characteristic polynomial, and does not use the ad-hoc combinatorial concept of clow sequences. This answers the 26-year-old open question by Mahajan-Vinay from 1999 about the necessity of clow sequences.We prove all results in a combinatorial way that on a first sight looks similar to Mahajan-Vinay, but it is closer to Straubing's and Zeilberger's constructions."
2511.05035,"Modular composition is the problem of computing the coefficient vector of the polynomial $f(g(x)) \bmod h(x)$, given as input the coefficient vectors of univariate polynomials $f$, $g$, and $h$ over an underlying field $\mathbb{F}$. While this problem is known to be solvable in nearly-linear time over finite fields due to work of Kedlaya & Umans, no such near-linear-time algorithms are known over infinite fields, with the fastest known algorithm being from a recent work of Neiger, Salvy, Schost & Villard that takes $O(n^{1.43})$ field operations on inputs of degree $n$. In this work, we show that for any infinite field $\mathbb{F}$, modular composition is in the border of algebraic circuits with division gates of nearly-linear size and polylogarithmic depth. Moreover, this circuit family can itself be constructed in near-linear time.Our techniques also extend to other algebraic problems, most notably to the problem of computing greatest common divisors of univariate polynomials. We show that over any infinite field $\mathbb{F}$, the GCD of two univariate polynomials can be computed (piecewise) in the border sense by nearly-linear-size and polylogarithmic-depth algebraic circuits with division gates, where the circuits themselves can be constructed in near-linear time. While univariate polynomial GCD is known to be computable in near-linear time by the Knuth--Schönhage algorithm, or by constant-depth algebraic circuits from a recent result of Andrews & Wigderson, obtaining a parallel algorithm that simultaneously achieves polylogarithmic depth and near-linear work remains an open problem of great interest. Our result shows such an upper bound in the setting of border complexity."
2511.05176,"We show that Reed-Solomon codes of dimension $k$ and block length $n$ over any finite field $\mathbb{F}$ can be deterministically list decoded from agreement $\sqrt{(k-1)n}$ in time $\text{poly}(n, \log |\mathbb{F}|)$.Prior to this work, the list decoding algorithms for Reed-Solomon codes, from the celebrated results of Sudan and Guruswami-Sudan, were either randomized with time complexity $\text{poly}(n, \log |\mathbb{F}|)$ or were deterministic with time complexity depending polynomially on the characteristic of the underlying field. In particular, over a prime field $\mathbb{F}$, no deterministic algorithms running in time $\text{poly}(n, \log |\mathbb{F}|)$ were known for this problem.Our main technical ingredient is a deterministic algorithm for solving the bivariate polynomial factorization instances that appear in the algorithm of Sudan and Guruswami-Sudan with only a $\text{poly}(\log |\mathbb{F}|)$ dependence on the field size in its time complexity for every finite field $\mathbb{F}$. While the question of obtaining efficient deterministic algorithms for polynomial factorization over finite fields is a fundamental open problem even for univariate polynomials of degree $2$, we show that additional information from the received word can be used to obtain such an algorithm for instances that appear in the course of list decoding Reed-Solomon codes."
2511.06171,"Several recent works [DHLNSY25, CPPS25a, CPPS25b] have studied a model of property testing of Boolean functions under a \emph{relative-error} criterion. In this model, the distance from a target function $f: \{0,1\}^n \to \{0,1\}$ that is being tested to a function $g$ is defined relative to the number of inputs $x$ for which $f(x)=1$; moreover, testing algorithms in this model have access both to a black-box oracle for $f$ and to independent uniform satisfying assignments of $f$. The motivation for this model is that it provides a natural framework for testing \emph{sparse} Boolean functions that have few satisfying assignments, analogous to well-studied models for property testing of sparse graphs.The main result of this paper is a lower bound for testing \emph{halfspaces} (i.e., linear threshold functions) in the relative error model: we show that $\tilde{\Omega}(\log n)$ oracle calls are required for any relative-error halfspace testing algorithm over the Boolean hypercube $\{0,1\}^n$. This stands in sharp contrast both with the constant-query testability (independent of $n$) of halfspaces in the standard model [MORS10], and with the positive results for relative-error testing of many other classes given in [DHLNSY25, CPPS25a, CPPS25b]. Our lower bound for halfspaces gives the first example of a well-studied class of functions for which relative-error testing is provably more difficult than standard-model testing."
