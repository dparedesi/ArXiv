paper_id,abstract
2501.00473,"Despite enormous efforts devoted to understand the characteristics and impacts of retracted papers, little is known about the mechanisms underlying the dynamics of their harm and the dynamics of its propagation. Here, we propose a citation-based framework to quantify the harm caused by retracted papers, aiming to uncover why their harm persists and spreads so widely. We uncover an ''attention escape'' mechanism, wherein retracted papers postpone significant harm, more prominently affect indirectly citing papers, and inflict greater harm on citations in journals with an impact factor less than 10. This mechanism allows retracted papers to inflict harm outside the attention of authors and publishers, thereby evading their intervention. This study deepens understanding of the harm caused by retracted papers, emphasizes the need to activate and enhance the attention of authors and publishers, and offers new insights and a foundation for strategies to mitigate their harm and prevent its spread."
2501.02856,"Many countries and institutions are striving to develop tools to monitor their open science policies. Since 2018, with the launch of its National Plan for Open Science, France has been progressively implementing a monitoring framework for its public policy, relying exclusively on reliable, open, and controlled data. Currently, this monitoring focuses on research outputs, particularly publications, as well as theses and clinical trials. Publications serve as a basis for analyzing other dimensions, including research data, code, and software. The metadata associated with publications is therefore particularly valuable, but the methodology for leveraging it raises several challenges. Here, we briefly outline how we have used this metadata to construct the French Open Science Monitor."
2501.03632,"Data are at the heart of electronic resource management in academic libraries. Assessing the usage data of electronic resources has become a prevalent approach to demonstrate the value of digital collections, justify library expenditures, and gain insights into how users interact with library materials. This study analyzes the usage statistics of electronic books (ebooks) generated locally by the OpenURL link resolver in an academic library, and statistics collected by platform vendors based on Release 5 of the Counting Online Usage of Networked Electronic Resource (COUNTER R5). Three content provider platforms (Cambridge Core, EBSCOhost, and ScienceDirect) were analyzed as data sources. The COUNTER and link resolver statistics were examined to determine the degree of association between these two metrics. The Spearman correlation coefficient was moderate (rs > 0.561 and < 0.678) and statistically significant (p < .01). This suggests that these metrics capture different aspects of the usage of ebooks in different contexts. Other factors, such as the types of access to electronic resources and the units of content delivered, were also examined. The study concludes with a discussion regarding the scope and limitations of link resolver and COUNTER R5 as library metrics for measuring the usage of ebooks."
2501.03771,"We report evidence of a new set of sneaked references discovered in the scientific literature. Sneaked references are references registered in the metadata of publications without being listed in reference section or in the full text of the actual publications where they ought to be found. We document here 80,205 references sneaked in metadata of the International Journal of Innovative Science and Research Technology (IJISRT). These sneaked references are registered with Crossref and all cite -- thus benefit -- this same journal. Using this dataset, we evaluate three different methods to automatically identify sneaked references. These methods compare reference lists registered with Crossref against the full text or the reference lists extracted from PDF files. In addition, we report attempts to scale the search for sneaked references to the scholarly literature."
2501.04008,"Since decades, the modelling of metadata has been core to the functioning of any academic library. Its importance has only enhanced with the increasing pervasiveness of Generative Artificial Intelligence (AI)-driven information activities and services which constitute a library's outreach. However, with the rising importance of metadata, there arose several outstanding problems with the process of designing a library metadata model impacting its reusability, crosswalk and interoperability with other metadata models. This paper posits that the above problems stem from an underlying thesis that there should only be a few core metadata models which would be necessary and sufficient for any information service using them, irrespective of the heterogeneity of intra-domain or inter-domain settings. To that end, this paper advances a contrary view of the above thesis and substantiates its argument in three key steps. First, it introduces a novel way of thinking about a library metadata model as an ontology-driven composition of five functionally interlinked representation levels from perception to its intensional definition via properties. Second, it introduces the representational manifoldness implicit in each of the five levels which cumulatively contributes to a conceptually entangled library metadata model. Finally, and most importantly, it proposes a Generative AI-driven Human-Large Language Model (LLM) collaboration based metadata modelling approach to disentangle the entanglement inherent in each representation level leading to the generation of a conceptually disentangled metadata model. Throughout the paper, the arguments are exemplified by motivating scenarios and examples from representative libraries handling cancer information."
2501.04014,"The European Union's Artificial Intelligence Act (AI Act) requires providers and deployers of high-risk AI applications to register their systems into the EU database, wherein the information should be represented and maintained in an easily-navigable and machine-readable manner. Given the uptake of open data and Semantic Web-based approaches for other EU repositories, in particular the use of the Data Catalogue vocabulary Application Profile (DCAT-AP), a similar solution for managing the EU database of high-risk AI systems is needed. This paper introduces AICat - an extension of DCAT for representing catalogues of AI systems that provides consistency, machine-readability, searchability, and interoperability in managing open metadata regarding AI systems. This open approach to cataloguing ensures transparency, traceability, and accountability in AI application markets beyond the immediate needs of high-risk AI compliance in the EU. AICat is available online atthis https URLunder the CC-BY-4.0 license."
2501.04015,"The current research conducts a comprehensive analysis of citation networks focusing on publications by authors affiliated with Egyptian institutions. Leveraging the Semantic Scholar platform and its API, a citation network and a co-authorship network graphs are constructed to visualize the interconnections among these publications and their authors. This is done using the Python package for graph analysis (Networkx). The primary objective is to identify influential Egyptian publications and assess the centrality of nodes within the citation network. Through meticulous data collection including web scraping techniques, we obtained a cleaned dataset comprising publications by authors affiliated with Egyptian institutions. The analysis addresses challenges related to data quality, technical intricacies, and time constraints, resulting in a reliable and robust dataset. The findings provide valuable information on the impact of Egyptian publications, offering insights into the scholarly influence of authors associated with Egyptian institutions. This research equips researchers and academics interested in evaluating the impact of Egyptian publications with valuable data for future studies, collaborations, and policy decisions."
2501.05821,"This study focuses on analysing the coverage of publications' metadata available in the Current Research Information System (CRIS) infrastructure of the University of Bologna (UNIBO), implemented by the IRIS platform, within an authoritative source of open research information, i.e. OpenCitations. The analysis considers data regarding the publication entities alongside the citation links. We precisely quantify the proportion of UNIBO IRIS publications included in OpenCitations, examine their types, and evaluate the number of citations in OpenCitations that involve IRIS publications. Our methodology filters and transforms data dumps of IRIS and OpenCitations, creating novel datasets used for the analysis. Our findings reveal that only 36% of IRIS is covered in OpenCitations, with journal articles exhibiting the highest coverage. We identified 5,129,406 citation links pointing to UNIBO IRIS publications. From a purely quantitative perspective, comparing our results with broader proprietary services like Scopus and Web of Science reveals a comparable quantitative coverage in the number of IRIS bibliographic resources included in all the systems analysed (OpenCitations, Scopus and Web of Science) as well as in the number of citations received by them."
2501.06656,"For analysis of bibliographic data, we can obtain from bibliographic databases the corresponding collection of bibliographic networks. Recently OpenAlex, a new open-access bibliographic database, became available. We present OpenAlex2Pajek, an R package for converting OpenAlex data into a collection of Pajek's networks. For an illustration, we created a temporal weighted network describing the co-authorship between world countries for years from 1990 to 2023. We present some analyses of this network."
2501.06833,"In English literature, the 19th century witnessed a significant transition in styles, themes, and genres. Consequently, the novels from this period display remarkable diversity. This paper explores these variations by examining the evolution of term usage in 19th century English novels through the lens of information retrieval. By applying a query expansion-based approach to a decade-segmented collection of fiction from the British Library, we examine how related terms vary over time. Our analysis employs multiple standard metrics including Kendall's tau, Jaccard similarity, and Jensen-Shannon divergence to assess overlaps and shifts in expanded query term sets. Our results indicate a significant degree of divergence in the related terms across decades as selected by the query expansion technique, suggesting substantial linguistic and conceptual changes throughout the 19th century novels."
2501.06843,"The Global Research infrastructure (GRI) is made up of the repositories and organizations that provide persistent identifiers (PIDs) and metadata for many kinds of research objects and connect these objects to funders, research institutions, researchers, and one another using PIDs. The INFORMATE Project has combined three data sources to focus on understanding how the global research infrastructure might help the US National Science Foundation (NSF) and other federal agencies identify and characterize the impact of their support. In this paper we present INFORMATE observations of three data systems. The NSF Award database represents NSF funding while the NSF Public Access Repository (PAR) and CHORUS, as a proxy for the GRI, represent two different view of results of that funding. We compare the first at the level of awards and the second two at the level of published research articles. Our findings demonstrate that CHORUS datasets include significantly more NSF awards and more related papers than does PAR. Our findings also suggest that time plays a significant role in the inclusion of award metadata across the sources analyzed. Data in those sources travel very different journeys, each presenting different obstacles to metadata completeness and suggesting necessary actions on the parts of authors and publishers to ensure that publication and funding metadata are captured. We discuss these actions, as well as implications our findings have for emergent technologies such as artificial intelligence and natural language processing."
2501.06956,"In the rapidly evolving landscape of technological innovation, safeguarding intellectual property rights through patents is crucial for fostering progress and stimulating research and development investments. This report introduces a ground-breaking Patent Novelty Assessment and Claim Generation System, meticulously crafted to dissect the inventive aspects of intellectual property and simplify access to extensive patent claim data. Addressing a crucial gap in academic institutions, our system provides college students and researchers with an intuitive platform to navigate and grasp the intricacies of patent claims, particularly tailored for the nuances of Chinese patents. Unlike conventional analysis systems, our initiative harnesses a proprietary Chinese API to ensure unparalleled precision and relevance. The primary challenge lies in the complexity of accessing and comprehending diverse patent claims, inhibiting effective innovation upon existing ideas. Our solution aims to overcome these barriers by offering a bespoke approach that seamlessly retrieves comprehensive claim information, finely tuned to the specifics of the Chinese patent landscape. By equipping users with efficient access to comprehensive patent claim information, our transformative platform seeks to ignite informed exploration and innovation in the ever-evolving domain of intellectual property. Its envisioned impact transcends individual colleges, nurturing an environment conducive to research and development while deepening the understanding of patented concepts within the academic community."
2501.07267,"Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications -- such as author-publication history, author affiliation, research topics, and citation counts -- we achieve an F1 score of 0.76, demonstrating robust classification of author roles."
2501.08352,"Considering the lack of a unified framework for image description and deep cultural analysis at the subject level in the field of Ancient Chinese Paintings (ACP), this study utilized the Beijing Palace Museum's ACP collections to develop a semantic model integrating the iconological theory with a new workflow for term extraction and mapping. Our findings underscore the model's effectiveness. SDM can be used to support further art-related knowledge organization and cultural exploration of ACPs."
2501.08401,"This study examines gender disparities in communication research through citation metrics, authorship patterns, team composition, and faculty salaries. Using data from 62,359 papers across 121 communication journals, we find that while female authors are increasingly represented, citation gaps persist, with sole-authored papers by women receiving fewer citations than those by men, especially in smaller teams. Team composition analysis reveals a tendency toward gender homophily, with single-gender teams being more common. In top U.S. communication journals, female authors face underrepresentation and citation disparities favoring male authors. Salary analysis from leading U.S. public universities shows that female faculty earn lower salaries at the Assistant Professor level, though disparities lessen at higher ranks. These findings highlight the need for greater efforts to promote gender equity through inclusive collaboration, equitable citation practices, and fair compensation."
2501.09666,"INTRODUCTION: Wikipedia is a major source of information, particularly for medical and health content, citing over 4 million scholarly publications. However, the representation of research-based knowledge across different languages on Wikipedia has been under explored. This study analyses the largest database of Wikipedia citations collected to date, examining the uniqueness of content and research representation across languages. METHOD: The study included nearly 3.5 million unique research articles and their Wikipedia mentions from 21 languages. These were categorized into three groups: Group A (publications uniquely cited by a single non-English Wikipedia), Group B (co-cited by English and non-English Wikipedias), and Group C (co-cited by multiple non-English Wikipedias). Descriptive and comparative statistics were conducted by Wikipedia language, group, and discipline. RESULTS: Significant differences were found between twenty non-English languages and English Wikipedia (p<0.001). While English Wikipedia is the largest, non-English Wikipedias cite an additional 1.5 million publications. CONCLUSION: English Wikipedia should not be seen as a comprehensive body of information. Non-English Wikipedias cover unique subjects and disciplines, offering a more complete representation of research collectively. The uniqueness of voice in non-English Wikipedias correlates with their size, though other factors may also influence these differences."
2501.09897,"High-quality biomedical datasets are essential for medical research and disease treatment innovation. The NIH-funded Bridge2AI project strives to facilitate such innovations by uniting top-tier, diverse teams to curate datasets designed for AI-driven biomedical research. We examined 1,699 dataset papers from the Nucleic Acids Research (NAR) database issues and the Bridge2AI Talent Knowledge Graph. By treating each paper's authors as a team, we explored the relationship between team attributes (team power and fairness) and dataset paper quality, measured by scientific impact (Relative Citation Ratio percentile) and clinical translation power (APT, likelihood of citation by clinical trials and guidelines). Utilizing the SHAP explainable AI framework, we identified correlations between team attributes and the success of dataset papers in both citation impact and clinical translation. Key findings reveal that (1) PI (Principal Investigator) leadership and team academic prowess are strong predictors of dataset success; (2) team size and career age are positively correlated with scientific impact but show inverse patterns for clinical translation; and (3) higher female representation correlates with greater dataset success. Although our results are correlational, they offer valuable insights into forming high-performing data generation teams. Future research should incorporate causal frameworks to deepen understanding of these relationships."
2501.10035,"This study introduces a novel methodology for mapping scientific communities at scale, addressing challenges associated with network analysis in large bibliometric datasets. By leveraging enriched publication metadata from the French research portal scanR and applying advanced filtering techniques to prioritize the strongest interactions between entities, we construct detailed, scalable network maps. These maps are enhanced through systematic disambiguation of authors, affiliations, and topics using persistent identifiers and specialized algorithms. The proposed framework integrates Elasticsearch for efficient data aggregation, Graphology for network spatialization (Force Atltas2) and community detection (Louvain algorithm) and VOSviewer for network vizualization. A Large Language Model (Mistral Nemo) is used to label the communities detected and OpenAlex data helps to enrich the results with citation counts estimation to detect hot topics. This scalable approach enables insightful exploration of research collaborations and thematic structures, with potential applications for strategic decision-making in science policy and funding. These web tools are effective at the global (national) scale but are also available (and can be integrated via iframes) on the perimeter of any French research institution (from large research organisms to any laboratory). The scanR community analysis tool is available online [this https URL](this https URL). All tools and methodologies are open-source on the repo [this https URL](this https URL)"
2501.10415,"A key issue hindering discoverability, attribution and reusability of open research software is that its existence often remains hidden within the manuscript of research papers. For these resources to become first-class bibliographic records, they first need to be identified and subsequently registered with persistent identifiers (PIDs) to be made FAIR (Findable, Accessible, Interoperable and Reusable). To this day, much open research software fails to meet FAIR principles and software resources are mostly not explicitly linked from the manuscripts that introduced them or used them. SoFAIR is a 2-year international project (2024-2025) which proposes a solution to the above problem realised over the content available through the global network of open repositories. SoFAIR will extend the capabilities of widely used open scholarly infrastructures (CORE, Software Heritage, HAL) and tools (GROBID) operated by the consortium partners, delivering and deploying an effective solution for the management of the research software lifecycle, including: 1) ML-assisted identification of research software assets from within the manuscripts of scholarly papers, 2) validation of the identified assets by authors, 3) registration of software assets with PIDs and their archival."
2501.10892,"Canada's research productivity in Library and Information Science (LIS) is significant: studies have found that Canada ranks third globally in terms of output. As the LIS field continues to grow, the pace of output accelerates, and the scope of this work expands. The recently launched Canadian Publications in Library and Information Science Database compiles all Canadian scientific publications, including those authored by faculty members and academic librarians. This database offers the advantage of encompassing articles and librarian publications that may not be typically included in traditional bibliometric surveys, such as those conducted using databases like Web of Science, Scopus, and Library and Information Science Abstracts (LISA). Using this data, this study maps the scholarly contributions of Canadian LIS scholars and academic librarians to the field of LIS and examines whether Canadian LIS research is characterized by silos. This paper examines the similarities and differences in research output, impact, topics, and publication venues between academic librarians and scholars in Canada, as well as the extent to which academics and practitioners engage in research collaborations or reference each other's work. We find that while there is some degree of overlap in research topics and publication venues between LIS academics and academic librarians, the two groups appear to act as distinct research communities with distinct topical foci and publishing habits. The two groups also do not appear to engage with each other strongly, either through collaboration or citing each other's work."
2501.1099,"Citations in the scientific literature system do not simply reflect relationships between knowledge but are influenced by non-objective and societal factors. Citation bias, irresponsible citation, and citation manipulation are widespread and have become a serious and growing problem. However, it has been difficult to assess the consequences of mixing societal factors into the literature system because there was no observable literature system unmixed with societal factors for comparison. In this paper, we construct a mathematical theorem network, representing a logic-based and objective knowledge system, to address this problem. By comparing the mathematical theorem network and the scientific citation networks, we find that these two types of networks are significantly different in their structure and function. In particular, the reward function in citation networks is impaired: The scientific citation network fails to provide more recognition for more disruptive results, while the mathematical theorem network can achieve. We develop a network generation model that can create two types of links$\unicode{x2014}$logical and societal$\unicode{x2014}$to account for these differences. The model parameter $q$, which we call the human influence factor, can control the number of societal links and thus regulate the degree of mixing of societal factors in the networks. Under this design, the model successfully reproduces the differences among real networks. These results suggest that the presence of societal factors undermines the function of the scientific reward system. To improve the status quo, we advocate for reforming the reference list format in papers, urging journals to require authors to separately disclose logical references and social references."
2501.12221,"The number of published scholarly articles is growing at a significant rate, making scholarly knowledge organization increasingly important. Various approaches have been proposed to organize scholarly information, including describing scholarly knowledge semantically leveraging knowledge graphs. Transforming unstructured knowledge, presented within articles, to structured and semantically represented knowledge generally requires human intelligence and labor since natural language processing methods alone typically do not render sufficient precision and recall for many applications. With the recent developments of Large Language Models (LLMs), it becomes increasingly possible to provide truly intelligent user interfaces guiding humans in the transformation process. We present an approach to integrate non-intrusive LLMs guidance into existing user interfaces. More specifically, we integrate LLM-supported user interface components into an existing scholarly knowledge infrastructure. Additionally, we provide our experiences with LLM integration, detailing best practices and obstacles. Finally, we evaluate the approach using a small-scale user evaluation with domain experts."
2501.12786,"Recipes of popular origin and handwritten cookbooks are often overlooked by scholars. RagÃ¹ is a pilot project that tries to fill in this gap by gathering and digitising a collection of cookbooks belonging to the Italian traditional cuisine, and making it accessible via a digital platform. The project aims at contributing to two research lines: a) to identify agile methods for publishing data in a low-cost crowdsourcing project, and b) to devise an effective storytelling journey for the RagÃ¹ project."
2501.16197,"HERITRACE is a data editor designed for galleries, libraries, archives and museums, aimed at simplifying data curation while enabling non-technical domain experts to manage data intuitively without losing its semantic integrity. While the semantic nature of RDF can pose a barrier to data curation due to its complexity, HERITRACE conceals this intricacy while preserving the advantages of semantic representation. The system natively supports provenance management and change tracking, ensuring transparency and accountability throughout the curation process. Although HERITRACE functions effectively out of the box, it offers a straightforward customization interface for technical staff, enabling adaptation to the specific data model required by a given collection. Current applications include the ParaText project, and its adoption is already planned for OpenCitations. Future developments will focus on integrating the RDF Mapping Language (RML) to enhance compatibility with non-RDF data formats, further expanding its applicability in digital heritage management."
2501.17013,"In this paper, we provide a technical vision for key enabling elements for the architecture of the UK National Data Library (NDL) with a strong focus on building it as an AI-ready data infrastructure through standardised vocabularies, automated analysis tools, and interoperability services. We follow the ODI Multilayer Interoperability Framework (MIF) for data stewardship, covering central socio-technical aspects for the NDL including user-centric approaches to service design and governance."
2501.1715,"In the era of big science, many national governments are helping to build well-funded teams of scientists to serve nationalistic ambitions, providing financial incentives for certain outcomes for purposes other than advancing science. That in turn can impact the behavior of scientists and create distortions in publication rates, frequency, and publication venues targeted. To that end, we provide evidence that indicates significant inequality using standard Gini Index metrics in the publication rates of individual scientists across various groupings (e.g. country, institution type, ranking-level) based on an intensive analysis of thousands of papers published in several well-known ACM conferences (HRI, IUI, KDD, CHI, SIGGRAPH, UIST, and UBICOMP) over 15 years between 2010 to 2024. Furthermore, scientists who were affiliated with the top-5 countries (in terms of research expenditure) were found to be contributing significantly more to the inequality in publication rates than others, which raises a number of questions for the scientific community. We discuss some of those questions later in the paper. We also detected several examples in the dataset of potential serious ethical problems in publications likely caused by such incentive systems. Finally, a topic modeling analysis revealed that some countries are pursuing a much narrower range of scientific topics relative to others, indicating those incentives may also be limiting genuine scientific curiosity. In summary, our findings raise awareness of systems put in place by certain national governments that may be eroding the pursuit of truth through science and gradually undermining the integrity of the global scientific community."
2501.17456,"Novelty evaluation is vital for the promotion and management of innovation. With the advancement of information techniques and the open data movement, some progress has been made in novelty measurements. Tracking and reviewing novelty measures provides a data-driven way to assess contributions, progress, and emerging directions in the science field. As academic papers serve as the primary medium for the dissemination, validation, and discussion of scientific knowledge, this review aims to offer a systematic analysis of novelty measurements for scientific papers. We began by comparing the differences between scientific novelty and four similar concepts, including originality, scientific innovation, creativity, and scientific breakthrough. Next, we reviewed the types of scientific novelty. Then, we classified existing novelty measures according to data types and reviewed the measures for each type. Subsequently, we surveyed the approaches employed in validating novelty measures and examined the current tools and datasets associated with these measures. Finally, we proposed several open issues for future studies."
2501.18129,"Gender biases in scholarly metrics remain a persistent concern, despite numerous bibliometric studies exploring their presence and absence across productivity, impact, acknowledgment, and self-citations. However, methodological inconsistencies, particularly in author name disambiguation and gender identification, limit the reliability and comparability of these studies, potentially perpetuating misperceptions and hindering effective interventions. A review of 70 relevant publications over the past 12 years reveals a wide range of approaches, from name-based and manual searches to more algorithmic and gold-standard methods, with no clear consensus on best practices. This variability, compounded by challenges such as accurately disambiguating Asian names and managing unassigned gender labels, underscores the urgent need for standardized and robust methodologies. To address this critical gap, we propose the development and implementation of ``Scholarly Data Analysis (SoDA) Cards."" These cards will provide a structured framework for documenting and reporting key methodological choices in scholarly data analysis, including author name disambiguation and gender identification procedures. By promoting transparency and reproducibility, SoDA Cards will facilitate more accurate comparisons and aggregations of research findings, ultimately supporting evidence-informed policymaking and enabling the longitudinal tracking of analytical approaches in the study of gender and other social biases in academia."
2502.00874,"The rapid growth of submissions to top-tier Artificial Intelligence (AI) and Machine Learning (ML) conferences has prompted many venues to transition from closed to open review platforms. Some have fully embraced open peer reviews, allowing public visibility throughout the process, while others adopt hybrid approaches, such as releasing reviews only after final decisions or keeping reviews private despite using open peer review systems. In this work, we analyze the strengths and limitations of these models, highlighting the growing community interest in transparent peer review. To support this discussion, we examine insights from Paper Copilot, a website launched two years ago to aggregate and analyze AI / ML conference data while engaging a global audience. The site has attracted over 200,000 early-career researchers, particularly those aged 18-34 from 177 countries, many of whom are actively engaged in the peer review process. Drawing on our findings, this position paper advocates for a more transparent, open, and well-regulated peer review aiming to foster greater community involvement and propel advancements in the field."
2502.01407,"Data sharing is fundamental to scientific progress, enhancing transparency, reproducibility, and innovation across disciplines. Despite its growing significance, the variability of data-sharing practices across research fields remains insufficiently understood, limiting the development of effective policies and infrastructure. This study investigates the evolving landscape of data-sharing practices, specifically focusing on the intentions behind data release, reuse, and referencing. Leveraging the PubMed open dataset, we developed a model to identify mentions of datasets in the full-text of publications. Our analysis reveals that data release is the most prevalent sharing mode, particularly in fields such as Commerce, Management, and the Creative Arts. In contrast, STEM fields, especially the Biological and Agricultural Sciences, show significantly higher rates of data reuse. However, the humanities and social sciences are slower to adopt these practices. Notably, dataset referencing remains low across most disciplines, suggesting that datasets are not yet fully recognized as research outputs. A temporal analysis highlights an acceleration in data releases after 2012, yet obstacles such as data discoverability and compatibility for reuse persist. Our findings can inform institutional and policy-level efforts to improve data-sharing practices, enhance dataset accessibility, and promote broader adoption of open science principles across research domains."
2502.01417,"In this research-in-progress paper, we apply a computational measure correlating with originality from creativity science: Divergent Semantic Integration (DSI), to a selection of 99,557 scientific abstracts and titles selected from the Web of Science. We observe statistically significant differences in DSI between subject and field of research, and a slight rise in DSI over time. We model the base 10 logarithm of the citation count after 5 years with DSI and find a statistically significant positive correlation in all fields of research with an adjusted $R^2$ of 0.13."
2502.01525,"Although web advertisements represent an inimitable part of digital cultural heritage, serious archiving and replay challenges persist. To explore these challenges, we created a dataset of 279 archived ads. We encountered five problems in archiving and replaying them. For one, prior to August 2023, Internet Archive's Save Page Now service excluded not only well-known ad services' ads, but also URLs with ad related file and directory names. Although after August 2023, Save Page Now still blocked the archiving of ads loaded on a web page, it permitted the archiving of an ad's resources if the user directly archived the URL(s) associated with the ad. Second, Brozzler's incompatibility with Chrome prevented ads from being archived. Third, during crawling and replay sessions, Google's and Amazon's ad scripts generated URLs with different random values. This precluded archived ads' replay. Updating replay systems' fuzzy matching approach should enable the replay of these ads. Fourth, when loading Flashtalking web page ads outside of ad iframes, the ad script requested a non-existent URL. This, prevented the replay of ad resources. But as was the case with Google and Amazon ads, updating replay systems' fuzzy matching approach should enable Flashtalking ads' replay. Finally, successful replay of ads loaded in iframes with the src attribute of ""about:blank"" depended upon a given browser's service worker implementation. A Chromium bug stopped service workers from accessing resources inside of this type of iframe, which in turn prevented replay. Replacing the ""about:blank"" value for the iframe's src attribute with a blob URL before an ad was loaded solved this problem. Resolving these replay problems will improve the replay of ads and other dynamically loaded embedded web resources that use random values or ""about:blank"" iframes."
2502.02321,"This paper aims to bridge between the current scientific discourse about the dynamics of data communities in research infrastructures and practical experiences at a data archive which provides services for such data communities. We describe and analyse policies and practices within DANS-KNAW, the Dutch national centre of expertise and repository for research data concerning the interaction with communities in general. We take the case of the emerging DANS Data Station Life Sciences to study how a data archive navigates between observation of data research needs and anticipation of research data archival solutions. This paper offers a unique view of the complex dynamics between data communities (including lay experts) and data service providers. It adds nuances to understanding the emergence of a data community and the role of data service providers, both supporting and shaping, in this process."
2502.03059,"We report on a preliminary investigation into the current scope of research in information management, adopting a conceptual approach derived from previous work by HjÃ¸rland in information science and by Palvia in information systems. We created a data-set of 107 articles resulting from a search in Web of Science, using the search strategy of the term information management in the titles of articles, and then restricting the analysis to those journals we identified as having an information science orientation, rather than an information systems orientation. The analysis reveals the International Journal of Information Management as the most significant journal in the field, but also draws attention to the rise of interest in the field through contributions to two Brazilian journals and one Spanish journal. The thematic analysis revealed that the dominant research themes from the information science perspective were empirical user studies, studies of the structural and institutional approach, and information system usage and adoption. Further work will be undertaken to explore the relevance of the approach in the analysis of other document sets from areas such as health care, construction and engineering."
2502.04944,"A small amount of unscrupulous people, concerned by their career prospects, resort to paper mill services to publish articles in renowned journals and conference proceedings. These include patchworks of synonymized contents using paraphrasing tools, featuring tortured phrases, increasingly polluting the scientific literature. The Problematic Paper Screener (PPS) has been developed to allow articles (re)assessment on PubPeer. Since most of the known tortured phrases are found in publications in science, technology, engineering, and mathematics (STEM), we extend this work by exploring their presence in the humanities and social sciences (HSS). To do so, we used the PPS to look for tortured abbreviations, generated from the two social science thesauri ELSST and THESOZ. We also used two case studies to find new tortured abbreviations, by screening the Hindawi EDRI journal and the GESIS SSOAR repository. We found a total of 32 multidisciplinary problematic documents, related to Education, Psychology, and Economics. We also generated 121 new fingerprints to be added to the PPS. These articles and future screening have to be investigated by social scientists, as most of it is currently done by STEM domain experts."
2502.05781,"We introduce reputable citations (RC), a method to screen and segment a collection of papers by decoupling popularity and influence. We demonstrate RC using recent works published in a large set of mathematics journals from Clarivate's Incites Essential Science Indicators, leveraging Clarivate's Web of Science for citation reports and assigning prestige values to institutions based on well-known international rankings. We compare researchers drawn from two samples: highly cited researchers (HC) and mathematicians whose influence is acknowledged by peers (Control). RC scores distinguish the influence of researchers beyond citations, revealing highly cited mathematical work of modest influence. The control group, comprising peer-acknowledged researchers, dominates the top tier of RC scores despite having fewer total citations than the HC group. Influence, as recognized by peers, does not always correlate with high citation counts, and RC scores offer a nuanced distinction between the two. With development, RC scores could automate screening of citations to identify exceptional and influential research, while addressing manipulative practices. The first application of RC reveals mathematics works that may be cited for reasons unrelated to genuine research advancements, suggesting a need for continued development of this method to mitigate such trends."
2502.0619,"Using large-scale citation data and a breakthrough metric, the study systematically evaluates the inevitability of scientific breakthroughs. We find that scientific breakthroughs emerge as multiple discoveries rather than singular events. Through analysis of over 40 million journal articles, we identify multiple discoveries as papers that independently displace the same reference using the Disruption Index (D-index), suggesting functional equivalence. Our findings support Merton's core argument that scientific discoveries arise from historical context rather than individual genius. The results reveal a long-tail distribution pattern of multiple discoveries across various datasets, challenging Merton's Poisson model while reinforcing the structural inevitability of scientific progress."
2502.08171,"Although indicators based on scholarly citations are widely used to support the evaluation of academic journals, alternatives are needed for scholarly book acquisitions. This article assesses the value of research quality scores from ChatGPT 4o-mini for 9,830 social sciences, arts, and humanities books from 2019 indexed in Scopus, based on their titles and descriptions but not their full texts. Although most books scored the same (3* on a 1* to 4* scale), the citation rates correlate positively but weakly with ChatGPT 4o-mini research quality scores in both the social sciences and the arts and humanities. Part of the reason for the differences was the inclusion of textbooks, short books, and edited collections, all of which tended to be less cited and lower scoring. Some topics also tend to attract many/few citations and/or high/low ChatGPT scores. Descriptions explicitly mentioning theory and/or some methods also associated with higher scores and more citations. Overall, the results provide some evidence that both ChatGPT scores and citation counts are weak indicators of the research quality of books. Whilst not strong enough to support individual book quality judgements, they may help academic librarians seeking to evaluate new book collections, series, or publishers for potential acquisition."
2502.10496,"The study aims to identify the process of transfer from science to technology that occurs in the main Portuguese public universities. The methodology was based on the analysis of the scientific literature cited in patents. Data was obtained from the Lens patent database. 10,514 scientific articles cited in patents were retrieved. A descriptive analysis of the data was performed. Science maps were created to visualize the main research trends. The results showed a valuable impact of academic research in certain scientific disciplines, such as Chemistry, Biology, Materials Sciences and Medicine. The main research fronts were cancer, nanoparticles, biomaterials, tissue engineering or molecular biology. In conclusion, the research produced by Portuguese universities has generated relevant knowledge for patented inventions and the science-technology flow within specific areas."
2502.11111,"Multi-Valued Logic (MVL) has more than one logic level defined to represent data whereas binary logic has 2 logic levels. It has been shown that the MVL circuits use the circuit resources more effectively at different voltage levels with less circuitry and greater efficiency. Recently, graphene nano-ribbon field effect transistor (GNRFET) has drawn a lot of interest due to its higher electron mobility. This paper presents quaternary decoder implemented in GNRFET and analyzed latency, power, performance etc. also compared the power and delay characteristics of the design implemented both in CMOS and Graphene Nano Ribbon Field Effect Transistor (GNRFET) in the 32nm technology node."
2502.11923,"This editorial explores the significance of research visibility within the evolving landscape of academic communication, mainly focusing on the role of search engines as online meta-markets shaping the impact of research. With the rapid expansion of scientific output and the increasing reliance on algorithm-driven platforms such as Google and Google Scholar, the online visibility of scholarly work has become an essential factor in determining its reach and influence. The need for more rigorous research into academic search engine optimization (A-SEO), a field still in its infancy despite its growing relevance, is also discussed, highlighting key challenges in the field, including the lack of robust research methodologies, the skepticism within the academic community regarding the commercialization of science, and the need for standardization in reporting and measurement techniques. This editorial thus invites a multidisciplinary dialogue on the future of research visibility, with significant implications for academic publishing, science communication, research evaluation, and the global scientific ecosystem."
2502.12795,"The effective and targeted provision of health information to consumers, specifically tailored to their needs and preferences, is indispensable in healthcare. With access to appropriate health information and adequate understanding, consumers are more likely to make informed and healthy decisions, become more proficient in recognizing symptoms, and potentially experience improvements in the prevention or treatment of their medical conditions. Most of today's health information, however, is provided in the form of static documents. In this paper, we present a novel and innovative visual health information system based on adaptive document visualizations. Depending on the user's information needs and preferences, the system can display its content with document visualization techniques at different levels of detail, aggregation, and visual granularity. Users can navigate using content organization along sections or automatically computed topics, and choose abstractions from full texts to word clouds. Our first contribution is a formative user study which demonstrated that the implemented document visualizations offer several advantages over traditional forms of document exploration. Informed from that, we identified a number of crucial aspects for further system development. Our second contribution is the introduction of an interaction provenance visualization which allows users to inspect which content, in which representation, and in which order has been received. We show how this allows to analyze different document exploration and navigation patterns, useful for automatic adaptation and recommendation functions. We also define a baseline taxonomy for adapting the document presentations which can, in principle, be leveraged by the observed user patterns. The interaction provenance view, furthermore, allows users to reflect on their exploration and inform future usage of the system."
2502.12891,"Special functions are essential in theoretical and applied mathematics and have various applications in the applied sciences. Mathematicians have studied them for centuries, but there is still no bibliometric analysis that summarises the datasets of publications showing different network visualisations, such as co-author and keyword visualisations, basic keyword statistics and other data analyses. This work appears to be the first attempt to fill this gap by presenting different network visualisations based on 4025 documents with the keyword ""special function"" in their title, abstract or keywords belonging to the field of mathematics in the Scopus database. We also show that special functions are rarely used in geometric modelling, a mathematical foundation for CAD, industrial design, architecture, and other applied fields, and we discuss how different visualisations for special functions can be generated in the Julia programming language. The generated image and video visualisations can be helpful to academics to see the impact of different authors, to define their new research topics, to see connections or lack thereof between various topics, to find the most popular special functions, or for teaching purposes to show the importance of special functions and links to other topics in modern pure and applied mathematics and other sciences."
2502.13567,"A study of the Revista General de Informacion y Documentacion, from 2005 to 2022. The objective is aimed at qualifying the structure of the research field and assessing the trajectory of the thematic areas covered. Applying as methodology the analysis of co-words, the construction of bibliometric networks and the creation of scientific maps. 514 documents are extracted from the Web of Science (WoS) database. The keywords assigned by the authors of the documents are selected and divided into three subperiods: 2005-2010, 2011-2016 and 2017-2022. In the results, 1701 author keywords and 37 bibliometric networks are obtained. In the period 2005-2010, the structure of the research field is represented on the scientific map with very few central and specialized topics, considering an initial and underdeveloped organization. In the period 2011-2016, the structure of the research field is distributed on the scientific map with a more varied number of central and specialized topics, but still insufficient, considering an organization in the process of development. In the period 2017-2022, the structure of the research field is shown on the map with all kinds of family of topics (central, specialized, transversal, emerging or disappearing), being valued as a dynamic, complex and heterogeneous organization. Regarding the evolution of the thematic areas, the map shows solid progress between the last two periods. The morphology of the thematic field treated in RGID is outlined in three phases: foundation, process of development and consolidation."
2502.13934,"Citations are a key indicator of research impact but are shaped by factors beyond intrinsic research quality, including prestige, social networks, and thematic similarity. While the Matthew Effect explains how prestige accumulates and influences citation distributions, our study contextualizes this by showing that other mechanisms also play a crucial role. Analyzing a large dataset of disambiguated authors (N=43,467) and citation linkages (N=264,436) in U.S. economics, we find that close ties in the collaboration network are the strongest predictor of citation, closely followed by thematic similarity between papers. This reinforces the idea that citations are not only a matter of prestige but mostly of social networks and intellectual proximity. Prestige remains important for understanding highly cited papers, but for the majority of citations, proximity--both social and semantic--plays a more significant role. These findings shift attention from extreme cases of highly cited research toward the broader distribution of citations, which shapes career trajectories and the production of knowledge. Recognizing the diverse factors influencing citations is critical for science policy, as this work highlights inequalities that are not based on preferential attachment, but on the role of self-citations, collaborations, and mainstream versus no mainstream research subjects."
2502.14402,"This paper analyzes the scientific production on the COVID-19 effect in the area of Information Sciences from a bibliometric perspective. The objectives focused on: 1) determining the most productive authors, countries, institutions and journals; 2) identifying the sources that constitute the core of scientific production; 3) examining the manuscripts with the greatest impact; and 4) visualizing the thematic and conceptual structure of the scientific domain analyzed. Bibliometric indicators and factor analysis techniques were used for data analysis. A total of 1,175 publications indexed in the Web of Science (WoS) core collection from 2020 to 2022 were retrieved. The results showed that the most relevant countries were the United States, United Kingdom, China and Spain. The core of the scientific production was formed by the publications: Journal of the American Medical Informatics Association, Information Professional, Scientometrics and Journal of Health Communication. The papers with the greatest impact were concentrated in those dedicated to the analysis of the role of telemedicine in medical care. The conceptual structure showed the main research fronts, such as the role of telehealth, academic libraries and digital literacy in the fight against the pandemic, the role of social networks in the health crisis, as well as the problem of misinformation and fake news"
2502.1457,"This study explores the connection between patent citations and scientific publications across six fields: Biochemistry, Genetics, Pharmacology, Engineering, Mathematics, and Physics. Analysing 117,590 papers from 2014 to 2023, the research emphasises how publication year, open access (OA) status, and discipline influence patent citations. Openly accessible papers, particularly those in hybrid OA journals or green OA repositories, are significantly more likely to be cited in patents, seven times more than those mentioned in blogs, and over twice as likely compared to older publications. However, papers with policy-related references are less frequently cited, indicating that patents may prioritise commercially viable innovations over those addressing societal challenges. Disciplinary differences reveal distinct innovation patterns across sectors. While academic visibility via blogs or platforms like Mendeley increases within scholarly circles, these have limited impact on patent citations. The study also finds that increased funding, possibly tied to applied research trends and fully open access journals, negatively affects patent citations. Social media presence and the number of authors have minimal impact. These findings highlight the complex factors shaping the integration of scientific research into technological innovations."
2502.15692,"Familiarizing oneself with a new scientific field and its existing literature can be daunting due to the large amount of available articles. Curated lists of academic references, or reading lists, compiled by experts, offer a structured way to gain a comprehensive overview of a domain or a specific scientific challenge. In this work, we introduce ACL-rlg, the largest open expert-annotated reading list dataset. We also provide multiple baselines for evaluating reading list generation and formally define it as a retrieval task. Our qualitative study highlights the fact that traditional scholarly search engines and indexing methods perform poorly on this task, and GPT-4o, despite showing better results, exhibits signs of potential data contamination."
2502.18427,"This article compares (1) citation analysis with OpenAlex and Scopus, testing their citation counts, document type/coverage and subject classifications and (2) three citation-based indicators: raw counts, (field and year) Normalised Citation Scores (NCS) and Normalised Log-transformed Citation Scores (NLCS). Methods (1&2): The indicators calculated from 28.6 million articles were compared through 8,704 correlations on two gold standards for 97,816 UK Research Excellence Framework (REF) 2021 articles. The primary gold standard is ChatGPT scores, and the secondary is the average REF2021 expert review score for the department submitting the article. Results: (1) OpenAlex provides better citation counts than Scopus and its inclusive document classification/scope does not seem to cause substantial field normalisation problems. The broadest OpenAlex classification scheme provides the best indicators. (2) Counterintuitively, raw citation counts are at least as good as nearly all field normalised indicators, and better for single years, and NCS is better than NLCS. (1&2) There are substantial field differences. Thus, (1) OpenAlex is suitable for citation analysis in most fields and (2) the major citation-based indicators seem to work counterintuitively compared to quality judgements. Field normalisation seems ineffective because more cited fields tend to produce higher quality work, affecting interdisciplinary research or within-field topic differences."
2502.1936,"Sustaining knowledge infrastructures (KIs) remains a persistent issue that requires continued engagement from diverse stakeholders. This is due to the complexity of KIs and sustainability, as well as to new questions and values that are arising in relation to KI maintenance. In this commentary, we draw on existing literature and our experiences at a workshop for researchers exploring KI evaluation to pose five directions of thinking which are especially relevant for KI project managers to consider when thinking about how to make their KIs stand the test of time."
2502.19663,"English is widely used as a lingua franca in scholarly communication, yet preserving local languages is vital to reaching a broader audience. Disseminating research in multiple languages can help ensure equitable access, a responsibility shared by both publishers and authors. This study examines the practices of both groups to identify any notable differences. Several academic social networks, preprint servers, and repositories are analysed to evaluate the resources currently available and their existing policies. Additionally, journals that actively promote multilingual dissemination are reviewed to understand their implementation strategies and how these align with the standards set by the DOI Registration Agency (DOI RA). From the author's perspective, differing policies across platforms can heavily influence decisions, mainly because not all platforms provide relationship metadata. Publishers face similar challenges, underscoring the urgent need for standardisation. Moreover, the lack of consistency creates opportunities for unethical practices in academia, such as counting total of citations originating from the same article in different languages. This highlights the importance of a more comprehensive approach to evaluating research beyond citation and document counts. Collaboration among publishers, authors, and other stakeholders is essential to fostering greater understanding and preventing misconceptions in the academic landscape."
2502.19679,"Large Language Models, despite their power, have a fundamental architectural vulnerability stemming from their causal transformer design -- order sensitivity. This architectural constraint may distorts classification outcomes when prompt elements like label options are reordered, revealing a theoretical gap between accuracy metrics and true model reliability. The paper conceptualizes this vulnerability through the lens of survey methodology, where respondent biases parallel LLM positional dependencies. Empirical evidence using the F1000 biomedical dataset across three scales of LLaMA3.1 models (8B, 70B, 405B) demonstrates that these architectural constraints produce inconsistent annotations under controlled perturbations. The paper advances a practical solution for social science - Independent Probability Assessment - which decouples label evaluation to circumvent positional bias inherent in sequential processing. This approach yields an information-theoretic reliability measure (R-score) that quantifies annotation robustness at the case level. The findings establish that architectural vulnerabilities in causal transformers require methodological innovations beyond accuracy metrics to ensure valid social science inference, as demonstrated through downstream regression analyses where order-sensitive annotations significantly alter substantive conclusions about scientific impact."
2503.01031,"Academic codes associated with research papers are valuable resources for scholars. In specialized fields outside computer science, code availability is often limited, making effective code retrieval essential. Google Scholar is a crucial academic search tool. If a code published in the paper is not retrievable via Google Scholar, its accessibility and impact are significantly reduced. This study takes the term ""accelerated degradation"" combined with ""reliability"" as an example, and finds that, for papers published by Elsevier, only GitHub links included in abstracts are comprehensively retrieved by Google Scholar. When such links appear within the main body of a paper, even in the ""Data Availability"" section, they may be ignored and become unsearchable. These findings highlight the importance of strategically placing GitHub links in abstracts to enhance code discoverability on Google Scholar."
2503.02671,"Scholars, awards committees, and laypeople frequently discuss the merit of written works. Literary professionals and journalists differ in how much perspectivism they concede in their book reviews. Here, we quantify how strongly book reviews are determined by the actual book contents vs. idiosyncratic reader tendencies. In our analysis of 624,320 numerical and textual book reviews, we find that the contents of professionally published books are not predictive of a random reader's reading enjoyment. Online reviews of popular fiction and non-fiction books carry up to ten times more information about the reviewer than about the book. For books of a preferred genre, readers might be less likely to give low ratings, but still struggle to converge in their relative assessments. We find that book evaluations generalize more across experienced review writers than casual readers. When discussing specific issues with a book, one review text had poor predictability of issues brought up in another review of the same book. We conclude that extreme perspectivism is a justifiable position when researching literary quality, bestowing literary awards, and designing recommendation systems."
2503.03251,"As research in the Scientometric deepens, the impact of data quality on research outcomes has garnered increasing attention. This study, based on Web of Science (WoS) and Crossref datasets, systematically evaluates the differences between data sources and the effects of data merging through matching, comparison, and integration. Two core metrics were employed: Reference Coverage Rate (RCR) and Article Scientific Prestige (ASP), which respectively measure citation completeness (quantity) and academic influence (quality). The results indicate that the WoS dataset outperforms Crossref in its coverage of high-impact literature and ASP scores, while the Crossref dataset provides complementary value through its broader coverage of literature. Data merging significantly improves the completeness of the citation network, with particularly pronounced benefits in smaller disciplinary clusters such as Education and Arts. However, data merging also introduces some low-quality citations, resulting in a polarization of overall data quality. Moreover, the impact of data merging varies across disciplines; high-impact clusters such as Science, Biology, and Medicine benefit the most, whereas clusters like Social Sciences and Arts are more vulnerable to negative effects. This study highlights the critical role of data sources in Scientometric research and provides a framework for assessing and improving data quality."
2503.033,"Finding enjoyable fiction books can be challenging, partly because stories are multi-faceted and one's own literary taste might be difficult to ascertain. Here, we introduce the ISAAC method (Introspection-Support, AI-Annotation, and Curation), a pipeline which supports fiction readers in gaining awareness of their literary preferences and finding enjoyable books. ISAAC consists of four steps: a user supplies book ratings, an AI agent researches and annotates the provided books, patterns in book enjoyment are reviewed by the user, and the AI agent recommends new books. In this proof-of-concept self-study, the authors test whether ISAAC can highlight idiosyncratic patterns in their book enjoyment, spark a deeper reflection about their literary tastes, and make accurate, personalized recommendations of enjoyable books and underexplored literary niches. Results highlight substantial advantages of ISAAC over existing methods such as an integration of automation and intuition, accurate and customizable annotations, and explainable book recommendations. Observed disadvantages are that ISAAC's outputs can elicit false self-narratives (if statistical patterns are taken at face value), that books cannot be annotated if their online documentation is lacking, and that people who are new to reading have to rely on assumed book ratings or movie ratings to power the ISAAC pipeline. We discuss additional opportunities of ISAAC-style book annotations for the study of literary trends, and the scientific classification of books and readers."
2503.03611,"This paper pays a tribute to Loet's work in a specific way. More than 20 years ago Loet Leydesdorff and myself designed a programme for future innovation studies 'Measuring the knowledge base - a programme of innovation studies'. Although, the funding programme we envisioned eventually did not materialise, the proposal text set out the main lines of our research collaboration over the coming decades. This paper revisits main statements of this programme and discusses their remaining validity in the light of more recent research. Core to the Leydesdorff and Scharnhorst text was a system-theoretical, evolutionary perspective on science dynamics, newly emerging structures and phenomena and addressed the question to which extent they could be meaningful studied using quantitative approaches. This paper looks into three cases - all examples of newly emerging institutional structures and related practices in science. They are all located at the interface between research and research infrastructures. While discussing how the programmatic ideas written up at the beginning of the 2000s still informs measurement attempts in those three cases, the paper also touches upon questions of epistemological foundations of quantitative studies in general. The main conclusion is that combining measurement experiments with philosophical reflection remains important."
2503.04324,"In this study, we investigated a phenomenon that one intuitively would assume does not exist: self-citations on the paper basis. Actually, papers citing themselves do exist in the Web of Science (WoS) database. In total, we obtained 44,857 papers that have self-citation relations in the WoS raw dataset. In part, they are database artefacts but in part they are due to papers citing themselves in the conclusion or appendix. We also found cases where paper self-citations occur due to publisher-made highlights promoting and citing the paper. We analyzed the self-citing papers according to selected metadata. We observed accumulations of the number of self-citing papers across publication years. We found a skewed distribution across countries, journals, authors, fields, and document types. Finally, we discuss the implications of paper self-citations for bibliometric indicators."
2503.0555,"Google Scholar is a vital tool for engineering scholars, enabling efficient literature searches and facilitating academic dissemination. Elsevier, as one of the largest publishers of engineering journals, produces essential research that scholars rely on. The pre-proof policy, adopted by Elsevier for certain journals, allows articles to be published online in their accepted draft form before final proofreading and formatting. However, this study empirically demonstrates that the pre-proof publication policy hinders comprehensive indexing by Google Scholar. Articles published under this policy are only partially indexed, often limited to titles and abstracts, while crucial sections such as introductions, methods, results, discussions, conclusions, appendices, and data availability statements remain unsearchable. This problem has persisted for years, resulting in reduced visibility and accessibility of certain Elsevier articles. To improve academic dissemination, both Elsevier and Google Scholar must address this problem by modifying publishing policies or enhancing indexing practices. Additionally, this paper explores strategies that authors can use to mitigate the issue and ensure broader discoverability of their research."
2503.06871,"The development of digital humanities necessitates scholars to adopt more data-intensive methods and engage in multidisciplinary collaborations. Understanding their collaborative data behaviors becomes essential for providing more curated data, tailored tools, and a collaborative research environment. This study explores how interdisciplinary researchers collaborate on data activities by conducting focus group interviews with 19 digital humanities research groups. Through inductive coding, the study identified seven primary and supportive data activities and found that different collaborative modes are adopted in various data activities. The collaborative modes include humanities-driven, technically-driven, and balanced, depending on how team members naturally adjusted their responsibilities based on their expertise. These findings establish a preliminary framework for examining collaborative data behavior and interdisciplinary collaboration in digital humanities."
2503.09811,"Understanding the mechanisms driving the distribution of scientific citations is a key challenge in assessing the scientific impact of authors. We investigate the influence of the preferential attachment rule (PAR) in this process by analysing individual citation events from the DBLP dataset and two Scopus-based datasets, enabling us to estimate the probability of citations being assigned preferentially. Our findings reveal that, for the aggregated dataset, PAR dominates the citation distribution process, with approximately 70% of citations adhering to this mechanism. However, analysis at the individual level shows significant variability, with some authors experiencing a greater prevalence of preferential citations, particularly in the context of external citations. In contrast, self-citations exhibit notably different behaviour, with only 20% following PAR. We also demonstrate that the prominence of PAR increases with an author's citability (average citations per paper), suggesting that more citable authors are preferentially cited, while less-cited authors experience more random citation patterns. Furthermore, we show that self-citations may influence bibliometric indices, such as the h-index. Our results confirm the distinct dynamics of self-citations compared to external citations, raising questions about the mechanisms driving self-citation patterns. These findings provide new insights into citation behaviours and highlight the limitations of existing approaches."
2503.11853,"Introduction: Scholarly research spans multiple languages, making multilingual metadata crucial for organizing and accessing knowledge across linguistic boundaries. These multilingual metadata already exist and are propagated throughout scholarly publishing infrastructure, but the extent to which they are correctly recorded, or how they affect metadata quality more broadly is little understood.Methods: Our study quantifies the prevalence of multilingual records across a sample of publisher metadata and offers an understanding of their completeness, quality, and alignment with metadata standards. Utilizing the Crossref API to generate a random sample of 519,665 journal article records, we categorize each record into four distinct language types: English monolingual, non-English monolingual, multilingual, and uncategorized. We then investigate the prevalence of programmatically-detectable errors and the prevalence of multilingual records within the sample to determine whether multilingualism influences the quality of article metadata.Results: We find that English-only records are still in the vast majority among metadata found in Crossref, but that, while non-English and multilingual records present unique challenges, they are not a source of significant metadata quality issues and, in few instances, are more complete or correct than English monolingual records.Discussion & Conclusion: Our findings contribute to discussions surrounding multilingualism in scholarly communication, serving as a resource for researchers, publishers, and information professionals seeking to enhance the global dissemination of knowledge and foster inclusivity in the academic landscape."
2503.13238,"The 2010-2011 Arab Spring reverberated far beyond politics, reshaping how the Middle East and North Africa region (MENA) is studied. Analyzing 3.7 million Scopus-indexed articles published between 2002 and 2019, we find that mentions of ten of these countries in titles or abstracts rose significantly after 2011 relative to the global baseline, with Egypt receiving the greatest attention in the region. We link this surge to two intertwined mechanisms: an increase in research funding directed at the MENA region and the emigration of researchers who continued publishing on their countries of origin. Our analysis reveals that Saudi Arabia has emerged as a regional hub for studying the affected countries, attracting funding and scholars, and thereby playing a significant role in shaping the scientific narrative on the region. These findings demonstrate how political upheaval can reshape global knowledge flows by altering who studies whom, with what resources, and in which disciplines."
2503.13448,"Author Name Disambiguation (AND) is a critical task for digital libraries aiming to link existing authors with their respective publications. Due to the lack of persistent identifiers used by researchers and the presence of intrinsic linguistic challenges, such as homonymy, the development of Deep Learning algorithms to address this issue has become widespread. Many AND deep learning methods have been developed, and surveys exist comparing the approaches in terms of techniques, complexity, performance. However, none explicitly addresses AND methods in the context of deep learning in the latest years (i.e. timeframe 2016-2024). In this paper, we provide a systematic review of state-of-the-art AND techniques based on deep learning, highlighting recent improvements, challenges, and open issues in the field. We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning."
2503.13449,"Character recognition is a technique that enables the automated extraction of characters from texts, while coreference resolution establishes connections between various mentions of the same character, collectively facilitating the creation of expansive character networks (Moretti, 2011). Together, these technologies make it possible to navigate and analyze large literary corpora, opening new avenues for in-depth exploration and understanding of literature. We have created a system specifically for the French language, based on BookNLP-fr (the French counterpart of BookNLP) and NetworkX (a Python package for the manipulation and visualization of complex networks). This allows us to establish connections between series of literary works based on structural features (such as typical relationships between characters) or specific subgenres (for instance, adventure novels featuring a group of young heroes). In this paper, as an illustration, we show the networks obtained at different stages of the short novel Boule de Suif from Maupassant (a French 19th century novelist). These figures effectively illustrate how the relationships between the characters develop over the course of the story."
2503.13451,"Computer science research spans a diverse array of topics, with scholars exploring numerous subfields. This paper examines the self-reported research interests of the top 3,260 most cited computer science authors on Google Scholar. Using the scholarly Python library, we systematically retrieved and classified their interests into predefined categories based on the Computer Science Ontology (CSO). The analysis highlights a hierarchy of primary research areas, including Artificial Intelligence, Software Engineering, Data Mining, and Computer Systems. Additionally, it investigates the distribution of these interests, identifying emerging trends, established fields, and areas with relatively less attention. These findings provide a current snapshot of research priorities and serve as a foundation for guiding future studies in computer science."
2503.13452,"This report, authored in 2003, presents an innovative approach to the management and utilization of audiovisual archives in the humanities and social sciences. Developed by the research team ESCoM, under the auspices of the Maison des Sciences de l'Homme (MSH) in Paris, this program predated platforms like YouTube and was groundbreaking in its vision for the digital preservation, segmentation, and classification of audiovisual content. Its objectives included creating a heritage of scientific knowledge, developing advanced tools for its annotation and reuse, and facilitating the dissemination of specialized research to a broadthis http URLits core, the report outlines the development of an integrated environment that allows users to index, annotate, and classify audiovisual segments through personalized ontologies and thematic grids. The proposed methods rely on cutting-edge concepts, such as semantic web technologies, knowledge representation, and conceptual graph editing, to enable researchers and educators to create tailored archives and new multimedia resources. This forward-thinking approach aligns with modern practices of content reuse and republication, demonstrating a vision well ahead of itsthis http URLprogram also emphasizes the importance of segmenting and indexing audiovisual materials based on user-defined criteria, enabling researchers to identify and highlight specific thematic or conceptual elements within a vast pool of data. By facilitating this level of granularity, the system supports personalized academic and professional applications, including multimedia presentations, educational resources, and research dissemination. It introduces tools such as enhanced media players, ontology builders, and annotation editors to make this process accessible andthis http URL, the report discusses the Opales project, a collaborative initiative that exemplifies this innovative framework. The project developed a prototype environment integrating tools for creating ''hyper-documents'' and supporting multilingual, multi-platform content dissemination. Despite the technological and methodological challenges of the time, the report's vision of interactive, richly annotated audiovisual archives has set the stage for the development of contemporary digital knowledge ecosystems. Its emphasis on semantic representation and user-centric customization continues to resonate in the digital humanities today."
2503.13453,"E-Semiotics is a conceptual and practical framework for designing, developing, and managing digital information and knowledge products. It applies semiotic principles to digital environments, focusing on the structural, contextual, and narrative organization of information. Central to E-Semiotics is the concept of ''scenario building,'' which acts as a template or guide for creating and maintaining digital products and services, ensuring usability, adaptability, andthis http URLapproach distinguishes itself from traditional semiotics by addressing the unique features of digital media, such as interactivity, hypertextuality, and modularity. It requires a dual competency in semiotics and technology, making it particularly relevant for developing interactive digital products like e-learning systems, digital libraries, and web portals. E-Semiotics also integrates seamlessly with knowledge management, offering conceptual models and technological tools to optimize the storage, retrieval, and dissemination ofthis http URLmethodology includes both a semiotic approach, which focuses on understanding the structural and contextual dimensions of information, and a technological approach, which ensures interoperability, reusability, and scalability of digital tools. It has broad applications in areas such as multi-support publishing, semantic web development, and the creation of dynamic websites and web services. These applications empower organizations, particularly small and medium-sized ones, to leverage digital technologies without extensive technical expertise.E-Semiotics faces challenges like conceptual complexity and economic barriers, but its potential lies in democratizing access to digital tools and fostering innovation. It bridges the gap between theory and practice, offering scalable solutions that respond to evolving user needs. This framework is poised to play a critical role in the digital transformation of communication and knowledge systems, supporting organizations in adapting to the demands of a rapidly changing digital landscape."
2503.13454,"Building Performance Simulation (BPS) uses advanced computational and data science methods. Reproducibility, the ability to obtain the same results by using the same data and methods, is essential in BPS research to ensure the reliability and validity of scientific results. The benefits of reproducible research include enhanced scientific integrity, faster scientific advancements, and valuable educational resources. Despite its importance, reproducibility in BPS is often overlooked due to technical complexities, insufficient documentation, and cultural barriers such as the lack of incentives for sharing code and data. This paper encourages the reproducibility of articles on computational science and proposes to recognize reproductible code and data, with persistent Digital Object Identifier (DOI), as peer-reviewed archival publications. Practical workflows for achieving reproducibility in BPS are presented for the use of MATLAB and Python."
2503.13456,"The h-index has become a widely used metric for evaluating the productivity and citation impact of researchers. Introduced by physicist Jorge E. Hirsch in 2005, the h-index measures both the quantity (number of publications) and quality (citations) of a researcher's output. While it has gained popularity for its simplicity and practicality, the h-index is not without its limitations. We examine the strengths and weaknesses of this metric, presenting preliminary experimental results that demonstrate the limitations of the h-index. We also propose a potential solution. The primary aim of this work is to shed light on the shortcomings of the h-index and its implications for ranking scientists, motivating them, allocating funding, and advancing science."
2503.13459,"The visibility of research projects is crucial for maximizing their scientific and societal impact. Current Research Information Systems (CRIS) centralize data, enhancing access, the dissemination of results, and interdisciplinary collaboration. This article examines how CRIS improves global reach, funding opportunities, and adherence to open science principles, contrasting these advantages with the limitations faced by projects without such systems. CRIS proves to be essential tools for optimizing information management, strengthening research, and positioning institutions within the global scientific community."
2503.1346,"Citations demonstrate the credibility, impact, and connection of a paper with the academic community. Self-citations support research continuity but, if excessive, may inflate metrics and raise bias concerns. The aim of the study is to examine the role of self-citations towards the research impact of India. To study this, 3.58 million papers affiliated with India from 1947 to 2024 in the Scopus database were downloaded, and 2.96 million were filtered according to document type and publication year up to 2023. Further filtering based on high citation counts identified the top 1% of highly cited papers, totaling 29,556. The results indicate that the impact of Indian research, measured by highly cited papers, has grown exponentially since 2000, reaching a peak during the 2011-2020 decade. Among the citations received by these 29,556 papers, 6% are self-citations. Papers with a high proportion of self-citations (>90%) are predominantly from recent decades and are associated with smaller team sizes. The findings also reveal that smaller teams are primarily domestic, whereas larger teams are more likely to involve international collaborations. Domestic collaborations dominate smaller team sizes in terms of both self-citations and publications, whereas international collaborations gain prominence as team sizes increase. The results indicate that while domestic collaborations produce a higher number of highly cited papers, international collaborations are more likely to generate self-citations. The top international collaborators in highly cited papers are the USA, followed by UK, and Germany."
2503.13463,"ML/AI is the field of computer science and computer engineering that arguably received the most attention and funding over the last decade. Data is the key element of ML/AI, so it is becoming increasingly important to ensure that users are fully aware of the quality of the datasets that they use, and of the process generating them, so that possible negative impacts on downstream effects can be tracked, analysed, and, where possible, mitigated. One of the tools that can be useful in this perspective is dataset documentation. The aim of this work is to investigate the state of dataset documentation practices, measuring the completeness of the documentation of several popular datasets in ML/AI repositories. We created a dataset documentation schema -- the Documentation Test Sheet (DTS) -- that identifies the information that should always be attached to a dataset (to ensure proper dataset choice and informed use), according to relevant studies in the literature. We verified 100 popular datasets from four different repositories with the DTS to investigate which information was present. Overall, we observed a lack of relevant documentation, especially about the context of data collection and data processing, highlighting a paucity of transparency."
2503.13464,"Research data management (RDM) strategies and practices play a pivotal role in adhering to the paradigms of reproducibility and transparency by enabling research sharing in accordance with the principles of Open Science. Discipline-specificity is an essential factor when understanding RDM declinations, to tailor a comprehensive support service and to enhance interdisciplinarity.In this paper we present the results of a mapping carried out to gather information on research data generated and managed within the University of Bologna (UniBO). The aim is to identify differences and commonalities between disciplines and potential challenges for institutional support.We analyzed the data management plans (DMPs) of European competitive projects drafted by researchers affiliated with UniBO. We applied descriptive statistics to the collected variables to answer three main questions: How diverse is the range of data managed within the University of Bologna? Which trends of problems and patterns in terms of data management can influence/improve data stewardship service? Is there an interdisciplinary approach to data production within the University?The research work evidenced many points of contact between different disciplines in terms of data produced, formats used and modest predilection for data reuse. Hot topics such as data confidentiality, needed either on privacy or intellectual property rights (IPR) premises, and long-term preservation pose challenges to all researchers.These results show an increasing attention to RDM while highlighting the relevance of training and support to face the relatively new challenges posed by this approach."
2503.13484,"The realization of digital preservation in compliance with legislation is extremely important, especially in a sensitive domain like healthcare, where guaranteeing document reliability, authenticity, integrity and readability over time is essential to have an immediate return in terms of efficiency of the whole care setting. In this perspective, the present paper highlights critical issues, through detailed surveys addressed to both national health facilites and digital preservers, defining the state of the art of digital preservation practices in the Italian healthcare setting. The final aim is to identify the strategic areas that need technical and regulatory interventions, in order to offer a major boost of innovation to the domain. Results show an extremely variegated context that is not always compliant to the articulated legislation. These results will be used to integrate the new Italian Guidelines on the creation, management and preservation of digital documents published by the Agency of Digital Italy"
2503.13485,"Deep learning has had a great impact on various fields of computer science by enabling data-driven representation learning in a decade. Because science and technology policy decisions for a nation can be made on the impact of each technology, quantifying research impact is an important task. The number of citations and impact factor can be used to measure the impact for individual research. What would have happened without the research, however, is fundamentally a counterfactual phenomenon. Thus, we propose an approach based on causal inference to quantify the research impact of a specific technical topic. We leverage difference-in-difference to quantify the research impact by applying to bibliometric data. First, we identify papers of a specific technical topic using keywords or category tags from Microsoft Academic Graph, which is one of the largest academic publication dataset. Next, we build a paper citation network between each technical field. Then, we aggregate the cross-field citation count for each research field. Finally, the impact of a specific technical topic for each research field is estimated by applying difference-in-difference. Evaluation results show that deep learning significantly affects computer vision and natural language processing. Besides, deep learning significantly affects cross-field citation especially for speech recognition to computer vision and natural language processing to computer vision. Moreover, our method revealed that the impact of deep learning was 3.1 times of the impact of interpretability for ML models."
2503.15772,"The integrity of peer review is fundamental to scientific progress, but the rise of large language models (LLMs) has introduced concerns that some reviewers may rely on these tools to generate reviews rather than writing them independently. Although some venues have banned LLM-assisted reviewing, enforcement remains difficult as existing detection tools cannot reliably distinguish between fully generated reviews and those merely polished with AI assistance. In this work, we address the challenge of detecting LLM-generated reviews. We consider the approach of performing indirect prompt injection via the paper's PDF, prompting the LLM to embed a covert watermark in the generated review, and subsequently testing for presence of the watermark in the review. We identify and address several pitfalls in naÃ¯ve implementations of this approach. Our primary contribution is a rigorous watermarking and detection framework that offers strong statistical guarantees. Specifically, we introduce watermarking schemes and hypothesis tests that control the family-wise error rate across multiple reviews, achieving higher statistical power than standard corrections such as Bonferroni, while making no assumptions about the nature of human-written reviews. We explore multiple indirect prompt injection strategies--including font-based embedding and obfuscated prompts--and evaluate their effectiveness under various reviewer defense scenarios. Our experiments find high success rates in watermark embedding across various LLMs. We also empirically find that our approach is resilient to common reviewer defenses, and that the bounds on error rates in our statistical tests hold in practice. In contrast, we find that Bonferroni-style corrections are too conservative to be useful in this setting."
2503.16623,"Scientific publications significantly impact academic-related decisions in computer science, where top-tier conferences are particularly influential. However, efforts required to produce a publication differ drastically across various subfields. While existing citation-based studies compare venues within areas, cross-area comparisons remain challenging due to differing publication volumes and citation practices.To address this gap, we introduce the concept of ICLR points, defined as the average effort required to produce one publication at top-tier machine learning conferences such as ICLR, ICML, and NeurIPS. Leveraging comprehensive publication data from DBLP (2019--2023) and faculty information from CSRankings, we quantitatively measure and compare the average publication effort across 27 computer science sub-areas. Our analysis reveals significant differences in average publication effort, validating anecdotal perceptions: systems conferences generally require more effort per publication than AI conferences.We further demonstrate the utility of the ICLR points metric by evaluating publication records of universities, current faculties and recent faculty candidates. Our findings highlight how using this metric enables more meaningful cross-area comparisons in academic evaluation processes. Lastly, we discuss the metric's limitations and caution against its misuse, emphasizing the necessity of holistic assessment criteria beyond publication metrics alone."
2503.17419,"The National Academic Depository of India is a distinctive, novel and progressive step visualized by Ministry of Human Resources Development, Govt. of India towards maintaining a database to hold the academic awards issued by Educational Institutions in an electronic and digital form. NAD promises to abolish the difficulties / inefficiencies of collecting, maintaining, and presenting physical paper certificates that can be easily copied / created and the verification processes which are costly, time consuming and disorganized. The depository can eradicate the need to store academic awards in physical form. It can verify the awards issued by different Institutions to the students in an easy way. The secure digital depository is a good proposal to do away with fake and forged certificates. The concept of academic depository is identical to the concept of financial securities. The pilot project is successfully completed with the help of Central Board of Secondary Education and some universities. In order to become fully functional, the depository has to conquer a few challenges with respect to academic diversities in terms of duration of courses and equivalence. National Academic Depository is a revolutionary effort towards the vision of Digital India."
2503.18215,"News and social media are widely used to disseminate science, but do they also help raise awareness of problems in research? This study investigates whether high levels of news and social media attention might accelerate the retraction process and increase the visibility of retracted articles. To explore this, we analyzed 15,642 news mentions, 6,588 blog mentions, and 404,082 X mentions related to 15,461 retracted articles. Articles receiving high levels of news and X mentions were retracted more quickly than non-mentioned articles in the same broad field and with comparable publication years, author impact, and journal impact. However, this effect was not statistically signicant for articles with high levels of blog mentions. Notably, articles frequently mentioned in the news experienced a significant increase in annual citation rates after their retraction, possibly because media exposure enhances the visibility of retracted articles, making them more likely to be cited. These findings suggest that increased public scrutiny can improve the efficiency of scientific self-correction, although mitigating the influence of retracted articles remains a gradual process."
2503.18236,"The evaluation of a researcher's performance has traditionally relied on various bibliometric measures, with the h-index being one of the most prominent. However, the h-index only accounts for the number of citations received in a publication and does not account for other factors such as the number of authors or their specific contributions in collaborative works. Therefore, the h-index has been placed on scrutiny as it has motivated academic integrity issues where non-contributing authors get authorship merely for raising their h-index. In this study, we comprehensively evaluate existing metrics in their ability to account for authorship contribution by their position and introduce a novel variant of the h-index, known as the h-leadership index. The h-leadership index aims to advance the fair evaluation of academic contributions in multi-authored publications by giving importance to authorship position beyond the first and last authors, motivated by Stanford's ranking of the top 2 \% of world scientists. We assign weighted citations based on a modified complementary unit Gaussian curve, ensuring that the contributions of middle authors are appropriately recognised. We apply the h-leadership index to analyse the top 50 researchers across the Group of 8 (Go8) universities in Australia, demonstrating its potential to provide a more balanced assessment of research performance. We provide open-source software for extending the work further."
2503.18506,"Purpose: The study examines the Open Access (OA) landscape of Indian state agricultural universities, focusing on OA growth, leading institutions, prolific authors, preferred sources, funding, APC usage, and trending topics. It aims to identify research gaps, guide future research, and support policymakers in developing effective OA policies Design/methodology/approach The experiment utilized the OpenAlex database to collect global open access (OA) publications from Indian state agricultural universities over the past ten years (2014-2023). Using the Research Organization Registry ID, 97,536 publications were extracted. Data analysis was performed with OpenRefine, and ArcGIS 10.8 and Microsoft Excel were used for visualization. Findings: The global OA research output from state agricultural universities amounted to 65,889 publications across five OA categories: Green OA (7.35%), Diamond OA (6.74%), Gold OA (57.27%), Hybrid OA (9.24%), and Bronze OA (19.41%). Notably, 78.34% of articles were published in 864 low-impact domestic journals. Tamil Nadu Agricultural University produced the most publications in Gold, Diamond, Hybrid, and Bronze OA categories, while Punjab Agricultural University excelled in Green OA and received the highest funding, incurring the most article processing charges (APCs). Collaborative research focusing on agricultural policies, rice water management, soil fertility, and crop productivity had a greater impact. Originality/value The experiment is the first effort to evaluate the OA global academic research outputs of Indian state agriculture universities. The findings offer institutions, state governments, and funding agencies the opportunity to prioritise open-access publishing to promote sustainable agricultural research. Research limitations/implications The study is limited to the publications data indexed in the OpenAlex database."
2503.19848,"The tendency of generative artificial intelligence (AI) systems to ""hallucinate"" false information is well-known; AI-generated citations to non-existent sources have made their way into the reference lists of peer-reviewed publications. Here, I propose a solution to this problem, taking inspiration from the Transparency and Openness Promotion (TOP) data sharing guidelines, the clash of generative AI with the American judiciary, and the precedent set by submissions of prior art to the United States Patent and Trademark Office. Journals should require authors to submit the full text of each cited source along with their manuscripts, thereby preventing authors from citing any material whose full text they cannot produce. This solution requires limited additional work on the part of authors or editors while effectively immunizing journals against hallucinated references."
2503.21114,"Uncertainty of scientific findings are typically reported through statistical metrics such as $p$-values, confidence intervals, etc. The magnitude of this objective uncertainty is reflected in the language used by the authors to report their findings primarily through expressions carrying uncertainty-inducing terms or phrases. This language uncertainty is a subjective concept and is highly dependent on the writing style of the authors. There is evidence that such subjective uncertainty influences the impact of science on public audience. In this work, we turned our focus to scientists themselves, and measured/analyzed the subjective uncertainty and its impact within scientific communities across different disciplines. We showed that the level of this type of uncertainty varies significantly across different fields, years of publication and geographical locations. We also studied the correlation between subjective uncertainty and several bibliographical metrics, such as number/gender of authors, centrality of the field's community, citation count, etc. The underlying patterns identified in this work are useful in identification and documentation of linguistic norms in scientific communication in different communities/societies."
2503.21267,"Introduction: Taylor & Francis journal Bioengineered has been targeted by paper mills. The goal of this study is to identify problematic articles published in Bioengineered during the period 2010 to 2024.Methods: Dimensions was used to search for articles that contained the terms mouse OR mice OR rat OR rats in title or abstract, published in Bioengineered between January 1st 2010 to December 31st 2024. All articles were assessed by eye and by using software to detect inappropriate image duplication and manipulation. An article was classified as problematic if it contained inappropriate image duplication or manipulation or had been previously retracted. Problematic articles were reported on PubPeer by the authors, if they had not been reported previously. All included articles were assessed for post-publication editorial decisions.Results: We have excluded all articles published in 2024 from further analysis, as these were all retraction notices. We assessed the remaining 878 articles, of which 226 (25.7%) were identified as problematic, of which 35 had been previously retracted. One retracted article was later de-retracted. One article received a correction. None of the included articles received an expression of concern or the Taylor & Francis under investigation pop-up.Conclusions: Taylor & Francis lack of editorial action has left the scientific community vulnerable to reading and citing hundreds of problematic articles published in Bioengineered. To uphold scientific integrity, Taylor & Francis should use the findings of this study as a starting point to systematically identify all compromised articles in Bioengineered and take appropriate editorial action."
2503.21423,"This article investigates the dynamics of academic publishing resilience and volatility at Slovenia's University of Maribor (UM) from 2004 to 2023. This period was marked by significant economic pressures and policy shifts, including changes to higher education legislation and university funding. Using UM's employment data and OpenAlex publication records, the study examines the relationship between employed researcher numbers and unique authors publishing under the UM affiliation. Despite a substantial decrease in researcher employment during the 2009-2013 economic recession and austerity phase, the number of unique authors publishing with UM affiliation surprisingly increased. This growth was driven by factors such as a shift towards project-based funding, contributions from an expanding doctoral student cohort, and increased international collaborations. Analysis of author turnover reveals a notable contrast: high short-term volatility (annual churn rates of ~40-50%) versus significant mid-term stability (5-year churn rates of ~8-10%). Survival analysis confirms this trend, showing high initial attrition among publishing authors but long-term persistence for a core group. Furthermore, co-authorship network analysis indicates the UM research network has become more resilient over time. A critical finding is a fundamental shift in network structure around 2016, transitioning from dissassortative to assortative mixing, signaling profound changes in collaboration dynamics. The findings carry implications for research policy and university management, highlighting the necessity of balancing short-term performance indicators with the long-term stability and resilience essential for a thriving research community."
2503.21645,"This study provides a comparative evaluation of global diplomatic mission directories.this http URL,this http URL, andthis http URLare strategically selected among the top ten global services. After analyzing nearly all available online global diplomatic directory services, these three platforms are selected as they represent fundamentally different approaches to creating worldwide diplomatic mission databases. Using official diplomatic lists from over 150 countries as benchmarks, we assessed data coverage, accuracy, and update frequency across these platforms. DiplomaticMonitor consistently outperforms its counterparts in structure, completeness, and timeliness, accurately reflecting ambassadorial appointment cycles and maintaining high precision across contact and personnel records. EmbassyPages, despite strong search engine visibility and widespread usage, exhibits significant data currency issues, with markedly diminished ambassadorial accuracy attributable to delayed refresh cycles. WikiData offers valuable historical documentation and open-source accessibility but lacks the consistency and verification protocols necessary for reliable real-time diplomatic information. Our findings highlight the critical challenge posed by the absence of a standardized global diplomatic mission registry. In this fragmented landscape, methodologically rigorous third-party platforms can occasionally surpass government-published records in quality and utility. The research demonstrates that in contemporary digital diplomacy, data reliability correlates less with institutional provenance than with disciplined, transparent, and consistent data stewardship practices."
2503.22594,"In the context of academic publishing and peer review, this study investigates the relationship between post-publication expert evaluations, their agreement levels, and the subsequent scientific and public recognition of the reviewed research. Using expert statements from the Science Media Center Germany as a dataset, we analyze Research in Context reviews to examine the alignment between qualitative post-publication assessments and bibliometric as well as altmetric indicators. We employ a Large Language Model to translate unstructured expert reviews into a structured rating scheme. Furthermore, we correlate these evaluations with citation counts from the Web of Science and alternative impact metrics such as the Altmetric Attention Score, news mentions, and Mendeley readership statistics from the Altmetric Explorer. We investigate the alignment of positive or critical post-publication reviews and high or low citation or altmetric counts."
2503.22967,"Starting in February 2024, the HKUST Library further extended the scope of AI literacy to AI utilization, which focuses on fostering student involvement in utilizing state-of-the-art technologies in the projects that initiated by the Library, named ""Digital Scholarship (DS) CoLab"". A key focus of the DS CoLab scheme has been on cultivating talents and enabling students to utilize advanced technologies in practical context. It aims to reinforce the library's role as a catalyst and hub for fostering multidisciplinary collaboration and cultivate the ""can do spirit"" among university members. The Library offers 1-2 projects per year for students to engage with advanced technologies in practical contexts while supporting the Library in tackling challenges and streamlining operational tasks. The tool that introduced in this paper was mainly developed by two of the authors, Sherry Yip Sau Lai and Berry Han Liuruo, as part-time student helpers under one of our DS CoLab scheme in the 2024 Spring Semester (February to May 2024). This paper details the complete journey from ideation to implementation of developing a Chinese Named-Entity Recognition (NER) Tool from the group up within one semester, from the initial research and planning stages to execution and come up a viable product. The collaborative spirit fostered by this project, with students playing a central role, exemplifies the power and potential of innovative educational models that prioritize hands-on learning with student involvement."
2503.23285,"Understanding the changing structure of science over time is essential to elucidating how science evolves. We develop diachronic embeddings of scholarly periodicals to quantify ""semantic changes"" of periodicals across decades, allowing us to track the evolution of research topics and identify rapidly developing fields. By mapping periodicals within a physical-life-health triangle, we reveal an evolving interdisciplinary science landscape, finding an overall trend toward specialization for most periodicals but increasing interdisciplinarity for bioscience periodicals. Analyzing a periodical's trajectory within this triangle over time allows us to visualize how its research focus shifts. Furthermore, by monitoring the formation of local clusters of periodicals, we can identify emerging research topics such as AIDS research and nanotechnology in the 1980s. Our work offers novel quantification in the science of science and provides a quantitative lens to examine the evolution of science, which may facilitate future investigations into the emergence and development of research fields."
2503.234,"Probably Not. Journal Citation Indicator (JCI) was introduced to address the limitations of traditional metrics like the Journal Impact Factor (JIF), particularly its inability to normalize citation impact across different disciplines. This study reveals that JCI faces significant challenges in field normalization for Art & Humanities journals, as evidenced by much lower correlations with a more granular, paper-level metric, CNCI-CT. A detailed analysis of Architecture journals highlights how journal-level misclassification and the interdisciplinary nature of content exacerbate these issues, leading to less reliable evaluations. We recommend improving journal classification systems or adopting paper-level normalization methods, potentially supported by advanced AI techniques, to enhance the accuracy and effectiveness of JCI for Art & Humanities disciplines."
2503.23414,"This paper investigates the presence and impact of questionable, AI-generated academic papers on widely used preprint repositories, with a focus on their role in citation manipulation. Motivated by suspicious patterns observed in publications related to our ongoing research on GenAI-enhanced cybersecurity, we identify clusters of questionable papers and profiles. These papers frequently exhibit minimal technical content, repetitive structure, unverifiable authorship, and mutually reinforcing citation patterns among a recurring set of authors. To assess the feasibility and implications of such practices, we conduct a controlled experiment: generating a fake paper using GenAI, embedding citations to suspected questionable publications, and uploading it to one such repository (ResearchGate). Our findings demonstrate that such papers can bypass platform checks, remain publicly accessible, and contribute to inflating citation metrics like the H-index and i10-index. We present a detailed analysis of the mechanisms involved, highlight systemic weaknesses in content moderation, and offer recommendations for improving platform accountability and preserving academic integrity in the age of GenAI."
2504.02767,"The spread of scientific knowledge depends on how researchers discover and cite previous work. The adoption of large language models (LLMs) in the scientific research process introduces a new layer to these citation practices. However, it remains unclear to what extent LLMs align with human citation practices, how they perform across domains, and may influence citation dynamics. Here, we show that LLMs systematically reinforce the Matthew effect in citations by consistently favoring highly cited papers when generating references. This pattern persists across scientific domains despite significant field-specific variations in existence rates, which refer to the proportion of generated references that match existing records in external bibliometric databases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers, we find that LLM recommendations diverge from traditional citation patterns by preferring more recent references with shorter titles and fewer authors. Emphasizing their content-level relevance, the generated references are semantically aligned with the content of each paper at levels comparable to the ground truth references and display similar network effects while reducing author self-citations. These findings illustrate how LLMs may reshape citation practices and influence the trajectory of scientific discovery by reflecting and amplifying established trends. As LLMs become more integrated into the scientific research process, it is important to understand their role in shaping how scientific communities discover and build upon prior work."
2504.02769,"Research performance is often measured using bibliometric indicators, such as publication count, total citations, and $h$-index. These metrics influence career advancements, salary adjustments, administrative opportunities, funding prospects, and professional recognition. However, the reliance on these metrics has also made them targets for manipulation, misuse, and abuse. One primary ethical concern is authorship abuse, which includes paid, ornamental, exploitative, cartel, and colonial authorship. These practices are prevalent because they artificially enhance multiple bibliometric indicators all at once. Our study confirms a significant rise in the mean and median number of authors per publication across multiple disciplines over the last 34 years. While it is important to identify the cases of authorship abuse, a thorough investigation of every paper proves impractical. In this study, we propose a credit allocation scheme based on the reciprocals of the Fibonacci numbers, designed to adjust credit for individual contributions while systematically reducing credit for potential authorship abuse. The proposed scheme aligns with rigorous authorship guidelines from scientific associations, which mandate significant contributions across most phases of a study, while accommodating more lenient guidelines from scientific publishers, which recognize authorship for minimal contributions. We recalibrate the traditional bibliometric indicators to emphasize author contribution rather than participation in publications. Additionally, we propose a new indicator, $T^{\prime}$-index, to assess researchers' leading and contributing roles in their publications. Our proposed credit allocation scheme mitigates the effects of authorship abuse and promotes a more ethical scientific ecosystem."
2504.04464,"Although citation-based indicators are widely used for research evaluation, they are not useful for recently published research, reflect only one of the three common dimensions of research quality, and have little value in some social sciences, arts and humanities. Large Language Models (LLMs) have been shown to address some of these weaknesses, with ChatGPT 4o-mini showing the most promising results, although on incomplete data. This article reports by far the largest scale evaluation of ChatGPT 4o-mini yet, and also evaluates its larger sibling ChatGPT 4o. Based on comparisons between LLM scores, averaged over 5 repetitions, and departmental average quality scores for 107,212 UK-based refereed journal articles, ChatGPT 4o is marginally better than ChatGPT 4o-mini in most of the 34 field-based Units of Assessment (UoAs) tested, although combining both gives better results than either one. ChatGPT 4o scores have a positive correlation with research quality in 33 of the 34 UoAs, with the results being statistically significant in 31. ChatGPT 4o scores had a higher correlation with research quality than long term citation rates in 21 out of 34 UoAs and a higher correlation than short term citation rates in 26 out of 34 UoAs. The main limitation is that it is not clear whether ChatGPT leverages public information about departmental research quality to cheat with its scores. In summary, the results give the first large scale evidence that ChatGPT 4o is competitive with citations as a new research quality indicator, but ChatGPT 4o-mini, which is more cost-effective."
2504.04677,"Initially developed to capture technical innovation and later adapted to identify scientific breakthroughs, the Disruption Index (D-index) offers the first quantitative framework for analyzing transformative research. Despite its promise, prior studies have struggled to clarify its theoretical foundations, raising concerns about potential bias. Here, we show that-contrary to the common belief that the D-index measures absolute innovation-it captures relative innovation: a paper's ability to displace its most-cited reference. In this way, the D-index reflects scientific progress as the replacement of older answers with newer ones to the same fundamental question-much like light bulbs replacing candles. We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence. To facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex."
2504.05206,"Entity rankings (e.g., institutions, journals) are a core component of academia and related industries. Existing approaches to institutional rankings have relied on a variety of data sources, and approaches to computing outcomes, but remain controversial. One limitation of existing approaches is reliance on scholarly output (e.g., number of publications associated with a given institution during a time period). We propose a new approach to rankings - one that relies not on scholarly output, but rather on the type of citations received (an implementation of the Scite Index). We describe how the necessary data can be gathered, as well as how relevant metrics are computed. To demonstrate the utility of our approach, we present rankings of fields, journals, and institutions, and discuss the various ways Scite's data can be deployed in the context of rankings. Implications, limitations, and future directions are discussed."
2504.05361,"The concept of FAIR Digital Objects represents a foundational step towards realizing machine-actionable, interoperable data infrastructures across scientific and industrial domains. As digital spaces become increasingly heterogeneous, scalable mechanisms for data processing and interpretability are essential. This paper provides a comparative analysis of various typing mechanisms to associate FAIR Digital Objects with their operations, addressing the pressing need for a structured approach to manage data interactions within the FAIR Digital Objects ecosystem. By examining three core models -- record typing, profile typing, and attribute typing -- this work evaluates each model's complexity, flexibility, versatility, and interoperability, shedding light on their strengths and limitations. With this assessment, we aim to offer insights for adopting FDO frameworks that enhance data automation and promote the seamless exchange of digital resources across domains."
2504.05905,"The proliferation of surveys and review articles in academic journals has impacted citation metrics like impact factor and h-index, skewing evaluations of journal and researcher quality. This work investigates the implications of this trend, focusing on the field of Computer Science, where a notable increase in review publications has led to inflated citation counts and rankings. While reviews serve as valuable literature overviews, they should not overshadow the primary goal of research -to advance scientific knowledge through original contributions. We advocate for prioritizing citations of primary research in journal articles to uphold citation integrity and ensure fair recognition of substantive contributions. This approach preserves the reliability of citation-based metrics and supports genuine scientific advancement."
2504.05976,"Creating an inclusive art environment requires engaging multiple senses for a fully immersive experience. Culture is inherently synesthetic, enriched by all senses within a shared time and space. In an optimal synesthetic setting, people of all abilities can connect meaningfully; when one sense is compromised, other channels can be enhanced to compensate. This is the power of multimodality. Digital technology is increasingly able to capture aspects of multimodality. To document multimodality aspects of cultural practices and products for the long-term remains a challenge. Many artistic products from the performing arts tend to be multimodal, and are often immersive, so only a multimodal repository can offer a platform for this work. To our knowledge there is no single, comprehensive repository with a knowledge base to serve arts and disability. By knowledge base, we mean classifications, taxonomies, or ontologies (in short, knowledge organisation systems). This paper presents innovative ways to develop a knowledge base which capture multimodal features of archived representations of cultural assets, but also indicate various forms how to interact with them including machine-readable description. We will demonstrate how back-end and front-end applications, in a combined effort, can support accessible archiving and data management for complex digital objects born out of artistic practices and make them available for wider audiences."
2504.06268,"Open science movement has established reproducibility, transparency, and validation of research outputs as essential norms for conducting scientific research. It advocates for open access to research outputs, especially research data, to enable verification of published findings and its optimum reuse. The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles support the philosophy of open science and have emerged as a foundational framework for making digital assets machine-actionable and enhancing their reusability and value in various domains, particularly in scientific research and data management. In response to the growing demand for making data FAIR, various FAIR implementation frameworks have been developed by various organizations to educate and make the scientific community more aware of FAIR and its principles and to make the adoption and implementation of FAIR easier. This paper provides a comprehensive review of the openly available FAIR implementation frameworks based on a parametric evaluation of these frameworks. The current work identifies 13 frameworks and compares them against their coverage of the four foundational principles of FAIR, including an assessment of these frameworks against 36 parameters related to technical specifications, basic features, and FAIR implementation features and FAIR coverage. The study identifies that most of the frameworks only offer a step-by-step guide to FAIR implementation and seem to be adopting the technology-first approach, mostly guiding the deployment of various tools for FAIR implementation. Many frameworks are missing the critical aspects of explaining what, why, and how for the four foundational principles of FAIR, giving less consideration to the social aspects of FAIR. The study concludes that more such frameworks should be developed, considering the people-first approach rather than the technology-first."
2504.06314,"This study aims to evaluate the accuracy of authorship attributions in scientific publications, focusing on the fairness and precision of individual contributions within academic works. The study analyzes 81,823 publications from the journal PLOS ONE, covering the period from January 2018 to June 2023. It examines the authorship attributions within these publications to try and determine the prevalence of inappropriate authorship. It also investigates the demographic and professional profiles of affected authors, exploring trends and potential factors contributing to inaccuracies in authorship. Surprisingly, 9.14% of articles feature at least one author with inappropriate authorship, affecting over 14,000 individuals (2.56% of the sample). Inappropriate authorship is more concentrated in Asia, Africa, and specific European countries like Italy. Established researchers with significant publication records and those affiliated with companies or nonprofits show higher instances of potential monetary authorship. Our findings are based on contributions as declared by the authors, which implies a degree of trust in their transparency. However, this reliance on self-reporting may introduce biases or inaccuracies into the dataset. Further research could employ additional verification methods to enhance the reliability of the findings. These findings have significant implications for journal publishers, highlighting the necessity for robust control mechanisms to ensure the integrity of authorship attributions. Moreover, researchers must exercise discernment in determining when to acknowledge a contributor and when to include them in the author list. Addressing these issues is crucial for maintaining the credibility and fairness of academic publications."
2504.07726,"Quantum Machine Learning (QML) is the intersection of two revolutionary fields: quantum computing and machine learning. It promises to unlock unparalleled capabilities in data analysis, model building, and problem-solving by harnessing the unique properties of quantum mechanics. This research endeavors to conduct a comprehensive bibliometric analysis of scientific information pertaining to QML covering the period from 2000 to 2023. An extensive dataset comprising 9493 scholarly works is meticulously examined to unveil notable trends, impact factors, and funding patterns within the domain. Additionally, the study employs bibliometric mapping techniques to visually illustrate the network relationships among key countries, institutions, authors, patent citations and significant keywords in QML research. The analysis reveals a consistent growth in publications over the examined period. The findings highlight the United States and China as prominent contributors, exhibiting substantial publication and citation metrics. Notably, the study concludes that QML, as a research subject, is currently in a formative stage, characterized by robust scholarly activity and ongoing development."
2504.07828,"The temporal dimension of citation accumulation poses fundamental challenges for quantitative research evaluations, particularly in assessing disruptive and consolidating research through the disruption index (D). While prior studies emphasize minimum citation windows (mostly 3-5 years) for reliable citation impact measurements, the time-sensitive nature of D - which quantifies a paper' s capacity to eclipse prior knowledge - remains underexplored. This study addresses two critical gaps: (1) determining the temporal thresholds required for publications to meet citation/reference prerequisites, and (2) identifying ""optimal"" citation windows that balance early predictability and longitudinal validity. By analyzing millions of publications across four fields with varying citation dynamics, we employ some metrics to track D stabilization patterns. Key findings reveal that a 10-year window achieves >80% agreement with final D classifications, while shorter windows (3 years) exhibit instability. Publications with >=30 references stabilize 1-3 years faster, and extreme cases (top/bottom 5% D values) become identifiable within 5 years - enabling early detection of 60-80% of highly disruptive and consolidating works. The findings offer significant implications for scholarly evaluation and science policy, emphasizing the need for careful consideration of citation window length in research assessment (based on D)."
2504.08171,"Digital computational outputs are now ubiquitous in the research workflow and the way in which these data are stored and cataloged is becoming more standardized across fields of research. However, even with accessible data and code, the barrier to recreating figures and reproducing scientific findings remains high. What is generally missing is the computational environment and associated pipelines in which the data and code are executed to generate figures. The archival, reproducible, and transparent science (ARTS) open framework incorporates containers, version control systems, and persistent archives through which all data, code, and figures related to a research project can be stored together, easily recreated, and serve as an accessible platform for long-term sharing and validation. If the underlying principles behind this framework are broadly adopted, it will improve the reproducibility and transparency of research."
2504.08619,"Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science conferences over the past six years (2019-2024). We approach this analysis from four distinct perspectives: (1) We investigate how LLM research is driving topic shifts within major conferences. (2) We adopt a topic modeling approach to identify various areas of LLM-related topic growth and reveal the topics of concern at different conferences. (3) We explore distinct contribution patterns of academic and industrial institutions. (4) We study the influence of national origins on LLM development trajectories. Synthesizing the findings from these diverse analytical angles, we derive ten key insights that illuminate the dynamics and evolution of the LLM research ecosystem."
2504.10424,"Many scholarly societies face challenges in adapting their publishing to an open access model where neither authors nor readers pay any fees. Some have argued that one of the main barriers is the actual cost of publishing. The goal of this paper is to show that the actual costs can be extremely low while still maintaining scholarly quality. We accomplish this by building a journal publishing workflow that minimizes the amount of required human labor. We recently built a software system for this and launched a journal using the system, and we estimate estimate our cost to publish this journal is approximately \$705 per year, plus \$1 per article and about 10 minutes of volunteer labor per article. We benefited from two factors, namely the fact that authors in our discipline use LaTeX to prepare their manuscripts, and we had volunteer labor to develop software and run the journal. We have made most of this software open source in the hopes that it can help others."
2504.12195,"Purpose. The increasing emphasis on data quantity in research infrastructures has highlighted the need for equally robust mechanisms ensuring data quality, particularly in bibliographic and citation datasets. This paper addresses the challenge of maintaining high-quality open research information within OpenCitations, a community-guided Open Science Infrastructure, by introducing tools for validating and monitoring bibliographic metadata and citation data.Methods. We developed a custom validation tool tailored to the OpenCitations Data Model (OCDM), designed to detect and explain ingestion errors from heterogeneous sources, whether due to upstream data inconsistencies or internal software bugs. Additionally, a quality monitoring tool was created to track known data issues post-publication. These tools were applied in two scenarios: (1) validating metadata and citations from Matilda, a potential future source, and (2) monitoring data quality in the existing OpenCitations Meta dataset.Results. The validation tool successfully identified a variety of structural and semantic issues in the Matilda dataset, demonstrating its precision. The monitoring tool enabled the detection of recurring problems in the OpenCitations Meta collection, as well as their quantification. Together, these tools proved effective in enhancing the reliability of OpenCitations' published data.Conclusion. The presented validation and monitoring tools represent a step toward ensuring high-quality bibliographic data in open research infrastructures, though they are limited to the data model adopted by OpenCitations. Future developments are aimed at expanding to additional data sources, with particular regard to crowdsourced data."
2504.13387,"Since the introduction of Bitcoin in 2008, blockchain technology has garnered widespread attention. Scholars from various research fields, countries, and institutions have published a significant number of papers on this subject. However, there is currently a lack of comprehensive analysis specifically focusing on the scientific publications in the field of blockchain.To conduct a comprehensive analysis, we compiled a corpus of 41,497 publications in blockchain research from 2008 to 2023 using the Clarivate databases. Through bibliometric and citation analyses, we gained valuable insights into the field. Our study offers an overview of the blockchain research landscape, including country, institution, authorship, and subject categories. Additionally, we identified Emerging Research Areas (ERA) using the co-citation clustering approach, examining factors such as recency, growth, and contributions from different countries/regions. Furthermore, we identified influential publications based on citation velocity and analyzed five representative Research Fronts in detail. This analysis provides a fine-grained examination of specific areas within blockchain research. Our findings contribute to understanding evolving trends, emerging applications, and potential directions for future research in the multidisciplinary field of blockchain."
2504.14512,"Field normalization plays a crucial role in scientometrics to ensure fair comparisons across different disciplines. In this paper, we revisit the effectiveness of several widely used field normalization methods. Our findings indicate that source-side normalization (as employed in SNIP) does not fully eliminate citation bias across different fields and the imbalanced paper growth rates across fields are a key factor for this phenomenon. To address the issue of skewness, logarithmic transformation has been applied. Recently, a combination of logarithmic transformation and mean-based normalization, expressed as ln(c+1)/mu, has gained popularity. However, our analysis shows that this approach does not yield satisfactory results. Instead, we find that combining logarithmic transformation (ln(c+1)) with z-score normalization provides a better alternative. Furthermore, our study suggests that the better performance is achieved when combining both source-side and target-side field normalization methods."
2504.15038,"This study compares open metadata from hoaddata, an openly available dataset based on Crossref, OpenAlex and the cOAlition S Journal Checker Tool, with proprietary bibliometric databases Scopus and Web of Science to estimate the impact of transformative agreements on hybrid open access publishing. Analysing over 13,000 hybrid journals between 2019-2023, the research found substantial growth in open access due to these agreements, although most articles remain paywalled. The results were consistent across all three data sources, showing strong correlations in country-level metrics despite differences in journal coverage and metadata availability. By 2023, transformative agreements enabled the majority of open access in hybrid journals, with particularly high adoption in European countries. The analysis revealed strong alignment between first and corresponding authorship when measuring agreement uptake by publisher and country. This comparative approach supports the use of open metadata for large-scale hybrid open access studies, while using multiple data sources together provides a more robust understanding of hybrid open access adoption than any single database can offer, overcoming individual limitations in coverage and metadata quality."
2504.15504,"Failures of retraction are common in science. Why do these failures occur? And, relatedly, what makes findings harder or easier to retract? We use data from Microsoft Academic Graph, Retraction Watch, and Altmetric -- including retracted papers, citation records, and Altmetric scores and mentions -- to test recently proposed answers to these questions. A recent previous study by LaCroix et al. employ simple network models to argue that the social spread of scientific information helps explain failures of retraction. One prediction of their models is that widely known or well established results, surprisingly, should be easier to retract, since their retraction is more relevant to more scientists. Our results support this conclusion. We find that highly cited papers show more significant reductions in citation after retraction and garner more attention to their retractions as they occur."
2504.17189,"Recent advances in machine learning and artificial intelligence have provided more alternatives for the implementation of repetitive or monotonous tasks. However, the development of AI tools has not been straightforward, and use case exploration and workflow integration are still ongoing challenges. In this work, we present a detailed qualitative analysis of the performance and user experience of popular commercial AI chatbots when used for document classification with limited data. We report the results for a real-world example of metadata augmentation in academic libraries environment. We compare the results of AI chatbots with other machine learning and natural language processing methods such as XGBoost and BERT-based fine tuning, and share insights from our experience. We found that AI chatbots perform similarly among them while outperforming the machine learning methods we tested, showing their advantage when the method relies on local data for training. We also found that while working with AI chatbots is easier than with code, getting useful results from them still represents a challenge for the user. Furthermore, we encountered alarming conceptual errors in the output of some chatbots, such as not being able to count the number of lines of our inputs and explaining the mistake as ``human error''. Although this is not complete evidence that AI chatbots can be effectively used for metadata classification, we believe that the information provided in this work can be useful to librarians and data curators in developing pathways for the integration and use of AI tools for data curation or metadata augmentation tasks."
2504.18889,"This study examines the use of evidence in policymaking by analysing a range of journal and article attributes, as well as online engagement metrics. It employs a large-scale citation analysis of nearly 150,000 articles covering diverse policy topics. The findings highlight that scholarly citations exert the strongest positive influence on policy citations. Articles from journals with a higher citation impact and larger Mendeley readership are cited more frequently in policy documents. Other online engagements, such as news and blog mentions, also boost policy citations, while mentions on social media X have a negative effect. The finding that highly cited and widely read papers are also frequently referenced in policy documents likely reflects the perception among policymakers that such research is more trustworthy. In contrast, papers that derive their influence primarily from social media tend to be cited less often in policy contexts."
2504.18896,"Open science is increasingly recognised worldwide, with preprint posting emerging as a key strategy. This study explores the factors influencing researchers' adoption of preprint publication, particularly the perceived effectiveness of this practice and research intensity indicators such as publication and review frequency. Using open data from a comprehensive survey with 5,873 valid responses, we conducted regression analyses to control for demographic variables. Researchers' productivity, particularly the number of journal articles and books published, greatly influences the frequency of preprint deposits. The perception of the effectiveness of preprints follows this. Preprints are viewed positively in terms of early access to new research, but negatively in terms of early feedback. Demographic variables, such as gender and the type of organisation conducting the research, do not have a significant impact on the production of preprints when other factors are controlled for. However, the researcher's discipline, years of experience and geographical region generally have a moderate effect on the production of preprints. These findings highlight the motivations and barriers associated with preprint publication and provide insights into how researchers perceive the benefits and challenges of this practice within the broader context of open science."
2504.20065,"We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works."
2504.20081,"Citation metrics serve as the cornerstone of scholarly impact evaluation despite their well-documented vulnerability to inflation through self-citation practices. This paper introduces the Self-Citation Adjusted Index (SCAI), a sophisticated metric designed to recalibrate citation counts by accounting for discipline-specific self-citation patterns. Through comprehensive analysis of 5,000 researcher profiles across diverse disciplines, we demonstrate that excessive self-citation inflates traditional metrics by 10-20%, potentially misdirecting billions in research funding. Recent studies confirm that self-citation patterns exhibit significant gender disparities, with men self-citing up to 70% more frequently than women, exacerbating existing inequalities in academic recognition. Our open-source implementation provides comprehensive tools for calculating SCAI and related metrics, offering a more equitable assessment of research impact that reduces the gender citation gap by approximately 8.5%. This work contributes to the paradigm shift toward transparent, nuanced, and equitable research evaluation methodologies in academia, with direct implications for funding allocation decisions that collectively amount to over $100 billion annually in the United States alone."
2504.20125,"A key factor for lunar mission planning is the ability to assess the local availability of raw materials. However, many potentially relevant measurements are scattered across a variety of scientific publications. In this paper we consider the viability of obtaining lunar composition data by leveraging LLMs to rapidly process a corpus of scientific publications. While leveraging LLMs to obtain knowledge from scientific documents is not new, this particular application presents interesting challenges due to the heterogeneity of lunar samples and the nuances involved in their characterization. Accuracy and uncertainty quantification are particularly crucial since many materials properties can be sensitive to small variations in composition. Our findings indicate that off-the-shelf LLMs are generally effective at extracting data from tables commonly found in these documents. However, there remains opportunity to further refine the data we extract in this initial approach; in particular, to capture fine-grained mineralogy information and to improve performance on more subtle/complex pieces of information."
2504.206,"We propose a citation index $\nu$ (``nu'') and show that it lies between the classical $h$-index and $g$-index. This idea is then generalized to a monotone parametric family $(\nu_\alpha)$ ($\alpha\ge 0$), whereby $h=\nu_0$ and $\nu=\nu_1$, while the limiting value $\nu_\infty$ is expressed in terms of the maximum citation."
2504.211,"Language is a major source of systemic inequities in science, particularly among scholars whose first language is not English. Studies have examined scientists' linguistic practices in specific contexts; few, however, have provided a global analysis of multilingualism in science. Using two major bibliometric databases (OpenAlex and Dimensions), we provide a large-scale analysis of linguistic diversity in science, considering both the language of publications (N=87,577,942) and of cited references (N=1,480,570,087). For the 1990-2023 period, we find that only Indonesian, Portuguese and Spanish have expanded at a faster pace than English. Country-level analyses show that this trend is due to the growing strength of the Latin American and Indonesian academic circuits. Our results also confirm the own-language preference phenomenon (particularly for languages other than English), the strong connection between multilingualism and bibliodiversity, and that social sciences and humanities are the least English-dominated fields. Our findings suggest that policies recognizing the value of both national-language and English-language publications have had a concrete impact on the distribution of languages in the global field of scholarly communication."
2505.02455,"Historical study of the Holocaust is commonly hampered by the dispersed and fragmented nature of important archival sources relating to this event. The EHRI project set out to mitigate this problem by building a trans-national network of archives, researchers, and digital practitioners, and one of its main outcomes was the creation of the EHRI Portal, a ""virtual observatory"" that gathers in one centralised platform descriptions of Holocaust-related archival sources from around the world. In order to build the Portal a strong data identification and integration effort was required, culminating in the project's third phase with the creation of the EHRI-3 data integration lab. The focus of the lab was to lower the bar to participation in the EHRI Portal by providing support to institutions in conforming their archival metadata with that required for integration, ultimately opening the process up to smaller institutions (and even so-called ""micro-archives"") without the necessary resources to undertake this process themselves. In this paper we present our experiences from running the data integration lab and discuss some of the challenges (both of a technical and social nature), how we tried to overcome them, and the overall lessons learnt. We envisage this work as an archetype upon which other practitioners seeking to pursue similar data integration activities can build their own efforts."
2505.03835,"The adoption of open science has quickly changed how artificial intelligence (AI) policy research is distributed globally. This study examines the regional trends in the citation of preprints, specifically focusing on the impact of two major disruptive events: the COVID-19 pandemic and the release of ChatGPT, on research dissemination patterns in the United States, Europe, and South Korea from 2015 to 2024. Using bibliometrics data from the Web of Science, this study tracks how global disruptive events influenced the adoption of preprints in AI policy research and how such shifts vary by region. By marking the timing of these disruptive events, the analysis reveals that while all regions experienced growth in preprint citations, the magnitude and trajectory of change varied significantly. The United States exhibited sharp, event-driven increases; Europe demonstrated institutional growth; and South Korea maintained consistent, linear growth in preprint adoption. These findings suggest that global disruptions may have accelerated preprint adoption, but the extent and trajectory are shaped by local research cultures, policy environments, and levels of open science maturity. This paper emphasizes the need for future AI governance strategies to consider regional variability in research dissemination and highlights opportunities for further longitudinal and comparative research to deepen our understanding of open-access adoption in AI policy development."
2505.04309,"This paper explores methods for building a comprehensive citation graph using big data techniques to evaluate scientific impact more accurately. Traditional citation metrics have limitations, and this work investigates merging large citation datasets to create a more accurate picture. Challenges of big data, like inconsistent data formats and lack of unique identifiers, are addressed through deduplication efforts, resulting in a streamlined and reliable merged dataset with over 119 million records and 1.4 billion citations. We demonstrate that merging large citation datasets builds a more accurate citation graph facilitating a more robust evaluation of scientific impact."
2505.06107,"Most web and digital trace data do not include information about an individual's nationality due to privacy concerns. The lack of data on nationality can create challenges for migration research. It can lead to a left-censoring issue since we are uncertain about the migrant's country of origin. Once we observe an emigration event, if we know the nationality, we can differentiate it from return migration. We propose methods to detect the nationality with the least available data, i.e., full names. We use the detected nationality in comparison with the country of academic origin, which is a common approach in studying the migration of researchers. We gathered 2.6 million unique name-nationality pairs from Wikipedia and categorized them into families of nationalities with three granularity levels to use as our training data. Using a character-based machine learning model, we achieved a weighted F1 score of 84% for the broadest and 67% for the most granular, country-level categorization. In our empirical study, we used the trained and tested model to assign nationality to 8+ million scholars' full names in Scopus data. Our results show that using the country of first publication as a proxy for nationality underestimates the size of return flows, especially for countries with a more diverse academic workforce, such as the USA, Australia, and Canada. We found that around 48% of emigration from the USA was return migration once we used the country of name origin, in contrast to 33% based on academic origin. In the most recent period, 79% of scholars whose affiliation has consistently changed from the USA to China, and are considered emigrants, have Chinese names in contrast to 41% with a Chinese academic origin. Our proposed methods for addressing left-censoring issues are beneficial for other research that uses digital trace data to study migration."
2505.06448,"Global university rankings have reshaped how academic success is defined, incentivizing metrics such as publication counts and citation rates at the expense of scholarly integrity. This study examines 18 universities in India, Lebanon, Saudi Arabia, and the United Arab Emirates, selected from among the world's 1,000 most-publishing institutions for their extraordinary research growth and sharp declines in first and corresponding authorship. These institutions exhibit bibliometric patterns consistent with strategic metric optimization, including publication surges of up to 965%, a proliferation of hyper-prolific authors, dense reciprocal co-authorship and citation networks, elevated shares of output in delisted journals, and rising retraction rates. These patterns are analyzed in light of Goodhart's Law and institutional isomorphism, illustrating how performance pressures can reshape academic behavior. To systematically assess and monitor such risks, the study introduces the Research Integrity Risk Index (RI2), a composite indicator based on retraction rates and reliance on delisted journals. RI2 effectively identifies institutions with bibliometric profiles that diverge from global norms and may warrant closer examination. The findings highlight the urgent need for integrity-sensitive reforms in how rankings, funders, and institutions assess scholarly performance."
2505.06721,"Understanding how co-authors distribute credit is critical for accurately assessing scholarly collaboration. In this study, we uncover the implicit structures within scientific teamwork by systematically analyzing author contributions across a large corpus of research publications. We introduce a computational framework designed to convert free-text contribution statements into 14 standardized CRediT categories, identifying clear and consistent positional patterns in task assignments. By analyzing over 400,000 scientific articles from prominent sources such as PLOS One and Nature, we extracted and standardized more than 5.6 million author-task assignments corresponding to 1.58 million author mentions. Our analysis reveals substantial disparities in workload distribution. Notably, in small teams with three co-authors, the most engaged contributor performs over three times more tasks than the least engaged, a disparity that grows linearly with team size. This demonstrates a consistent pattern of central and peripheral roles within modern collaborative teams. Moreover, our analysis shows distinct positional biases in task allocation: technical responsibilities, such as software development and formal analysis, broadly fall to authors positioned earlier in the author list, whereas managerial tasks, including supervision and funding acquisition, increasingly concentrate among authors positioned toward the end. This gradient underscores a significant division of labor, where early-listed authors mainly undertake most hands-on activities. In contrast, senior authors mostly assume roles involving leadership and oversight. Our findings highlight the structured and hierarchical organization within scholarly collaborations, providing deeper insights into the specific roles and dynamics that govern academic teamwork"
2505.06938,"Taking its point of departure in the recent developments in the field of digital humanities and the increasing automatisation of scholarly workflows, this study explores the implications of digital approaches to textual traditions for the broader field of textual scholarship. It argues that the relative simplicity of creating computergenerated stemmas allows us to view the stemma codicum as a research tool rather than the final product of our scholarly investigation. Using the Old Norse saga of HrÃ³mundur as a case study, this article demonstrates that stemmas can serve as a starting point for exploring textual traditions further. In doing so, they enable us to address research questions that otherwise remain unanswered. The article is accompanied by datasets used to generate stemmas for the HrÃ³mundar saga tradition as well as two custom Python scripts. The scripts are designed to convert XML-based textual data, encoded according to the TEI Guidelines, into the input format used for the analysis in the PHYLIP package to generate unrooted trees of relationships between texts."
2505.07577,"Accurate affiliation matching, which links affiliation strings to standardized organization identifiers, is critical for improving research metadata quality, facilitating comprehensive bibliometric analyses, and supporting data interoperability across scholarly knowledge bases. Existing approaches fail to handle the complexity of affiliation strings that often include mentions of multiple organizations or extraneous information. In this paper, we present AffRo, a novel approach designed to address these challenges, leveraging advanced parsing and disambiguation techniques. We also introduce AffRoDB, an expert-curated dataset to systematically evaluate affiliation matching algorithms, ensuring robust benchmarking. Results demonstrate the effectiveness of AffRp in accurately identifying organizations from complex affiliation strings."
2505.07912,"Democratic societies need accessible, reliable information. Videos and Podcasts have established themselves as the medium of choice for civic dissemination, but also as carriers of misinformation. The emerging Science Communication Knowledge Infrastructure (SciCom KI) curating non-textual media is still fragmented and not adequately equipped to scale against the content flood. Our work sets out to support the SciCom KI with a central, collaborative platform, the SciCom Wiki, to facilitate FAIR (findable, accessible, interoperable, reusable) media representation and the fact-checking of their content, particularly for videos and podcasts. Building an open-source service system centered around Wikibase, we survey requirements from 53 stakeholders, refine these in 11 interviews, and evaluate our prototype based on these requirements with another 14 participants. To address the most requested feature, fact-checking, we developed a neurosymbolic computational fact-checking approach, converting heterogenous media into knowledge graphs. This increases machine-readability and allows comparing statements against equally represented ground-truth. Our computational fact-checking tool was iteratively evaluated through 10 expert interviews, a public user survey with 43 participants verified the necessity and usability of our tool. Overall, our findings identified several needs to systematically support the SciCom KI. The SciCom Wiki, as a FAIR digital library complementing our neurosymbolic computational fact-checking framework, was found suitable to address the raised requirements. Further, we identified that the SciCom KI is severely underdeveloped regarding FAIR knowledge and related systems facilitating its collaborative creation and curation. Our system can provide a central knowledge node, yet a collaborative effort is required to scale against the imminent (mis-)information flood."
2505.08533,"Publishing research data aims to improve the transparency of research results and facilitate the reuse of datasets. In both cases, referencing the datasets that were used is recommended. Research data repositories can support data referencing through various measures and also benefit from it, for example using this information to demonstrate their impact. However, the literature shows that the practice of formally citing research data is not widespread, data metrics are not yet established, and effective incentive structures are lacking. This article examines how often and in what form datasets published via the research data repository RADAR are referenced. For this purpose, the data sources Google Scholar, DataCite Event Data and the Data Citation Corpus were analyzed. The analysis shows that 27.9 % of the datasets in the repository were referenced at least once. 21.4 % of these references were (also) present in the reference lists and are therefore considered data citations. Datasets were referenced often in data availability statements. A comparison of the three data sources showed that there was little overlap in the coverage of references. In most cases (75.8 %), data and referencing objects were published in the same year. Two definition approaches were considered to investigate data reuse. 118 RADAR datasets were referenced more than once. Only 21 references had no overlaps in the authorship information -- these datasets were referenced by researchers that were not involved in data collection."
2505.11633,"This demo paper reports on a new workflow \textit{GhostWriter} that combines the use of Large Language Models and Knowledge Graphs (semantic artifacts) to support navigation through collections. Situated in the research area of Retrieval Augmented Generation, this specific workflow represents the creation of local and adaptable chatbots. Based on the tool-suite \textit{EverythingData} at the backend, \textit{GhostWriter} provides an interface that enables querying and ``chatting'' with a collection. Applied iteratively, the workflow supports the information needs of researchers when interacting with a collection of papers, whether it be to gain an overview, to learn more about a specific concept and its context, and helps the researcher ultimately to refine their research question in a controlled way. We demonstrate the workflow for a collection of articles from the \textit{method data analysis} journal published by GESIS -- Leibniz-Institute for the Social Sciences. We also point to further application areas."
2505.12134,"This study examines the effect of article processing charge (APC) waivers on the participation of Ukrainian researchers in fully Gold Open Access (Gold OA) journals published by the five largest academic publishers - Elsevier, SAGE, Springer Nature, Taylor & Francis, and Wiley - during the period 2019-2024. These publishers were selected because, in response to the full-scale war launched against Ukraine in 2022, all five introduced emergency 100% APC-waiver policies for Ukrainian authors. Using bibliometric data from the Web of Science Core Collection, the study analyses publication trends in Ukrainian-authored articles in fully Gold OA journals of these publishers before and after 2022. The results show a marked post-2022 increase in Ukraine's Gold OA output, particularly in journals published by Springer Nature and Elsevier. Disciplinary and publisher-specific patterns are evident, with especially strong growth in the medical and applied sciences. The findings underscore the potential of targeted support measures during times of crisis, while also illustrating the inherent limitations of APC-based publishing models in fostering equitable scholarly communication."
2505.13276,"This paper presents CHAD-KG, a knowledge graph designed to describe bibliographic metadata and digitisation paradata of cultural heritage objects in exhibitions, museums, and collections. It also documents the related data model and materialisation engine. Originally based on two tabular datasets, the data was converted into RDF according to CHAD-AP, an OWL application profile built on standards like CIDOC-CRM, LRMoo, CRMdig, and Getty AAT. A reproducible pipeline, developed with a Morph-KGC extension, was used to generate the graph. CHAD-KG now serves as the main metadata source for the Digital Twin of the temporary exhibition titled \emph{The Other Renaissance - Ulisse Aldrovandi and The Wonders Of The World}, and other collections related to the digitisation work under development in a nationwide funded project, i.e. Project CHANGES (this https URL). To ensure accessibility and reuse, it offers a SPARQL endpoint, a user interface, open documentation, and is published on Zenodo under a CC0 license. The project improves the semantic interoperability of cultural heritage data, with future work aiming to extend the data model and materialisation pipeline to better capture the complexities of acquisition and digitisation, further enrich the dataset and broaden its relevance to similar initiatives."
2505.14328,"This paper introduces a pipeline for integrating semantic metadata, 3D models, and storytelling, enhancing cultural heritage digitization. Using the Aldrovandi Digital Twin case study, it outlines a reusable workflow combining RDF-driven narratives and data visualization for creating interactive experiences to facilitate access to cultural heritage."
2505.14838,"Understanding the impact of scientific publications is crucial for identifying breakthroughs and guiding future research. Traditional metrics based on citation counts often miss the nuanced ways a paper contributes to its field. In this work, we propose a new task: generating nuanced, expressive, and time-aware impact summaries that capture both praise (confirmation citations) and critique (correction citations) through the evolution of fine-grained citation intents. We introduce an evaluation framework tailored to this task, showing moderate to strong human correlation on subjective metrics such as insightfulness. Expert feedback from professors reveals a strong interest in these summaries and suggests future improvements."
2505.15042,"Software is often developed using versioned controlled software, such as Git, and hosted on centralized Web hosts, such as GitHub and GitLab. These Web hosted software repositories are made available to users in the form of traditional HTML Web pages for each source file and directory, as well as a presentational home page and various descriptive pages. We examined more than 12,000 Web hosted Git repository project home pages, primarily from GitHub, to measure how well their presentational components are preserved in the Internet Archive, as well as the source trees of the collected GitHub repositories to assess the extent to which their source code has been preserved. We found that more than 31% of the archived repository home pages examined exhibited some form of minor page damage and 1.6% exhibited major page damage. We also found that of the source trees analyzed, less than 5% of their source files were archived, on average, with the majority of repositories not having source files saved in the Internet Archive at all. The highest concentration of archived source files available were those linked directly from repositories' home pages at a rate of 14.89% across all available repositories and sharply dropping off at deeper levels of a repository's directory tree."
2505.15384,"This work aims to study a count response random variable, the number of citations of a research paper, affected by some explanatory variables through a suitable regression model. Due to the fact that the count variable exhibits substantial variation since the sample variance is larger than the sample mean, the classical Poisson regression model seems not to be appropriate. We concentrate attention on the negative binomial regression model, which allows the variance of each measurement to be a function of its predicted value. Nevertheless, the process of citations of papers may be divided into two parts. In the first stage, the paper has no citations, and the second part provides the intensity of the citations. A hurdle model for separating the documents with citations and those without citations is considered. The dataset for the empirical application consisted of 43,190 research papers in the field of Economics and Business from 2014-2021, obtained from The Lens database. Citation counts and social attention scores for each article were gathered from Altmetric database. The main findings indicate that both collaboration and funding have a positive impact on citation counts and reduce the likelihood of receiving zero citations. Higher journal impact factors lead to higher citation counts, while lower peer review ratings lead to fewer citations and a higher probability of zero citations. Mentions in news, blogs, and social media have varying but generally limited effects on citation counts. Open access via repositories (green OA) correlates with higher citation counts and a lower probability of zero citations. In contrast, OA via the publisher's website without an explicit open license (bronze OA) is associated with higher citation counts but also with a higher probability of zero citations."
2505.1655,"FAIR Digital Objects support research data management aligned with the FAIR principles. To be machine-actionable, they must support operations that interact with their contents. This can be achieved by associating operations with FAIR-DO data types. However, current typing models and Data Type Registries lack support for type-associated operations. In this work, we introduce a typing model that describes type-associated and technology-agnostic FAIR Digital Object Operations in a machine-actionable way, building and improving on the existing concepts. In addition, we introduce the Integrated Data Type and Operations Registry with Inheritance System, a prototypical implementation of this model that integrates inheritance mechanisms for data types, a rule-based validation system, and the computation of type-operation associations. Our approach significantly improves the machine-actionability of FAIR Digital Objects, paving the way towards dynamic, interoperable, and reproducible research workflows."
2505.17945,"In the field of artificial intelligence (AI) research, there seems to be a rapprochement between academics and industrial forces. The aim of this study is to assess whether and to what extent industrial domination in the field as well as the ever more frequent switch between academia and industry resulted in the adoption of industrial norms and practices by academics. Using bibliometric information and data on scientific code, we aimed to understand academic and industrial researchers' practices, the way of choosing, investing, and succeeding across multiple and concurrent artifacts. Our results show that, although both actors write papers and code, their practices and the norms guiding them differ greatly. Nevertheless, it appears that the presence of industrials in academic studies leads to practices leaning toward the industrial side, but also to greater success in both artifacts, suggesting that if convergence is, then it is passing through those mixed teams rather than through pure academic or industrial studies."
2505.1818,"Clustering scientific publications can reveal underlying research structures within bibliographic databases. Graph-based clustering methods, such as spectral, Louvain, and Leiden algorithms, are frequently utilized due to their capacity to effectively model citation networks. However, their performance may degrade when applied to real-world data. This study evaluates the performance of these clustering algorithms on a citation graph comprising approx. 700,000 papers and 4.6 million citations extracted from Web of Science. The results show that while scalable methods like Louvain and Leiden perform efficiently, their default settings often yield poor partitioning. Meaningful outcomes require careful parameter tuning, especially for large networks with uneven structures, including a dense core and loosely connected papers. These findings highlight practical lessons about the challenges of large-scale data, method selection and tuning based on specific structures of bibliometric clustering tasks."
2505.18196,"In a commentary published in mid-2024 (to which the present work is a direct response), a number of scientists argue that U.S. funding agencies have ""politicized"" the process by which grants are awarded, in service of diversifying the scientific workforce. The commentary in question, however, makes numerous unfounded assertions while recycling citations to a fusillade of opinion essays written by the same cabal of authors, in an effort to resemble a work of serious scholarship. Basic fact-checking is provided here, demonstrating numerous claims that are unsupported by the source material and readily debunked. The present work also serves to document the reality of inclusion and diversity plans for scientific grant proposals to U.S. funding agencies, as they existed at the end of 2024. It is intended as a bulwark against retroactive false narratives, as the U.S. moves into a period of intense antagonism towards diversity, equity, and inclusion activities."
2505.18207,"In scientific research, ``limitations'' refer to the shortcomings, constraints, or weaknesses of a study. A transparent reporting of such limitations can enhance the quality and reproducibility of research and improve public trust in science. However, authors often underreport limitations in their papers and rely on hedging strategies to meet editorial requirements at the expense of readers' clarity and confidence. This tendency, combined with the surge in scientific publications, has created a pressing need for automated approaches to extract and generate limitations from scholarly papers. To address this need, we present a full architecture for computational analysis of research limitations. Specifically, we (1) create a dataset of limitations from ACL, NeurIPS, and PeerJ papers by extracting them from the text and supplementing them with external reviews; (2) we propose methods to automatically generate limitations using a novel Retrieval Augmented Generation (RAG) technique; (3) we design a fine-grained evaluation framework for generated limitations, along with a meta-evaluation of these techniques."
2505.18222,"This paper aims to address a gap in major Islamic topics by developing an ontology for the Book of Purification in Islam. Many authoritative Islamic texts begin with the Book of Purification, as it is essential for performing prayer (the second pillar of Islam after Shahadah, the profession of faith) and other religious duties such as Umrah and Hajj.The ontology development strategy followed six key steps: (1) domain identification, (2) knowledge acquisition, (3) conceptualization, (4) classification, (5) integration and implementation, and (6) ontology generation. This paper includes examples of the constructed tables and classifications.The focus is on the design and analysis phases, as technical implementation is beyond the scope of this study. However, an initial implementation is provided to illustrate the steps of the proposed strategy.The developed ontology ensures reusability by formally defining and encoding the key concepts, attributes, and relationships related to the Book of Purification. This structured representation is intended to support knowledge sharing and reuse."
2505.20103,"Citations are crucial in scientific research articles as they highlight the connection between the current study and prior work. However, this process is often time-consuming for researchers. In this study, we propose the SciRGC framework, which aims to automatically recommend citation articles and generate citation sentences for citation locations within articles. The framework addresses two key challenges in academic citation generation: 1) how to accurately identify the author's citation intent and find relevant citation papers, and 2) how to generate high-quality citation sentences that align with human preferences. We enhance citation recommendation accuracy in the citation article recommendation module by incorporating citation networks and sentiment intent, and generate reasoning-based citation sentences in the citation sentence generation module by using the original article abstract, local context, citation intent, and recommended articles as inputs. Additionally, we propose a new evaluation metric to fairly assess the quality of generated citation sentences. Through comparisons with baseline models and ablation experiments, the SciRGC framework not only improves the accuracy and relevance of citation recommendations but also ensures the appropriateness of the generated citation sentences in context, providing a valuable tool for interdisciplinary researchers."
2505.20944,"This study explores the effects of Russia's full-scale invasion of Ukraine on the international collaboration of Ukrainian scholars. First and foremost, Ukrainian scholars deserve respect for continuing to publish despite life-threatening conditions, mental strain, shelling and blackouts. In 2022-2023, universities gained more from international collaboration than the NASU. The percentage of internationally co-authored articles remained unchanged for the NASU, while it increased for universities. In 2023, 40.8% of articles published by the NASU and 32,2% of articles published by universities were internationally co-authored. However, these figures are still much lower than in developed countries (60-70%). The citation impact of internationally co-authored articles remained statistically unchanged for the NASU but increased for universities. The highest share of internationally co-authored articles published by the NASU in both periods was in the physical sciences and engineering. However, the citation impact of these articles declined in 2022-2023, nearly erasing their previous citation advantage over university publications. Universities consistently outperformed the NASU in the citation impact of internationally co-authored articles in biomedical and health sciences across both periods. International collaboration can help Ukrainian scholars to go through this difficult time. In turn, they can contribute to the strengthening of Europe."
2505.21162,"Citations play a fundamental role in the scientific ecosystem, serving as a foundation for tracking the flow of knowledge, acknowledging prior work, and assessing scholarly influence. In scientometrics, they are also central to the construction of quantitative indicators. Not all citations, however, serve the same function: some provide background, others introduce methods, or compare results. Therefore, understanding citation intent allows for a more nuanced interpretation of scientific impact. In this paper, we adopted a GAN-based method to classify citation intents. Our results revealed that the proposed method achieves competitive classification performance, closely matching state-of-the-art results with substantially fewer parameters. This demonstrates the effectiveness and efficiency of leveraging GAN architectures combined with contextual embeddings in intent classification task. We also investigated whether filtering citation intents affects the centrality of papers in citation networks. Analyzing the network constructed from the unArXiv dataset, we found that paper rankings can be significantly influenced by citation intent. All four centrality metrics examined- degree, PageRank, closeness, and betweenness - were sensitive to the filtering of citation types. The betweenness centrality displayed the greatest sensitivity, showing substantial changes in ranking when specific citation intents were removed."
2505.22702,"This study examines the shift in the scientific community from X (formerly Twitter) to Bluesky, its impact on scientific communication, and consequently on social metrics (altmetrics). We analysed 14,497 publications from multidisciplinary and Library and Information Science (LIS) journals between January 2024 and March 2025. The results reveal a notable increase in Bluesky activity for multidisciplinary journals in November 2024, likely influenced by political and platform changes, with mentions multiplying for journals like Nature and Science. In LIS, the adoption of Bluesky is different and shows marked variation between European and United States journals. Although Bluesky remains a minority platform compared to X over the whole period, when focusing on user engagement after the United States elections, we see a much more even distribution between the two platforms. In two LIS journals, Bluesky even surpasses X, while in most others, the difference in user engagement was no longer as pronounced, marking a significant change from previous patterns in altmetrics."
2505.24091,"The Environmental Governance and Data Initiative (EDGI) regularly crawled US federal environmental websites between 2016 and 2020 to capture changes between two presidential administrations. However, because it does not include the previous administration ending in 2008, the collection is unsuitable for answering our research question, Were the website terms deleted by the Trump administration (2017--2021) added by the Obama administration (2009--2017)? Thus, like many researchers using the Wayback Machine's holdings for historical analysis, we do not have access to a complete collection suiting our needs. To answer our research question, we must extend the EDGI collection back to January, 2008. This includes discovering relevant pages that were not included in the EDGI collection that persisted through 2020, not just going further back in time with the existing pages. We pieced together artifacts collected by various organizations for their purposes through many means (Save Page Now, Archive-It, and more) in order to curate a dataset sufficient for our intentions. In this paper, we contribute a methodology to extend existing web archive collections temporally to enable longitudinal analysis, including a dataset extended with this methodology. We use our new dataset to analyze our question, Were the website terms deleted by the Trump administration added by the Obama administration? We find that 81 percent of the pages in the dataset changed between 2008 and 2020, and that 87 percent of the pages with terms deleted by the Trump administration were terms added during the Obama administration."
2506.03165,"This paper reviews literature pertaining to the development of data science as a discipline, current issues with data bias and ethics, and the role that the discipline of information science may play in addressing these concerns. Information science research and researchers have much to offer for data science, owing to their background as transdisciplinary scholars who apply human-centered and social-behavioral perspectives to issues within natural science disciplines. Information science researchers have already contributed to a humanistic approach to data ethics within the literature and an emphasis on data science within information schools all but ensures that this literature will continue to grow in coming decades. This review article serves as a reference for the history, current progress, and potential future directions of data ethics research within the corpus of information science literature."
2506.0318,"Digitizing cultural heritage collections has become crucial for preservation of historical artifacts and enhancing their availability to the wider public. Galleries, libraries, archives and museums (GLAM institutions) are actively digitizing their holdings and creates extensive digital collections. Those collections are often enriched with metadata describing items but not exactly their contents. The Jagiellonian Digital Library, standing as a good example of such an effort, offers datasets accessible through protocols like OAI-PMH. Despite these improvements, metadata completeness and standardization continue to pose substantial obstacles, limiting the searchability and potential connections between collections. To deal with these challenges, we explore an integrated methodology of computer vision (CV), artificial intelligence (AI), and semantic web technologies to enrich metadata and construct knowledge graphs for digitized manuscripts and incunabula."
2506.03187,"Interdisciplinary scientific research is increasingly important in knowledge production, funding policies, and academic discussions on scholarly communication. While many studies focus on interdisciplinary corpora defined a priori -- usually through keyword-based searches within assumed interdisciplinary domains -- few explore interdisciplinarity as an emergent intersection between two distinct fields. Thus, methodological proposals for building databases at the intersection of two fields of knowledge are scarce. The goal of this article is to develop and compare different strategies for defining an interdisciplinary corpus between two bodies of knowledge. As a case study, we focus on the intersection between neuroscience and computer science. To this end, we develop and compare four retrieval strategies, two of them based on keywords and two based on citation and reference patterns. Our results show that the reference-based strategy provides better retrieval, pseudorecall, and F1. While we focus on comparing strategies for the study of the intersection between the fields of neuroscience and computer science, this methodological reflection is applicable to a wide range of interdisciplinary domains."
2506.03221,"As the volume of scientific literature grows, efficient knowledge organization becomes increasingly challenging. Traditional approaches to structuring scientific content are time-consuming and require significant domain expertise, highlighting the need for tool support. We present ExtracTable, a Human-in-the-Loop (HITL) workflow and framework that assists researchers in transforming unstructured publications into structured representations. The workflow combines large language models (LLMs) with user-defined schemas and is designed for downstream integration into knowledge graphs (KGs). Developed and evaluated in the context of the Open Research Knowledge Graph (ORKG), ExtracTable automates key steps such as document preprocessing and data extraction while ensuring user oversight through validation. In an evaluation with ORKG community participants following the Quality Improvement Paradigm (QIP), ExtracTable demonstrated high usability and practical value. Participants gave it an average System Usability Scale (SUS) score of 84.17 (A+, the highest rating). The time to progress from a research interest to literature-based insights was reduced from between 4 hours and 2 weeks to an average of 24:40 minutes. By streamlining corpus creation and structured data extraction for knowledge graph integration, ExtracTable leverages LLMs and user models to accelerate literature reviews. However, human validation remains essential to ensure quality, and future work will address improving extraction accuracy and entity linking to existing knowledge resources."
2506.03321,"We investigated the feasibility of predicting Medical Subject Headings (MeSH) Publication Types (PTs) from MEDLINE citation metadata using pre-trained Transformer-based models BERT and DistilBERT. This study addresses limitations in the current automated indexing process, which relies on legacy NLP algorithms. We evaluated monolithic multi-label classifiers and binary classifier ensembles to enhance the retrieval of biomedical literature. Results demonstrate the potential of Transformer models to significantly improve PT tagging accuracy, paving the way for scalable, efficient biomedical indexing."
2506.03527,"Accurately evaluating scholarly influence is essential for fair academic assessment, yet traditional bibliometric indicators - dominated by publication and citation counts - often favor hyperprolific authors over those with deeper, long-term impact. We propose the x-index, a novel citation-based metric that conceptualizes citation as a process of knowledge diffusion and incorporates citation distance to reflect the structural reach of scholarly work. By weighting citations according to the collaborative proximity between citing and cited authors, the x-index captures both the depth and breadth of influence within evolving academic networks. Empirical analyses show that the x-index significantly improves the rankings of Turing Award recipients while reducing those of hyperprolific authors, better aligning rankings with recognized academic merit. It also demonstrates superior discriminatory power among early-career researchers and reveals stronger sensitivity to institutional research quality. These results suggest that the x-index offers a more equitable and forward-looking alternative to existing metrics, with practical applications in talent identification, funding decisions, and academic recommendation systems."
2506.03587,"The rapid growth of scholarly literature makes it increasingly difficult for researchers to keep up with new knowledge. Automated tools are now more essential than ever to help navigate and interpret this vast body of information. Scientific papers pose unique difficulties, with their complex language, specialized terminology, and diverse formats, requiring advanced methods to extract reliable and actionable insights. Large language models (LLMs) offer new opportunities, enabling tasks such as literature reviews, writing assistance, and interactive exploration of research. This special issue of the TAL journal highlights research addressing these challenges and, more broadly, research on natural language processing and information retrieval for scholarly and scientific documents."
2506.03726,"Although bibliometrics has become an essential tool in the evaluation of research performance, bibliometric analyses are sensitive to a range of methodological choices. Subtle choices in data selection, indicator construction, and modeling decisions can substantially alter results. Ensuring robustness, meaning that findings hold up under different reasonable scenarios, is therefore critical for credible research and research evaluation. To address this issue, this study introduces multiverse analysis to bibliometrics. Multiverse analysis is a statistical tool that enables analysts to transparently discuss modeling assumptions and thoroughly assess model robustness. Whereas standard robustness checks usually cover only a small subset of all plausible models, multiverse analysis includes all plausible models. We illustrate the benefits of multiverse analysis by testing the hypothesis posed by Wu et al. (2019) that small teams produce more disruptive research than large teams. While we found robust evidence of a negative effect of team size on disruption scores, the effect size is so small that its practical relevance seems questionable. Our findings underscore the importance of assessing the multiverse robustness of bibliometric results to clarify their practical implications."
2506.06753,"Intergovernmental organizations (IGOs) increasingly rely on scientific evidence, yet the pathways through which scientific research enters policy remain opaque. By linking 230,737 scientific papers cited in IGO policy documents (2015-2023) to their authors and collaboration networks, we identify a small group of policy-influential scientists (PI-Sci) who dominate this knowledge flow. These scientists form tightly interconnected, internationally spanning co-authorship networks and achieve policy citations shortly after publication, a distinctive feature of cumulative advantage at the science-policy interface. The concentration of influence varies by field: tightly clustered in established domains like climate modeling, and more dispersed in emerging areas like AI governance. Many PI-Sci serve on high-level advisory bodies (e.g., IPCC), and major IGOs frequently co-cite the same PI-Sci papers, indicating synchronized knowledge diffusion through shared expert networks. These findings reveal how network structure and elite brokerage shape the translation of research into global policy, highlighting opportunities to broaden the scope of knowledge that informs policy."
2506.07547,"Preprints have been considered primarily as a supplement to journal-based systems for the rapid dissemination of relevant scientific knowledge and have historically been supported by studies indicating that preprints and published reports have comparable authorship, references, and quality. However, as preprints increasingly serve as an independent medium for scholarly communication rather than precursors to the version of record, it remains uncertain how preprint usage is shaping scientific discourse. Our research revealed that the preprint citations exhibit significantly higher inequality than journal citations, consistently among categories. This trend persisted even when controlling for age and the mean citation count of the journal matched to each of the preprint categories. We also found that the citation inequality in preprints is not solely driven by a few highly cited papers or those with no impact, but rather reflects a broader systemic effect. Whether the preprint is subsequently published in a journal or not does not significantly affect the citation inequality. Further analyses of the structural factors show that preferential attachment does not significantly contribute to citation inequality in preprints, whereas author prestige plays a substantial role. Notably, the gap in citation inequality between the preprint category and the journal is more pronounced in fields where preprints are more established, such as mathematics, physics, and high-energy physics. This highlights a potential vulnerability in preprint ecosystems where reputation-driven citation may hinder scientific diversity."
2506.07748,"Artificial Intelligence (AI) technologies like ChatGPT now threaten bibliometrics as the primary generators of research quality indicators. They are already used in at least one research quality evaluation system and evidence suggests that they are used informally by many peer reviewers. Since using bibliometrics to support research evaluation continues to be controversial, this article reviews the corresponding advantages and disadvantages of AI-generated quality scores. From a technical perspective, generative AI based on Large Language Models (LLMs) equals or surpasses bibliometrics in most important dimensions, including accuracy (mostly higher correlations with human scores), and coverage (more fields, more recent years) and may reflect more research quality dimensions. Like bibliometrics, current LLMs do not ""measure"" research quality, however. On the clearly negative side, LLM biases are currently unknown for research evaluation, and LLM scores are less transparent than citation counts. From a systemic perspective, the key issue is how introducing LLM-based indicators into research evaluation will change the behaviour of researchers. Whilst bibliometrics encourage some authors to target journals with high impact factors or to try to write highly cited work, LLM-based indicators may push them towards writing misleading abstracts and overselling their work in the hope of impressing the AI. Moreover, if AI-generated journal indicators replace impact factors, then this would encourage journals to allow authors to oversell their work in abstracts, threatening the integrity of the academic record."
2506.08199,"Scientific document embeddings contain a variety of rich features which can be harnessed for downstream tasks such as recommendation, ranking, and clustering. We explore which tangible insights can be drawn from scientific document embeddings to understand trends in computer science research featured across nine well-known venues. We collect approximately 60,000 scientific documents published between 2015 and 2023 and analyze their embeddings, which we produce with the SPECTER pre-trained language model. In particular, we examine whether similarity between two venues can be measured using the embeddings of the scientific documents they admit for publication. Our findings indicate that some venues within computer science are indistinguishable when only considering the distributions of their document embeddings. We additionally examine whether any two venues are becoming increasingly similar over time and identify a trend of convergence within some venues in our analysis. We discuss the implications of these results and the potential impact on new scientific contributions."
2506.09056,"The exponential increase in academic publications has made it increasingly difficult for researchers to remain up to date and systematically synthesize knowledge scattered across vast and fragmented research domains. Literature reviews, particularly those supported by bibliometric methods, have become essential in organizing prior findings and guiding future research directions. While numerous tools exist for bibliometric analysis and network science, there is currently no single platform that integrates the full range of features from both domains. Researchers are often required to navigate multiple software environments, many of which lack customizable visualizations, cross-database integration, and AI-assisted result summarization. Addressing these limitations, this study introduces MetaInfoSci atthis http URL, a comprehensive, web-based platform designed to unify bibliometric, scientometric, and network analytical capabilities. The platform supports tailored query design, merges data from diverse sources, enables rich and adaptable visual outputs, and provides automated, AI-driven summaries of analytical results. This integrated approach aims to enhance the accessibility, efficiency, and depth of scientific literature analysis for scholars across disciplines."
2506.0953,"In today's data-driven research landscape, dataset visibility and accessibility play a crucial role in advancing scientific knowledge. At the same time, data citation is essential for maintaining academic integrity, acknowledging contributions, validating research outcomes, and fostering scientific reproducibility. As a critical link, it connects scholarly publications with the datasets that drive scientific progress. This study investigates whether repository visibility influences data citation rates. We hypothesize that repositories with higher visibility, as measured by search engine metrics, are associated with increased dataset citations. Using OpenAlex data and repository impact indicators (including the visibility index from Sistrix, the h-index of repositories, and citation metrics such as mean and median citations), we analyze datasets in Social Sciences and Economics to explore their relationship. Our findings suggest that datasets hosted on more visible web domains tend to receive more citations, with a positive correlation observed between web domain visibility and dataset citation counts, particularly for datasets with at least one citation. However, when analyzing domain-level citation metrics, such as the h-index, mean, and median citations, the correlations are inconsistent and weaker. While higher visibility domains tend to host datasets with greater citation impact, the distribution of citations across datasets varies significantly. These results suggest that while visibility plays a role in increasing citation counts, it is not the sole factor influencing dataset citation impact. Other elements, such as dataset quality, research trends, and disciplinary norms, can also contribute to citation patterns."
2506.10942,"Understanding the flow of information across today's fragmented digital media landscape requires scalable, cross-platform infrastructure. In this paper, we present the Canadian Media Ecosystem Observatory, a national-scale infrastructure designed to monitor political and media discourse across platforms in near real time.Media Ecosystem Observatory (MEO) data infrastructure features custom crawlers for major platforms, a unified indexing pipeline, and a normalization layer that harmonizes heterogeneous schemas into a common data model. Semantic embeddings are computed for each post to enable similarity search and vector-based analyses such as topic modeling and clustering. Processed and raw data are made accessible through API, dashboards and website, supporting both automated and ad hoc research workflows. We illustrate the utility of the observatory through example analyses of major Canadian political events, including Meta's 2023 news ban and the recent federal elections. As a whole, the system offers a model for digital trace infrastructure and an evolving research platform for studying the dynamics of modern media ecosystems."
2506.13525,"The large language model (LLM) ChatGPT's quality scores for journal articles correlate more strongly with human judgements than some citation-based indicators in most fields. Averaging multiple ChatGPT scores improves the results, apparently leveraging its internal probability model. To leverage these probabilities, this article tests two novel strategies: requesting percentage likelihoods for scores and extracting the probabilities of alternative tokens in the responses. The probability estimates were then used to calculate weighted average scores. Both strategies were evaluated with five iterations of ChatGPT 4o-mini on 96,800 articles submitted to the UK Research Excellence Framework (REF) 2021, using departmental average REF2021 quality scores as a proxy for article quality. The data was analysed separately for each of the 34 field-based REF Units of Assessment. For the first strategy, explicit requests for tables of score percentage likelihoods substantially decreased the value of the scores (lower correlation with the proxy quality indicator). In contrast, weighed averages of score token probabilities slightly increased the correlation with the quality proxy indicator and these probabilities reasonably accurately reflected ChatGPT's outputs. The token probability approach is therefore the most accurate method for ranking articles by research quality as well as being cheaper than comparable ChatGPT strategies."
2506.1443,"The transition to Open Science necessitates robust and reliable metadata. While national initiatives, such as the French Open Science Monitor, aim to track this evolution using open data, reliance on proprietary databases persists in many places. Open platforms like OpenAlex still require significant human intervention for data accuracy. This paper introduces Works-magnet, a project by the French Ministry of Higher Education and Research (MESR) Data Science & Engineering Team. Works-magnet is designed to accelerate the curation of bibliographic and research data metadata, particularly affiliations, by making automated AI calculations visible and correctable. It addresses challenges related to metadata heterogeneity, complex processing chains, and the need for human curation in a diverse research landscape. The paper details Works-magnet's concepts, and the observed limitations, while outlining future directions for enhancing open metadata quality and reusability. The works-magnet app is open source on githubthis https URL"
2506.14715,"Procedural Knowledge Libraries (PKLs) are frameworks for capturing the full arc of scientific inquiry, not just its outcomes. Whereas traditional libraries store static end products, PKLs preserve the process that leads to those results, including hypotheses, failures, decisions, and iterations. By addressing the loss of tacit knowledge -- typically buried in notebooks, emails, or memory -- PKLs lay a foundation for reproducible, collaborative, and adaptive research. PKLs provide executable, version-controlled records that contextualize each step of a research process. For example, a researcher using Jupyter notebooks could share not just final outputs, but also the reasoning, discarded approaches, and intermediate analyses that informed them. This work proposes a framework for implementing PKLs within the Jupyter ecosystem, supported by a lens-based transformation model and procedural storage schema."
2506.1504,"This study aims to improve the accuracy of long-term citation impact prediction by integrating early citation counts, Mendeley readership, and various non-scientific factors, such as journal impact factor, authorship and reference list characteristics, funding and open-access status. Traditional citation-based models often fall short by relying solely on early citations, which may not capture broader indicators of a publication's potential influence. By incorporating non-scientific predictors, this model provides a more nuanced and comprehensive framework that outperforms existing models in predicting long-term impact. Using a dataset of Italian-authored publications from the Web of Science, regression models were developed to evaluate the impact of these predictors over time. Results indicate that early citations and Mendeley readership are significant predictors of long-term impact, with additional contributions from factors like authorship diversity and journal impact factor. The study finds that open-access status and funding have diminishing predictive power over time, suggesting their influence is primarily short-term. This model benefits various stakeholders, including funders and policymakers, by offering timely and more accurate assessments of emerging research. Future research could extend this model by incorporating broader altmetrics and expanding its application to other disciplines and regions. The study concludes that integrating non-citation-based factors with early citations captures a more complex view of scholarly impact, aligning better with real-world research influence."
2506.15055,"""Digital libraries"" is an umbrella term that encompasses the automation of library services, online catalogs, information retrieval systems, multi-media databases, data archives, and other internet-facing collections of digital resources. Clifford Lynch has played pivotal roles in the technical development, institutionalization, policy, practice, and dissemination of digital libraries for more than 40 years. Beginning with his foundational role in building MELVYL for the University of California in the early 1980s -- the first internet-native online open access library catalog -- through his convening roles in open access and open data in the 21st century, his career is marked by multiple milestones of innovation. Clifford Lynch's career has traced the trajectory of digital libraries and knowledge infrastructures. Over the course of these 40 years, research libraries have faced four categories of challenges: invisible infrastructure, content and collections, preservation and access, and institutional boundaries. These challenges have become yet more complex in an era of open access, open data, and evolving regimes of intellectual property and scholarly publishing. As the digital library communities have merged and diverged over this time span, we collectively face challenges for at least the next 40 years to sustain access to current resources while growing the next generations of digital libraries and librarians."
2506.15237,"The issue of gender bias in scientific publications has been a topic of ongoing debate. One aspect of this debate concerns whether women receive equal credit for their contributions compared to men. Conventional wisdom suggests that women are more likely to be acknowledged than listed as co-authors. In this study, we analyze data from over 20,000 authors and 60,000 acknowledged individuals across nine disciplines in open-access journals. Our results confirm persistent gender disparities: women are more frequently acknowledged than credited as co-authors, especially in roles involving investigation and analysis. To account for status and disciplinary effects, we examined collaboration pair composed of highly cited and less-cited scholars. In collaborations, highly cited scholars are more likely to be listed as an author regardless of gender. Notably, highly cited women in such pairs are even more likely to be co-authors than their men counterparts. Our findings suggest that power dynamics and perceived success heavily influence the distribution of credit in scientific publishing. These results underscore the role of status dynamics in shaping authorship and call for a more nuanced understanding of how gender, power, and recognition interact in scientific publishing. Our findings offer valuable insights for scholars, editors, and funding committed to advancing equity in science."
2506.15959,"New ideas are often thought to arise from recombining existing knowledge. Yet despite rapid publication growth - and expanding opportunities for recombination - scientific breakthroughs remain rare. This gap between productivity and progress challenges recombinant growth theory as the prevailing account of innovation. We argue that the limitation of this theory lies in treating ideas solely as complements, overlooking that breakthroughs often arise when ideas act as substitutes. To test this, we integrate scientist interviews, bibliometric validation, and machine learning analysis of 41 million papers (1965-2024). Interviews reveal that breakthroughs are marked not by novelty (Atypicality) alone but by their ability to displace dominant ideas (Disruption). Large-scale analysis confirms that novelty and disruption represent distinct innovation mechanisms: they are negatively correlated across domains, periods, team sizes, and paper versions. Novel papers extend dominant ideas across topics and attract immediate attention; disruptive papers displace them within the same topic and generate lasting influence. Hence, progress slows not from lack of effort but because most research extends rather than overturns ideas. Applying this perspective reveals distinct roles of theories and methods in scientific change: methods more often drive breakthroughs, whereas theories tend to be novel but rarely disruptive, reinforcing the dominance of established ideas."
2506.17508,"This paper presents KnoVo (Knowledge Evolution), an intelligent framework designed for quantifying and analyzing the evolution of research novelty in the scientific literature. Moving beyond traditional citation analysis, which primarily measures impact, KnoVo determines a paper's novelty relative to both prior and subsequent work within its multilayered citation network. Given a target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to dynamically extract dimensions of comparison (e.g., methodology, application, dataset). The target paper is then compared to related publications along these same extracted dimensions. This comparative analysis, inspired by tournament selection, yields quantitative novelty scores reflecting the relative improvement, equivalence, or inferiority of the target paper in specific aspects. By aggregating these scores and visualizing their progression, for instance, through dynamic evolution graphs and comparative radar charts, KnoVo facilitates researchers not only to assess originality and identify similar work, but also to track knowledge evolution along specific research dimensions, uncover research gaps, and explore cross-disciplinary connections. We demonstrate these capabilities through a detailed analysis of 20 diverse papers from multiple scientific fields and report on the performance of various open-source LLMs within the KnoVo framework."
2506.18069,"We developed a proof-of-concept method for the automatic analysis of the structure and content of incunabula pages. A custom dataset comprising 500 annotated pages from five different incunabula was created using resources from the Jagiellonian Digital Library. Each page was manually labeled with five predefined classes: Text, Title, Picture, Table, and Handwriting. Additionally, the publicly available DocLayNet dataset was utilized as supplementary training data. To perform object detection, YOLO11n and YOLO11s models were employed and trained using two strategies: a combined dataset (DocLayNet and the custom dataset) and the custom dataset alone. The highest performance (F1 = 0.94) was achieved by the YOLO11n model trained exclusively on the custom data. Optical character recognition was then conducted on regions classified as Text, using both Tesseract and Kraken OCR, with Tesseract demonstrating superior results. Subsequently, image classification was applied to the Picture class using a ResNet18 model, achieving an accuracy of 98.7% across five subclasses: Decorative_letter, Illustration, Other, Stamp, and Wrong_detection. Furthermore, the CLIP model was utilized to generate semantic descriptions of illustrations. The results confirm the potential of machine learning in the analysis of early printed books, while emphasizing the need for further advancements in OCR performance and visual content interpretation."
2506.18517,"The openCost project aims to enhance transparency in research funding by making publication-related costs publicly accessible, following FAIR principles. It introduces a metadata schema for cost data, allowing aggregation and analysis across institutions. The project promotes open access and cost-efficient models, benefiting academic institutions, funders, and policymakers."
2506.18804,"Science is driven by community endeavors across diverse fields and specializations, forming a complex structure that renders conventional performance evaluation methods inadequate. Using established indicators, the network-based normalized citation score and the disruptive index, combined with the GENEPY algorithm, we evaluate the complexity rank of countries based on their breakthrough performance across 89 subfields of physical sciences, drawing on nearly 60 million articles (1900-2023). This quality-focused integrated approach reveals pronounced asymmetries: while countries such as the United States, Israel, and several in Europe sustain long-term structural advantages, emerging nations show rapid gains in later decades. A power-law relationship between aggregated breakthrough performance and countries' R&D expenditure underscores the unequal and scale-dependent nature of global science. These results demonstrate that scientific advancement arises not from uniform growth but from asymmetric complexity, offering actionable insights for policymakers and funding agencies aiming to foster sustainable, high-quality research ecosystems."
2506.20225,"Preprints have become increasingly essential in the landscape of open science, facilitating not only the exchange of knowledge within the scientific community but also bridging the gap between science and technology. However, the impact of preprints on technological innovation, given their unreviewed nature, remains unclear. This study fills this gap by conducting a comprehensive scientometric analysis of patent citations to bioRxiv preprints submitted between 2013 and 2021, measuring and accessing the contribution of preprints in accelerating knowledge transfer from science to technology. Our findings reveal a growing trend of patent citations to bioRxiv preprints, with a notable surge in 2020, primarily driven by the COVID-19 pandemic. Preprints play a critical role in accelerating innovation, not only expedite the dissemination of scientific knowledge into technological innovation but also enhance the visibility of early research results in the patenting process, while journals remain essential for academic rigor and reliability. The substantial number of post-online-publication patent citations highlights the critical role of the open science model-particularly the ""open access"" effect of preprints-in amplifying the impact of science on technological innovation. This study provides empirical evidence that open science policies encouraging the early sharing of research outputs, such as preprints, contribute to more efficient linkage between science and technology, suggesting an acceleration in the pace of innovation, higher innovation quality, and economic benefits."
2506.20918,"In this project, we semantically enriched and enhanced the metadata of long text documents, theses and dissertations, retrieved from the HathiTrust Digital Library in English published from 1920 to 2020 through a combination of manual efforts and large language models. This dataset provides a valuable resource for advancing research in areas such as computational social science, digital humanities, and information science. Our paper shows that enriching metadata using LLMs is particularly beneficial for digital repositories by introducing additional metadata access points that may not have originally been foreseen to accommodate various content types. This approach is particularly effective for repositories that have significant missing data in their existing metadata fields, enhancing search results and improving the accessibility of the digital repository."
2506.21232,"A 20-year analysis of CrossRef metadata demonstrates that global scholarly output -- encompassing publications, retractions, and preprints -- exhibits strikingly inertial growth, well-described by exponential, quadratic, and logistic models with nearly indistinguishable goodness-of-fit. Retraction dynamics, in particular, remain stable and minimally affected by the COVID-19 shock, which contributed less than 1% to total notices. Since 2004, publications doubled every 9.8 years, retractions every 11.4 years, and preprints at the fastest rate, every 5.6 years. The findings underscore a system primed for ongoing stress at unchanged structural bottlenecks. Although model forecasts diverge beyond 2024, the evidence suggests that the future trajectory of scholarly communication will be determined by persistent systemic inertia rather than episodic disruptions -- unless intentionally redirected by policy or AI-driven reform."
2506.21331,"Everyday, a vast stream of research documents is submitted to conferences, anthologies, journals, newsletters, annual reports, daily papers, and various periodicals. Many such publications use independent external specialists to review submissions. This process is called peer review, and the reviewers are called referees. However, it is not always possible to pick the best referee for reviewing. Moreover, new research fields are emerging in every sector, and the number of research papers is increasing dramatically. To review all these papers, every journal assigns a small team of referees who may not be experts in all areas. For example, a research paper in communication technology should be reviewed by an expert from the same field. Thus, efficiently selecting the best reviewer or referee for a research paper is a big challenge.In this research, we propose and implement program that uses a new strategy to automatically select the best reviewers for a research paper. Every research paper contains references at the end, usually from the same area. First, we collect the references and count authors who have at least one paper in the references. Then, we automatically browse the web to extract research topic keywords. Next, we search for top researchers in the specific topic and count their h-index, i10-index, and citations for the first n authors. Afterward, we rank the top n authors based on a score and automatically browse their homepages to retrieve email addresses. We also check their co-authors and colleagues online and discard them from the list. The remaining top n authors, generally professors, are likely the best referees for reviewing the research paper."
2506.21819,"Scientific publications, primarily digitized as PDFs, remain static and unstructured, limiting the accessibility and reusability of the contained knowledge. At best, scientific knowledge from publications is provided in tabular formats, which lack semantic context. A more flexible, structured, and semantic representation is needed to make scientific knowledge understandable and processable by both humans and machines. We propose an evolution model of knowledge representation, inspired by the 5-star Linked Open Data (LOD) model, with five stages and defined criteria to guide the stepwise transition from a digital artifact, such as a PDF, to a semantic representation integrated in a knowledge graph (KG). Based on an exemplary workflow implementing the entire model, we developed a hybrid approach, called SciMantify, leveraging tabular formats of scientific knowledge, e.g., results from secondary studies, to support its evolving semantification. In the approach, humans and machines collaborate closely by performing semantic annotation tasks (SATs) and refining the results to progressively improve the semantic representation of scientific knowledge. We implemented the approach in the Open Research Knowledge Graph (ORKG), an established platform for improving the findability, accessibility, interoperability, and reusability of scientific knowledge. A preliminary user experiment showed that the approach simplifies the preprocessing of scientific knowledge, reduces the effort for the evolving semantification, and enhances the knowledge representation through better alignment with the KG structures."
2506.22729,"Persistence is often regarded as a virtue in science. In this paper, however, we challenge this conventional view by highlighting its contextual nature, particularly how persistence can become a liability during periods of paradigm shift. We focus on the deep learning revolution catalyzed by AlexNet in 2012. Analyzing the 20-year career trajectories of over 5,000 scientists who were active in top machine learning venues during the preceding decade, we examine how their research focus and output evolved. We first uncover a dynamic period in which leading venues increasingly prioritized cutting-edge deep learning developments that displaced relatively traditional statistical learning methods. Scientists responded to these changes in markedly different ways. Those who were previously successful or affiliated with old teams adapted more slowly, experiencing what we term a rigidity penalty - a reluctance to embrace new directions leading to a decline in scientific impact, as measured by citation percentile rank. In contrast, scientists who pursued strategic adaptation - selectively pivoting toward emerging trends while preserving weak connections to prior expertise - reaped the greatest benefits. Taken together, our macro- and micro-level findings show that scientific breakthroughs act as mechanisms that reconfigure power structures within a field."
2506.23366,"Scientific behavior is often characterized by a tension between building upon established knowledge and introducing novel ideas. Here, we investigate whether this tension is reflected in the relationship between the similarity of a scientific paper to previous research and its eventual citation rate. To operationalize similarity to previous research, we introduce two complementary metrics to characterize the local geometry of a publication's semantic neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed number of previously-published papers and the minimum distance enclosing those papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as the average directional difference between a paper and its nearest neighbors. We tested the predictive relationship between these two metrics and its subsequent citation rate using a Bayesian hierarchical regression approach, surveying $\sim 53,000$ publications across nine academic disciplines and five different document embeddings. While the individual effects of $\rho$ on citation count are small and variable, incorporating density-based predictors consistently improves out-of-sample prediction when added to baseline models. These results suggest that the density of a paper's surrounding scientific literature may carry modest but informative signals about its eventual impact. Meanwhile, we find no evidence that publication asymmetry improves model predictions of citation rates. Our work provides a scalable framework for linking document embeddings to scientometric outcomes and highlights new questions regarding the role that semantic similarity plays in shaping the dynamics of scientific reward."
2507.00961,"We present Digital Collections Explorer, a web-based, open-source exploratory search platform that leverages CLIP (Contrastive Language-Image Pre-training) for enhanced visual discovery of digital collections. Our Digital Collections Explorer can be installed locally and configured to run on a visual collection of interest on disk in just a few steps. Building upon recent advances in multimodal search techniques, our interface enables natural language queries and reverse image searches over digital collections with visual features. This paper describes the system's architecture, implementation, and application to various cultural heritage collections, demonstrating its potential for democratizing access to digital archives, especially those with impoverished metadata. We present case studies with maps, photographs, and PDFs extracted from web archives in order to demonstrate the flexibility of the Digital Collections Explorer, as well as its ease of use. We demonstrate that the Digital Collections Explorer scales to hundreds of thousands of images on a MacBook Pro with an M4 chip. Lastly, we host a public demo of Digital Collections Explorer."
2507.01228,"The ability to find data is central to the FAIR principles underlying research data stewardship. As with the ability to reuse data, efforts to ensure and enhance findability have historically focused on discoverability of data by other researchers, but there is a growing recognition of the importance of stewarding data in a fashion that makes them FAIR for a wide range of potential reusers and stakeholders. Research institutions are one such stakeholder and have a range of motivations for discovering data, specifically those affiliated with a focal institution, from facilitating compliance with funder provisions to gathering data to inform research data services. However, many research datasets and repositories are not optimized for institutional discovery (e.g., not recording or standardizing affiliation metadata), which creates downstream obstacles to workflows designed for theoretically comprehensive discovery and to metadata-conscious data generators. Here I describe an open-source workflow for institutional tracking of research datasets at the University of Texas at Austin. This workflow comprises a multi-faceted approach that utilizes multiple open application programming interfaces (APIs) in order to address some of the common challenges to institutional discovery, such as variation in whether affiliation metadata are recorded or made public, and if so, how metadata are standardized, structured, and recorded. It is presently able to retrieve more than 4,000 affiliated datasets across nearly 70 distinct platforms, including objects without DOIs and objects without affiliation metadata. However, there remain major gaps that stem from suboptimal practices of both researchers and data repositories, many of which were identified in previous studies and which persist despite significant investment in efforts to standardize and elevate the quality of datasets and their metadata."
2507.0152,"This paper aims to comprehensively grasp the research status and development trends of soil microplastics (MPs). It collects studies from the Web of Science Core Collection covering the period from 2013 to 2024. Employing CiteSpace and VOSviewer, the paper conducts in - depth analyses of literature regarding the environmental impacts of microplastics. These analyses involve keyword co - occurrence, clustering, burst term identification, as well as co - occurrence analysis of authors and institutions. Microplastics can accumulate in soil, transfer through food chains, and ultimately affect human health, making the research on them essential for effective pollution control. Focusing on the international research on the impacts of microplastics on soil and ecosystems, the study reveals a steadily increasing trend in the number of publications each year, reaching a peak of 956 articles in 2024. A small number of highly productive authors contribute significantly to the overall research output. The keyword clustering analysis results in ten major clusters, including topics such as plastic pollution and microbial communities. The research on soil microplastics has evolved through three distinct stages: the preliminary exploration phase from 2013 to 2016, the expansion phase from 2017 to 2020, and the integration phase from 2021 to 2024. For future research, multi - level assessments of the impacts of microplastics on soil ecosystems and organisms should be emphasized, in order to fully uncover the associated hazards and develop practical solutions."
2507.01651,"Neuroscience and AI have an intertwined history, largely relayed in the literature of both fields. In recent years, due to the engineering orientations of AI research and the monopoly of industry for its large-scale applications, the mutual expansion of neuroscience and AI in fundamental research seems challenged. In this paper, we bring some empirical evidences that, on the contrary, AI and neuroscience are continuing to grow together, but with a pronounced interest in the fields of study related to neurodegenerative diseases since the 1990s. With a temporal knowledge cartography of neuroscience drawn with advanced document embedding techniques, we draw the dynamical shaping of the discipline since the 1970s and identified the conceptual articulation of AI with this particular subfield mentioned before. However, a further analysis of the underlying citation network of the studied corpus shows that the produced AI technologies remain confined in the different subfields and are not transferred from one subfield to another. This invites us to discuss the genericity capability of AI in the context of an intradisciplinary development, especially in the diffusion of its associated metrology."
2507.04132,"This article presents and validates an ideal, four-stage workflow for the high-accuracy transcription and analysis of challenging medieval legal documents. The process begins with a specialized Handwritten Text Recognition (HTR) model, itself created using a novel ""Clean Ground Truth"" curation method where a Large Language Model (LLM) refines the training data. This HTR model provides a robust baseline transcription (Stage 1). In Stage 2, this baseline is fed, along with the original document image, to an LLM for multimodal post-correction, grounding the LLM's analysis and improving accuracy. The corrected, abbreviated text is then expanded into full, scholarly Latin using a prompt-guided LLM (Stage 3). A final LLM pass performs Named-Entity Correction (NEC), regularizing proper nouns and generating plausible alternatives for ambiguous readings (Stage 4). We validate this workflow through detailed case studies, achieving Word Error Rates (WER) in the range of 2-7% against scholarly ground truths. The results demonstrate that this hybrid, multi-stage approach effectively automates the most laborious aspects of transcription while producing a high-quality, analyzable output, representing a powerful and practical solution for the current technological landscape."
2507.04444,"Data search for scientific research is more complex than a simple web search. The emergence of large language models (LLMs) and their applicability for scientific tasks offers new opportunities for researchers who are looking for data, e.g., to freely express their data needs instead of fitting them into restrictions of data catalogues and portals. However, this also creates uncertainty about whether LLMs are suitable for this task. To answer this question, we conducted a user study with 32 researchers. We qualitatively and quantitively analysed participants' information interaction behaviour while searching for data using LLMs in two data search tasks, one in which we prompted the LLM to behave as a persona. We found that participants interact with LLMs in natural language, but LLMs remain a tool for them rather than an equal conversational partner. This changes slightly when the LLM is prompted to behave as a persona, but the prompting only affects participants' user experience when they are already experienced in LLM use."
2507.05903,"The AI-Reporter represents a paradigmatic shift in scientific publication practice. This document demonstrates through a concrete case study how our system transforms academic presentations into publication-ready chapters -- in less than three minutes. Using Arno Simons' lecture on Large Language Models from the ``Large Language Models for the History, Philosophy, and Sociology of Science'' workshop (NEPI) as an example, we show how technological innovation bridges the gap between ephemeral presentation and permanent scientific documentation."
2507.1181,"Scientific innovation is undergoing a paradigm shift driven by the rapid advancement of Large Language Models (LLMs). As science faces mounting challenges including information overload, disciplinary silos, and diminishing returns on conventional research methods, LLMs are emerging as powerful agents capable not only of enhancing scientific workflows but also of participating in and potentially leading the innovation process. Existing surveys mainly focus on different perspectives, phrases, and tasks in scientific research and discovery, while they have limitations in understanding the transformative potential and role differentiation of LLM. This survey proposes a comprehensive framework to categorize the evolving roles of LLMs in scientific innovation across three hierarchical levels: Evaluator, Collaborator, and Scientist. We distinguish between LLMs' contributions to structured scientific research processes and open-ended scientific discovery, thereby offering a unified taxonomy that clarifies capability boundaries, evaluation criteria, and human-AI interaction patterns at each level. Through an extensive analysis of current methodologies, benchmarks, systems, and evaluation metrics, this survey delivers an in-depth and systematic synthesis on LLM-driven scientific innovation. We present LLMs not only as tools for automating existing processes, but also as catalysts capable of reshaping the epistemological foundations of science itself. This survey offers conceptual clarity, practical guidance, and theoretical foundations for future research, while also highlighting open challenges and ethical considerations in the pursuit of increasingly autonomous AI-driven science. Resources related to this survey can be accessed on GitHub at:this https URL."
2507.12255,"Team science dominates scientific knowledge production, but what makes academic teams successful? Using temporal data on 25.2 million publications and 31.8 million authors, we propose a novel network-driven approach to identify and study the success of persistent teams. Challenging the idea that persistence alone drives success, we find that team freshness - new collaborations built on prior experience - is key to success. High impact research tends to emerge early in a team's lifespan. Analyzing complex team overlap, we find that teams open to new collaborative ties consistently produce better science. Specifically, team re-combinations that introduce new freshness impulses sustain success, while persistence impulses from experienced teams are linked to earlier impact. Together, freshness and persistence shape team success across collaboration stages."
2507.13143,"In research, measuring instruments play a crucial role in producing the data that underpin scientific discoveries. Information about instruments is essential in data interpretation and, thus, knowledge production. However, if at all available and accessible, such information is scattered across numerous data sources. Relating the relevant details, e.g. instrument specifications or calibrations, with associated research assets (data, but also operating infrastructures) is challenging. Moreover, understanding the (possible) use of instruments is essential for researchers in experiment design and execution. To address these challenges, we propose a Knowledge Graph (KG) based approach for representing, publishing, and using information, extracted from various data sources, about instruments and associated scholarly artefacts. The resulting KG serves as a foundation for exploring and gaining a deeper understanding of the use and role of instruments in research, discovering relations between instruments and associated artefacts (articles and datasets), and opens the possibility to quantify the impact of instruments in research."
2507.14752,"We document strategies and lessons learned from sampling the web by collecting 27.3 million URLs with 3.8 billion archived pages spanning 26 years (1996-2021) from the Internet Archive's (IA) Wayback Machine. Our goal is to revisit fundamental questions regarding the size, nature, and prevalence of the publicly archivable web, in particular, to reconsider the question: ""How long does a web page last?"" Addressing this question requires obtaining a sample of the web. We proposed several dimensions to sample URLs from the Wayback Machine's holdings: time of first archive, HTML vs. other MIME types, URL depth (top-level pages vs. deep links), and top-level domain (TLD). We sampled 285 million URLs from IA's ZipNum index file, which contains every 6000th line of the CDX index. These indexes also include URLs of embedded resources such as images, CSS, and JavaScript. To limit our sample to ""web pages"" (i.e., pages intended for human interaction), we filtered for likely HTML pages based on filename extension. We then queried IA's CDX API to determine the time of first capture and MIME type of each URL. We grouped 92 million text/html URLs based on year of first capture. Archiving speed and capacity have increased over time, so we found more URLs archived in later years. To counter this, we extracted top-level URLs from deep links to upsample earlier years. Our target was 1 million URLs per year, but due to sparseness during 1996-2021, we clustered those years, collecting 1.2 million URLs for that range. Popular domains like Yahoo and Twitter were over-represented, so we performed logarithmic-scale downsampling. Our final dataset contains TimeMaps of 27.3 million URLs, comprising 3.8 billion archived pages. We convey lessons learned from sampling the archived web to inform future studies."
2507.155,"The sustainability of the global academic ecosystem relies on researcher demographics and gender balance, yet assessing these dynamics in a timely manner for policy is challenging. Here, we propose a researcher population pyramids framework for tracking global demographic and gender trajectories using publication data. This framework provides a timely snapshot of historical and present demographics and gender balance, revealing three contrasting research systems: Emerging systems (e.g., Arab countries) exhibit high researcher inflows with widening gender gaps in cumulative productivity; Mature systems (e.g., the United States) show modest inflows with narrowing gender gaps; and Rigid systems (e.g., Japan) lag in both. Furthermore, by simulating future scenarios, the framework makes potential trajectories visible. If 2023 demographic patterns persist, Arab countries' systems could resemble mature or even rigid ones by 2050. Our framework provides a robust diagnostic tool for policymakers worldwide to foster sustainable talent pipelines and gender equality in academia."
2507.1559,"Since the 60s, musicology has been increasingly impacted by computational tools in various ways, from systematic analysis approaches to modeling of creativity. This article presents a comprehensive assessment of the current state of Computational Musicology tools based on survey data collected from practitioners in the field. We gathered information on tool usage patterns, common analytical tasks, user satisfaction levels, data characteristics, and prioritized features across four distinct domains: symbolic music, music-related imagery, audio, and text. Our findings reveal significant gaps between current tooling capabilities and user needs, highlighting some limitations of these tools across all domains. This assessment contributes to the ongoing dialogue between tool developers and music scholars, aiming to enhance the effectiveness and accessibility of computational methods in musicological research."
2507.17114,"This study examines the social media uptake of scientific journals on two different platforms - X and WeChat - by comparing the adoption of X among journals indexed in the Science Citation Index-Expanded (SCIE) with the adoption of WeChat among journals indexed in the Chinese Science Citation Database (CSCD). The findings reveal substantial differences in platform adoption and user engagement, shaped by local contexts. While only 22.7% of SCIE journals maintain an X account, 84.4% of CSCD journals have a WeChat official account. Journals in Life Sciences & Biomedicine lead in uptake on both platforms, whereas those in Technology and Physical Sciences show high WeChat uptake but comparatively lower presence on X. User engagement on both platforms is dominated by low-effort interactions rather than more conversational behaviors. Correlation analyses indicate weak-to-moderate relationships between bibliometric indicators and social media metrics, confirming that online engagement reflects a distinct dimension of journal impact, whether on an international or a local platform. These findings underscore the need for broader social media metric frameworks that incorporate locally dominant platforms, thereby offering a more comprehensive understanding of science communication practices across diverse social media and contexts."
2507.17127,"Scientific retractions reflect issues within the scientific record, arising from human error or misconduct. Although gender differences in retraction rates have been previously observed in various contexts, no comprehensive study has explored this issue across all fields of science. This study examines gender disparities in scientific misconduct or errors, specifically focusing on differences in retraction rates between male and female first authors in relation to their research productivity. Using a dataset comprising 11,622 retracted articles and 19,475,437 non-retracted articles from the Web of Science and Retraction Watch, we investigate gender differences in retraction rates from the perspectives of retraction reasons, subject fields, and countries. Our findings indicate that male first authors have higher retraction rates, particularly for scientific misconduct such as plagiarism, authorship disputes, ethical issues, duplication, and fabrication/falsification. No significant gender differences were found in retractions attributed to mistakes. Furthermore, male first authors experience significantly higher retraction rates in biomedical and health sciences, as well as in life and earth sciences, whereas female first authors have higher retraction rates in mathematics and computer science. Similar patterns are observed for corresponding authors. Understanding these gendered patterns of retraction may contribute to strategies aimed at reducing their prevalence."
2507.18201,"With the evolution of process approaches within organizations, the increasing importance of quality management systems (like ISO 9001) and the recent introduction of ISO 30401 for knowledge management, we examine how these different elements converge within the framework of an Integrated Management System. The article specifically demonstrates how an ISO30401-compliant knowledge management system can be implemented by deploying the mechanisms of the SECI model through the steps of the PDCA cycle as applied in the processes of the integrated management system."
2507.1884,"Amid the migration of academics from X, the social media platform Bluesky has emerged as a potential alternative. To assess its viability and relevance for science communication, this study presents the first large-scale analysis of scholarly article dissemination on Bluesky, exploring its potential as a new source of social media metrics. We collected and analysed over 2.6 million Bluesky posts referencing 532,302 scholarly articles from January 2023 to July 2025, integrating metadata from the OpenAlex database. Temporal trends, disciplinary coverage, language use, textual characteristics, and user engagement were examined. A sharp increase in scholarly activity on Bluesky was observed from November 2024 to January 2025, coinciding with broader academic shifts away from X. As on X, Bluesky posts primarily concern the health, social, and environmental sciences and are predominantly written in English. Nevertheless, Bluesky posts demonstrate substantially higher levels of interaction (likes, reposts, replies, and quotes) and greater textual originality than previously reported for X, suggesting both stronger interactive and more interpretive engagement. These findings highlight Bluesky's emerging role as a credible platform for science communication and a promising source for altmetrics. The platform may facilitate not only early visibility of research outputs but also more meaningful scholarly dialogue in the evolving social media landscape."
2507.19092,"The digitization of historical folkloristic materials presents unique challenges due to diverse text layouts, varying print and handwriting styles, and linguistic variations. This study explores different optical character recognition (OCR) approaches for Slovene folkloristic and historical text digitization, integrating both traditional methods and large language models (LLMs) to improve text transcription accuracy while maintaining linguistic and structural integrity. We compare single-stage OCR techniques with multi-stage pipelines that incorporate machine learning-driven post-processing for text normalization and layout reconstruction. While LLM-enhanced methods show promise in refining recognition outputs and improving readability, they also introduce challenges related to unintended modifications, particularly in the preservation of dialectal expressions and historical structures. Our findings provide insights into selecting optimal digitization strategies for large-scale folklore archives and outline recommendations for developing robust OCR pipelines that balance automation with the need for textual authenticity in digital humanities research."
2507.19302,"Citation indexes play a crucial role for understanding how science is produced, disseminated, and used. However, these databases often face a critical trade-off: those offering extensive and high-quality coverage are typically proprietary, whereas publicly accessible datasets frequently exhibit fragmented coverage and inconsistent data quality. OpenAlex was developed to address this challenge, providing a freely available database with broad open coverage, with a particular emphasis on non-English speaking countries. Yet, few studies have assessed the quality of the OpenAlex dataset. This paper assesses the coverage, by OpenAlex, of China's papers, which shows an abnormal trend, and compares it with other countries that do not have English as their main language. Our analysis reveals that while OpenAlex increases the coverage of China's publications, primarily those disseminated by a national database, this coverage is incomplete and discontinuous when compared to other countries' records in the database. We observe similar issues in other non-English-speaking countries, with coverage varying across regions. These findings indicate that although OpenAlex expands coverage of research outputs, continuity issues persist and disproportionately affect certain countries. We emphasize the need for researchers to use OpenAlex data cautiously, being mindful of its potential limitations in cross-national analyses."
2507.20131,"Permanent Data Encoding (PDE) is a visual language framework designed for long-term, human-readable, and electrically independent knowledge preservation. By encoding semantic content into compact 2-3 character alphanumeric codes, paired with public dictionaries and rule-based expansion structures, PDE enables information to be visually interpreted and logically reconstructed without reliance on digital systems. Unlike QR codes or binary data, PDE offers a transparent and self-contained method of encoding meaning. This paper outlines the PDE syntax, dictionary protocol, use cases in disaster resilience and AI integration, and its implications as a cross-generational semantic infrastructure."
2507.22019,"URI redirections are integral to web management, supporting structural changes, SEO optimization, and security. However, their complexities affect usability, SEO performance, and digital preservation. This study analyzed 11 million unique redirecting URIs, following redirections up to 10 hops per URI, to uncover patterns and implications of redirection practices. Our findings revealed that 50% of the URIs terminated successfully, while 50% resulted in errors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to HTTPS transitions, were prevalent, reflecting adherence to SEO best practices. Non-canonical redirects, often involving domain or path changes, highlighted significant web migrations, rebranding, and security risks. Notable patterns included ""sink"" URIs, where multiple redirects converged, ranging from traffic consolidation by global websites to deliberate ""Rickrolling."" The study also identified 62,000 custom 404 URIs, almost half being soft 404s, which could compromise SEO and user experience. These findings underscore the critical role of URI redirects in shaping the web while exposing challenges such as outdated URIs, server instability, and improper error handling. This research offers a detailed analysis of URI redirection practices, providing insights into their prevalence, types, and outcomes. By examining a large dataset, we highlight inefficiencies in redirection chains and examine patterns such as the use of ""sink"" URIs and custom error pages. This information can help webmasters, researchers, and digital archivists improve web usability, optimize resource allocation, and safeguard valuable online content."
2507.22391,"Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR). Although it is hard for most investigators to remember all the guiding principles associated with FAIR data, there is one overarching requirement: The data need to be annotated with rich, discipline-specific, standardized metadata. The Center for Expanded Data Annotation and Retrieval (CEDAR) builds technology that enables scientists to encode metadata standards as templates that enumerate the attributes of different kinds of experiments. These metadata templates capture preferences regarding how data should be described and what a third party needs to know to make sense of the datasets. CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards. Like the declarative knowledge bases that underpinned intelligent systems decades ago, CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings. They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards, and for deploying those standards in a range of intelligent systems to promote open science."
2507.22479,"This paper introduces a document type classifier with the purpose to optimise the distinction between research and non-research journal publications in OpenAlex. Based on open metadata, the classifier can detect non-research or editorial content within a set of classified articles and reviews (e.g. paratexts, abstracts, editorials, letters). The classifier achieves an F1-score of 0,95, indicating a potential improvement in the data quality of bibliometric research in OpenAlex when applying the classifier on real data. In total, 4.589.967 out of 42.701.863 articles and reviews could be reclassified as non-research contributions by the classifier, representing a share of 10,75%"
2508.00826,"Making scientific papers accessible may require reprocessing old papers to create output compliant with accessibility standards. An important step there is to convert the visual formatting to the logical one. In this report we describe our attempt at zero shot conversion of arXiv papers. Our results are mixed: while it is possible to do conversion, the reliability is not too good. We discuss alternative approaches to this problem."
2508.00827,"Building upon a formal, event-centric model for the diachronic evolution of legal norms grounded in the IFLA Library Reference Model (LRMoo), this paper addresses the essential first step of publishing this model's foundational entity-the abstract legal Work (F1)-on the Semantic Web. We propose a detailed, property-by-property mapping of the LRMoo F1 Work to the widely adoptedthis http URLvocabulary. Using Brazilian federal legislation from thethis http URLportal as a practical case study, we demonstrate how to create interoperable, machine-readable descriptions via JSON-LD, focusing on stable URN identifiers, core metadata, and norm relationships. This structured mapping establishes a stable, URI-addressable anchor for each legal norm, creating a verifiable ""ground truth"". It provides the essential, interoperable foundation upon which subsequent layers of the model, such as temporal versions (Expressions) and internal components, can be built. By bridging formal ontology with web-native standards, this work paves the way for building deterministic and reliable Legal Knowledge Graphs (LKGs), overcoming the limitations of purely probabilistic models."
2508.00829,"Scientific knowledge is a key driver of technological innovation, shaping industrial development and policy decisions worldwide. Understanding how patents incorporate scientific research is essential for assessing the role of academic discoveries in technological progress. Non-Patent References (NPRs) provide a useful indicator of this relationship by revealing the extent to which patents draw upon scientific literature. Here, we show that reliance on scientific research in patents varies significantly across regions. Oceania and Europe display stronger engagement with scientific knowledge, while the Americas exhibit lower reliance. Moreover, NPRs are more likely to be open access than the average scientific publication, a trend that intensifies when Sci-Hub availability is considered. These results highlight the transformative impact of Open Science on global innovation dynamics. By facilitating broader access to research, Open Science strengthens the link between academia and industry, underscoring the need for policies that promote equitable and science-based innovation, particularly in developing regions."
2508.00836,"The rapid growth of preprint servers has accelerated scientific dissemination but has also shifted the technical burden of manuscript preparation to authors. This challenge is particularly acute in computational research, where manuscripts must remain synchronised with evolving data and code. We present Rxiv-Maker, a framework that resolves this by converting simple Markdown files into professionally typeset, publication-ready PDFs. Its core feature is the ability to execute embedded code, creating a self-updating manuscript where figures and statistical values are generated directly from source data during compilation. This ensures that the final document is always current and fully reproducible. By integrating with standard tools like Git and Visual Studio (VS) Code, Rxiv-Maker provides an efficient, transparent, and collaborative authoring experience, applying principles of software engineering to academic writing to foster open and verifiable science."
2508.00838,"Web-enabled LLMs frequently answer queries without crediting the web pages they consume, creating an ""attribution gap"" - the difference between relevant URLs read and those actually cited. Drawing on approximately 14,000 real-world LMArena conversation logs with search-enabled LLM systems, we document three exploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI GPT-4o responses are generated without explicitly fetching any online content; 2) No citation: Gemini provides no clickable citation source in 92% of answers; 3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant pages per query but cites only three to four. A negative binomial hurdle model shows that the average query answered by Gemini or Sonar leaves about 3 relevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained by its selective log disclosures rather than by better attribution. Citation efficiency - extra citations provided per additional relevant web page visited - varies widely across models, from 0.19 to 0.45 on identical queries, underscoring that retrieval design, not technical limits, shapes ecosystem impact. We recommend a transparent LLM search architecture based on standardized telemetry and full disclosure of search traces and citation logs."
2508.00842,"The decline of single authorship in peer-reviewed journals within the current collaboration-oriented knowledge production framework has prompted deeper reflection on the shifting power structures in academic systems. This paper aims to explore the underlying institutional logic and evaluation mechanisms contributing to the marginalization of single-author research in the management field. It further investigates how the discourse of collaborative advantage conceals structural power redistribution and ideological disembedding. Through an analysis of authorship data from top-tier journals, a critical reading of institutional incentive texts, and an empirical review of authorial configurations, the study building on the work of Harzing, Wuchty, and Lariviere constructs a three-dimensional causal chain: collaboration incentives, responsibility dilution, and originality weakening. Findings suggest that single authorship is not explicitly excluded but is gradually sidelined from central publication channels by funding policies, review practices, and performance metrics. Independent thought is thus structurally marginalized within institutionalized collaboration. The paper advocates for a paradigm shift from instrumental rationality to value-based rationality and calls for the restoration of legitimacy and public value for independent research through reforms in evaluation frameworks, journal governance, and research ethics, aiming to safeguard academic diversity and intellectual autonomy."
2508.00859,"High-quality, ""rich"" metadata are essential for making research data findable, interoperable, and reusable. The Center for Expanded Data Annotation and Retrieval (CEDAR) has long addressed this need by providing tools to design machine-actionable metadata templates that encode community standards in a computable form. To make these capabilities more accessible within real-world research workflows, we have developed the CEDAR Embeddable Editor (CEE)-a lightweight, interoperable Web Component that brings structured, standards-based metadata authoring directly into third-party platforms. The CEE dynamically renders metadata forms from machine-actionable templates and produces semantically rich metadata in JSON-LD format. It supports ontology-based value selection via the BioPortal ontology repository, and it includes external authority resolution for persistent identifiers such as ORCIDs for individuals and RORs for research organizations. Crucially, the CEE requires no custom user-interface development, allowing deployment across diverse platforms. The CEE has been successfully integrated into generalist scientific data repositories such as Dryad and the Open Science Framework, demonstrating its ability to support discipline-specific metadata creation. By supporting the embedding of metadata authoring within existing research environments, the CEE can facilitate the adoption of community standards and help improve metadata quality across scientific disciplines."
2508.00862,"Various technologies have been applied to monitor the proximity between two construction entities, preventing struck-by accidents and thereby enhancing onsite safety. This study comprehensively reviews related efforts dedicated to proximity monitoring and warning (PMW) based on 97 relevant articles published between 2010 and 2024. The bibliometric analysis reveals the technical roadmap over time, as well as the five most influential leaders and the two largest research networks they have established. The qualitative review is then conducted from four perspectives: influencing factor study, hazard level definition and determination, proximity perception, and alarm issuing and receiving. Finally, the limitations and challenges of current proximity perception are discussed, along with corresponding future research directions, including end-to-end three-dimensional (3D) object detection, real-time 3D reconstruction and updating for dynamic construction scenes, and multimodal fusion. This review presents the current research status, limitations, and future directions of PMW, guiding the future development of PMW systems."
2508.00867,"This article explores the integration of AI-generated subject terms into library cataloging, focusing on validation through the Library of Congress Linked Data Service. It examines the challenges of traditional subject cataloging under the Library of Congress Subject Headings system, including inefficiencies and cataloging backlogs. While generative AI shows promise in expediting cataloging workflows, studies reveal significant limitations in the accuracy of AI-assigned subject headings. The article proposes a hybrid approach combining AI technology with human validation through LOC Linked Data Service, aiming to enhance the precision, efficiency, and overall quality of metadata creation in library cataloging practices."
2508.00871,"In an age of fast-paced technological change, patents have evolved into not only legal mechanisms of intellectual property, but also structured storage containers of knowledge full of metadata, categories, and formal innovation. This chapter proposes to reframe patents in the context of information science, by focusing on patents as knowledge artifacts, and by seeing patents as fundamentally tied to the global movement of scientific and technological knowledge. With a focus on three areas, the inventions of AIs, biotech patents, and international competition with patents, this work considers how new technologies are challenging traditional notions of inventorship, access, and moralthis http URLchapter provides a critical analysis of AI's implications for patent authorship and prior art searches, ownership issues arising from proprietary claims in biotechnology to ethical dilemmas, and the problem of using patents for strategic advantage in a global context of innovation competition. In this analysis, the chapter identified the importance of organizing information, creating metadata standards about originality, implementing retrieval systems to access previous works, and ethical contemplation about patenting unseen relationships in innovation ecosystems. Ultimately, the chapter called for a collaborative, transparent, and ethically-based approach in managing knowledge in the patenting environment highlighting the role for information professionals and policy to contribute to access equity in innovation."
2508.01882,"Research evaluation is important for appointments, promotions, departmental assessments, and national science strategy monitoring. Whilst Global North universities often have sufficient senior researchers for effective peer review and enough trust in citation data to use it for supporting indicators, the same is less likely to be true in the Global South. Moreover, Global South research priorities may not align well with citation-based indicators. This article introduces a ChatGPT-based strategy designed to address both limitations, applying it to Mauritius. The strategy involves giving ChatGPT instructions about how to evaluate the quality of research from the perspective of a given Global South nation and then using it to score articles based on these criteria. Results from Mauritius show that ChatGPT's scores for 1,566 journal articles published between 2015 and 2021 have an almost zero correlation with both ChatGPT research quality scores and citation rates. A word association thematic analysis of articles with relatively high scores for value to Mauritius identified a range of plausible themes, including education, policy relevance, and industrial production. Higher scoring articles also tended to mention the country or an important commercial sector in the abstract. Whilst the evidence suggests that assessing the direct value to a country of journal articles using ChatGPT gives plausible results, this approach should be used cautiously because it has unknown accuracy and ignores the wider value of research contributions."
2508.02084,"Advanced bioimaging technologies have enabled the large-scale acquisition of multidimensional data, yet effective metadata management and interoperability remain significant challenges. To address these issues, we propose a new ontology-driven framework for the Systems Science of Biological Dynamics Database (SSBD) that adopts a two-tier architecture. The core layer provides a class-centric structure referencing existing biomedical ontologies, supporting both SSBD:repository -- which focuses on rapid dataset publication with minimal metadata -- and SSBD:database, which is enhanced with biological and imaging-related annotations. Meanwhile, the instance layer represents actual imaging dataset information as Resource Description Framework individuals that are explicitly linked to the core classes. This layered approach aligns flexible instance data with robust ontological classes, enabling seamless integration and advanced semantic queries. By coupling flexibility with rigor, the SSBD Ontology promotes interoperability, data reuse, and the discovery of novel biological mechanisms. Moreover, our solution aligns with the Recommended Metadata for Biological Images guidelines and fosters compatibility. Ultimately, our approach contributes to establishing a Findable, Accessible, Interoperable, and Reusable data ecosystem within the bioimaging community."
2508.02379,"This paper presents the results of the USRN Discovery Pilot Project, a collaboration of SPARC, the Confederation of Open Access Repositories (COAR), CORE and Antleaf, to enhance the discoverability of research papers in US repositories leveraging CORE as an indexing service for USRN repositories. The project conducted actions in three strategic areas: Assessing and quantitatively measuring discoverability and barriers to it at the beginning and end of the pilot project, conducting interventions to increase discoverability, and supporting interventions by technology and guidelines (provided by CORE services), to minimise effort and maximise effect. The key results of the project include: Around three-quarters of a million research outputs held in the selected US repositories have been made discoverable (a 50% increase) compared to the year before; The project has made available the CORE Data Provider's Guide as well as a selection of new and improved tools to support repositories in increasing their discoverability. These include the CORE Reindexing Button and Index Notification modules, Fresh Finds and the USRN Desirable Characteristics for Digital Publication Repositories checking tool. The project team is now exploring ways to scale out this work to include more repositories."
2508.0274,"Large language models (LLMs) are rapidly being adopted as research assistants, particularly for literature review and reference recommendation, yet little is known about whether they introduce demographic bias into citation workflows. This study systematically investigates gender bias in LLM-driven reference selection using controlled experiments with pseudonymous author names. We evaluate several LLMs (GPT-4o, GPT-4o-mini, Claude Sonnet, and Claude Haiku) by varying gender composition within candidate reference pools and analyzing selection patterns across fields. Our results reveal two forms of bias: a persistent preference for male-authored references and a majority-group bias that favors whichever gender is more prevalent in the candidate pool. These biases are amplified in larger candidate pools and only modestly attenuated by prompt-based mitigation strategies. Field-level analysis indicates that bias magnitude varies across scientific domains, with social sciences showing the least bias. Our findings indicate that LLMs can reinforce or exacerbate existing gender imbalances in scholarly recognition. Effective mitigation strategies are needed to avoid perpetuating existing gender disparities in scientific citation practices before integrating LLMs into high-stakes academic workflows."
2508.03828,"We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles with their citations and scraped web sources; articles are represented in a rich data structure, and scraped source texts are stored inline with precise character offsets of their citations in the article text. MegaWika 2 is a major upgrade from the original MegaWika, spanning six times as many articles and twice as many fully scraped citations. Both MegaWika and MegaWika 2 support report generation research ; whereas MegaWika also focused on supporting question answering and retrieval applications, MegaWika 2 is designed to support fact checking and analyses across time and language."
2508.03962,"The growing volume of scientific literature makes it challenging for scientists to move from a list of papers to a synthesized understanding of a topic. Because of the constant influx of new papers on a daily basis, even if a scientist identifies a promising set of papers, they still face the tedious task of individually reading through dozens of titles and abstracts to make sense of occasionally conflicting findings. To address this critical bottleneck in the research workflow, we introduce a summarization feature to BIP! Finder, a scholarly search engine that ranks literature based on distinct impact aspects like popularity and influence. Our approach enables users to generate two types of summaries from top-ranked search results: a concise summary for an instantaneous at-a-glance comprehension and a more comprehensive literature review-style summary for greater, better-organized comprehension. This ability dynamically leverages BIP! Finder's already existing impact-based ranking and filtering features to generate context-sensitive, synthesized narratives that can significantly accelerate literature discovery and comprehension."
2508.04024,"We discuss newly uncovered cases of identity theft in the scientific peer-review process within artificial intelligence (AI) research, with broader implications for other academic procedures. We detail how dishonest researchers exploit the peer-review system by creating fraudulent reviewer profiles to manipulate paper evaluations, leveraging weaknesses in reviewer recruitment workflows and identity verification processes. The findings highlight the critical need for stronger safeguards against identity theft in peer review and academia at large, and to this end, we also propose mitigating strategies."
2508.04213,"Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM) play a central role in providing the primary framework through which intelligent systems can explore and interpret the literature. However, these resources have traditionally been manually curated, a process that is time-consuming, prone to obsolescence, and limited in granularity. This paper presents Sci-OG, a semi-auto\-mated methodology for generating research topic ontologies, employing a multi-step approach: 1) Topic Discovery, extracting potential topics from research papers; 2) Relationship Classification, determining semantic relationships between topic pairs; and 3) Ontology Construction, refining and organizing topics into a structured ontology. The relationship classification component, which constitutes the core of the system, integrates an encoder-based language model with features describing topic occurrence in the scientific literature. We evaluate this approach against a range of alternative solutions using a dataset of 21,649 manually annotated semantic triples. Our method achieves the highest F1 score (0.951), surpassing various competing approaches, including a fine-tuned SciBERT model and several LLM baselines, such as the fine-tuned GPT4-mini. Our work is corroborated by a use case which illustrates the practical application of our system to extend the CSO ontology in the area of cybersecurity. The presented solution is designed to improve the accessibility, organization, and analysis of scientific knowledge, thereby supporting advancements in AI-enabled literature management and research exploration."
2508.06004,"Author-level citation metrics provide a practical, interpretable, and scalable signal of scholarly influence in a complex research ecosystem. It has been widely used as a proxy in hiring decisions. However, the past five years have seen the rapid emergence of large-scale publications in the field of large language models and foundation models, with papers featuring hundreds to thousands of co-authors and receiving tens of thousands of citations within months. For example, Gemini has 1361 authors and has been cited around 4600 times in 19 months. In such cases, traditional metrics, such as total citation count and the $h$-index, fail to meaningfully distinguish individual contributions. Therefore, we propose the following research question: How can one identify standout researchers among thousands of co-authors in large-scale LLM papers? This question is particularly important in scenarios such as academic hiring and funding decisions. In this paper, we introduce a novel citation metric designed to address this challenge by balancing contributions across large-scale and small-scale publications. We propose the SBCI index, analyze its theoretical properties, and evaluate its behavior on synthetic publication datasets. Our results demonstrate that the proposed metric provides a more robust and discriminative assessment of individual scholarly impact in the era of large-scale collaborations."
2508.06401,"This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025. A total of 128 articles met our inclusion criteria. The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP). RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights. Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG. To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured. This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research."
2508.07196,"Although both Google Gemini (1.5 Flash) and ChatGPT (4o and 4o-mini) give research quality evaluation scores that correlate positively with expert scores in nearly all fields, and more strongly that citations in most, it is not known whether this is true for smaller Large Language Models (LLMs). In response, this article assesses Google's Gemma-3-27b-it, a downloadable LLM (60Gb). The results for 104,187 articles show that Gemma-3-27b-it scores correlate positively with an expert research quality score proxy for all 34 Units of Assessment (broad fields) from the UK Research Excellence Framework 2021. The Gemma-3-27b-it correlations have 83.8% of the strength of ChatGPT 4o and 94.7% of the strength of ChatGPT 4o-mini correlations. Differently from the two larger LLMs, the Gemma-3-27b-it correlations do not increase substantially when the scores are averaged across five repetitions, its scores tend to be lower, and its reports are relatively uniform in style. Overall, the results show that research quality score estimation can be conducted by offline LLMs, so this capability is not an emergent property of the largest LLMs. Moreover, score improvement through repetition is not a universal feature of LLMs. In conclusion, although the largest LLMs still have the highest research evaluation score estimation capability, smaller ones can also be used for this task, and this can be helpful for cost saving or when secure offline processing is needed."
2508.08347,"Digital Humanities (DH) is an interdisciplinary field that integrates computational methods with humanities scholarship to investigate innovative topics. Each academic discipline follows a unique developmental path shaped by the topics researchers investigate and the methods they employ. With the help of bibliometric analysis, most of previous studies have examined DH across multiple dimensions such as research hotspots, co-author networks, and institutional rankings. However, these studies have often been limited in their ability to provide deep insights into the current state of technological advancements and topic development in DH. As a result, their conclusions tend to remain superficial or lack interpretability in understanding how methods and topics interrelate in the field. To address this gap, this study introduced a new concept of Topic-Method Composition (TMC), which refers to a hybrid knowledge structure generated by the co-occurrence of specific research topics and the corresponding method. Especially by analyzing the interaction between TMCs, we can see more clearly the intersection and integration of digital technology and humanistic subjects in DH. Moreover, this study developed a TMC-based workflow combining bibliometric analysis, topic modeling, and network analysis to analyze the development characteristics and patterns of research disciplines. By applying this workflow to large-scale bibliometric data, it enables a detailed view of the knowledge structures, providing a tool adaptable to other fields."
2508.0885,"This study examines how Russia's full-scale war against Ukraine affected APCs, publishing patterns, and citation impact of Gold OA articles authored by Ukrainian scholars between 2020 and 2023. Data from Scopus covers articles published before (2020-2021) and after (2022-2023) the war's onset. Statistical analysis revealed a small but significant correlation between APC amounts and citation impact, though the effect size was minimal, suggesting higher APCs did not substantially boost citations. APC waivers offered by major publishers such as Springer and Elsevier since 2022 have led to only a slight increase in articles authored solely by Ukrainian scholars in their journals. Despite these waivers, MDPI and Aluna Publishing House maintained the largest shares of such publications, likely due to low rejection rates, fast publication, and - in Aluna's case - reduced APCs for Ukrainian authors. Between 2020 and 2023, the number of articles authored solely by Ukrainian scholars in foreign journals fell by 25.7%, and total APC spending declined by 24.6%, from EUR 1.24 million to EUR 0.93 million. Medicine accounted for the largest share of both articles and APC expenditure, with the majority published in Aluna journals. Ensuring genuine equity in scholarly communication requires alternative publishing models beyond APC-based Gold OA, guaranteeing equal opportunities to publish regardless of institutional or national affiliation. Reform must also address evaluation systems that prioritise output metrics over research quality and academic cultures that favour speed and APC payments over rigour, even when high-quality, no-cost publishing options are available."
2508.12735,"Citation analysis is widely used in research evaluation to assess the impact of scientific papers. These analyses rest on the assumption that citation decisions by authors are accurate, representing flow of knowledge from cited to citing papers. However, in practice, researchers often cite for reasons other than attributing intellectual credit to previous research. Citations made for rhetorical reasons or without reading the cited work compromise the value of citations as instrument for research evaluation. Past research on threats to the accuracy of citations has mainly focused on citation bias as the primary concern. In this paper, we argue that citation noise - the undesirable variance in citation decisions - represents an equally critical but underexplored challenge in citation analysis. We define and differentiate two types of citation noise: citation level noise and citation pattern noise. Each type of noise is described in terms of how it arises and the specific ways it can undermine the validity of citation-based research assessments. By conceptually differing citation noise from citation accuracy and citation bias, we propose a framework for the foundation of citation analysis. We discuss strategies and interventions to minimize citation noise, aiming to improve the reliability and validity of citation analysis in research evaluation. We recommend that the current professional reform movement in research evaluation such as the Coalition for Advancing Research Assessment (CoARA) pick up these strategies and interventions as an additional building block for careful, responsible use of bibliometric indicators in research evaluation."
2508.13182,"Classification of scientific abstracts is useful for strategic activities but challenging to automate because the sparse text provides few contextual clues. Metadata associated with the scientific publication can be used to improve performance but still often requires a semi-supervised setting. Moreover, such schemes may generate labels that lack distinction -- namely, they overlap and thus do not uniquely define the abstract. In contrast, experts label and sort these texts with ease. Here we describe an application of a process we call artificial intuition to replicate the expert's approach, using a Large Language Model (LLM) to generate metadata. We use publicly available abstracts from the United States National Science Foundation to create a set of labels, and then we test this on a set of abstracts from the Chinese National Natural Science Foundation to examine funding trends. We demonstrate the feasibility of this method for research portfolio management, technology scouting, and other strategic activities."
2508.13234,"The acceleration of artificial intelligence (AI) in science is recognized and many scholars have begun to explore its role in interdisciplinary collaboration. However, the mechanisms and extent of this impact are still unclear. This study, using AlphaFold's impact on structural biologists, examines how AI technologies influence interdisciplinary collaborative patterns. By analyzing 1,247 AlphaFold-related papers and 7,700 authors from Scopus, we employ bibliometric analysis and causal inference to compare interdisciplinary collaboration between AlphaFold adopters and non-adopters. Contrary to the widespread belief that AI facilitates interdisciplinary collaboration, our findings show that AlphaFold increased structural biology-computer science collaborations by just 0.48%, with no measurable effect on other disciplines. Specifically, AI creates interdisciplinary collaboration demands with specific disciplines due to its technical characteristics, but this demand is weakened by technological democratization and other factors. These findings demonstrate that artificial intelligence (AI) alone has limited efficacy in bridging disciplinary divides or fostering meaningful interdisciplinary collaboration."
2508.14139,"Information overload and the rapid pace of scientific advancement make it increasingly difficult to evaluate and allocate resources to new research proposals. Is there a structure to scientific discovery that could inform such decisions? We present statistical evidence for such structure, by training a classifier that successfully predicts high-citation research papers between 2010-2024 in the Computer Science, Physics, and PubMed domains."
2508.14595,"The issue of whether nonlinear normalized citation counts can be added is critically important in scientometrics because it touches upon the theoretical foundation of underlying computation in the field. In this paper, we provide rigorous mathematical proofs for the key theorems underlying this fundamental issue. Based on these proofs, we ultimately arrive at the following conclusion: a nonlinear normalization method for citation counts must be a non-equidistant transformation; consequently, the resulting nonlinear normalized citation counts are no longer equidistant and therefore cannot be added. Furthermore, because our mathematical proofs are established over the real number domain, we also derive a more general conclusion that is applicable to data transformations over the real number domain across various scientific fields: a nonlinear transformation becomes a non-equidistant transformation only if it satisfies a certain regularity condition, for example, if this nonlinear transformation is a continuous function, monotonic over a certain interval, or its domain is restricted to rational numbers. In such cases, the resulting nonlinear data are no longer equidistant and therefore cannot be added. This general conclusion can be broadly applied to various linear and nonlinear transformation problems, which offers significant insights for addressing the misuse of nonlinear data."
2508.15556,"HERITRACE is a semantic data editor designed for cultural heritage institutions, addressing the gap between complex Semantic Web technologies and domain expert needs. ParaText Bibliographical Database, a specialized bibliographical database for ancient Greek exegesis, demonstrates HERITRACE's capabilities in Classical Philology. This paper examines how HERITRACE enables non-technical scholars to manage complex semantic data through SHACL-based form generation and validation, while ensuring comprehensive provenance tracking and change management via an OpenCitations Data Model adaptation."
2508.15645,"VIVer is a digital lexicography project with historical-literary and historical-linguistic aims that can be considered a case study of a Digital Humanities project. This paper presents the IT choices made to promote the dissemination and enhancement of the results, analysing the issues and advantages for wider adoption, beyond the specific VIVer project, serving as a model and inspiration for future projects."
2508.15916,"Information Ecosystem Reengineering (IER) -- the technological reconditioning of information sources, services, and systems within a complex information ecosystem -- is a foundational challenge in the digital transformation of public sector services and smart governance platforms. From a semantic knowledge management perspective, IER becomes especially entangled due to the potentially infinite number of possibilities in its conceptualization, namely, as a result of manifoldness in the multi-level mix of perception, language and conceptual interlinkage implicit in all agents involved in such an effort. This paper proposes a novel approach -- Representation Disentanglement -- to disentangle these multiple layers of knowledge representation complexity hindering effective reengineering decision making. The approach is based on the theoretically grounded and implementationally robust ontology-driven conceptual modeling paradigm which has been widely adopted in systems analysis and (re)engineering. We argue that such a framework is essential to achieve explainability, traceability and semantic transparency in public sector knowledge representation and to support auditable decision workflows in governance ecosystems increasingly driven by Artificial Intelligence (AI) and data-centric architectures."
2508.16276,"The recent surge in bibliometric studies published has been accompanied by increasing diversity in the completeness of reporting these studies' details, affecting reliability, reproducibility, and robustness. Our study systematises the reporting of bibliometric research using open peer reviews. We examined 182 peer reviews of 85 bibliometric studies published in library and information science (LIS) journals and conference proceedings, and non-LIS journals. We extracted 968 reviewer comments and inductively classified them into 11 broad thematic categories and 68 sub-categories, determining that reviewers largely focus on the completeness and clarity of reporting data, methods, and results. We subsequently derived 49 recommendations for the details authors should report and compared them with the GLOBAL, PRIBA, and BIBLIO reporting guidelines to identify (dis)similarities in content. Our recommendations addressed 60-80% of the guidelines' items, while the guidelines covered 45-65% of our recommendations. Our recommendations provided greater range and specificity, but did not incorporate the functions of guidelines beyond addressing academic content. We argue that peer reviews provide valuable information for the development of future guidelines. Further, our recommendations can be read as the implicit community standards for reporting bibliometric studies and could be used by authors to aid complete and accurate reporting of their manuscripts."
2508.16519,"The h index is a widely recognized metric for assessing the research impact of scholars, defined as the maximum value h such that the scholar has published h papers each cited at least h times. While it has proven useful measuring individual scholarly productivity and citation impact, the h index has limitations, such as an inability to account for interdisciplinary collaboration or demographic differences in citation patterns. Moreover, it is sometimes mistakenly treated as a measure of research quality, even though it only reflects how often work has been cited. While metric based evaluations of research have grown in importance in some areas of academia, such as medicine, these evaluations fail to consider other important aspects of intellectual work, such as representational and epistemic diversity in research. In this article, we propose a new metric called the c index, or the community index, which combines multiple dimensions of scholarly impact. This is important because a plurality of perspectives and lived experiences within author teams can promote epistemological reflection and humility as part of the creation and validation of scientific knowledge. The c index is a means of accounting for the often global, and increasingly interdisciplinary nature of contemporary research, in particular, the data that is collected, curated and analyzed in the process of scientific inquiry. While the c index provides a means of quantifying diversity within research teams, diversity is integral to the advancement of scientific excellence and should be actively fostered through formal recognition and valuation. We herein describe the mathematical foundation of the c index and demonstrate its potential to provide a more comprehensive representation and more multidimensional assessment of scientific contributions of research impact as compared to the h index."
2508.1809,"Large language models have demonstrated remarkable versatility across a wide range of natural language processing tasks and domains. One such task is Named Entity Recognition (NER), which involves identifying and classifying proper names in text, such as people, organizations, locations, dates, and other specific entities. NER plays a crucial role in extracting information from unstructured textual data, enabling downstream applications such as information retrieval from unstructured text.Traditionally, NER is addressed using supervised machine learning approaches, which require large amounts of annotated training data. However, historical texts present a unique challenge, as the annotated datasets are often scarce or nonexistent, due to the high cost and expertise required for manual labeling. In addition, the variability and noise inherent in historical language, such as inconsistent spelling and archaic vocabulary, further complicate the development of reliable NER systems for these sources.In this study, we explore the feasibility of applying LLMs to NER in historical documents using zero-shot and few-shot prompting strategies, which require little to no task-specific training data. Our experiments, conducted on the HIPE-2022 (Identifying Historical People, Places and other Entities) dataset, show that LLMs can achieve reasonably strong performance on NER tasks in this setting. While their performance falls short of fully supervised models trained on domain-specific annotations, the results are nevertheless promising. These findings suggest that LLMs offer a viable and efficient alternative for information extraction in low-resource or historically significant corpora, where traditional supervised methods are infeasible."
2508.18146,"Scopus is increasingly regarded as a high-quality and reliable data source for research and evaluation of scientific and scholarly activity. However, a puzzling phenomenon has been discovered occasionally: millions of records with author affiliation information collected in Scopus are oddly labeled as ""country-undefined"" by Scopus which is rarely to be detected in its counterpart Web of Science. This huge number of ""homeless"" records in Scopus is unacceptable for a widely used high-quality bibliographic database. By using data from the past 124 years, this brief communication tries to probe these affiliated but country-undefined records in Scopus. Our analysis identifies four primary causes for these ""homeless"" records: incomplete author affiliation addresses, Scopus' inability to recognize different variants of country/territory names, misspelled country/territory names in author affiliation addresses, and Scopus' insufficiency in correctly split and identify the clean affiliation addresses. To address this pressing issue, we put forward several recommendations to relevant stakeholders, with the aim of resettling millions of ""homeless"" records in Scopus and reducing its potential impact on Scopus-based literature retrieval, analysis, and evaluation."
2508.1862,"Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes."
2509.01304,"In a context where the social sciences and humanities are experimenting with non-anthropocentric analytical frames, this article proposes a semiotic (structural) reading of the hybridization between symbolic AI and neural (or sub-symbolic) AI based on a field of application: the design and use of a knowledge base for area studies. We describe the LaCAS ecosystem -- Open Archives in Linguistic and Cultural Studies (thesaurus; RDF/OWL ontology; LOD services; harvesting; expertise; publication), deployed at Inalco (National Institute for Oriental Languages and Civilizations) in Paris with the Okapi (Open Knowledge and Annotation Interface) software environment from Ina (National Audiovisual Institute), which now has around 160,000 documentary resources and ten knowledge macro-domains grouping together several thousand knowledge objects. We illustrate this approach using the knowledge domain ''Languages of the world'' (~540 languages) and the knowledge object ''Quechua (language)''. On this basis, we discuss the controlled integration of neural tools, more specifically generative tools, into the life cycle of a knowledge base: assistance with data localization/qualification, index extraction and aggregation, property suggestion and testing, dynamic file generation, and engineering of contextualized prompts (generic, contextual, explanatory, adjustment, procedural) aligned with a domain ontology. We outline an ecosystem of specialized agents capable of animating the database while respecting its symbolic constraints, by articulating model-driven and data-driven methods."
2509.0153,"Despite broad acclaim for basic research, science is undergoing an applied shift that marginalizes basic scientists. This gap reflects an incomplete understanding of their distinctive roles, which prevents translating philosophical appreciation into effective support. We introduce a scalable metric--the application score--to position research along the basic-applied spectrum and apply it to 62 million publications (1970-2023) to reveal the distinctive contributions of basic scientists. We find a structural asymmetry: involvement of basic scientists substantially increases citation impact, even more so in applied contexts, while applied scientists show no such effect in basic domains. This asymmetric effect arises from their intellectual leadership in conceptualization, writing, and experimental design, amplified in large, multidisciplinary, and intermediate career teams. Yet basic scientists remain concentrated in historically prestigious institutions, while new entrants shift toward applied work, indicating critical undersupply. These findings provide large-scale evidence for the indispensable role of basic scientists, guiding policy and institutional strategy to sustain the foundations of discovery and innovation."
2509.02356,"This Data Descriptor introduces the dataset Enevaeldens Nyheder Online (News during Absolutism Online). The Enevaeldens Nyheder Online (ENO) dataset provides a reconstruction of the contents of major newspapers in Denmark and Norway during the period of Absolutism (1660-1849). The dataset contains approx. 474 million words, created using neural networks designed to process digitised microfilm versions of Danish newspapers as well as a smaller selection of Norwegian publications that were all hitherto illegible for computers. The contributions details this process and its results, including a way to derive standalone texts from the editions, and the accompanying BERT-model trained on a beta-version of the dataset."
2509.02581,"Despite the growing availability of tools designed to support scholarly knowledge extraction and organization, many researchers still rely on manual methods, sometimes due to unfamiliarity with existing technologies or limited access to domain-adapted solutions. Meanwhile, the rapid increase in scholarly publications across disciplines has made it increasingly difficult to stay current, further underscoring the need for scalable, AI-enabled approaches to structuring and synthesizing scholarly knowledge. Various research communities have begun addressing this challenge independently, developing tools and frameworks aimed at building reliable, dynamic, and queryable scholarly knowledge bases. However, limited interaction across these communities has hindered the exchange of methods, models, and best practices, slowing progress toward more integrated solutions. This manuscript identifies ways to foster cross-disciplinary dialogue, identify shared challenges, categorize new collaboration and shape future research directions in scholarly knowledge and organization."
2509.03391,"Survey research has a long-standing history of being a human-powered field, but one that embraces various technologies for the collection, processing, and analysis of various behavioral, political, and social outcomes of interest, among others. At the same time, Large Language Models (LLMs) bring new technological challenges and prerequisites in order to fully harness their potential. In this paper, we report work-in-progress on a systematic literature review based on keyword searches from multiple large-scale databases as well as citation networks that assesses how LLMs are currently being applied within the survey research process. We synthesize and organize our findings according to the survey research process to include examples of LLM usage across three broad phases: pre-data collection, data collection, and post-data collection. We discuss selected examples of potential use cases for LLMs as well as its pitfalls based on examples from existing literature. Considering survey research has rich experience and history regarding data quality, we discuss some opportunities and describe future outlooks for survey research to contribute to the continued development and refinement of LLMs."
2509.04124,"Bibliometric measures, such as total citations and h-index, have become a cornerstone for evaluating academic performance; however, these traditional metrics, being non-weighted, inadequately capture the nuances of individual contributions. To address this constraint, we developed GScholarLens, an open-access browser extension that integrates seamlessly with Google Scholar to enable detailed bibliometric analysis. GScholarLens categorizes publications by authorship roles, adjusts citation weightings accordingly, and introduces Scholar h-index, Sh-index, an authorship-contribution normalized h-index. This tool proportionally weights citations based on authorship position using heuristic percentages, i.e., corresponding 100 percent, first 90 percent, second 50 percent, co-authors in publications with less than six authors 25 percent, and co-authors with more than six authors 10 percent. Currently, there is no empirical data available for author-contribution weights, however, this proof-of-concept framework can easily adapt more precise author-contribution weightage data decided by authors at the time of manuscript submission along with CRediT, which journals and publishers can mandate. Furthermore, this tool incorporates retraction detection by mapping data from retraction databases into the Google Scholar interface. By aligning bibliometric evaluation more closely with actual scholarly contribution, GScholarLens presents a better open-access framework for academic recognition, particularly within interdisciplinary and highly collaborative research environments. This tool is freely accessible atthis https URL."
2509.0419,"This paper examines how the role of cited papers evolves over time by analyzing nearly 900 highly cited papers (HCPs) published between 2000 and 2016 and the full text of over 220,000 papers citing them. We investigate multiple citation characteristics, including citation location within the full text, reference and in-text citation types, citation sentiment, and textual and bibliographic relatedness between citing and cited papers. Our findings reveal that as HCPs age, they tend to be cited earlier in papers citing them, mentioned fewer times in the full text, and more often cited alongside other references. Citation sentiment remains predominantly neutral, while both textual and bibliographic similarity between HCPs and their citing papers decline over time. These patterns indicate a shift from direct topical and methodological engagement toward more general, background, and symbolic referencing. The findings highlight the importance to consider citation context rather than relying solely on simple citation counts. Large-scale full-text analyses such as ours can help refine measures of scientific impact and advance scholarly search and science mapping by uncovering more nuanced connections between papers."
2509.04759,"In this work, we study how URL extraction results depend on input format. We compiled a pilot dataset by extracting URLs from 10 arXiv papers and used the same heuristic method to extract URLs from four formats derived from the PDF files or the source LaTeX files. We found that accurate and complete URL extraction from any single format or a combination of multiple formats is challenging, with the best F1-score of 0.71. Using the pilot dataset, we evaluate extraction performance across formats and show that structured formats like HTML and XML produce more accurate results than PDFs or Text. Combining multiple formats improves coverage, especially when targeting research-critical resources. We further apply URL extraction on two tasks, namely classifying URLs into open-access datasets and software and the others, and analyzing the trend of URLs usage in arXiv papers from 1992 to 2024. These results suggest that using a combination of multiple formats achieves better performance on URL extraction than a single format, and the number of URLs in arXiv papers has been steadily increasing since 1992 to 2014 and has been drastically increasing from 2014 to 2024. The dataset and the Jupyter notebooks used for the preliminary analysis are publicly available atthis https URL"
2509.06206,"Gender inequality in scientific careers has been extensively documented through aggregate measures such as total publications and cumulative citations, yet the temporal dynamics underlying these disparities remain largely unexplored. Here we developed a multi-dimensional framework to examine gender differences in scientific knowledge creation through three complementary temporal dimensions: stability (consistency of performance over time), volatility (degree of year-to-year fluctuation), and persistence (ability to maintain high performance for extended periods). Using comprehensive bibliometric data from SciSciNet covering 62.5 million authors whose careers began between 1960-2010, we constructed knowledge creation capability measures that captured how scientists absorb knowledge from diverse sources and contribute to field advancement. We found that female scientists demonstrated significantly higher knowledge production stability (0.170 vs. 0.119 for males) while simultaneously exhibiting greater year-to-year volatility (6.606 vs. 6.228), revealing a striking paradox in career dynamics. Female scientists showed persistence advantages under moderate performance requirements but faced disadvantages under extreme criteria demanding sustained peak performance. However, these patterns varied substantially across disciplines, with female advantages strongest in humanities and social sciences while STEM fields show mixed results."
2509.06212,"The mechanisms driving different types of scientific innovation through collaboration remain poorly understood. Here we develop a comprehensive framework analyzing over 14 million papers across 19 disciplines from 1960 to 2020 to unpack how collaborative synergy shapes research disruption. We introduce the synergy factor to quantify collaboration cost-benefit dynamics, revealing discipline-specific architectures where Physics peaks at medium team sizes while humanities achieve maximal synergy through individual scholarship. Our mediation analysis demonstrates that collaborative synergy, not team size alone, mediates 75% of the relationship between team composition and disruption. Key authors play a catalytic role, with papers featuring exceptional researchers showing 561% higher disruption indices. Surprisingly, high-citation authors reduce disruptive potential while those with breakthrough track records enhance it, challenging traditional evaluation metrics. We identify four distinct knowledge production modes: elite-driven, baseline, heterogeneity-driven, and low-cost. These findings reveal substantial heterogeneity in optimal collaboration strategies across disciplines and provide evidence-based guidance for research organization, with implications for science policy and the design of research institutions in an increasingly collaborative scientific landscape."
2509.06412,"Navigating the vast and rapidly increasing sea of academic publications to identify institutional synergies, benchmark research contributions and pinpoint key research contributions has become an increasingly daunting task, especially with the current exponential increase in new publications. Existing tools provide useful overviews or single-document insights, but none supports structured, qualitative comparisons across institutions or publications.To address this, we demonstrate Compare, a novel framework that tackles this challenge by enabling sophisticated long-context comparisons of scientific contributions. Compare empowers users to explore and analyze research overlaps and differences at both the institutional and publication granularity, all driven by user-defined questions and automatic retrieval over online resources. For this we leverage on Retrieval-Augmented Generation over evolving data sources to foster long context knowledge synthesis. Unlike traditional scientometric tools, Compare goes beyond quantitative indicators by providing qualitative, citation-supported comparisons."
2509.08299,"Scientific progress fundamentally depends on researchers' ability to access and build upon the work of others. Yet, a majority of published work remains behind expensive paywalls, limiting access to universities that can afford subscriptions. Furthermore, even when articles are accessible, the underlying datasets could be restricted, available only through a ""reasonable request"" to the authors. One way researchers could overcome these barriers is by relying on informal channels, such as emailing authors directly, to obtain paywalled articles or restricted datasets. However, whether these informal channels are hindered by racial and/or institutional biases remains unknown. Here, we combine qualitative semi-structured interviews, large-scale observational analysis, and two randomized audit experiments to examine racial and institutional disparities in access to scientific knowledge. Our analysis of 250 million articles reveals that researchers in the Global South cite paywalled papers and upon-request datasets at significantly lower rates than their Global North counterparts, and that these access gaps are associated with reduced knowledge breadth and scholarly impact. To interrogate the mechanisms underlying this phenomenon, we conduct two randomized email audit studies in which fictional PhD students differing in racial background and institutional affiliation request access to paywalled articles (N = 18,000) and datasets (N = 11,840). We find that racial identity more strongly predicts response rate to paywalled article requests compared to institutional affiliation, whereas institutional affiliation played a larger role in shaping access to datasets. These findings reveal how informal gatekeeping can perpetuate structural inequities in science, highlighting the need for stronger data-sharing mandates and more equitable open access policies."
2509.0871,"This article proposes a novel methodological approach for developing use cases for CH e-infrastuctures documented using Jupyter Notebooks (JNs), enabling transparency and reproducibility. We also address the present problem of use cases that are not consistently documented to cover all key aspects that are derived from the use case literature review outside of CH field to define a useful use case.Purpose. Our primary objective is to explore the practices around creating and analysing use cases related to digital cultural heritage. Our review of the literature showed a substantial deviation in the depth and coverage of use cases and revealed the need for a more robust and consistent approach to creating use cases in a digital heritage context. We developed a framework to develop use cases to support the ongoing efforts to expand the use of eInfrastructures in the digital heritage domain as a first step.Design/methodology/approach. Our research design combines desk research of existing literature and analysing examples of use cases documented in projects. We examine the challenges and inconsistencies in the current practice of use case production in digital heritage. Finally, we synthesize a systematic process to generate use cases which is illustrated by five example use cases within the context.Our work impacts directly such infrastructures and communities as the International GLAM Labs Community, AI for Libraries, Archives, and Museums (AI4LAM) and Time Machine Organisation. This work advances the use of data research infrastructures within communities of researchers, scholars, students, GLAM (Galleries, Libraries, Archives, and Museums) institutions, and Cultural Heritage and Cultural and Creative Industries (CCIs)."
2509.09596,"This study investigates how Large Language Models (LLMs) are influencing the language of academic papers by tracking 12 LLM-associated terms across six major scholarly databases (Scopus, Web of Science, PubMed, PubMed Central (PMC), Dimensions, and OpenAlex) from 2015 to 2024. Using over 2.4 million PMC open-access publications (2021-July 2025), we also analysed full texts to assess changes in the frequency and co-occurrence of these terms before and after ChatGPT's initial public release. Across databases, delve (+1,500%), underscore (+1,000%), and intricate (+700%) had the largest increases between 2022 and 2024. Growth in LLM-term usage was much higher in STEM fields than in social sciences and arts and humanities. In PMC full texts, the proportion of papers using underscore six or more times increased by over 10,000% from 2022 to 2025, followed by intricate (+5,400%) and meticulous (+2,800%). Nearly half of all 2024 PMC papers using any LLM term also included underscore, compared with only 3%-14% of papers before ChatGPT in 2022. Papers using one LLM term are now much more likely to include other terms. For example, in 2024, underscore strongly correlated with pivotal (0.449) and delve (0.311), compared with very weak associations in 2022 (0.032 and 0.018, respectively). These findings provide the first large-scale evidence based on full-text publications and multiple databases that some LLM-related terms are now being used much more frequently and together. The rapid uptake of LLMs to support scholarly publishing is a welcome development reducing the language barrier to academic publishing for non-English speakers."
2509.10389,"Teams now drive most scientific advances, yet the impact of absolute beginners -- authors with no prior publications -- remains understudied. Analyzing over 28 million articles published between 1971 and 2020 across disciplines and team sizes, we uncover a universal and previously undocumented pattern: teams with a higher fraction of beginners are systematically more disruptive and innovative. Their contributions are linked to distinct knowledge-integration behaviors, including drawing on broader and less canonical prior work and producing more atypical recombinations. Collaboration structure further shapes outcomes: disruption is high when beginners work with early-career colleagues or with co-authors who have disruptive track records. Although disruption and citations are negatively correlated overall, highly disruptive papers from beginner-heavy teams are highly cited. These findings reveal a ""beginner's charm"" in science, highlighting the underrecognized yet powerful value of beginner fractions in teams and suggesting actionable strategies for fostering a thriving ecosystem of innovation in science and technology."
2509.11997,"The complex systems keyword diagram generated by the author in 2010 has been used widely in a variety of educational and outreach purposes, but it definitely needs a major update and reorganization. This short paper reports our recent attempt to update the keyword diagram using information collected from the following multiple sources: (a) collective feedback posted on social media, (b) recent reference books on complex systems and network science, (c) online resources on complex systems, and (d) keyword search hits obtained using OpenAlex, an open-access bibliographic catalogue of scientific publications. The data (a), (b) and (c) were used to incorporate the research community's and other public communities' perceptions of the relevant topics, whereas the data (d) was used to obtain more objective measurements of the keywords' relevance and associations from publications made in complex systems science. Results revealed differences and overlaps between public perception and actual usage of keywords in publications on complex systems. Four topical communities were obtained from the keyword association network, although they were highly intertwined with each other. We hope that the resulting network visualization of complex systems keywords provides a more up-to-date, accurate topic map of the field of complex systems as of today."
2509.1223,"This study examines the evolution of references to grain storage structures in medieval European charters, based on a quantitative and semantic analysis of the digitized CEMA (Cartae Europae Medii Aevi) corpus comprising more than 225,000 documents. The author applies text mining and distributional analysis methods to a lexicon of some forty terms designating storage locations (grangia, horreum, granarium, granica, etc.), cross-referencing these data with references to grain and analyzing their semantic contexts over the long term. The analysis reveals a paradigm shift between the early Middle Ages (decentralized, loosely regulated storage) and the 12th-13th centuries (centralization of storage by the ruling classes). Granaries became instruments of spatial polarization and social control, contributing to the accentuation of social domination in medieval Europe. This evolution was accompanied by a new conceptualization of storage, both material and spiritual."
2509.12245,"Information Technology (IT) is recognized as an independent and unique research field. However, there has been ambiguity and difficulty in identifying and differentiating IT research from other close variations. Given this context, this paper aimed to explore the roots of the Information Technology (IT) research domain by conducting a large-scale text mining analysis of 50,780 abstracts from awarded NSF CISE grants from 1985 to 2024. We categorized the awards based on their program content, labeling human-centric programs as IT research programs and infrastructure-centric programs as other research programs based on the IT definitions in the literature. This novel approach helped us identify the core concepts of IT research and compare the similarities and differences between IT research and other research areas. The results showed that IT differentiates itself from other close variations by focusing more on the needs of users, organizations, and societies."
2509.13236,"Despite their cultural and historical significance, Black digital archives continue to be a structurally underrepresented area in AI research and infrastructure. This is especially evident in efforts to digitize historical Black newspapers, where inconsistent typography, visual degradation, and limited annotated layout data hinder accurate transcription, despite the availability of various systems that claim to handle optical character recognition (OCR) well. In this short paper, we present a layout-aware OCR pipeline tailored for Black newspaper archives and introduce an unsupervised evaluation framework suited to low-resource archival contexts. Our approach integrates synthetic layout generation, model pretraining on augmented data, and a fusion of state-of-the-art You Only Look Once (YOLO) detectors. We used three annotation-free evaluation metrics, the Semantic Coherence Score (SCS), Region Entropy (RE), and Textual Redundancy Score (TRS), which quantify linguistic fluency, informational diversity, and redundancy across OCR regions. Our evaluation on a 400-page dataset from ten Black newspaper titles demonstrates that layout-aware OCR improves structural diversity and reduces redundancy compared to full-page baselines, with modest trade-offs in coherence. Our results highlight the importance of respecting cultural layout logic in AI-driven document understanding and lay the foundation for future community-driven and ethically grounded archival AI systems."
2509.17465,"In this work, we present PoliCorp (this https URL), a web portal designed to facilitate the search and analysis of political text corpora. PoliCorp provides researchers with access to rich textual data, enabling in-depth analysis of parliamentary discourse over time. The platform currently features a collection of transcripts from debates in the German parliament, spanning 76 years of proceedings. With the advanced search functionality, researchers can apply logical operations to combine or exclude search criteria, making it easier to filter through vast amounts of parliamentary debate data. The search can be customised by combining multiple fields and applying logical operators to uncover complex patterns and insights within the data. Additional data processing steps were performed to enable web-based search and incorporate extra features. A key feature that differentiates PoliCorp is its intuitive web-based interface that enables users to query processed political texts without requiring programming skills. The user-friendly platform allows for the creation of custom subcorpora via search parameters, which can be freely downloaded in JSON format for further analysis."
2509.22013,"This study examines how Russia's full-scale war against Ukraine affected foreign co-funding, authorship patterns, and the citation impact of articles funded by the Ministry of Education and Science of Ukraine (MESU), the National Academy of Sciences of Ukraine (NASU), and the National Research Foundation of Ukraine (NRFU). The analysis includes articles in Scopus-indexed journals between 2020 and 2023. The share of articles funded by these agencies increased from 8.6% in 2020-2021 to 11.9% in 2022-2023. During the war, the citation impact of MESU-funded articles rose, driven mainly by highly cited articles authored by Ukrainian scholars with foreign co-affiliations, and to a lesser extent by international collaborations. In contrast, the citation impact of NASU- and NRFU-funded articles remained stable. For NASU-funded articles, foreign co-funding was consistently associated with higher citation impact. However, MESU-funded articles published in 2022-2023 without foreign co-funding outperformed those with such funding. Notably, NASU-funded articles without foreign co-funding had citation impact statistically indistinguishable from unfunded articles, yet made up 59.6% of NASU-funded output in 2022-2023. These findings highlight the need to reform research funding allocation in Ukraine to prioritise potentially more impactful work and to strengthen international collaboration, which remains strongly linked to higher research visibility and influence."
2509.22616,"Once upon a time, scientists' worth was measured by their ideas, proofs, and perhaps how eloquently they debated Hilbert's problems at seminars. But now, citation metrics have come to center stage and handed us new masters: FWCI and CNCI. This paper critically, and with a touch of satire, examines how these seemingly objective metrics are shaping, and often distorting, the scientific landscape. Through examples and analysis, we highlight the consequences of relying too heavily on such indicators in evaluating researchers and scientific contributions."
2509.24283,"We present an overview of the SCIDOCA 2025 Shared Task, which focuses on citation discovery and prediction in scientific documents. The task is divided into three subtasks: (1) Citation Discovery, where systems must identify relevant references for a given paragraph; (2) Masked Citation Prediction, which requires selecting the correct citation for masked citation slots; and (3) Citation Sentence Prediction, where systems must determine the correct reference for each cited sentence. We release a large-scale dataset constructed from the Semantic Scholar Open Research Corpus (S2ORC), containing over 60,000 annotated paragraphs and a curated reference set. The test set consists of 1,000 paragraphs from distinct papers, each annotated with ground-truth citations and distractor candidates. A total of seven teams registered, with three submitting results. We report performance metrics across all subtasks and analyze the effectiveness of submitted systems. This shared task provides a new benchmark for evaluating citation modeling and encourages future research in scientific document understanding. The dataset and task materials are publicly available atthis https URL."
2509.24511,"In recent years, the surge in retractions has been accompanied by numerous papers receiving comments that raise concerns about their reliability. The prevalence of problematic papers undermines the reliability of scientific research and threatens the foundation of evidence-based medicine. In this study,we focus on the field of non-coding RNA(ncRNA) as a case study to explore the typical characteristics of problematic papers from various perspectives, aiming to provide insights for addressing large-scale fraudulent publications. Research on under-investigated ncRNAs is more likely to yield problematic papers. These problematic papers often exhibit significant textual similarity, and many others sharing this similarity also display suspicious instances of image duplication. Healthcare institutions are particularly prone to publishing problematic papers, especially those with a low publication volume. Most problematic papers are found in a limited number of journals, and many journals inadequately address the commented papers. Our findings suggest that numerous problematic papers may still remain unidentified. The revealed characteristics offer valuable insights for formulating strategies to address the issue of fraudulent papers at scale."
2509.25237,"""Quantum est in libris"" explores the intersection of the archaic and the modern. On one side, there are manuscript materials from the Estonian National Museum's (ERM) more than century-old archive describing the life experiences of Estonian people; on the other side, there is technology that transforms these materials into a dynamic and interactive experience. Connecting technology and cultural heritage is the visitor, who turns texts into inputs for a screen sculpture.Historical narratives are visually brought to life through the contemporary technological language. Because the video AI models we employed, Runway Gen-3 and Gen-4, have not previously interacted with Estonian heritage, we can observe how machines today ""read the world"" and create future heritage. ""Quantum est in libris"" introduces an exciting yet unsettling new dimension to the concept of cultural heritage: in a world where data are fluid and interpretations unstable, heritage status becomes fragile. In the digital environment, heritage issues are no longer just about preservation and transmission, but also about representation of the media, machine creativity, and interpretive error. Who or what shapes memory processes and memory spaces, and how?"
2509.26001,"We propose the first workshop on Building Innovative Research Systems for Digital Libraries (BIRDS) to take place at TPDL 2025 as a full-day workshop. BIRDS addresses practitioners working in digital libraries and GLAMs as well as researchers from computational domains such as data science, information retrieval, natural language processing, and data modelling. Our interdisciplinary workshop focuses on connecting members of both worlds. One of today's biggest challenges is the increasing information flood. Large language models like ChatGPT seem to offer good performance for answering questions on the web. So, shall we just build upon that idea and use chatbots in digital libraries? Or do we need to design and develop specialized and effective access paths? Answering these questions requires to connect different communities, practitioners from real digital libraries and researchers in the area of computer science. In brief, our workshop's goal is thus to support researchers and practitioners to build the next generation of innovative and effective digital library systems."
2510.01593,"This study presents a bibliometric analysis of industry--academia collaboration in artificial intelligence (AI) research, focusing on papers from two major international conferences, AAAI and IJCAI, from 2010 to 2023. Most previous studies have relied on publishers and other databases to analyze bibliographic information. However, these databases have problems, such as missing articles and omitted metadata. Therefore, we adopted a novel approach to extract bibliographic information directly from the article PDFs: we examined 20,549 articles and identified the collaborative papers through a classification process of author affiliation. The analysis explores the temporal evolution of collaboration in AI, highlighting significant changes in collaboration patterns over the past decade. In particular, this study examines the role of key academic and industrial institutions in facilitating these collaborations, focusing on emerging global trends. Additionally, a content analysis using document classification was conducted to examine the type of first author in collaborative research articles and explore the potential differences between collaborative and noncollaborative research articles. The results showed that, in terms of publication, collaborations are mainly led by academia, but their content is not significantly different from that of others. The affiliation metadata are available atthis https URL."
2510.01783,"The PreprintToPaper dataset connects bioRxiv preprints with their corresponding journal publications, enabling large-scale analysis of the preprint-to-publication process. It comprises metadata for 145,517 preprints from two periods, 2016-2018 (pre-pandemic) and 2020-2022 (pandemic), retrieved via the bioRxiv and Crossref APIs. Each record includes bibliographic information such as titles, abstracts, authors, institutions, submission dates, licenses, and subject categories, alongside enriched publication metadata including journal names, publication dates, author lists, and further information. Preprints are categorized into three groups: Published (formally linked to a journal article), Preprint Only (unpublished), and Gray Zone (potentially published but unlinked). To enhance reliability, title and author similarity scores were calculated, and a human-annotated subset of 299 records was created for evaluation of Gray Zone cases. The dataset supports diverse applications, including studies of scholarly communication, open science policies, bibliometric tool development, and natural language processing research on textual changes between preprints and their published versions. The dataset is publicly available in CSV format via Zenodo."
2510.01961,"The communication of technical insight in scientific manuscripts often relies on ad-hoc formatting choices, resulting in inconsistent visual emphasis and limited portability across document classes. This paper introduces ktbox, a modular LaTeX framework that unifies semantic color palettes, structured highlight boxes, taxonomy trees, and author metadata utilities into a coherent system for scholarly writing. The framework is distributed as a set of lightweight, namespaced components:this http URLfor semantic palettes,this http URLfor structured highlight and takeaway environments,this http URLfor taxonomy trees with fusion and auxiliary annotations, andthis http URLfor ORCID-linked author metadata. Each component is independently usable yet interoperable, ensuring compatibility with major templates such as IEEEtran, acmart, iclr conference, and beamer. Key features include auto-numbered takeaway boxes, wide-format highlights, flexible taxonomy tree visualizations, and multi-column layouts supporting embedded tables, enumerations, and code blocks. By adopting a clear separation of concerns and enforcing a consistent naming convention under the kt namespace, the framework transforms visual styling from cosmetic add-ons into reproducible, extensible building blocks of scientific communication, improving clarity, portability, and authoring efficiency across articles, posters, and presentations."
2510.02743,"Academic grant programs are widely used to motivate international research collaboration and boost scientific impact across borders. Among these, bi-national funding schemes -- pairing researchers from just two designated countries - are common yet understudied compared with national and multinational funding. In this study, we explore whether bi-national programs genuinely foster new collaborations, high-quality research, and lasting partnerships. To this end, we conducted a bibliometric case study of the German-Israeli Foundation (GIF), covering 642 grants, 2,386 researchers, and 52,847 publications. Our results show that GIF funding catalyzes collaboration during, and even slightly before, the grant period, but rarely produces long-lasting partnerships that persist once the funding concludes. By tracing co-authorship before, during, and after the funding period, clustering collaboration trajectories with temporally-aware K-means, and predicting cluster membership with ML models (best: XGBoost, 74% accuracy), we find that 45% of teams with no prior joint work become active while funded, yet activity declines symmetrically post-award; roughly one-third sustain collaboration longer-term, and a small subset achieve high, lasting output. Moreover, there is no clear pattern in the scientometrics of the team's operating as a predictor for long-term collaboration before the grant. This refines prior assumptions that international funding generally forges enduring networks. The results suggest policy levers such as sequential funding, institutional anchoring (centers, shared infrastructure, mobility), and incentives favoring genuinely new pairings have the potential to convert short-term boosts into resilient scientific bridges and inform the design of bi-national science diplomacy instruments."
2510.03307,"We introduce the QIC-Index, a novel metric to address the failure of publication-centric metrics to value research data sharing. The QIC-Index quantifies the impact of individual data objects by calculating a score based on their Quality (Q), Impact (I), and Collaboration (C). By rewarding the sharing of high-quality, impactful, and collaborative data, our framework aligns individual incentives with the goals of open science and aims to foster a more transparent and efficient research culture."
2510.0407,"The probability folder of Mathlib, Lean's mathematical library, makes a heavy use of Markov kernels. We present their definition and properties and describe the formalization of the disintegration theorem for Markov kernels. That theorem is used to define conditional probability distributions of random variables as well as posterior distributions. We then explain how Markov kernels are used in a more unusual way to get a common definition of independence and conditional independence and, following the same principles, to define sub-Gaussian random variables. Finally, we also discuss the role of kernels in our formalization of entropy and Kullback-Leibler divergence."
2510.04749,"The increasing volume of scholarly publications requires advanced tools for efficient knowledge discovery and management. This paper introduces ongoing work on a system using Large Language Models (LLMs) for the semantic extraction of key concepts from scientific documents. Our research, conducted within the German National Research Data Infrastructure for and with Computer Science (NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing. We outline our explorative work, which uses in-context learning with various LLMs to extract concepts from papers, initially focusing on the Business Process Management (BPM) domain. A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields. We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study. Additionally, we gathered insights from the computer science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services. These services aim to support structured literature reviews, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs."
2510.08723,"We systematize the intellectual scope of the ACM Computer Science and Law Symposium (CS&Law). In particular, we address the meaning and importance of the word ''and'' in the name of the symposium. We identify previously published papers (from CS&Law and other forums) that exemplify different aspects of the CS&Law scope and note that the scope is expected to evolve as the symposium and the community grow and change. To round out our systematization of the still nascent research area, we also discuss the mission of CS&Law: What might the symposium seek to accomplish beyond providing a forum for intellectual exchange and community formation?"
2510.09172,"Nowadays, software is one of the cornerstones when conducting research in several scientific fields which employ computer-based methodologies to answer new research questions. However, for these experiments to be completely reproducible, research software should comply with the FAIR principles, yet its metadata can be represented following different data models and spread across different locations. In order to bring some cohesion to the field, CodeMeta was proposed as a vocabulary to represent research software metadata in a unified and standardised manner. While existing tools can help users to generate CodeMeta files for some specific use cases, they fall short on flexibility and adaptability. Hence, in this work, I propose the use of declarative mapping rules to generate CodeMeta files, illustrated through the implementation of three crosswalks in ShExML which are then expanded and merged to cover the generation of CodeMeta files for two existing research software artefacts. Moreover, the outputs are validated using SHACL and ShEx and the whole generation workflow is automated requiring minimal user intervention upon a new version release. This work can, therefore, be used as an example upon which other developers can include a CodeMeta generation workflow in their repositories, facilitating the adoption of CodeMeta and, ultimately, increasing research software FAIRness."
2510.10336,"Public funding plays a central role in driving scientific discovery. To better understand the link between research inputs and outputs, we introduce FIND (Funding-Impact NSF Database), an open-access dataset that systematically links NSF grant proposals to their downstream research outputs, including publication metadata and abstracts. The primary contribution of this project is the creation of a large-scale, structured dataset that enables transparency, impact evaluation, and metascience research on the returns to public funding. To illustrate the potential of FIND, we present two proof-of-concept NLP applications. First, we analyze whether the language of grant proposals can predict the subsequent citation impact of funded research. Second, we leverage large language models to extract scientific claims from both proposals and resulting publications, allowing us to measure the extent to which funded projects deliver on their stated goals. Together, these applications highlight the utility of FIND for advancing metascience, informing funding policy, and enabling novel AI-driven analyses of the scientific process."
2510.14071,"Wikipedia is one of the largest online encyclopedias, which relies on scientific publications as authoritative sources. The increasing prevalence of open access (OA) publishing has expanded the public availability of scientific knowledge; however, its impact on the dynamics of knowledge contestation within collaborative environments such as Wikipedia remains underexplored. To address this gap, we analyze a large-scale dataset that combines Wikipedia edit histories with metadata from scientific publications cited in disputed Wikipedia articles. Our study investigates the characteristics of scientific publications involved in disputes and examines whether OA articles are more likely to be contested than paywalled ones.We find that scientific disputes on Wikipedia are more frequent in the social sciences and humanities, where topics often involve social values and interpretative variability. Publications with higher citation counts and publications in high-impact journals are more likely to be involved in disputes. OA publications are significantly more likely to be involved in disputes and tend to be contested sooner after publication than paywalled articles. This pattern suggests that increased accessibility accelerates both engagement and scrutiny. The relationship between OA status and dispute involvement also varies across disciplines, reflecting differences in Wikipedia editorial practices and norms.These findings highlight the dual role of OA in both expanding access to scientific knowledge and increasing its visibility in contexts of public negotiation and debate. This study contributes to a broader understanding of how scientific knowledge is collaboratively constructed and contested on open platforms, offering insights for research on open science, scholarly communication, and digital knowledge governance."
2510.16152,"Scientific literature is increasingly siloed by complex language, static disciplinary structures, and potentially sparse keyword systems, making it cumbersome to capture the dynamic nature of modern science. This study addresses these challenges by introducing an adaptable large language model (LLM)-driven framework to quantify thematic trends and map the evolving landscape of scientific knowledge. The approach is demonstrated over a 20-year collection of more than 1,500 engineering articles published by the Proceedings of the National Academy of Sciences (PNAS), marked for their breadth and depth of research focus. A two-stage classification pipeline first establishes a primary thematic category for each article based on its abstract. The subsequent phase performs a full-text analysis to assign secondary classifications, revealing latent, cross-topic connections across the corpus. Traditional natural language processing (NLP) methods, such as Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the resulting topical structure and also suggest that standalone word-frequency analyses may be insufficient for mapping fields with high diversity. Finally, a disjoint graph representation between the primary and secondary classifications reveals implicit connections between themes that may be less apparent when analyzing abstracts or keywords alone. The findings show that the approach independently recovers much of the journal's editorially embedded structure without prior knowledge of its existing dual-classification schema (e.g., biological studies also classified as engineering). This framework offers a powerful tool for detecting potential thematic trends and providing a high-level overview of scientific progress."
2510.16407,"In this article we propose a novel method to perform unsupervised clustering of different forms of Institute names. We use only author and affiliation metadata to perform the clustering without any string or pattern matching. After analysing only 50000 articles from Crossref database, we see encouraging results which can be scaled up to provide even better results. We compare our clustering with what a well-known method using string matching does and found that the results were complementary. This can help perform institute disambiguation better when integrated with existing systems, especially to provide aliases for cases where traditional string matching fails. The code of this open-source methodology can be found at:this https URL"
2510.16477,"This study of literature focusing on 'AI Policy' over the past decade, found that citations of preprints, publications on platforms such as arXiv, have increased from five percent to forty percent across three major regions: the U.S., U.K. & E.U., and South Korea. We compare regional responses of preprint citations across the global disruptions of COVID-19 and the release of ChatGPT. We discuss driving factors and risks of preprint normalization, which follows the trend in computer science."
2510.1784,"The digitalisation of research requires data management systems capable of supporting a broad spectrum of usage scenarios, ranging from document-oriented repositories to fully factographic environments. This paper introduces a methodological approach for the stepwise development of such systems, illustrated by the MatInf Research Data Management System (RDMS). The proposed framework combines a graph-based STAR paradigm-emphasising Statefulness, Traceability, Aim, and Result-with the SET methodology, which enables systematic Standardisation, Extraction, and Testing of research data. Together, these principles provide a pathway towards FAIR-compliant data infrastructures, facilitating reproducibility, re-use, and integration of heterogeneous materials science data. By demonstrating the gradual consolidation of research outputs into unified datasets, this study highlights how adaptive RDMS design can support accelerated scientific discovery and enhance collaborative research in large-scale projects."
2510.17853,"Large Language Models (LLMs) have emerged as promising assistants for scientific writing. However, there have been concerns regarding the quality and reliability of the generated text, one of which is the citation accuracy and faithfulness. While most recent work relies on methods such as LLM-as-a-Judge, the reliability of LLM-as-a-Judge alone is also in doubt. In this work, we reframe citation evaluation as a problem of citation attribution alignment, which is assessing whether LLM-generated citations match those a human author would include for the same text. We propose CiteGuard, a retrieval-aware agent framework designed to provide more faithful grounding for citation validation. CiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4% accuracy on the CiteME benchmark, on par with human-level performance (69.7%). It also enables the identification of alternative but valid citations."
2511.01113,"Documents are a common way to store and share information, with tables being an important part of many documents. However, there is no real common understanding of how to model documents and tables in particular. Because of this lack of standardization, most scientific approaches have their own way of modeling documents and tables, leading to a variety of different data structures and formats that are not directly compatible. Furthermore, most data models focus on either the spatial or the semantic structure of a document, neglecting the other aspect. To address this, we developed S2Doc, a flexible data structure for modeling documents and tables that combines both spatial and semantic information in a single format. It is designed to be easily extendable to new tasks and supports most modeling approaches for documents and tables, including multi-page documents. To the best of our knowledge, it is the first approach of its kind to combine all these aspects in a single format."
2511.01353,"The study explores the current state of artificial intelligence (AI) literacy levels among library professionals employing a quantitative approach consisting of 92 surveys of LIS professionals in the United Arab Emirates (UAE). Findings of the study revealed the presence of strong cognitive competencies, while there were gaps observed in behavioral and normative competencies, especially related to AI biases, AI-powered learning, and ethical considerations. There was a disconnect observed between the perceived importance of AI skills and the effectiveness of the current training programs."
2511.01439,"Competitive grant funding is associated with high costs and a potential bias to favor conservative research. This comment proposes integrating editorial preregistration, in the form of registered reports, into grant peer review processes as a reform strategy. Linking funding decisions to in principle accepted study protocols would reduce reviewer burden, strengthen methodological rigor, and provide an institutional foundation for (more) replication, theory driven research, and high risk research. Our proposal also minimizes strategic proposal writing and ensures scholarly output through the publication of preregistered protocols, regardless of funding outcomes. Possible implementation models include direct coupling of journal acceptance with funding, co review mechanisms, voucher systems, and lotteries. While challenges remain in aligning journal and funding agency procedures, the integration of preregistration and funding offers a promising pathway toward a more transparent and efficient research ecosystem."
2511.01485,"This study aims to present a scientometric analysis of the journal titled Cognition for a period of 20 years from 1999 to 2018. The present study was conducted with an aim to provide a summary of research activity in current journal and characterize its most aspects. The research coverage includes the year wise distribution of articles, authors, institutions, countries and citation analysis of the journal. The analysis showed that 2870 papers were published in journal of Cognition from 1999 to 2018. The study identified top 20 prolific authors, institutions and countries of the journal. Researchers from USA have been made the most percentage of contributions."
2511.01496,"This paper examines and explores the web impact factor through a webometric study of the present 12 University Websites of Jammu and Kashmir. Identifies the domain systems of the websites; analyzes the number of web pages and link pages, and calculates the External Link WIF or simple web impact factor (WIF) and external web impact factor of all the University websites. Also reflects that some university websites have higher number of web pages, but correspondingly their link pages are very small in number and websites fall behind in their simple and external link web impact factor. It found that the Cluster University of Jammu ranked 1 (0.9018) in Internal Link WIF of Websites in Jammu and Kashmir. Shri Mata Vaishno Devi University ranked 1 (0.7249) in External Link Web Impact Factor."
2511.01675,"We show that citation metrics of journal articles in many of the online-only Springer Nature journals and associated ones, such as Scientific Reports, Nature Communications, Communications journals, as well as many BMC, Discovery and npj journals, are distorted, going back to articles from 2001. We find that most likely due to an API response error, many references lead to the wrong article, typically to Article Number 1 of a given Volume. Beyond the negative effect of introducing incorrect reference information, this distorts the citation statistics of articles in these journals, with a few articles being massively over-cited compared to their peers, while many lose citations; e.g. both in Scientific Reports and in Nature Communications, 5 of the 10 top cited articles are article number 1s. We validate the distorted statistics by assessing data from multiple scientific literature databases: Crossref, OpenCitations, Semantic Scholar, and the journals' websites. The issue primarily arises from the inconsistent transition from page-based referencing of articles to article number-based referencing, as well as the improper handling of the change in the publisher's article metadata API. It seems that the most pressing problem has been present since approximately 2011, which we estimate affects the citation count of millions of authors."
2511.02255,"Summing or averaging nonlinearly field-normalized citation counts is a common but methodologically problematic practice, as it violates mathematical principles. The issue originates from the nonlinear transformation, which disrupts the equal-interval property of the data. Such unequal data do not satisfy the necessary conditions for summation. In our study, we normalized citation counts of papers from all sample universities using six linear and nonlinear methods, and then computed the total and average scores for each university under each method. By benchmarking against raw citations and linear normalized scores, we explore how large the error effect is from summing or averaging the nonlinear field normalized citation counts. Our empirical results indicate that the error exists but is relatively small. We further found that the magnitude of the error is significantly influenced by whether the sample publications are homogeneous or heterogeneous. This study has significant implications for whether the results obtained through nonlinear methods on a single level can be directly summed or averaged when calculating the overall impact of a research unit."
2511.02275,"The present study is undertaken to find out the publication trends on Alopecia Areata Disease during 2010-2019 from the global perspective. The study mainly focus on distribution of research output, top journals for publications, most prolific authors, authorship pattern, and citations pattern on Alopecia Areata Disease. The results indicate that highest growth rate of publications occurred during the year 2019. Columbia University topped the scene among all institutes. The maximum publications were more than four authored publications. Christiano AM and Clynes R were found to be the most prolific authors. It is also found that most of the prolific authors (by number of publications) do appear in highly cited publications list. Alopecia Areata Disease researchers mostly preferred using article publications to communicate their findings."
2511.02296,"The significance of libraries in preserving and maintaining history and traditional culture cannot be overlooked. It is from this purpose that libraries are to envisage in their programmes cultural activities which must be collected, documented and preserved for posterity. The usefulness of preserved information lies in the fact that the generation to come will be able to establish their identity. This will also assist them with a foundation to build from. This study focus on the growth and development of Library and Culture research in forms of publications reflected in Web of Science database, during the span of 2010-2019. A total 890 publications were found and the highest 124 (13.93%) publications published inthis http URLanalysis maps comprehensively the parameters of total output, growth of output, authorship, institution wise and country-level collaboration patterns, major contributors (individuals, top publication sources, institutions, and countries). It exposed that the most prolific author is Lo P secured first place by contributing 4 (0.45%) publications, followed by Bressan V 3 (0.34%) publications in Library and Culture literature. Journal of Academic Librarianship produced the highest number of records 29 (3.26%) followed by Australian Library Journal having contributed 21 (2.36%).It is identified the domination of Wuhan University; School Information Management had contributed 6 (0.67%) of total research output. Authors from USA published the highest number of publications with a total of 244 (27.42%), followed by UK and Australia with 118 (13.26%) and 76 (8.54%) publications were produced respectively."
2511.02601,"Automated label generation for clusters of scientific documents is a common task in bibliometric workflows. Traditionally, labels were formed by concatenating distinguishing characteristics of a cluster's documents; while straightforward, this approach often produces labels that are terse and difficult to interpret. The advent and widespread accessibility of generative language models, such as ChatGPT, make it possible to automatically generate descriptive and human-readable labels that closely resemble those assigned by human annotators. Language-model label generation has already seen widespread use in bibliographic databases and analytical workflows. However, its rapid adoption has outpaced the theoretical, practical, and empirical foundations. In this study, we address the automated label generation task and make four key contributions: (1) we define two distinct types of labels: characteristic and descriptive, and contrast descriptive labeling with related tasks; (2) we provide a formal descriptive labeling that clarifies important steps and design considerations; (3) we propose a structured workflow for label generation and outline practical considerations for its use in bibliometric workflows; and (4) we develop an evaluative framework to assess descriptive labels generated by language models and demonstrate that they perform at or near characteristic labels, and highlight design considerations for their use. Together, these contributions clarify the descriptive label generation task, establish an empirical basis for the use of language models, and provide a framework to guide future design and evaluation efforts."
2511.03209,The main aim of this study was to assess and evaluate user satisfaction with library resources and services among library users associated with Solapur University. The current research shows the level of users satisfaction with different library resources and services offered by college libraries. The research found that a vast number of respondents were pleased with library facilities and services. The research is designed to achieve users satisfaction in the library to investigate the level of satisfaction towards library resources and services with regards to 26 colleges of Solapur University based in Maharashtra. Information in the form of data has been collected from colleges and on the basis of users results; analysis needs to analyze users satisfaction.
2511.03215,"The present study attempts to highlight the research output generated in Russia in coronary artery disease (CAD) research during the period 1990-2019 to understand the distribution of research output, top journals for publications, and most prolific authors, authorship pattern, and citation pattern. This study is based on secondary data extracted from the Science Citation Index (SCI), which is an integral component of the Web of Science. Descriptive and inferential statistical techniques were applied in the study. There were 5058 articles by Russian scholars in coronary artery disease during 1990-2019; they preferred to publish in Russian journals. The research contributions were in the form of research articles, meeting abstracts and reviews with a consistent drop in the number of editorial material and article; proceedings paper with time. Co-authorship was the norm in coronary artery disease research, with a steady increase in the number of multi-author documents in recent years."
2511.04075,"This paper presents a scientometric analysis of research output from the University of Lagos, focusing on the two decades spanning 2004 to 2023. Using bibliometric data retrieved from the Web of Science, we examine trends in publication volume, collaboration patterns, citation impact, and the most prolific authors, departments, and research domains at the university. The study reveals a consistent increase in research productivity, with the highest publication output recorded in 2023. Health Sciences, Engineering, and Social Sciences are identified as dominant fields, reflecting the university's interdisciplinary research strengths. Collaborative efforts, both locally and internationally, show a positive correlation with higher citation impact, with the United States and the United Kingdom being the leading international collaborators. Notably, open-access publications account for a significant portion of the university's research output, enhancing visibility and citation rates. The findings offer valuable insights into the university's research performance over the past two decades, providing a foundation for strategic planning and policy formulation to foster research excellence and global impact."
2511.04082,"DESIDOC Journal of Library & Information Technology (DJLIT) formerly known as DESIDOC Bulletin of Information Technology is a peer-reviewed, open access, bimonthly journal. This paper presents a Scientometric analysis of the DESIDOC Journal. The paper analyses the pattern of growth of the research output published in the journal, pattern of authorship, author productivity, and, subjects covered to the papers over the period (2013-2017). It is found that 227 papers were published during the period of study (2001-2012). The maximum numbers of articles were collaborative in nature. The subject concentration of the journal noted is Scientometrics. The maximum numbers of articles (65%) have ranged their thought contents between 6 and 10 pages. The study applied standard formula and statistical tools to bring out the factual result."
2511.04129,"The COVID-19 outbreak rapidly became a pandemic in the first quarter of 2020, posing an unprecedented threat and challenge to healthcare systems and the public. Governments in nearly every country focused on immunization programs for the general population using mRNA vaccines against this disease, marking the first large-scale use of this technology. Previously overlooked research papers on mRNA vaccine preparation or administration gained prominence. The impact was documented bibliographically through a surge in citations these papers received. These reports exemplify the Sleeping Beauty bibliometric phenomenon, while the articles that triggered this awakening act as the Sweet Prince, leading to the resurgence of the previous papers' bibliometric impact. Here, a backward reference search was performed in the Scopus bibliographic database to identify Sleeping Beauties by applying the Beauty Coefficient metric. A total of 915 original research articles were published in 2020, citing 21,979 referenced papers, including 1,181 focused on mRNA vaccines, with 671 of these being original research reports. By setting a threshold of at least 30 citations received before 2020, 272 papers published between 2005 and 2022 were examined. The finding that nearly half of the papers included were published in scientific journals between 2020 and 2022 is explained by the fact that these works received a significant number of citations as preprints or prepublications. We found that 28 papers from this bibliographic portfolio exhibited a Beauty Coefficient following the Sleeping Beauty bibliometric phenomenon. Our findings reveal that disruptive technological innovations may be built upon previously neglected reports that experienced sharp citation increases, driven by their crucial applicability to worldwide distresses."
2511.04211,"The article examines the theoretical, methodological, and technical foundations of research on audiovisual corpora within the field of digital humanities. It outlines the main transversal issues underlying the processes of constructing, exploiting, and interpreting such corpora, which are conceived as specific forms of textual data in the broad sense - that is, as sets of semiotic traces (written, visual, sound, or multimodal) that make it possible to document, analyze, and transmit domains of knowledge. The analysis is organized around five complementary themes. The first concerns the status and structure of textual data lato sensu: any data, regardless of its medium, participates in a meaningful representation of a domain and therefore requires a unified theoretical and methodological framework based on a transdisciplinary semiotic approach. The second theme addresses the documentary value of data and corpora, understood as the relevance of materials for documenting a research object in relation to the goals and perspectives of the projects in which they are used. This value depends both on provenance and reasoned selection, and on the pragmatic context of their use. The third theme distinguishes between data collections, corpora, and archives. A data collection constitutes a potential reservoir of materials, whereas a corpus results from a reasoned and contextualized selection from this collection, elaborated in relation to a specific project. Archives, in turn, refer both to open data repositories and to research resources that support experimentation, editorialization, and valorization. This distinction highlights the dynamic nature of the processes of constituting and reusing digital resources. The fourth theme explores the semantic enrichment of data, understood as the set of semiotic and technical operations that give meaning to data, establish relationships among them, and make them usable by social actors. This approach raises issues related to the description, classification, and interconnection of data, notably through the use of ontologies, metadata, and models for knowledge representation. Semantic enrichment thus forms part of a broader reflection on value creation and data reuse in diverse contexts. Finally, the fifth theme addresses the instrumentation of research, that is, the digital environments, tools, and infrastructures that support the production, management, and dissemination of corpora. These technical systems - collaborative platforms, annotation, analysis, and publication tools - form an essential component of the digital humanities ecosystem and condition new forms of archiving, circulation, and knowledge transmission. In conclusion, the article emphasizes three central questions: how to construct meaning from data; how to model strategies of appropriation, reuse, and re-publication; and how to organize new semiotic and cultural ecosystems of research. The overall challenge lies in understanding how audiovisual data can be transformed into genuine objects of knowledge and value within contemporary scientific, technical, and cultural frameworks."
2511.04683,"Academic citation integrity faces persistent challenges, with research indicating 20% of citations contain errors and manual verification requiring months of expert time. This paper presents a novel AI-powered methodology for systematic, comprehensive reference auditing using agentic AI with tool-use capabilities. We develop a zero-assumption verification protocol that independently validates every reference against multiple academic databases (Semantic Scholar, Google Scholar, CrossRef) without assuming any citation is correct. The methodology was validated across 30 academic documents (2,581 references) spanning undergraduate projects to doctoral theses and peer-reviewed publications. Results demonstrate 91.7% average verification rate on published PLOS papers, with successful detection of fabricated references, retracted articles, orphan citations, and predatory journals. Time efficiency improved dramatically: 90-minute audits for 916-reference doctoral theses versus months of manual review. The system achieved <0.5% false positive rate while identifying critical issues manual review might miss. This work establishes the first validated AI-agent methodology for academic citation integrity, demonstrating practical applicability for supervisors, students, and institutional quality assurance."
2511.0482,"The domination of scientific publishing in the Global North by major commercial publishers is harmful to science. We need the most powerful members of the research community, funders, governments and Universities, to lead the drive to re-communalise publishing to serve science not the market."
2511.05051,"This paper focused on the utilization of social media by library professionals and library users. It provides an understanding of social media, the most popular social media platforms utilized in the libraries. It also mentions the reasons for the adoption of social media in libraries be it academic, public, school libraries and other types of libraries. This is a review paper on the use of social media among library professionals and patrons. The findings reveal the contributions of social media to the libraries. Social media makes things easy for library professionals and library users. It enables them to connect, create awareness to new information, disseminate information instantly, and helps to market the library resources and services. Therefore, it is recommended amongst others that the library management board should encourage the use of social media in libraries."
2511.05211,"This study presents a comprehensive scientometric analysis of research productivity on Coronary Artery Disease (CAD) among the BRICS countries, Brazil, Russia, India, China, and South Africa, using data retrieved from the Web of Science database for the period 1990 to 2019. A total of 50,036 records were analyzed to assess publication growth trends, authorship patterns, collaboration levels, and citation impact. The findings reveal a steady increase in CAD-related publications, with China emerging as the leading contributor, followed by Brazil, Russia, India, and South Africa. English dominated as the primary language of communication, accounting for over 93% of publications. Authorship and collaboration analysis indicate a high degree of joint research, with 97.91% of studies being co-authored and a degree of collaboration of 0.98, underscoring the collective nature of scientific inquiry in this domain. The study validates the applicability of Lotkas Law for author productivity, Bradfords Law for journal distribution, and Zipfs Law for keyword frequency, while the Price Square Root Law was found inapplicable. The predominant publication format was journal articles (79.7%), and Kardiologiya (Russia) emerged as the most prolific journal. The results demonstrate significant growth in CAD research output and collaboration within BRICS, though notable disparities persist among member nations. The study recommends enhancing individual author productivity, expanding international collaboration, and supporting CAD research through strategic institutional and governmental initiatives. These findings provide valuable insights for policymakers, funding agencies, and the academic community to strengthen cardiovascular research capacity within developing economies."
2511.07168,"Author Name Disambiguation (AND) is a long-standing challenge in bibliometrics and scientometrics, as name ambiguity undermines the accuracy of bibliographic databases and the reliability of research evaluation. This study addresses the problem of cross-source disambiguation by linking academic career records from CercaUniversitÃ , the official registry of Italian academics, with author profiles in Scopus. We introduce LEAD (LLM-enhanced Engine for Author Disambiguation), a novel hybrid framework that combines semantic features extracted through Large Language Models (LLMs) with structural evidence derived from co-authorship and citation networks. Using a gold standard of 606 ambiguous cases, we compare five methods: (i) Label Spreading on co-authorship networks; (ii) Bibliographic Coupling on citation networks; (iii) a standalone LLM-based approach; (iv) an LLM-enriched configuration; and (v) the proposed hybrid pipeline. LEAD achieves the best performance (F1 = 96.7%, accuracy = 95.7%) with lower computational cost than full LLM models. Bibliographic Coupling emerges as the fastest and strongest single-source method. These findings demonstrate that integrating semantic and structural signals within a selective hybrid strategy offers a robust and scalable solution to cross-database author identification. Beyond the Italian case, this work highlights the potential of hybrid LLM-based methods to improve data quality and reliability in scientometric analyses."
