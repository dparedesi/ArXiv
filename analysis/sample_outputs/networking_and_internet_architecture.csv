paper_id,abstract
2501.00354,"Large constellations of Earth Observation Low Earth Orbit satellites collect enormous amounts of image data every day. This amount of data needs to be transferred to data centers for processing via ground stations. Ground Station as a Service (GSaaS) emerges as a new cloud service to offer satellite operators easy access to a network of ground stations on a pay-per-use basis. However, renting ground station and data center resources still incurs considerable costs, especially for large satellite constellations. The current practice of sticking to a single GSaaS provider also suffers high data latency and low robustness to weather variability due to limited ground station availability. To address these limitations, we propose SkyGS, a system that schedules both communication and computation by federating GSaaS and cloud computing services across multiple cloud providers. We formulate the resulting problem as a system cost minimization problem with a long-term data latency threshold constraint. In SkyGS, we apply Lyapunov optimization to decompose the long-term optimization problem into a series of real-time optimization problems that do not require prior knowledge. As the decomposed problem is still of exponential complexity, we transform it into a bipartite graph-matching problem and employ the Hungarian algorithm to solve it. We analyze the performance theoretically and evaluate SkyGS using realistic simulations based on real-world satellites, ground stations, and data centers data. The comprehensive experiments demonstrate that SkyGS can achieve cost savings by up to 63% & reduce average data latency by up to 95%."
2501.00372,"The increasing complexity of 6G systems demands innovative tools for network management, simulation, and optimization. This work introduces the integration of ns-3 with Sionna RT, establishing the foundation for the first open source full-stack Digital Network Twin (DNT) capable of supporting multi-RAT. By incorporating a deterministic ray tracer for precise and site-specific channel modeling, this framework addresses limitations of traditional stochastic models and enables realistic, dynamic, and multilayered wireless network simulations. Tested in a challenging vehicular urban scenario, the proposed solution demonstrates significant improvements in accurately modeling wireless channels and their cascading effects on higher network layers. With up to 65% observed differences in application-layer performance compared to stochastic models, this work highlights the transformative potential of ray-traced simulations for 6G research, training, and network management."
2501.00859,"Multirotor Aerial Vehicles (MRAVs) when integrated into wireless communication systems and equipped with a Reflective Intelligent Surface (RIS) enhance coverage and enable connectivity in obstructed areas. However, due to limited degrees of freedom (DoF), traditional under-actuated MRAVs with RIS are unable to control independently both the RIS orientation and their location, which significantly limits network performance. A new design, omnidirectional MRAV (o-MRAV), is introduced to address this issue. In this paper, an o-MRAV is deployed to assist a terrestrial base station in providing connectivity to obstructed users. Our objective is to maximize the minimum data rate among users by optimizing the o-MRAV's orientation, location, and RIS phase shift. To solve this challenging problem, we first smooth the objective function and then apply the Parallel Successive Convex Approximation (PSCA) technique to find efficient solutions. Our simulation results show significant improvements of 28% and 14% in terms of minimum and average data rates, respectively, for the o-MRAVs compared to traditional u-MRAVs."
2501.00883,"The dispersed node locations and complex topologies of edge networks, combined with intricate dynamic microservice dependencies, render traditional centralized microservice architectures (MSAs) unsuitable. In this paper, we propose a decentralized microservice architecture (DMSA), which delegates scheduling functions from the control plane to edge nodes. DMSA redesigns and implements three core modules of microservice discovery, monitoring, and scheduling for edge networks to achieve precise awareness of instance deployments, low monitoring overhead and measurement errors, and accurate dynamic scheduling, respectively. Particularly, DMSA has customized a microservice scheduling scheme that leverages multi-port listening and zero-copy forwarding to guarantee high data forwarding efficiency. Moreover, a dynamic weighted multi-level load balancing algorithm is proposed to adjust scheduling dynamically with consideration of reliability, priority, and response delay. Finally, we have implemented a physical verification platform for DMSA. Extensive empirical results demonstrate that compared to state-of-the-art and traditional scheduling schemes, DMSA effectively counteracts link failures and network fluctuations, improving the service response delay and execution success rate by approximately $60\% \sim 75\%$ and $10\%\sim15\%$, respectively."
2501.0095,"The future mobile network has the complex mission of distributing available radio resources among various applications with different requirements. The radio access network slicing enables the creation of different logical networks by isolating and using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources available among the slices to fulfill their service-level agreement (SLA) requirements, prioritizing critical slices while minimizing the number of intent violations. Moreover, ensuring that the RRS can deal with a high diversity of network scenarios is essential. Several recent papers present advances in machine learning-based RRS. However, the scenarios and slice variety are restricted, which inhibits solid conclusions about the generalization capabilities of the models after deployment in real networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a radio access network (RAN) slicing context. The proposed method protects high-priority slices when the available radio resources cannot fulfill all the slices. It uses transfer learning to reduce the number of training steps required. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines. The results show that by using transfer learning, the required number of training steps could be reduced by a factor of eight without hurting performance."
2501.01027,"Remote patient monitoring is crucial in modern healthcare, but current systems struggle with real-time analysis and prediction of vital signs. This paper presents a novel architecture combining deep learning with 5G network capabilities to enable real-time vital sign monitoring and prediction. The proposed system utilizes a hybrid CNN-LSTM model optimized for edge deployment, paired with 5G Ultra-Reliable Low-Latency Communication (URLLC) for efficient data transmission. The architecture achieves end-to-end latency of 14.4ms while maintaining 96.5% prediction accuracy across multiple vital signs. Our system shows significant improvements over existing solutions, reducing latency by 47% and increasing prediction accuracy by 4.2% compared to current state-of-the-art systems. Performance evaluations conducted over three months with data from 1000 patients validate the system's reliability and scalability in clinical settings. The results demonstrate that integrating deep learning with 5G technology can effectively address the challenges of real-time patient monitoring, leading to early detection of deteriorating conditions and improved clinical outcomes. This research establishes a framework for reliable, real-time vital sign monitoring and prediction in digital healthcare."
2501.01038,"Integrated sensing and communication (ISAC) is emerging as a key enabler for vehicle-to-everything (V2X) systems. However, designing efficient beamforming schemes for ISAC signals to achieve accurate sensing and enhance communication performance in the dynamic and uncertain environments of V2X networks presents significant challenges. While artificial intelligence technologies offer promising solutions, the energy-intensive nature of neural networks imposes substantial burdens on communication infrastructures. To address these challenges, this work proposes an energy-efficient and intelligent ISAC system for V2X networks. Specifically, we first leverage a Markov Decision Process framework to model the dynamic and uncertain nature of V2X networks. This framework allows the roadside unit to develop beamforming schemes relying solely on its current sensing information, eliminating the need for numerous pilot signals and extensive CSI acquisition. We then introduce an advanced deep reinforcement learning (DRL) algorithm, enabling the joint optimization of beamforming and power allocation to guarantee both communication rate and sensing accuracy in dynamic and uncertain V2X scenario. To alleviate the energy demands of neural networks, we integrate spiking neural networks (SNNs) into the DRL algorithm. The event-driven, sparse spike-based processing of SNNs significantly improves energy efficiency while maintaining strong performance. Extensive simulation results validate the effectiveness of the proposed scheme with lower energy consumption, superior communication performance, and improved sensing accuracy."
2501.01141,"This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating large language models (LLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90\% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model's focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36\% compared to DDPG and accelerates convergence by reducing required steps by up to 47\% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4\% improvement in QoE when scaling from 4 to 8 vehicles."
2501.012,"This paper provides an in-depth review and discussion of the state of the art in redundancy mitigation for the vehicular Collective Perception Service (CPS). We focus on the evolutionary differences between the redundancy mitigation rules proposed in 2019 in ETSI TR 103 562 versus the 2023 technical specification ETSI TS 103 324, which uses a Value of Information (VoI) based mitigation approach. We also critically analyse the academic literature that has sought to quantify the communication challenges posed by the CPS and present a unique taxonomy of the redundancy mitigation approaches proposed using three distinct classifications: object inclusion filtering, data format optimisation, and frequency management. Finally, this paper identifies open research challenges that must be adequately investigated to satisfactorily deploy CPS redundancy mitigation measures. Our critical and comprehensive evaluation serves as a point of reference for those undertaking research in this area."
2501.01271,"This paper investigates a fundamental yet under-explored trade-off between energy efficiency (EE) and spectral efficiency (SE) in distributed massive MIMO (D-mMIMO) systems. Unlike conventional EE-SE trade-off studies that primarily focus on transmission power, D-mMIMO systems introduce new energy consumption factors including fronthaul signaling and distributed signal processing, which are heavily influenced by AP-UE association. This work highlights the critical need for a system-level EE-SE trade-off framework that accounts for these unique aspects of D-mMIMO. We formulate a joint optimization problem that maximizes EE while satisfying uplink sum-SE constraints, through the coordinated design of power allocation and AP-UE association strategies. By explicitly considering both transmission and infrastructure-related energy costs, our approach enables energy-aware network design without compromising throughput. Numerical simulations demonstrate the substantial impact of dynamic AP-UE association and power control on the EE-SE trade-off, providing actionable insights for an efficient deployment of large-scale distributed MIMO networks in next-generation wireless systems."
2501.01398,"Augmented reality applications are bitrate intensive, delay-sensitive, and computationally demanding. To support them, mobile edge computing systems need to carefully manage both their networking and computing resources. To this end, we present a proof of concept resource management scheme that adapts the bandwidth at the base station and the GPU frequency at the edge to efficiently fulfill roundtrip delay constrains. Resource adaptation is performed using a Multi-Armed Bandit algorithm that accounts for the monotonic relationship between allocated resources and performance. We evaluate our scheme by experimentation on an OpenAirInterface 5G testbed where the considered application is OpenRTiST. The results indicate that our resource management scheme can substantially reduce both bandwidth usage and power consumption while delivering high quality of service. Overall, this work demonstrates that intelligent resource control can potentially establish systems that are not only more efficient but also more sustainable."
2501.01683,"Efficient global Internet scanning is crucial for network measurement and security analysis. While existing target generation algorithms demonstrate remarkable performance in large-scale detection, their efficiency notably diminishes in few-seed scenarios. This decline is primarily attributed to the intricate configuration rules and sampling bias of seed addresses. Moreover, instances where BGP prefixes have few seed addresses are widespread, constituting 63.65% of occurrences. We introduce 6Vision as a solution to tackle this challenge by introducing a novel approach of encoding IPv6 addresses into images, facilitating comprehensive analysis of intricate configuration rules. Through a process of feature stitching, 6Vision not only improves the learnable features but also amalgamates addresses associated with configuration patterns for enhanced learning. Moreover, it integrates an environmental feedback mechanism to refine model parameters based on identified active addresses, thereby alleviating the sampling bias inherent in seed addresses. As a result, 6Vision achieves high-accuracy detection even in few-seed scenarios. The HitRate of 6Vision shows a significant improvement ranging from 181% to 2,490% compared to existing algorithms, while the CoverNum increases by a factor of 1.18 to 11.20 times. Additionally, 6Vision can function as a preliminary detection module for existing algorithms, yielding a conversion gain (CG) ranging from 242% to 2,081%. Ultimately, we achieve a conversion rate (CR) of 28.97% for few-seed scenarios. We develop the IPv6 hitlist Patch, which augments current target generation algorithms for large-scale address detection, thereby effectively supporting IPv6 network measurement and security analysis."
2501.01828,"Recently, over-the-air federated learning (FL) has attracted significant attention for its ability to enhance communication efficiency. However, the performance of over-the-air FL is often constrained by device selection strategies and signal aggregation errors. In particular, neglecting straggler devices in FL can lead to a decline in the fairness of model updates and amplify the global model's bias toward certain devices' data, ultimately impacting the overall system performance. To address this issue, we propose a joint device selection and transmit power optimization framework that ensures the appropriate participation of straggler devices, maintains efficient training performance, and guarantees timely updates. First, we conduct a theoretical analysis to quantify the convergence upper bound of over-the-air FL under age-of-information (AoI)-based device selection. Our analysis further reveals that both the number of selected devices and the signal aggregation errors significantly influence the convergence upper bound. To minimize the expected weighted sum peak age of information, we calculate device priorities for each communication round using Lyapunov optimization and select the highest-priority devices via a greedy algorithm. Then, we formulate and solve a transmit power and normalizing factor optimization problem for selected devices to minimize the time-average mean squared error (MSE). Experimental results demonstrate that our proposed method offers two significant advantages: (1) it reduces MSE and improves model performance compared to baseline methods, and (2) it strikes a balance between fairness and training efficiency while maintaining satisfactory timeliness, ensuring stable model performance."
2501.01837,"Electric Vertical Take-off and Landing vehicles (eVTOLs) are driving Advanced Air Mobility (AAM) toward transforming urban transportation by extending travel from congested ground networks to low-altitude airspace. This transition promises to reduce traffic congestion and significantly shorten commute times. To ensure aviation safety, eVTOLs must fly within prescribed flight corridors. These corridors are managed by ground-based Air Traffic Control (ATCo) stations, which oversee air-ground communication and flight scheduling. However, one critical challenge remains: the lack of high rate air-ground communication and safe flight planning within these corridors. The introduction of 6G-oriented Stacked Intelligent Metasurface (SIM) technology presents a high rate communication solution. With advanced phase-shifting capabilities, SIM enables precise wireless signal control and supports beam-tracking communication with eVTOLs. Leveraging this technology, we propose a Composite Potential Field (CPF) approach. This method dynamically integrates target, separation, and communication fields to optimize both SIM communication efficiency and flight safety. Simulation results validate the effectiveness of this DT-based approach. Compared to the potential field flight control benchmark, it improves the transmission rate by 8.3\%. Additionally, it reduces flight distance deviation from the prescribed corridor by 10\% compared to predetermined optimization methods."
2501.02256,"To better explore the oceans, seamless communication coverage of the vast 3D underwater space is desired. Unlike terrestrial networks using radio signals, underwater acoustic communications face a unique challenge: nodes in underwater shadow zones cannot connect to the network, even within the line of sight. These shadow zones can extend for tens of kilometers, causing communication nodes to disconnect. Existing efforts focus on passive avoidance of shadow zones, but this strategy cannot ensure seamless coverage in dynamic ocean environments. This paper addresses the shadow zone problem by utilizing acoustic Reconfigurable Intelligent Surfaces (RIS) to actively control the underwater channel. Shadow zones are analytically modeled, and optimal RIS deployment strategies are developed for both deep-sea and shallow-sea environments. The acoustic RIS is redesigned considering practical engineering limitations and validated through pool tests. Bellhop-based simulations show that without RIS deployment, coverage is limited to less than 20%, regardless of source strength. However, with optimal RIS deployment, energy coverage can reach almost 100%."
2501.02469,"Minimal infrastructure requirements make LoRa suitable for service delivery in remote areas. Additionally, web applications have become a de-facto standard for modern service delivery. However, Long Range (LoRa) fails to enable HTTP access due to its limited bandwidth, payload size limitations, and high collisions in multi-user setups. We propose LoRaConnect to enable HTTP access over LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices connect and access HTTP resources over LoRa backhaul. It implements caching and synchronization mechanisms to address LoRa's aforementioned limitations. It also implements a message-slicing method in the application layer to overcome LoRa's payload limitations. We evaluate the proposed system using actual hardware in three experimental setups to assess the baseline performance, ideal scenario, and practical application scenario with Frequency Hopping Spread Spectrum (FHSS). Additionally, it implements a ping operation to demonstrate Internet capability and extensible nature. LoRaWeb achieves an average throughput of 1.18 KB/S approximately, with an access delay of only 1.3 S approximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves an access delay of approximately 6.7 S for a 10KB webpage in the ideal case and an average end-to-end delay of only 612 ms approximately in the FHSS-based setup. Comparison with benchmark suggests multi-fold improvement."
2501.02556,"This paper extends the classical network calculus to spatial scenarios, focusing on wireless networks with differentiated services and varying transmit power levels. Building on a spatial network calculus, a prior extension of network calculus to spatial settings, we propose a generalized framework by introducing regulations for stationary marked point processes. The regulations correspond to two key constraints: the total transmit power of all transmitters within a spatial region and the cumulative received power at a receiver, which we refer to as ball regulation and shot-noise regulation, respectively. Then we prove the equivalence of ball regulation and shot-noise regulation for stationary marked point processes and establish a universal lower bound on the performance of all network links under these constraints. This framework is applicable to diverse network scenarios, as demonstrated by the analysis of performance guarantees for networks with multi-class users. In addition, we propose an SINR-based power control scheme adapted to user traffic, which ensures differentiated quality of service (QoS) for different user classes."
2501.02572,"Extended reality (XR), blending virtual and real worlds, is a key application of future networks. While AI advancements enhance XR capabilities, they also impose significant computational and energy challenges on lightweight XR devices. In this paper, we developed a distributed queue model for multi-task DNN inference, addressing issues of resource competition and queue coupling. In response to the challenges posed by the high energy consumption and limited resources of XR devices, we designed a dual time-scale joint optimization strategy for model partitioning and resource allocation, formulated as a bi-level optimization problem. This strategy aims to minimize the total energy consumption of XR devices while ensuring queue stability and adhering to computational and communication resource constraints. To tackle this problem, we devised a Lyapunov-guided Proximal Policy Optimization algorithm, named LyaPPO. Numerical results demonstrate that the LyaPPO algorithm outperforms the baselines, achieving energy conservation of 24.79% to 46.14% under varying resource capacities. Specifically, the proposed algorithm reduces the energy consumption of XR devices by 24.29% to 56.62% compared to baseline algorithms."
2501.02775,"Recently, Low Earth Orbit (LEO) satellite networks (i.e., non-terrestrial network (NTN)), such as Starlink, have been successfully deployed to provide broader coverage than terrestrial networks (TN). Due to limited spectrum resources, TN and NTN may soon share the same spectrum. Therefore, fine-grained spectrum monitoring is crucial for spectrum sharing and interference avoidance. To this end, constructing a 4D radio map (RM) including three spatial dimensions and signal spectra is important. However, this requires the large deployment of sensors, and high-speed analog-to-digital converters for extensive spatial signal collection and wide power spectrum acquisition, respectively. To address these challenges, we propose a deep unsupervised learning framework without ground truths labeling requirement, DeepRM, comprised of neural compressive sensing (CS) and tensor decomposition (TD) algorithms. Firstly, we map the CS process into the optimization of a neural networksassociated loss function, and design a sparsity-performance balance training algorithm to reconstruct a wide power spectrum under limited sub-Nquist samples. Secondly, according to the output of neural CS algorithm, we also utilize neural networks to perform TD, and construct the 3D RM for each frequency, even under very sparse sensor deployment. Extensive evaluations show that DeepRM achieves lower error than its corresponding state-of-the-art baselines, especially with limited samples."
2501.02787,"Emerging technologies in sixth generation (6G) of wireless communications, such as terahertz communication and ultra-massive multiple-input multiple-output, present promising prospects. Despite the high data rate potential of millimeter wave communications, millimeter wave (mmWave) communications in urban low altitude economy (LAE) environments are constrained by challenges such as signal attenuation and multipath interference. Specially, in urban environments, mmWave communication experiences significant attenuation due to buildings, owing to its short wavelength, which necessitates developing innovative approaches to improve the robustness of such communications in LAE networking. In this paper, we explore the use of an unmanned aerial vehicle (UAV)-carried intelligent reflecting surface (IRS) to support low altitude mmWave communication. Specifically, we consider a typical urban low altitude communication scenario where a UAV-carried IRS establishes a line-of-sight (LoS) channel between the mobile users and a source user (SU) despite the presence of obstacles. Subsequently, we formulate an optimization problem aimed at maximizing the transmission rates and minimizing the energy consumption of the UAV by jointly optimizing phase shifts of the IRS and UAV trajectory. Given the non-convex nature of the problem and its high dynamics, we propose a deep reinforcement learning-based approach incorporating neural episodic control, long short-term memory, and an IRS phase shift control method to enhance the stability and accelerate the convergence. Simulation results show that the proposed algorithm effectively resolves the problem and surpasses other benchmark algorithms in various performances."
2501.02952,"Multi-access edge computing (MEC) is emerging as a promising paradigm to provide flexible computing services close to user devices (UDs). However, meeting the computation-hungry and delay-sensitive demands of UDs faces several challenges, including the resource constraints of MEC servers, inherent dynamic and complex features in the MEC system, and difficulty in dealing with the time-coupled and decision-coupled optimization. In this work, we first present an edge-cloud collaborative MEC architecture, where the MEC servers and cloud collaboratively provide offloading services for UDs. Moreover, we formulate an energy-efficient and delay-aware optimization problem (EEDAOP) to minimize the energy consumption of UDs under the constraints of task deadlines and long-term queuing delays. Since the problem is proved to be non-convex mixed integer nonlinear programming (MINLP), we propose an online joint communication resource allocation and task offloading approach (OJCTA). Specifically, we transform EEDAOP into a real-time optimization problem by employing the Lyapunov optimization framework. Then, to solve the real-time optimization problem, we propose a communication resource allocation and task offloading optimization method by employing the Tammer decomposition mechanism, convex optimization method, bilateral matching mechanism, and dependent rounding method. Simulation results demonstrate that the proposed OJCTA can achieve superior system performance compared to the benchmark approaches."
2501.0321,"Entanglement-based networks (EBNs) enable general-purpose quantum communication by combining entanglement and its swapping in a sequence that addresses the challenges of achieving long distance communication with high fidelity associated with quantum technologies. In this context, entanglement distribution refers to the process by which two nodes in a quantum network share an entangled state, serving as a fundamental resource for communication. In this paper, we study the performance of entanglement distribution mechanisms over a physical topology comprising end nodes and quantum switches, which are crucial for constructing large-scale links. To this end, we implemented a switch-based topology in NetSquid and conducted a series of simulation experiments to gain insight into practical and realistic quantum network engineering challenges. These challenges include, on the one hand, aspects related to quantum technology, such as memory technology, gate durations, and noise; and, on the other hand, factors associated with the distribution process, such as the number of switches, distances, purification, and error correction. All these factors significantly impact the end-to-end fidelity across a path, which supports communication between two quantum nodes. We use these experiments to derive some guidelines towards the design and configuration of future EBNs."
2501.03465,"LoRa bridges the gap between remote locations and mainstream networks, enabling large-scale Internet of Things (IoT) deployments. Despite the recent advancements around LoRa, Internet access over this technology is still largely unexplored. Most existing solutions only handle packets within the local LoRa network and do not interact with web applications. This limits the scalability and the ability to deliver essential web services in disconnected regions. This work proposes and implements ILoRa to extend the public Internet to disconnected areas for essential service delivery. ILoRa enables accessing Application Programming Interfaces (APIs) and web pages on the Internet over a LoRa backbone network. It comprises a ILoRa coordinator code (ICN) and access point nodes (APNs). The ICN interfaces the LoRa network with the public Internet and interprets content. The APN tethers a WiFi hotspot to which devices connect and access the web content. This work further proposes data handling methods for ICNs and APNs. An actual hardware-based implementation validates the proposed system. The implementation achieves a throughput of 1.06 kbps tested for an Internet-based API returning JSON data of 930 B. Furthermore, the APN consumed approximately $0.162$A current, and the resource utilization on the ICN was minimal."
2501.03601,"With the increasing number of connected devices and complex networks involved, current domain-specific security techniques become inadequate for diverse large-scale Internet of Things (IoT) systems applications. While cross-domain authentication and authorization brings lots of security improvement, it creates new challenges of efficiency and security. Zero trust architecture (ZTA), an emerging network security architecture, offers a more granular and robust security environment for IoT systems. However, extensive cross-domain data exchange in ZTA can cause reduced authentication and authorization efficiency and data privacy concerns. Therefore, in this paper, we propose a dynamic authentication and granularized authorization scheme based on ZTA integrated with decentralized federated learning (DFL) for cross-domain IoT networks. Specifically, device requests in the cross-domain process are continuously monitored and evaluated, and only necessary access permissions are granted. To protect user data privacy and reduce latency, we integrate DFL with ZTA to securely and efficiently share device data across different domains. Particularly, the DFL model is compressed to reduce the network transmission load. Meanwhile, a dynamic adaptive weight adjustment mechanism is proposed to enable the DFL model to adapt to data characteristics from different domains. We analyze the performance of the proposed scheme in terms of security proof, including confidentiality, integrity and availability. Simulation results demonstrate the superior performance of the proposed scheme in terms of lower latency and higher throughput compared to other existing representative schemes."
2501.0368,"Coordination among multiple access points (APs) is integral to IEEE 802.11bn (Wi-Fi 8) for managing contention in dense networks. This letter explores the benefits of Coordinated Spatial Reuse (C-SR) and proposes the use of reinforcement learning to optimize C-SR group selection. We develop a hierarchical multi-armed bandit (MAB) framework that efficiently selects APs for simultaneous transmissions across various network topologies, demonstrating reinforcement learning's promise in Wi-Fi settings. Among several MAB algorithms studied, we identify the upper confidence bound (UCB) as particularly effective, offering rapid convergence, adaptability to changes, and sustained performance."
2501.03731,"This letter proposes a Bayesian channel estimation method that leverages on the a priori information provided by the Electromagnetic Digital Twin's (EM-DT) representation of the environment. The proposed approach is compared with several conventional techniques in terms of Normalized Mean Square Error (NMSE), spectral efficiency, and number of pilots. Simulations prove more than $10\,$dB gain in NMSE and a spectral efficiency comparable to that of the ideal channel state information, for different signal-to-noise ratio (SNR) values. Additionally, the Bayesian EM-DT-empowered channel estimation enables a remarkable pilot reduction compared to maximum likelihood methods at low SNR."
2501.03905,"Mixture-of-Expert (MoE) models outperform conventional models by selectively activating different subnets, named experts, on a per-token basis. This gated computation generates dynamic communications that cannot be determined beforehand, challenging the existing GPU interconnects that remain static during the distributed training process. In this paper, we advocate for a first-of-its-kind system, called MixNet, that unlocks topology reconfiguration during distributed MoE training. Towards this vision, we first perform a production measurement study and show that the MoE dynamic communication pattern has strong locality, alleviating the requirement of global reconfiguration. Based on this, we design and implement a regionally reconfigurable high-bandwidth domain on top of existing electrical interconnects using optical circuit switching (OCS), achieving scalability while maintaining rapid adaptability. We have built a fully functional MixNet prototype with commodity hardware and a customized collective communication runtime that trains state-of-the-art MoE models with in-training topology reconfiguration across 32 A100 GPUs. Large-scale packet-level simulations show that MixNet delivers comparable performance as the non-blocking fat-tree fabric while boosting the training cost efficiency (e.g., performance per dollar) of four representative MoE models by 1.2x-1.5x and 1.9x-2.3x at 100 Gbps and 400 Gbps link bandwidths, respectively."
2501.04149,"This project compares the performance of simultaneous transmit and receive (STR) and enhanced multi-link single radio (EMLSR) within Multi-Link Operation (MLO) in Wi-Fi 7 networks. Using the ns-3 simulator, we evaluate both techniques under various scenarios, including changes in modulation coding scheme (MCS), bandwidth, link quality, and interference levels. Key performance metrics such as latency, throughput, and energy efficiency are analyzed to determine the trade-offs between STR and EMLSR. The results demonstrate that STR achieves higher throughput and lower latency due to dual-link utilization, making it suitable for high-load environments. In contrast, EMLSR balances energy efficiency with responsiveness, making it advantageous for power-sensitive applications. This analysis provides insights into the strengths and limitations of STR and EMLSR, guiding optimal deployment strategies for future Wi-Fi 7 networks."
2501.04246,"Encrypted traffic classification technology is a crucial decision-making information source for network management and security protection. It has the advantages of excellent response timeliness, large-scale data bearing, and cross-time-and-space analysis. The existing research on encrypted traffic classification has gradually transitioned from the closed world to the open world, and many classifier optimization and feature engineering schemes have been proposed. However, encrypted traffic classification has yet to be effectively applied to the actual network environment. The main reason is that applications on the Internet are constantly updated, including function adjustment and version change, which brings severe feature concept drift, resulting in rapid failure of the classifier. Hence, the entire model must be retrained only past very fast time, with unacceptable labeled sample constructing and model training cost. To solve this problem, we deeply study the characteristics of Internet application updates, associate them with feature concept drift, and then propose self-evolving encrypted traffic classification. We propose a feature concept drift determination method and a drift-oriented self-evolving fine-tuning method based on the Laida criterion to adapt to all applications that are likely to be updated. In the case of no exact label samples, the classifier evolves through fully fine-tuning continuously, and the time interval between two necessary retraining is greatly extended to be applied to the actual network environment. Experiments show that our approach significantly improves the classification performance of the original classifier on the following stage dataset of the following months (9\% improvement on F1-score) without any hard-to-acquire labeled sample. Under the current experimental environment, the life of the classifier is extended to more than eight months."
2501.04267,"Processing computer vision applications (CVA) on mobile devices is challenging due to limited battery life and computing power. While cloud-based remote processing of CVA offers abundant computational resources, it introduces latency issues that can hinder real-time applications. To overcome this problem, computational offloading to edge servers has been adopted by industry and academic research. Furthermore, 5G access can also benefit CVA with lower latency and higher bandwidth than previous cellular generations. As the number of Mobile Operators and Internet Service providers relying on 5G access is growing, it is of paramount importance to elaborate a solution for supporting real time applications with the assistance of the edge computing. Besides that, open-source based platforms for Multi-access Edge Computing (MEC) and 5G core can be deployed to rapid prototyping and testing applications. This paper aims at providing an end-to-end solution of open-source MEC and 5G Core platforms along with a commercial 5G Radio. We first conceived a 5G-edge computing environment to assist near to user processing of computer vision applications. Then a sentiment analysis application is developed and integrated to the proposed 5G-Edge architecture. Finally, we conducted a performance evaluation of the proposed solution and compare it against a remote cloud-based approach in order to highlight the benefits of our proposal. The proposed architecture achieved a 260\% throughput performance increase and reduced response time by 71.3\% compared to the remote-cloud-based offloading."
2501.04356,"Telecommunication technologies are important enablers for both digital and ecological transitions. By offering digital alternatives to traditional modes of transportation and communication, they help reduce carbon footprints while improving access to fundamental services. Particularly in rural and remote areas, telecommunications facilitate access to education, healthcare, and employment, helping to bridge the digital divide. Additionally, telecommunications can promote sustainability by supporting renewable energy usage, gender equality, and circular economies. However, defining the role of telecommunications in sustainability remains complex due to the historical focus on performance rather than long-term societal goals. Given the significance of this theme, this paper aims to provide the reader with a deeper look at the concept of sustainability within the telecommunications sector by examining relevant initiatives and projects. It reviews the major approaches for measuring sustainability and outlines practical approaches for implementing these assessments. Furthermore, the paper explores the proposed network architectures that incorporate Key Value Indicators and discusses major technologies in this area, such as Network Digital Twins and Intent-Based Networking. Through this analysis, the paper aims to contribute to creating sustainable telecommunication networks and broader industries."
2501.04496,"Wireless communication has profoundly transformed the way we experience the world. For instance, at most events, attendees commonly utilize their smartphones to document and share their experiences. This shift in user behavior largely stems from the cellular network s capacity for communication. However, as networks become increasingly sophisticated, new opportunities arise to leverage the network for services beyond mere communication, collectively termed Beyond Communication Services (BCS). These services encompass joint communications and sensing, network as a service, and distributed computing. This paper presents examples of BCS and identifies the enablers necessary to facilitate their realization in sixth generation (6G). These enablers encompass exposing data and network capabilities, optimizing protocols and procedures for BCS, optimizing compute offloading protocols and signalling, and employing application and device-driven optimization strategies."
2501.04557,"The design and comparison of satellite-terrestrial routing (STR) and inter-satellite routing (ISR) in low Earth orbit satellite constellations is a widely discussed topic. The signal propagation distance under STR is generally longer than that under ISR, resulting in greater path loss. The global deployment of gateways introduces additional costs for STR. In contrast, transmissions under ISR rely on the energy of satellites, which could be more costly. Additionally, ISLs require more complex communication protocol design, extra hardware support, and increased computational power. To maximize energy efficiency, we propose two optimal routing relay selection algorithms for ISR and STR, respectively. Furthermore, we derive the analytical expressions for the routing availability probability and energy efficiency, quantifying the performance of the algorithms. The analyses enable us to assess the performance of the proposed algorithms against existing methods through numerical results, compare the performance of STR and ISR, and provide useful insights for constellation design."
2501.04624,"Since the advent of software-defined networking (SDN), Traffic Engineering (TE) has been highlighted as one of the key applications that can be achieved through software-controlled protocols (e.g. PCEP and MPLS). Being one of the most complex challenges in networking, TE problems involve difficult decisions such as allocating flows, either via splitting them among multiple paths or by using a reservation system, to minimize congestion. However, creating an optimized solution is cumbersome and difficult as traffic patterns vary and change with network scale, capacity, and demand. AI methods can help alleviate this by finding optimized TE solutions for the best network performance. SDN-based TE tools such as Teal, Hecate and more, use classification techniques or deep reinforcement learning to find optimal network TE solutions that are demonstrated in simulation. Routing control conducted via source routing tools, e.g., PolKA, can help dynamically divert network flows. In this paper, we propose a novel framework that leverages Hecate to practically demonstrate TE on a real network, collaborating with PolKA, a source routing tool. With real-time traffic statistics, Hecate uses this data to compute optimal paths that are then communicated to PolKA to allocate flows. Several contributions are made to show a practical implementation of how this framework is tested using an emulated ecosystem mimicking a real P4 testbed scenario. This work proves valuable for truly engineered self-driving networks helping translate theory to practice."
2501.04792,"The stabilizability of wireless networked control systems (WNCSs) is a deterministic binary valued parameter proven to hold if the communication data rate is higher than the sum of the logarithm of unstable eigenvalues of the open-loop control system. In this analysis, it is assumed that the communication system provides a fixed deterministic transmission rate between the sensors and controllers. Due to the stochastic parameters of communication channels, such as small-scale fading, the instantaneous rate is an intrinsically stochastic parameter. In this sense, it is a common practice in the literature to use the deterministic ergodic rate in analyzing the asymptotic stabilizability. Theoretically, there exists no work in the literature investigating how the ergodic rate can be incorporated into the analysis of asymptotic stabilizability. Considering the stochastic nature of channel parameters, we introduce the concept of probability of stabilizability by interconnecting communication link reliability with the system's unstable eigenvalues and derive a closed-form expression that quantifies this metric. Numerical results are provided to visualize how communication and control systems' parameters affect the probability of stabilizability of the overall system."
2501.04956,"As a ubiquitous deployment paradigm, integrating microservice architecture (MSA) into edge networks promises to enhance the flexibility and scalability of services. However, it also presents significant challenges stemming from dispersed node locations and intricate network topologies. In this paper, we have proposed a topology-aware MSA characterized by a three-tier network traffic model encompassing the service, microservices, and edge node layers. This model meticulously characterizes the complex dependencies between edge network topologies and microservices, mapping microservice deployment onto link traffic to accurately estimate communication delay. Building upon this model, we have formulated a weighted sum communication delay optimization problem considering different types of services. Then, a novel topology-aware and individual-adaptive microservices deployment (TAIA-MD) scheme is proposed to solve the problem efficiently, which accurately senses the network topology and incorporates an individual-adaptive mechanism in a genetic algorithm to accelerate the convergence and avoid local optima. Extensive simulations show that, compared to the existing deployment schemes, TAIA-MD improves the communication delay performance by approximately 30% to 60% and effectively enhances the overall network performance. Furthermore, we implement the TAIA-MD scheme on a practical microservice physical platform. The experimental results demonstrate that TAIA-MD achieves superior robustness in withstanding link failures and network fluctuations."
2501.05262,"Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases."
2501.05673,"Network services are increasingly managed by considering chained-up virtual network functions and relevant traffic flows, known as the Service Function Chains (SFCs). To deal with sequential arrivals of SFCs in an online fashion, we must consider two closely-coupled problems - an SFC placement problem that maps SFCs to servers/links in the network and an SFC scheduling problem that determines when each SFC is executed. Solving the whole SFC problem targeting these two optimizations jointly is extremely challenging. In this paper, we propose a novel network diffuser using conditional generative modeling for this SFC placing-scheduling optimization. Recent advances in generative AI and diffusion models have made it possible to generate high-quality images/videos and decision trajectories from language description. We formulate the SFC optimization as a problem of generating a state sequence for planning and perform graph diffusion on the state trajectories to enable extraction of SFC decisions, with SFC optimization constraints and objectives as conditions. To address the lack of demonstration data due to NP-hardness and exponential problem space of the SFC optimization, we also propose a novel and somewhat maverick approach -- Rather than solving instances of this difficult optimization, we start with randomly-generated solutions as input, and then determine appropriate SFC optimization problems that render these solutions feasible. This inverse demonstration enables us to obtain sufficient expert demonstrations, i.e., problem-solution pairs, through further optimization. In our numerical evaluations, the proposed network diffuser outperforms learning and heuristic baselines, by $\sim$20\% improvement in SFC reward and $\sim$50\% reduction in SFC waiting time and blocking rate."
2501.05742,"The low-altitude economy (LAE) plays an indispensable role in cargo transportation, healthcare, infrastructure inspection, and especially post-disaster communication. Specifically, unmanned aerial vehicles (UAVs), as one of the core technologies of the LAE, can be deployed to provide communication coverage, facilitate data collection, and relay data for trapped users, thereby significantly enhancing the efficiency of post-disaster response efforts. In this paper, we design an efficient and robust UAV-swarm enabled collaborative self-organizing network to facilitate post-disaster communications. Specifically, a ground device transmits data to UAV swarms, which then use collaborative beamforming (CB) technique to form virtual antenna arrays and relay the data to a remote access point (AP) efficiently. Then, we formulate a rescue-oriented post-disaster transmission rate maximization optimization problem (RPTRMOP). Then, we propose a two-stage optimization approach to address it. In the first stage, the optimal traffic routing and the theoretical upper bound on the transmission rate of the network are derived. In the second stage, we transform the formulated RPTRMOP into a variant named V-RPTRMOP, and a diffusion model-enabled particle swarm optimization (DM-PSO) algorithm is proposed to deal with the V-RPTRMOP. Simulation results show the effectiveness of the proposed two-stage optimization approach in improving the transmission rate of the constructed network, which demonstrates the great potential for post-disaster communications. Moreover, the robustness of the constructed network is also validated via evaluating the impact of two unexpected situations on the system transmission rate."
2501.06176,"Since its first release, WiFi has been highly successful in providing wireless local area networks. The ever-evolving IEEE 802.11 standards continue to add new features to keep up with the trend of increasing numbers of mobile devices and the growth of Internet of Things (IoT) applications. Unfortunately, the lack of open-source IEEE 802.11 testbeds in the community limits the development and performance evaluation of those new features. Motivated by an existing popular open-source software-defined radio (SDR) package for single-user single-stream transmission based on the IEEE 802.11/a/g/p standard, in this paper we present GR-WiFi, an open-source package for single-user and multi-user multi-input multi-output (MIMO) transmissions based on 802.11n and 802.11ac standards. The distinct features of GR-WiFi include the support of parallel data streams to single or multiple users, and the compatible preamble processing to allow the co-existence of conventional, high-throughput (HT) and very-high-throughput (VHT) traffics. The performance of GR-WiFi is evaluated through both extensive simulation and real-world experiments."
2501.06191,"Deep Learning (DL) modeling has been a recent topic of interest. With the accelerating need to embed Deep Learning Networks (DLNs) to the Internet of Things (IoT) applications, many DL optimization techniques were developed to enable applying DL to IoTs. However, despite the plethora of DL optimization techniques, there is always a trade-off between accuracy, latency, and cost. Moreover, there are no specific criteria for selecting the best optimization model for a specific scenario. Therefore, this research aims at providing a DL optimization model that eases the selection and re-using DLNs on IoTs. In addition, the research presents an initial design for a DL optimization model management framework. This framework would help organizations choose the optimal DL optimization model that maximizes performance without sacrificing quality. The research would add to the IS design science knowledge as well as the industry by providing insights to many IT managers to apply DLNs to IoTs such as machines and robots."
2501.06194,"Nowadays, the use of soft computational techniques in power systems under the umbrella of machine learning is increasing with good reception. In this paper, we first present a deep learning approach to find the optimal configuration for HetNet systems. We used a very large number of radial configurations of a test system for training purposes. We also studied the issue of joint carrier/power allocation in multilayer hierarchical networks, in addition to ensuring the quality of experience for all subscribers, to achieve optimal power efficiency. The proposed method uses an adaptive load equilibrium model that aims to achieve ""almost optimal"" equity among all servers from the standpoint of the key performance indicator. Unlike current model-based energy efficiency methods, we propose a joint resource allocation, energy efficiency, and flow control algorithm to solve common nonconvex and hierarchical optimization problems. Also, by referring to the allocation of continuous resources based on SLA, we extended the proposed algorithm to common flow/power control and operational power optimization algorithm to achieve optimal energy efficiency along with ensuring user's throughput limitations. Also, simulation results show that the proposed controlled power/flow optimization approach can significantly increase energy efficiency compared to conventional designs using network topology adjustment capability."
2501.06205,"The evolution of Artificial Intelligence (AI) and its subset Deep Learning (DL), has profoundly impacted numerous domains, including autonomous driving. The integration of autonomous driving in military settings reduces human casualties and enables precise and safe execution of missions in hazardous environments while allowing for reliable logistics support without the risks associated with fatigue-related errors. However, relying on autonomous driving solely requires an advanced decision-making model that is adaptable and optimum in any situation. Considering the presence of numerous interconnected autonomous vehicles in mission-critical scenarios, Ultra-Reliable Low Latency Communication (URLLC) is vital for ensuring seamless coordination, real-time data exchange, and instantaneous response to dynamic driving environments. The advent of 6G strengthens the Internet of Automated Defense Vehicles (IoADV) concept within the realm of Internet of Military Defense Things (IoMDT) by enabling robust connectivity, crucial for real-time data exchange, advanced navigation, and enhanced safety features through IoADV interactions. On the other hand, a critical advancement in this space is using pre-trained Generative Large Language Models (LLMs) for decision-making and communication optimization for autonomous driving. Hence, this work presents opportunities and challenges with a vision of realizing the full potential of these technologies in critical defense applications, especially through the advancement of IoADV and its role in enhancing autonomous military operations."
2501.06242,"5G technology enhances industries with high-speed, reliable, low-latency communication, revolutionizing mobile broadband and supporting massive IoT connectivity. With the increasing complexity of applications on User Equipment (UE), offloading resource-intensive tasks to robust servers is essential for improving latency and speed. The 3GPP's Multi-access Edge Computing (MEC) framework addresses this challenge by processing tasks closer to the user, highlighting the need for an intelligent controller to optimize task offloading and resource allocation. This paper introduces a novel methodology to efficiently allocate both communication and computational resources among individual UEs. Our approach integrates two critical 5G service imperatives: Ultra-Reliable Low Latency Communication (URLLC) and Massive Machine Type Communication (mMTC), embedding them into the decision-making framework. Central to this approach is the utilization of Proximal Policy Optimization, providing a robust and efficient solution to the challenges posed by the evolving landscape of 5G technology. The proposed model is evaluated in a simulated 5G MEC environment. The model significantly reduces processing time by 4% for URLLC users under strict latency constraints and decreases power consumption by 26% for mMTC users, compared to existing baseline models based on the reported simulation results. These improvements showcase the model's adaptability and superior performance in meeting diverse QoS requirements in 5G networks."
2501.06244,"With the growing demand for Earth observation, it is important to provide reliable real-time remote sensing inference services to meet the low-latency requirements. The Space Computing Power Network (Space-CPN) offers a promising solution by providing onboard computing and extensive coverage capabilities for real-time inference. This paper presents a remote sensing artificial intelligence applications deployment framework designed for Low Earth Orbit satellite constellations to achieve real-time inference performance. The framework employs the microservice architecture, decomposing monolithic inference tasks into reusable, independent modules to address high latency and resource heterogeneity. This distributed approach enables optimized microservice deployment, minimizing resource utilization while meeting quality of service and functional requirements. We introduce Robust Optimization to the deployment problem to address data uncertainty. Additionally, we model the Robust Optimization problem as a Partially Observable Markov Decision Process and propose a robust reinforcement learning algorithm to handle the semi-infinite Quality of Service constraints. Our approach yields sub-optimal solutions that minimize accuracy loss while maintaining acceptable computational costs. Simulation results demonstrate the effectiveness of our framework."
2501.06309,"In our earlier work, Network-Centric Optimal Hybrid Mobility for IPv6 wireless sensor networks, in which the work sought to control mobility of sensor nodes from an external network was proposed. It was a major improvement on earlier works such as Cluster Sensor Proxy Mobile IPv6 (CSPMIPv6) and Network of Proxies (NoP). In this work, the Network-Centric optimal hybrid mobility scenario was used to detect and fill sensing holes occurring as a result damaged or energy depleted sensing nodes. Various sensor networks self-healing and recovery, and deployment algorithms such as Enhanced Virtual Forces Algorithm with Boundary Forces (EVFA-B); Coverage - Aware Sensor Automation protocol (CASA); Sensor Self-Organizing Algorithm (SSOA); VorLag and the use of the use of anchor and relay nodes were reviewed. With node density thresholds set for various scenarios, the recovery efficiency using various parameters were measured. Comparably, our method provides the most efficient node relocation and self-healing mechanism for sensor networks. Compared to Sensor Self-Organizing Algorithm (SSOA), Hybrid Mobile IP showed superiority in coverage, shorter period of recovery, less computational cost and lower energy depletion. With processing and mobility costs shifted to the external network, Hybrid Mobile IP extends the life span of the network."
2501.06428,"This research investigates how CDNs (Content Delivery Networks) can improve the digital experience, as consumers increasingly expect fast, efficient, and effortless access to online resources. CDNs play a crucial role in reducing latency, enhancing scalability, and optimizing delivery mechanisms, which is evident across various platforms and regions. The study focuses on key CDN concerns, such as foundational and modern CDN architectures, edge computing, hybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing topics, including caching, load balancing, and the novel features of HTTP/3 and QUIC.Current trends, such as integrating CDNs with 5G networks, serverless architectures, and AI-driven traffic management, are examined to demonstrate how CDN technology is likely to evolve. The study also addresses challenges related to security, cost, and global regulations. Practical examples from the e-commerce, streaming, and gaming industries highlight how enhanced CDNs are transforming these sectors.The conclusions emphasize the need to evolve CDN strategies to meet growing user expectations and adapt to the rapidly changing digital landscape. Additionally, the research identifies future research opportunities, particularly in exploring the impact of QC, the enhancement of AI services, and the sustainability of CDN solutions. Overall, the study situates architectural design, performance strategies, and emerging trends to address gaps and create a more efficient and secure approach for improving digital experiences."
2501.06446,"A large number of heterogeneous wireless networks share the unlicensed spectrum designated as the ISM (Industry, Scientific, and Medicine) radio band. These networks do not adhere to a common medium access rule and differ in their specifications considerably. As a result, when concurrently active, they cause cross-technology interference (CTI) on each other. The effect of this interference is not reciprocal, the networks using high transmission power and advanced transmission schemes often causing disproportionate disruptions to those with modest communication and computation resources. CTI corrupts packets, incurs packet retransmission cost, introduces end-to-end latency and jitter, and make networks unpredictable. The purpose of this paper is to closely examine its impact on low-power networks which are based on the IEEE 802.15.4 standard. It discusses latest developments on CTI detection, coexistence and avoidance mechanisms as well on messaging schemes which attempt to enable heterogeneous networks directly communicate with one another to coordinate packet transmission and channel assignment."
2501.06464,"An expansion of Internet of Things (IoTs) has led to significant challenges in wireless data harvesting, dissemination, and energy management due to the massive volumes of data generated by IoT devices. These challenges are exacerbated by data redundancy arising from spatial and temporal correlations. To address these issues, this paper proposes a novel data-driven collaborative beamforming (CB)-based communication framework for IoT networks. Specifically, the framework integrates CB with an overlap-based multi-hop routing protocol (OMRP) to enhance data transmission efficiency while mitigating energy consumption and addressing hot spot issues in remotely deployed IoT networks. Based on the data aggregation to a specific node by OMRP, we formulate a node selection problem for the CB stage, with the objective of optimizing uplink transmission energy consumption. Given the complexity of the problem, we introduce a softmax-based proximal policy optimization with long short-term memory (SoftPPO-LSTM) algorithm to intelligently select CB nodes for improving transmission efficiency. Simulation results validate the effectiveness of the proposed OMRP and SoftPPO-LSTM methods, demonstrating significant improvements over existing routing protocols and node selection strategies. The results also reveal that the combined OMRP with the SoftPPO-LSTM method effectively mitigates hot spot problems and offers superior performance compared to traditional strategies."
2501.06604,"The increasing demand for high-speed and reliable wireless networks has driven advancements in technologies such as millimeter-wave and 5G radios, which requires efficient planning and timely deployment of wireless access points. A critical tool in this process is the radio map, a graphical representation of radio-frequency signal strengths that plays a vital role in optimizing overall network performance. However, existing methods for estimating radio maps face challenges due to the need for extensive real-world data collection or computationally intensive ray-tracing analyses, which is costly and time-consuming. Inspired by the success of generative AI techniques in large language models and image generation, we explore their potential applications in the realm of wireless networks. In this work, we propose RM-Gen, a novel generative framework leveraging conditional denoising diffusion probabilistic models to synthesize radio maps using minimal and readily collected data. We then introduce an environment-aware method for selecting critical data pieces, enhancing the generative model's applicability and usability. Comprehensive evaluations demonstrate that RM-Gen achieves over 95% accuracy in generating radio maps for networks that operate at 60 GHz and sub-6GHz frequency bands, outperforming the baseline GAN and pix2pix models. This approach offers a cost-effective, adaptable solution for various downstream network optimization tasks."
2501.06637,"Terahertz communications are envisioned as a key enabler for 6G networks. The abundant spectrum available in such ultra high frequencies has the potential to increase network capacity to huge data rates. However, they are extremely affected by blockages, to the point of disrupting ongoing communications. In this paper, we elaborate on the relevance of predicting visibility between users and access points (APs) to improve the performance of THz-based networks by minimizing blockages, that is, maximizing network availability, while at the same time keeping a low reconfiguration overhead. We propose a novel approach to address this problem, by combining a neural network (NN) for predicting future user-AP visibility probability, with a probability threshold for AP reselection to avoid unnecessary reconfigurations. Our experimental results demonstrate that current state-of-the-art handover mechanisms based on received signal strength are not adequate for THz communications, since they are ill-suited to handle hard blockages. Our proposed NN-based solution significantly outperforms them, demonstrating the interest of our strategy as a research line."
2501.06688,"Consider a network where a wireless base station (BS) connects multiple source-destination pairs. Packets from each source are generated according to a renewal process and are enqueued in a single-packet queue that stores only the freshest packet. The BS decides, at each time slot, which sources to schedule. Selected sources transmit their packet to the BS via unreliable links. Successfully received packets are forwarded to corresponding destinations. The connection between the BS and destinations is assumed unreliable and delayed. Information freshness is captured by the Age of Information (AoI) metric. The objective of the scheduling decisions is leveraging the delayed and unreliable AoI knowledge to keep the information fresh. In this paper, we derive a lower bound on the achievable AoI by any scheduling policy. Then, we develop an optimal randomized policy for any packet generation processes. Next, we develop minimum mean square error estimators of the AoI and system times, and a Max-Weight Policy that leverages these estimators. We evaluate the AoI of the Optimal Randomized Policy and the Max-Weight Policy both analytically and through simulations. The numerical results suggest that the Max-Weight Policy with estimation outperforms the Optimal Randomized Policy even when the BS has no AoI knowledge."
2501.0688,"Online Cloud gaming demands real-time, high-quality video transmission across variable wide-area networks (WANs). Neural-enhanced video transmission algorithms employing super-resolution (SR) for video quality enhancement have effectively challenged WAN environments. However, these SR-based methods require intensive fine-tuning for the whole video, making it infeasible in diverse online cloud gaming. To address this, we introduce River, a cloud gaming delivery framework designed based on the observation that video segment features in cloud gaming are typically repetitive and redundant. This permits a significant opportunity to reuse fine-tuned SR models, reducing the fine-tuning latency of minutes to query latency of milliseconds. To enable the idea, we design a practical system that addresses several challenges, such as model organization, online model scheduler, and transfer strategy. River first builds a content-aware encoder that fine-tunes SR models for diverse video segments and stores them in a lookup table. When delivering cloud gaming video streams online, River checks the video features and retrieves the most relevant SR models to enhance the frame quality. Meanwhile, if no existing SR model performs well enough for some video segments, River will further fine-tune new models and update the lookup table. Finally, to avoid the overhead of streaming model weight to the clients, River designs a prefetching strategy that predicts the models with the highest possibility of being retrieved. Our evaluation based on real video game streaming demonstrates River can reduce redundant training overhead by 44% and improve the Peak-Signal-to-Noise-Ratio by 1.81dB compared to the SOTA solutions. Practical deployment shows River meets real-time requirements, achieving approximately 720p 20fps on mobile devices."
2501.06943,"Open radio access networks (e.g., O-RAN) facilitate fine-grained control (e.g., near-RT RIC) in next-generation networks, necessitating advanced AI/ML techniques in handling online resource orchestration in real-time. However, existing approaches can hardly adapt to time-evolving network dynamics in network slicing, leading to significant online performance degradation. In this paper, we propose AdaSlicing, a new adaptive network slicing system, to online learn to orchestrate virtual resources while efficiently adapting to continual network dynamics. The AdaSlicing system includes a new soft-isolated RAN virtualization framework and a novel AdaOrch algorithm. We design the AdaOrch algorithm by integrating AI/ML techniques (i.e., Bayesian learning agents) and optimization methods (i.e., the ADMM coordinator). We design the soft-isolated RAN virtualization to improve the virtual resource utilization of slices while assuring the isolation among virtual resources at runtime. We implement AdaSlicing on an O-RAN compliant network testbed by using OpenAirInterface RAN, Open5GS Core, and FlexRIC near-RT RIC, with Ettus USRP B210 SDR. With extensive network experiments, we demonstrate that AdaSlicing substantially outperforms state-of-the-art works with 64.2% cost reduction and 45.5% normalized performance improvement, which verifies its high adaptability, scalability, and assurance."
2501.06989,"With its significant security potential, the quantum internet is poised to revolutionize technologies like cryptography and communications. Although it boasts enhanced security over traditional networks, the quantum internet still encounters unique security challenges essential for safeguarding its Confidentiality, Integrity, and Availability (CIA). This study explores these challenges by analyzing the vulnerabilities and the corresponding mitigation strategies across different layers of the quantum internet, including physical, link, network, and application layers. We assess the severity of potential attacks, evaluate the expected effectiveness of mitigation strategies, and identify vulnerabilities within diverse network configurations, integrating both classical and quantum approaches. Our research highlights the dynamic nature of these security issues and emphasizes the necessity for adaptive security measures. The findings underline the need for ongoing research into the security dimension of the quantum internet to ensure its robustness, encourage its adoption, and maximize its impact on society."
2501.07895,"This paper highlights the significance of resource-constrained Internet of Things (RCD-IoT) systems in addressing the challenges faced by industries with limited resources. This paper presents an energy-efficient solution for industries to monitor and control their utilities remotely. Integrating intelligent sensors and IoT technologies, the proposed RCD-IoT system aims to revolutionize industrial monitoring and control processes, enabling efficient utilization ofthis http URLproposed system utilized the IEEE 802.15.4 WiFi Protocol for seamless data exchange between Sensor Nodes. This seamless exchange of information was analyzed through Packet Tracer. The system was equipped with a prototyped, depicting analytical chemical process to analyze the significant performance metrics. System achieved average Round trip time (RTT) of just 12ms outperforming the already existing solutions presented even with higher Quality of Service (QoS) under the transmission of 1500 packets/seconds under different line of sight (LOS) and Non line of sight (NLOS) fadings."
2501.08045,"This article investigates the adaptive resource allocation scheme for digital twin (DT) synchronization optimization over dynamic wireless networks. In our considered model, a base station (BS) continuously collects factory physical object state data from wireless devices to build a real-time virtual DT system for factory event analysis. Due to continuous data transmission, maintaining DT synchronization must use extensive wireless resources. To address this issue, a subset of devices is selected to transmit their sensing data, and resource block (RB) allocation is optimized. This problem is formulated as a constrained Markov process (CMDP) problem that minimizes the long-term mismatch between the physical and virtual systems. To solve this CMDP, we first transform the problem into a dual problem that refines RB constraint impacts on device scheduling strategies. We then propose a continual reinforcement learning (CRL) algorithm to solve the dual problem. The CRL algorithm learns a stable policy across historical experiences for quick adaptation to dynamics in physical states and network capacity. Simulation results show that the CRL can adapt quickly to network capacity changes and reduce normalized root mean square error (NRMSE) between physical and virtual states by up to 55.2%, using the same RB number as traditional methods."
2501.08099,"With users demanding seamless connectivity, handovers (HOs) have become a fundamental element of cellular networks. However, optimizing HOs is a challenging problem, further exacerbated by the growing complexity of mobile networks. This paper presents the first countrywide study of HO optimization, through the prism of Smoothed Online Learning (SOL). We first analyze an extensive dataset from a commercial mobile network operator (MNO) in Europe with more than 40M users, to understand and reveal important features and performance impacts on HOs. Our findings highlight a correlation between HO failures/delays, and the characteristics of radio cells and end-user devices, showcasing the impact of heterogeneity in mobile networks nowadays. We subsequently model UE-cell associations as dynamic decisions and propose a realistic system model for smooth and accurate HOs that extends existing approaches by (i) incorporating device and cell features on HO optimization, and (ii) eliminating (prior) strong assumptions about requiring future signal measurements and knowledge of end-user mobility. Our algorithm, aligned with the O-RAN paradigm, provides robust dynamic regret guarantees, even in challenging environments, and shows superior performance in multiple scenarios with real-world and synthetic data."
2501.081,"Extended Reality (XR) enables a plethora of novel interactive shared experiences. Ideally, users are allowed to roam around freely, while audiovisual content is delivered wirelessly to their Head-Mounted Displays (HMDs). Therefore, truly immersive experiences will require massive amounts of data, in the range of tens of gigabits per second, to be delivered reliably at extremely low latencies. We identify Millimeter-Wave (mmWave) communications, at frequencies between 24 and 300 GHz, as a key enabler for such experiences. In this article, we show how the mmWave state of the art does not yet achieve sufficient performance, and identify several key active research directions expected to eventually pave the way for extremely-high-quality mmWave-enabled interactive multi-user XR."
2501.08229,"This research proposes a system as a solution for the challenges faced by Sri Lanka' s historic railway system, such as scheduling delays, overcrowding, manual ticketing, and management inefficiencies. It proposes a multi-subsystem approach, incorporating GPS tracking, RFID-based e-ticketing, seat reservation, and vision-based people counting. The GPS based real time train tracking system performs accurately within 24 meters, with the MQTT protocol showing twice the speed of the HTTP-based system. All subsystems use the MQTT protocol to enhance efficiency, reliability, and passenger experience. The study's data and methodology demonstrate the effectiveness of these innovations in improving scheduling, passenger flow, and overall system performance, offering promising solutions for modernizing Sri Lanka's railway infrastructure."
2501.08535,"Network-assisted congestion control leveraging Explicit Congestion Notification (ECN) is an effective way to deal with congestion issues on the Internet. However, we believe that the existing ECN mechanism in the TCP/IP protocol stack may require further optimization to effectively address the evolving congestion challenges introduced by emerging technologies like immersive AR/VR applications and the burgeoning field of the Internet of Things (IoT). To that end, we propose a multilevel congestion notification mechanism called Enhanced ECN (EECN) that leverages the existing two ECN bits in the IP header to notify two levels of congestion in the network and uses the corresponding two bits in the TCP header to negotiate EECN during the handshake and echo congestion experienced back to the sender. Additionally, we propose a congestion control mechanism that triggers different congestion control responses based on the average RTT and multilevel congestion feedback received from the network, which yields promising results, highlighting the effectiveness of utilizing multilevel congestion feedback. The proposed EECN mechanism reduces packet drop by 70% compared to ECN, by 95% compared to TCP New Reno without ECN, and by 40% compared to VCP. The packets marked are reduced by 96% compared to ECN and 76% compared to VCP. Furthermore, the proposed approach reduces flow completion time by 61% compared to ECN and enhances the throughput of short-lived network flows, which are particularly pronounced in IoT environments."
2501.08599,"Reconfigurable intelligent surfaces (RISs) offer a viable way to improve the performance of multi-hop device-to-device (D2D) communication. However, due to the substantial propagation and penetration losses of the millimeter waves (mmWaves), a direct line of sight (LoS) link and close proximity of a device pair are required for a high data rate. Static obstacles like trees and buildings can easily impede the direct LoS connectivity between a device pair. Hence, RIS placement plays a crucial role in establishing an indirect LoS link between them. Therefore, in this work, we propose a set cover-based RIS deployment strategy for both single and double RIS-assisted D2D communication. In particular, we have demonstrated that permitting reflections via two consecutive RISs can greatly lower the RIS density in the environment, preventing resource waste and enabling the service of more obstructed device pairs. After the RIS deployment, for information transfer, we also propose an energy-efficient group selection criteria. Moreover, we prove that sometimes double reflections are more beneficial than single reflection, which is counter-intuitive. Numerical results show that our approach outperforms a random and a recent deployment strategy."
2501.08644,"In future wireless communication systems, millimeter waves (mmWaves) will play an important role in meeting high data rates. However, due to their short wavelengths, these mmWaves present high propagation losses and are highly attenuated by blocking. In this chapter, we seek to increase the indoor radio coverage at 60 GHz in non line-of-sight (NLOS) environments. Firstly, a metallic passive reflector is used in an L-shaped corridor. Secondly, an array of grooved metallic antennas of size 20 cm x 20 cm (corresponding to 80 grooves) is used in a T-shaped corridor. Next, the study focuses on the blockage losses caused by the human body. The results obtained in these different configurations show that it is possible to use beamforming to exploit a reflected path when the direct path is blocked."
2501.08751,"Using Millimeter-Wave (mmWave) wireless communications is often named as the prime enabler for mobile interactive Extended Reality (XR), as it offers multi-gigabit data rates at millisecond-range latency. To achieve this, mmWave nodes must focus their energy towards each other, which is especially challenging in XR scenarios, where the transceiver on the user's XR device may rotate rapidly. To evaluate the feasibility of mmWave XR, we present the first throughput and latency evaluation of state-of-the-art mmWave hardware under rapid rotational motion, for different PHY and MAC-layer parameter configurations. We show that this parameter configuration has a significant impact on performance, and that specialized beamforming approaches for rapid rotational motion may be necessary to enable uninterrupted, high-quality mobile interactive XR experiences."
2501.08752,"Achieving extremely high-quality and truly immersive interactive Virtual Reality (VR) is expected to require a wireless link to the cloud, providing multi-gigabit throughput and extremely low latency. A prime candidate for fulfilling these requirements is millimeter-wave (mmWave) communications, operating in the 30 to 300 GHz bands, rather than the traditional sub-6 GHz. Evaluations with first-generation mmWave Wi-Fi hardware, based on the IEEE 802.11ad standard, have so far largely remained limited to lower-layer metrics. In this work, we present the first experimental analysis of the capabilities of mmWave for streaming VR content, using a novel testbed capable of repeatably creating blockage through mobility. Using this testbed, we show that (a) motion may briefly interrupt transmission, (b) a broken line of sight may degrade throughput unpredictably, and (c) TCP-based streaming frameworks need careful tuning to behave well over mmWave."
2501.0876,"Translating configurations between different network devices is a common yet challenging task in modern network operations. This challenge arises in typical scenarios such as replacing obsolete hardware and adapting configurations to emerging paradigms like Software Defined Networking (SDN) and Network Function Virtualization (NFV). Engineers need to thoroughly understand both source and target configuration models, which requires considerable effort due to the complexity and evolving nature of these specifications. To promote automation in network configuration translation, we propose INTA, an intent-based translation framework that leverages Large Language Model (LLM) agents. The key idea of INTA is to use configuration intent as an intermediate representation for translation. It first employs LLMs to decompose configuration files and extract fine-grained intents for each configuration fragment. These intents are then used to retrieve relevant manuals of the target device. Guided by a syntax checker, INTA incrementally generates target configurations. The translated configurations are further verified and refined for semantic consistency. We implement INTA and evaluate it on real-world configuration datasets from the industry. Our approach outperforms state-of-the-art methods in translation accuracy and exhibits strong generalizability. INTA achieves an accuracy of 98.15% in terms of both syntactic and view correctness, and a command recall rate of 84.72% for the target configuration. The semantic consistency report of the translated configuration further demonstrates its practical value in real-world network operations."
2501.08848,"Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators."
2501.0899,"Ambient internet of things (A-IoT) paradigm is under study in 3GPP with the intention to provide a sustainable solution for the IoT market without any need to replace the batteries and operate in harsh environments where it is difficult to replenish batteries. This article provides insight on 3rd Generation Partnership Project (3GPP) discussions in Release 18 and 19 with the focus on network architecture aspects. 3GPP has recently decided to start normative work in its Radio Access Network (RAN) Working Group (WG) and discussions are ongoing to start a work item in other WGs with more focus on architecture aspects. We explore and analyze various aspects of system design related to architecture requirements to support A-IoT service, different architecture options to consider, security and authentication mechanisms for A-IoT devices as well as key challenges for standardization of A-IoT service."
2501.09123,"Fifth Generation (5G) mobile networks considers an expansive set of heterogeneous services with stringent Quality of Service (QoS) requirements, and traffic demand with inherent spatial-temporal distribution, which places the backhaul network deployment under potential strain. In this paper, we propose to harness network slicing, Integrated Access and Backhaul (IAB) technology coupled with satellite connectivity to build a dynamic wireless backhaul network that can provide additional backhaul capacity to the base stations on demand when the wired backhaul link is temporarily out of capacity. To construct the network design, Deep Reinforcement Learning (DRL) models are used to select, for each network slice of the congested base station, an appropriate backhaul link from the pool of available IAB and satellite links that meets the QoS requirements (i.e., throughput and latency) of the slice. Simulation results show that around 20 episodes are sufficient to train a Double Deep Q-Network (DDQN) agent, with one fully-connected hidden layer and Rectified Linear Unit (ReLU) activation function, that adjusts the topology of the backhaul network."
2501.09125,"Network slicing, a key technology introduced in 5G standards, enables mobile networks to simultaneously support a wide range ofheterogeneous use cases with diverse quality of service (QoS) requirements. This work discusses the potential benefits of networkslicing for the automotive sector, encompassing manufacturing processes and vehicular communications. The review of the stateof the art reveals a clear gap regarding the application of network slicing from the perspective of industrial verticals such asautomotive use cases and their specific requirements. Departing from this observation, we first identify limitations of previouscellular technologies and open challenges for supporting the data services required. Then we describe network slicing as an enablerto face these challenges. We present an analysis of the cost equilibrium for network slicing to be effective for car manufacturers,and tests in real 5G networks that demonstrate the performance improvement in OTA updates coexisting with other services."
2501.09383,"Mobile edge Large Language Model (LLM) deployments face inherent constraints, such as limited computational resources and network bandwidth. Although Retrieval-Augmented Generation (RAG) mitigates some challenges by integrating external knowledge bases, inefficient cache management can still result in high retrieval latency and frequent cache updates. To address these issues, we propose an Adaptive Contextual Caching (ACC) framework that anticipates user needs by proactively caching semantically relevant data for mobile-edge LLMs. ACC utilizes a deep reinforcement learning (DRL) module to refine cache replacement policies, balancing user context, document similarity, and the overhead associated with cache misses. Experimental results demonstrate that ACC increases cache hit rates to over 80\% after only 11 training episodes, outperforming FIFO, LRU, and semantic-only caching while reducing retrieval latency by up to 40\%. In particular, ACC also reduces local caching overhead (i.e., the cost of updating the cache when a miss occurs) by as much as 55\%, enabling scalable, low-latency LLM services in resource-constrained edge environments."
2501.09391,"The rapid advancement of immersive technologies has propelled the development of the Metaverse, where the convergence of virtual and physical realities necessitates the generation of high-quality, photorealistic images to enhance user experience. However, generating these images, especially through Generative Diffusion Models (GDMs), in mobile edge computing environments presents significant challenges due to the limited computing resources of edge devices and the dynamic nature of wireless networks. This paper proposes a novel framework that integrates contract-inspired contest theory, Deep Reinforcement Learning (DRL), and GDMs to optimize image generation in these resource-constrained environments. The framework addresses the critical challenges of resource allocation and semantic data transmission quality by incentivizing edge devices to efficiently transmit high-quality semantic data, which is essential for creating realistic and immersive images. The use of contest and contract theory ensures that edge devices are motivated to allocate resources effectively, while DRL dynamically adjusts to network conditions, optimizing the overall image generation process. Experimental results demonstrate that the proposed approach not only improves the quality of generated images but also achieves superior convergence speed and stability compared to traditional methods. This makes the framework particularly effective for optimizing complex resource allocation tasks in mobile edge Metaverse applications, offering enhanced performance and efficiency in creating immersive virtual environments."
2501.09405,"The advent of Non-Terrestrial Networks (NTN) represents a compelling response to the International Mobile Telecommunications 2030 (IMT-2030) framework, enabling the delivery of advanced, seamless connectivity that supports reliable, sustainable, and resilient communication systems. Nevertheless, the integration of NTN with Terrestrial Networks (TN) necessitates considerable alterations to the existing cellular infrastructure in order to address the challenges intrinsic to NTN implementation. Additionally, Ambient Backscatter Communication (AmBC), which utilizes ambient Radio Frequency (RF) signals to transmit data to the intended recipient by altering and reflecting these signals, exhibits considerable potential for the effective integration of NTN and TN. Furthermore, AmBC is constrained by its limitations regarding power, interference, and other related factors. In contrast, the application of Artificial Intelligence (AI) within wireless networks demonstrates significant potential for predictive analytics through the use of extensive datasets. AI techniques enable the real-time optimization of network parameters, mitigating interference and power limitations in AmBC. These predictive models also enhance the adaptive integration of NTN and TN, driving significant improvements in network reliability and Energy Efficiency (EE). In this paper, we present a comprehensive examination of how the commixture of AI, AmBC, and NTN can facilitate the integration of NTN and TN. We also provide a thorough analysis indicating a marked enhancement in EE predicated on this triadic relationship."
2501.0941,"Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. Exploiting the heterogeneous capabilities of edge LLMs is crucial for diverse emerging applications, as it enables greater cost-effectiveness and reduced latency. In this work, we introduce \textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative inference framework for edge LLMs. We formulate the joint gating and expert selection problem to optimize inference performance under energy and latency constraints. Unlike conventional MoE problems, LLM expert selection is significantly more challenging due to the combinatorial nature and the heterogeneity of edge LLMs across various attributes. To this end, we propose a two-level expert selection mechanism through which we uncover an optimality-preserving property of gating parameters across expert selections. This property enables the decomposition of the training and selection processes, significantly reducing complexity. Furthermore, we leverage the objective's monotonicity and design a discrete monotonic optimization algorithm for optimal expert selection. We implement edge servers with NVIDIA Jetson AGX Orins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results validate that performance improvements of various LLM models and show that our MoE$^2$ method can achieve optimal trade-offs among different delay and energy budgets, and outperforms baselines under various system resource constraints."
2501.10006,"Delay- and Disruption-tolerant Networking (DTN) is essential for communication in challenging environments with intermittent connectivity, long delays, and disruptions. Ensuring high performance in these types of networks is crucial because windows for data transmission are sparse and often short. However, research on evaluating DTN implementations is limited. Moreover, existing research relies on manual testing methods that lack reproducibility and scalability. We propose a novel generic framework for reproducible performance evaluation of DTN implementations to address this issue. We validate the framework's accuracy using a physical testbed and compare the uD3TN and ION DTN implementations. This comparison reveals that uD3TN exhibits higher goodput and shorter bundle retention times. On the other hand, ION exhibited superior memory management and fault tolerance, albeit at the cost of sending and receiving performance. Through this comparison, our framework demonstrates the feasibility of developing a generic toolkit for evaluating DTN."
2501.10159,"In pervasive systems, mobile devices and other sensors access Gateways, which are Servers that communicate with the devices, provide low latency services, connect them with each other, and connect them to the Internet and backbone networks. Gateway Servers are often equipped with Attack Detection (AD) software that analyzes the incoming traffic to protect the system against Cyberattacks, which can overwhelm the Gateway and the system as a whole. This paper describes a traffiic shaping, attack detection and an optimum attack mitigation scheme to protect the Gateway and the system as a whole from Cyberattacks. The approach is described and evaluated in an experimental testbed. The key parameter of the optimum mitigation technique is chosen based on an analytical model whose predictions are validated through detailed experiments."
2501.10224,"Gateway Servers for the Internet of Vehicles (IoV) must meet stringent Security and Quality of Service (QoS) requirements, including cyberattack protection, low delays and minimal packet loss, to offer secure real-time data exchange for human and vehicle safety and efficient road traffic management. Therefore, it is vital to protect these systems from cyberattacks with adequate Attack Detection (AD) and Mitigation mechanisms. Such attacks often include packet Floods that impair the QoS of the networks and Gateways and even impede the Gateways capability to carry out AD. Thus, this paper first evaluates these effects using system measurements during Flood attacks. It then demonstrates how a Smart Quasi-Deterministic Policy Forwarder (SQF) at the entrance of the Gateway can regulate the incoming traffic to ensure that the Gateway supports the AD to operate promptly during an attack. Since Flood attacks create substantial packet backlogs, we propose a novel Adaptive Attack Mitigation (AAM) system that is activated after an attack is detected to dynamically sample the incoming packet stream, determine whether the attack is continuing, and also drop batches of packets at the input to reduce the effects of the attack. The AAM is designed to minimize a cost function that includes the sampling overhead and the cost of lost benign packets. We show experimentally that the Optimum AAM approach is effective in mitigating attacks and present theoretical and experimental results that validate the proposed approach."
2501.10285,"Virtualization in 5G and beyond networks allows the creation of virtual networks, or network slices, tailored to meet the requirements of various applications. However, this flexibility introduces several challenges for infrastructure providers (InPs) in slice admission control (AC) and resource allocation. To maximize revenue, InPs must decide in real-time whether to admit new slice requests (SRs) given slices' revenues, limited infrastructure resources, unknown relationship between resource allocation and Quality of Service (QoS), and the unpredictability of future SRs. To address these challenges, this paper introduces a novel data-driven framework for 5G slice admission control that offers a guaranteed upper bound on the competitive ratio, i.e., the ratio between the revenue obtained by an oracle solution and that of the online solution. The proposed framework leverages a pricing function to dynamically estimate resources' pseudo-prices that reflect resource scarcity. Such prices are further coupled with a resource allocation algorithm, which leverages a machine-learned slice model and employs a primal-dual algorithm to determine the minimum-cost resource allocation. The resource cost is then compared with the offered revenue to admit or reject a SR. To demonstrate the efficacy of our framework, we train the data-driven slice model using real traces collected from our 5G testbed. Our results show that our novel approach achieves up to 42% improvement in the empirical competitive ratio, i.e., ratio between the optimal and the online solution, compared to other benchmark algorithms."
2501.11198,"Internet of Things (IoT) devices have become increasingly ubiquitous with applications not only in urban areas but remote areas as well. These devices support industries such as agriculture, forestry, and resource extraction. Due to the device location being in remote areas, satellites are frequently used to collect and deliver IoT device data to customers. As these devices become increasingly advanced and numerous, the amount of data produced has rapidly increased potentially straining the ability for radio frequency (RF) downlink capacity. Free space optical communications with their wide available bandwidths and high data rates are a potential solution, but these communication systems are highly vulnerable to weather-related disruptions. This results in certain communication opportunities being inefficient in terms of the amount of data received versus the power expended. In this paper, we propose a deep reinforcement learning (DRL) method using Deep Q-Networks that takes advantage of weather condition forecasts to improve energy efficiency while delivering the same number of packets as schemes that don't factor weather into routing decisions. We compare this method with simple approaches that utilize simple cloud cover thresholds to improve energy efficiency. In testing the DRL approach provides improved median energy efficiency without a significant reduction in median delivery ratio. Simple cloud cover thresholds were also found to be effective but the thresholds with the highest energy efficiency had reduced median delivery ratio values."
2501.1141,"This paper proposes a novel split learning architecture designed to exploit the cyclical movement of Low Earth Orbit (LEO) satellites in non-terrestrial networks (NTNs). Although existing research focuses on offloading tasks to the NTN infrastructure, these approaches overlook the dynamic movement patterns of LEO satellites that can be used to efficiently distribute the learning task. In this work, we analyze how LEO satellites, from the perspective of ground terminals, can participate in a time-window-based model training. By splitting the model between a LEO and a ground terminal, the computational burden on the satellite segment is reduced, while each LEO satellite offloads the partially trained model to the next satellite in the constellation. This cyclical training process allows larger and more energy-intensive models to be deployed and trained across multiple LEO satellites, despite their limited energy resources. We formulate an optimization problem that manages radio and processing resources, ensuring the entire data is processed during each satellite pass while minimizing the energy consumption. Our results demonstrate that this approach offers a more scalable and energy-efficient way to train complex models, enhancing the capabilities of LEO satellite constellations in the context of Artificial Intelligence-driven applications."
2501.11484,"The Internet of Things (IoT) establishes connectivity between billions of heterogeneous devices that provide a variety of essential everyday services. The IoT faces several challenges, including energy efficiency and scalability, that require consideration of enabling technologies such as network softwarization. This technology is an appropriate solution for IoT, leveraging Software Defined Networking (SDN) and Network Function Virtualization (NFV) as two main techniques, especially when combined with Machine Learning (ML). Although many efforts have been made to optimize routing in softwarized IoT, the existing solutions do not take advantage of distributed intelligence. In this paper, we propose to optimize routing in softwarized IoT networks using Federated Deep Reinforcement Learning (FDRL), where distributed network softwarization and intelligence (i.e., FDRL) join forces to improve routing in constrained IoT networks. Our proposal introduces the combination of two novelties (i.e., distributed controller design and intelligent routing) to meet the IoT requirements (mainly performance and energy efficiency). The simulation results confirm the effectiveness of our proposal compared to the conventional counterparts."
2501.11574,"Co-existence of 5G New Radio (5G-NR) with IoT devices is considered as a promising technique to enhance the spectral usage and efficiency of future cellular networks. In this paper, a unified framework has been proposed for allocating in-band resource blocks (RBs), i.e., within a multi-cell network, to 5G-NR users in co-existence with NB-IoT and LTE-M devices. First, a benchmark (upper-bound) scheduler has been designed for joint sub-carrier (SC) and modulation and coding scheme (MCS) allocation that maximizes instantaneous throughput and fairness among users/devices, while considering synchronous RB allocation in the neighboring cells. A series of numerical simulations with realistic ICI in an urban scenario have been used to compute benchmark upper-bound solutions for characterizing performance in terms of throughput, fairness, and delay. Next, an edge learning based multi-agent deep reinforcement learning (DRL) framework has been developed for different DRL algorithms, specifically, a policy-based gradient network (PGN), a deep Q-learning based network (DQN), and an actor-critic based deep deterministic policy gradient network (DDPGN). The proposed DRL framework depends on interference allocation, where the actions are based on inter-cell-interference (ICI) instead of power, which can bypass the need for raw data sharing and/or inter-agent communication. The numerical results reveal that the interference allocation based DRL schedulers can significantly outperform their counterparts, where the actions are based on power allocation. Further, the performance of the proposed policy-based edge learning algorithms is close to the centralized ones."
2501.11984,"Long-range frequency-hopping spread spectrum (LR-FHSS) promises to enhance network capacity by integrating frequency hopping into existing Long Range Wide Area Networks (LoRaWANs). Due to its simplicity and scalability, LR-FHSS has generated significant interest as a potential candidate for direct-to-satellite IoT (D2S-IoT) applications. This paper explores methods to improve the reliability of data transfer on the uplink (i.e., from terrestrial IoT nodes to satellite) of LR-FHSS D2S-IoT networks.Because D2S-IoT networks are expected to support large numbers of potentially uncoordinated IoT devices per satellite, acknowledgment-cum-retransmission-aided reliability mechanisms are not suitable due to their lack of scalability. We therefore leverage message-replication, wherein every application-layer message is transmitted multiple times to improve the probability of reception without the use of receiver acknowledgments. We propose two message-replication schemes. One scheme is based on conventional replication, where multiple replicas of a message are transmitted, each as a separate link-layer frame. In the other scheme, multiple copies of a message is included in the payload of a single link-layer frame. We show that both techniques improve LR-FHSS reliability. Which method is more suitable depends on the network's traffic characteristics. We provide guidelines to choose the optimal method."
2501.11994,"The Single Carrier-Frequency Division Multiple Access (SC-FDMA) is a transmission technique used in the uplink of Long Term Evolution (LTE) and 5G systems, as it is characterized by reduced transmitted signal envelope fluctuations in comparison to Orthogonal Frequency Division Multiplexing (OFDM) technique used in the downlink. This allows for higher energy efficiency of User Equipments (UEs) while maintaining sufficient signal quality, measured by Error Vector Magnitude (EVM), at the transmitter. This paper proposes to model a nonlinear Power Amplifier (PA) influence while optimizing the transmit power in order to maximize the Signal to Noise and Distortion power Ratio (SNDR) at the receiver, removing the transmitter-based EVM constraint. An analytic model of SNDR for the OFDM system and a semi-analytical model for the SC-FDMA system are provided. Numerical investigations show that the proposed transmit power optimization allows for improved signal quality at the receiver for both OFDM and SC-FDMA systems. However, SC-FDMA still outperforms OFDM in this matter. Such a power amplifier-aware wireless transmitter optimization should be considered to boost the performance and sustainability of next-generation wireless systems, including Internet of Things (IoT) ones."
2501.12033,"Today, the rapid growth of applications reliant on datacenters calls for new advancements to meet the increasing traffic and computational demands. Traffic traces from datacenters are essential for further development and optimization of future datacenters. However, traces are rarely released to the public. Researchers often use simplified mathematical models that lack the depth needed to recreate intricate traffic patterns and, thus, miss optimization opportunities found in realistic traffic. In this preliminary work, we introduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on the generative pre-trained transformer (GPT) architecture used by many state-of-the-art large language models. We train our model on a small set of available traffic traces from different domains and offer a simple methodology to evaluate the fidelity of the generated traces to their original counterparts. We show that DTG-GPT can synthesize novel traces that mimic the spatiotemporal patterns found in real traffic traces. We further demonstrate that DTG-GPT can generate traces for networks of different scales while maintaining fidelity. Our findings indicate the potential that, in the future, similar models to DTG-GPT will allow datacenter operators to release traffic information to the research community via trained GPT models."
2501.12304,"The increasing number of wireless communication technologies and standards bring immense opportunities and challenges to provide seamless connectivity in Hybrid Vehicular Networks (HVNs). HVNs could not only enhance existing applications but could also spur an array of new services. However, due to sheer number of use cases and applications with diverse and stringent QoS performance requirements it is very critical to efficiently decide on which radio access technology (RAT) to select. In this paper a QoS aware RAT selection algorithm is proposed for HVN. The proposed algorithm switches between IEEE 802.11p based ad hoc network and LTE cellular network by considering network load and application's QoS requirements. The simulation-based studies show that the proposed RAT selection mechanism results in lower number of Vertical Handovers (VHOs) and significant performance improvements in terms of packet delivery ratio, latency and application-level throughput."
2501.12317,"Vehicular communication networks represent both an opportunity and a challenge for providing smart mobility services by using a hybrid solution that relies on cellular connectivity and short range communications. The evaluation of this kind of network is overwhelmingly carried out in the present literature with simulations. However, the degree of realism of the results obtained is limited because simulations simplify real world interactions too much in many cases. In this article, we define an outdoor testbed to evaluate the performance of short range vehicular communications by using real world personal portable devices (smartphones, tablets, and laptops), two different PHY standards (IEEE 802.11g and IEEE 802.11a), and vehicles. Our test results on the 2.4 GHz band show that smartphones can be used to communicate vehicles within a range up to 75 m, while tablets can attain up to 125 m in mobility conditions. Moreover, we observe that vehicles equipped with laptops exchange multimedia information with nodes located further than 150 m. The communications on the 5 GHz band achieved an effective transmission range of up to 100 m. This, together with the optimization of the protocols used, could take our commodity lightweight devices to a new realm of use in the next generation of ad hoc mobility communications for moving through the city."
2501.12502,"In the evolving landscape of wireless communications, semantic communication (SemCom) has recently emerged as a 6G enabler that prioritizes the transmission of meaning and contextual relevance over conventional bit-centric metrics. However, the deployment of SemCom systems in industrial settings presents considerable challenges, such as high radio frequency interference (RFI), that can adversely affect system performance. To address this problem, in this work, we propose a novel approach based on integrating sequence spreading techniques with SemCom to enhance system robustness against such adverse conditions and enable scalable multi-user (MU) SemCom. In addition, we propose a novel signal refining network (SRN) to refine the received signal after despreading and equalization. The proposed network eliminates the need for computationally intensive end-to-end (E2E) training while improving performance metrics, achieving a 25% gain in BLEU score and a 12% increase in semantic similarity compared to E2E training using the same bandwidth."
2501.12589,"The deployment of LoRa networks necessitates joint performance optimization, including packet delivery rate, energy efficiency, and throughput. Additionally, multiple LoRa parameters for packet transmission must be dynamically configured to tailor the performance metrics prioritization across varying channel environments. Because of the coupling relationship between LoRa parameters and metrics, existing works have opted to focus on certain parameters or specific metrics to circumvent the intricate coupling relationship, leading to limited adaptability. Therefore, we propose D-LoRa, a distributed parameter adaptation scheme, based on reinforcement learning towards network performance. We decompose the joint performance optimization problem into multiple independent Multi-Armed Bandit (MAB) problems with different reward functions. We have also built a comprehensive analytical model for the LoRa network that considers path loss, quasi-orthogonality of spreading factor, and packet collision. Experimental results show that our scheme can increase packet delivery rate by up to 28.8% and demonstrates superior adaptability across different performance metrics."
2501.12656,"On-ramp merging presents a critical challenge in autonomous driving, as vehicles from merging lanes need to dynamically adjust their positions and speeds while monitoring traffic on the main road to prevent collisions. To address this challenge, we propose a novel merging control scheme based on reinforcement learning, which integrates lateral control mechanisms. This approach ensures the smooth integration of vehicles from the merging lane onto the main road, optimizing both fuel efficiency and passenger comfort. Furthermore, we recognize the impact of vehicle-to-vehicle (V2V) communication on control strategies and introduce an enhanced protocol leveraging Cellular Vehicle-to-Everything (C-V2X) Mode 4. This protocol aims to reduce the Age of Information (AoI) and improve communication reliability. In our simulations, we employ two AoI-based metrics to rigorously assess the protocol's effectiveness in autonomous driving scenarios. By combining the NS3 network simulator with Python, we simulate V2V communication and vehicle control simultaneously. The results demonstrate that the enhanced C-V2X Mode 4 outperforms the standard version, while the proposed control scheme ensures safe and reliable vehicle operation during on-ramp merging."
2501.12659,"Modern telecommunication networks face an increasing complexity due to the rapidly growing number of networked devices and rising amounts of data. The literature advocates for self-managing networks as a means to tackle the resulting challenges. While self-managing networks provide potential solutions to these challenges, current research solely focuses on the perspective of network operators. However, modern telecommunication networks involve various stakeholders, such as service providers and end users, and necessitate interactions between them. By transitioning from a single-stakeholder to a multi-stakeholder perspective, we address the preferences of all involved parties, acknowledging potential conflicts of interest and constraints like information asymmetries. This broader perspective facilitates the development of more effective self-managing networks, significantly enhancing their performance metrics compared to approaches that solely prioritize the concerns of network operators."
2501.12783,"Serverless computing adopts a pay-as-you-go billing model where applications are executed in stateless and shortlived containers triggered by events, resulting in a reduction of monetary costs and resource utilization. However, existing platforms do not provide an upper bound for the billing model which makes the overall cost unpredictable, precluding many organizations from managing their budgets. Due to the diverse ranges of serverless functions and the heterogeneous capacity of edge devices, it is challenging to receive near-optimal solutions for deployment cost in a polynomial time. In this paper, we investigated the function scheduling problem with a budget constraint for serverless computing in wireless networks. Users and IoT devices are sending requests to edge nodes, improving the latency perceived by users. We propose two online scheduling algorithms based on reinforcement learning, incorporating several important characteristics of serverless functions. Via extensive simulations, we justify the superiority of the proposed algorithm by comparing with an ILP solver (Midaco). Our results indicate that the proposed algorithms efficiently approximate the results of Midaco within a factor of 1.03 while our decision-making time is 5 orders of magnitude less than that of Midaco."
2501.12792,"While Time-Sensitive Networking (TSN) enhances the determinism, real-time capabilities, and reliability of Ethernet, future industrial networks will not only use wired but increasingly wireless communications. Wireless networks enable mobility, have lower costs, and are easier to deploy. However, for many industrial applications, wired connections remain the preferred choice, particularly those requiring strict latency bounds and ultra-reliable data flows, such as for controlling machinery or managing power electronics. The emergence of 5G, with its Ultra-Reliable Low-Latency Communication (URLLC) promises to enable high data rates, ultra-low latency, and minimal jitter, presenting a new opportunity for wireless industrial networks. However, as 5G networks include wired links from the base station towards the core network, a combination of 5G with time-sensitive networking is needed to guarantee stringent QoS requirements. In this paper, we evaluate 5G-TSN performance for different indoor factory applications and environments through simulations. Our findings demonstrate that 5G-TSN can address latency-sensitive scenarios in indoor factory environments."
2501.12829,"This study proposes a novel approach for dynamic load balancing in Software-Defined Networks (SDNs) using a Transformer-based Deep Q-Network (DQN). Traditional load balancing mechanisms, such as Round Robin (RR) and Weighted Round Robin (WRR), are static and often struggle to adapt to fluctuating traffic conditions, leading to inefficiencies in network performance. In contrast, SDNs offer centralized control and flexibility, providing an ideal platform for implementing machine learning-driven optimization strategies. The core of this research combines a Temporal Fusion Transformer (TFT) for accurate traffic prediction with a DQN model to perform real-time dynamic load balancing. The TFT model predicts future traffic loads, which the DQN uses as input, allowing it to make intelligent routing decisions that optimize throughput, minimize latency, and reduce packet loss. The proposed model was tested against RR and WRR in simulated environments with varying data rates, and the results demonstrate significant improvements in network performance. For the 500MB data rate, the DQN model achieved an average throughput of 0.275 compared to 0.202 and 0.205 for RR and WRR, respectively. Additionally, the DQN recorded lower average latency and packet loss. In the 1000MB simulation, the DQN model outperformed the traditional methods in throughput, latency, and packet loss, reinforcing its effectiveness in managing network loads dynamically. This research presents an important step towards enhancing network performance through the integration of machine learning models within SDNs, potentially paving the way for more adaptive, intelligent network management systems."
2501.13138,"While technologies such as Time-Sensitive Networking (TSN) improve deterministic behaviour, real-time functionality, and robustness of Ethernet, future industrial networks aim to be increasingly wireless. While wireless networks facilitate mobility, reduce cost, and simplify deployment, they do not always provide stringent latency constraints and highly dependable data transmission as required by many manufacturing systems. The advent of 5G, with its Ultra-Reliable Low-Latency Communication (URLLC) capabilities, offers potential for wireless industrial networks. 5G offers elevated data throughput, very low latency, and negligible jitter. As 5G networks typically include wired connections from the base station to the core network, integration of 5G with time-sensitive networking is essential to provide rigorous QoS standards. This paper assesses the scalability of 5G-TSN for various indoor factory applications and conditions using OMNET++ simulation. Our research shows that 5G-TSN has the potential to provide bounded delay for latency-sensitive applications in scalable indoor factory settings."
2501.13144,"Wireless physical layer assessment, such as measuring antenna radiation patterns, is complex and cost-intensive. Researchers often require a stationary setup with antennas surrounding the device under test. There remains a need for more cost-effective and open-source platforms that facilitate such research, particularly in automated testing contexts. This paper introduces the Gimbal-based platform for Wireless Evaluation (GWEn), a lightweight multi-axis positioner designed to portably evaluate wireless systems in real-world scenarios with minimal RF interference. We present an evaluation workflow that utilizes GWEn and show how it supports different types of wireless devices and communication systems, including Ultra-wideband, mmWave, and acoustic communication. GWEn is open-source, combining 3D-printed components with off-the-shelf parts, thus allowing researchers globally to replicate, utilize, and adapt the system according to their specific needs."
2501.1328,"Today's Low Earth Orbit (LEO) satellite networks, exemplified by SpaceX's Starlink, play a crucial role in delivering global internet access to millions of users. However, managing the dynamic and expansive nature of these networks poses significant challenges in designing optimal satellite topologies over time. In this paper, we introduce the \underline{D}ynamic Time-Expanded Graph (DTEG)-based \underline{O}ptimal \underline{T}opology \underline{D}esign (DoTD) algorithm to tackle these challenges effectively. We first formulate a novel space network topology optimization problem encompassing a multi-objective function -- maximize network capacity, minimize latency, and mitigate link churn -- under key inter-satellite link constraints. Our proposed approach addresses this optimization problem by transforming the objective functions and constraints into a time-dependent scoring function. This empowers each LEO satellite to assess potential connections based on their dynamic performance scores, ensuring robust network performance over time without scalability issues. Additionally, we provide proof of the score function's boundary to prove that it will not approach infinity, thus allowing each satellite to consistently evaluate others over time. For evaluation purposes, we utilize a realistic Mininet-based LEO satellite emulation tool that leverages Starlink's Two-Line Element (TLE) data. Comparative evaluation against two baseline methods -- Greedy and $+$Grid, demonstrates the superior performance of our algorithm in optimizing network efficiency and resilience."
2501.13431,"This paper investigates the trade-off between throughput and peak age of information (PAoI) outage probability in a multi-sensor information collection system. Each sensor monitors a physical process, periodically samples its status, and transmits the updates to a central access point over a shared radio resource. The trade-off arises from the interplay between each sensor's sampling frequency and the allocation of the shared resource. To optimize this trade-off, we formulate a joint optimization problem for each sensor's sampling delay and resource allocation, aiming to minimize a weighted sum of sampling delay costs (representing a weighted sum of throughput) while satisfying PAoI outage probability exponent constraints. We derive an optimal solution and particularly propose a closed-form approximation for large-scale systems. This approximation provides an explicit expression for an approximately optimal trade-off, laying a foundation for designing resource-constrained systems in applications that demand frequent updates and also stringent statistical timeliness guarantees."
2501.13508,"The safe and swift evacuation of passengers from Maritime Vessels, requires an effective Internet of Things(IoT) as well as an information and communication technology(ICT) infrastructure. However, during emergencies, delays in IoT and ICT systems that guide evacuees, can impair the evacuation process. This paper presents explores the impact of the key IoT and ICT elements. The methodology builds upon the deadline-aware adaptive navigation strategy (ANT), which offers the path segment that minimizes the evacuation time for each evacuee at each decision instant. The simulations on a real cruise ship configuration, show that delays in the delivery of correct instructions to evacuees can significantly hinder the effectiveness of the evacuation. Our findings stress the need to design robust and computationally fast IoT and ICT systems to support the evacuation of passengers in ships, and underscores the key role played by the IoT in the success of passenger evacuation and safety."
2501.1357,"Today's high-speed switches employ an on-chip shared packet buffer. The buffer is becoming increasingly insufficient as it cannot scale with the growing switching capacity. Nonetheless, the buffer needs to face highly intense bursts and meet stringent performance requirements for datacenter applications. This imposes rigorous demand on the Buffer Management (BM) scheme, which dynamically allocates the buffer across queues. However, the de facto BM scheme, designed over two decades ago, is ill-suited to meet the requirements of today's network. In this paper, we argue that shallow-buffer switches, intense bursts, along with dynamic traffic call for a highly agile BM that can quickly adjust the buffer allocation as traffic changes. However, the agility of the current BM is fundamentally limited by its non-preemptive nature. Nonetheless, we find that preemptive BM, considered unrealizable in history, is now feasible on modern switch chips. We propose Occamy, a preemptive BM that can quickly adjust buffer allocation. Occamy utilizes the redundant memory bandwidth to actively reclaim and reallocate the over-allocated buffer. Testbed experiments and large-scale simulations show that Occamy can improve the end-to-end performance by up to ~55%."
2501.13671,"The work proposes a new Combined Routing Protocol (CRP) for ad hoc networks that combines the benefits and annihilates the shortcomings of two well-known on-demand routing protocols in ad hoc networks: AODV (which provides a high probability of successfully discovering and maintaining a reliable route) and GPSR (which enables fast on-the-fly transmission based on the geographical coordinates of the destination node). The main idea of the new routing scheme applied in CRP is to use AODV protocol as a solution to the ""perimeter problem"" of GPSR. And vice versa we apply GPSR for moving the starting point of the AODV route discovering closer to the destination point, decreasing the number of hops and route building time, making the resultant route more stable. As the key result we see decreasing of the average packet delivery time in ad hoc networks with is extremely important for latency-critical applications, such as video streaming or command traffic."
2501.13716,"The rapid expansion of connected devices has amplified the need for robust and scalable security frameworks. This paper proposes a holistic approach to securing network-connected devices, covering essential layers: hardware, firmware, communication, and application. At the hardware level, we focus on secure key management, reliable random number generation, and protecting critical assets. Firmware security is addressed through mechanisms like cryptographic integrity validation and secure boot processes. For secure communication, we emphasize TLS 1.3 and optimized cipher suites tailored for both standard and resource-constrained devices. To overcome the challenges of IoT, compact digital certificates, such as CBOR, are recommended to reduce overhead and enhance performance. Additionally, the paper explores forward-looking solutions, including post-quantum cryptography, to future-proof systems against emerging threats. This framework provides actionable guidelines for manufacturers and system administrators to build secure devices that maintain confidentiality, integrity, and availability throughout their lifecycle."
2501.13744,"An important choice in the design of satellite networks is whether the routing decisions are made in a distributed manner onboard the satellite, or centrally on a ground-based controller. We study the tradeoff between centralized and distributed routing in large-scale satellite networks. In particular, we consider a centralized routing scheme that has access to global but delayed network state information and a distributed routing scheme that has access to local but real-time network state information. For both routing schemes, we analyze the throughput and delay performance of shortest-path algorithms in networks with and without buffers onboard the satellites. We show that distributed routing outperforms centralized routing when the rate of changes in the network link state is comparable to the inherent propagation and transmission delays. In particular, we show that in highly dynamic networks without buffers, the distributed scheme achieves higher throughput than a centralized scheme. In networks with buffers, the distributed scheme achieves lower delays with the same throughput."
2501.13843,"One of the main operational challenges faced by the operators of one-way car-sharing systems is to ensure vehicle availability across the regions of the service areas with uneven patterns of rental requests. Fleet balancing strategies are required to maximise the demand served while minimising the relocation costs. However, the design of optimal relocation policies is a complex problem, and global optimisation solutions are often limited to very small network sizes for computational reasons. In this work, we propose a multi-stage decision support system for vehicle relocation that decomposes the general relocation problem into three independent decision stages to allow scalable solutions. Furthermore, we adopt a rolling horizon control strategy to cope with demand uncertainty. Our approach is highly modular and flexible, and we leverage it to design user-based, operator-based and robotic relocation schemes. Besides, we formulate the relocation problem considering both conventional cars and a new class of compact stackable vehicles that can be driven in a road train. We compare the proposed relocation schemes with two recognised benchmarks using a large data set of taxi trips in New York. Our results show that our approach is scalable and outperforms the benchmark schemes in terms of quality of service, vehicle utilisation and relocation efficiency. Furthermore, we find that stackable vehicles can achieve a relocation performance close to that of autonomous cars, even with a small workforce of relocators."
2501.14117,"Future applications such as intelligent vehicles, the Internet of Things and holographic telepresence are already highlighting the limits of existing fifth-generation (5G) mobile networks. These limitations relate to data throughput, latency, reliability, availability, processing, connection density and global coverage, whether terrestrial, submarine or space-based. To remedy this, research institutes have begun to look beyond IMT2020, and as a result, 6G should provide effective solutions to 5G shortcomings. 6G will offer high quality of service and energy efficiency to meet the demands of future applications that are unimaginable to most people. In this article, we present the future applications and services promised by 6G."
2501.14205,"Large Language Models (LLMs) can perform zero-shot learning on unseen tasks and few-shot learning on complex reasoning tasks. However, resource-limited mobile edge networks struggle to support long-context LLM serving for LLM agents during multi-round interactions with users. Unlike stateless computation offloading and static service offloading in edge computing, optimizing LLM serving at edge servers is challenging because LLMs continuously learn from context which raises accuracy, latency, and resource consumption dynamics. In this paper, we propose a joint model caching and inference offloading framework that utilizes test-time deep reinforcement learning (T2DRL) to optimize deployment and execution strategies for long-context LLM serving. In this framework, we analyze the performance convergence and design an optimization problem considering the utilization of context windows in LLMs. Furthermore, the T2DRL algorithm can learn in both the training phase and the testing phase to proactively manage cached models and service requests and adapt to context changes and usage patterns during execution. To further enhance resource allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which dynamically matches supply and demand while maximizing social welfare. Finally, experimental results demonstrate that the T2DRL algorithm can reduce system costs by at least 30% compared to baselines while guaranteeing the performance of LLM agents in real-world perception and reasoning tasks."
2501.14367,"Mobile crowdsensing (MCS) networks enable large-scale data collection by leveraging the ubiquity of mobile devices. However, frequent sensing and data transmission can lead to significant resource consumption. To mitigate this issue, edge caching has been proposed as a solution for storing recently collected data. Nonetheless, this approach may compromise data freshness. In this paper, we investigate the trade-off between re-using cached task results and re-sensing tasks in cache-enabled MCS networks, aiming to minimize system latency while maintaining information freshness. To this end, we formulate a weighted delay and age of information (AoI) minimization problem, jointly optimizing sensing decisions, user selection, channel selection, task allocation, and caching strategies. The problem is a mixed-integer non-convex programming problem which is intractable. Therefore, we decompose the long-term problem into sequential one-shot sub-problems and design a framework that optimizes system latency, task sensing decision, and caching strategy subproblems. When one task is re-sensing, the one-shot problem simplifies to the system latency minimization problem, which can be solved optimally. The task sensing decision is then made by comparing the system latency and AoI. Additionally, a Bayesian update strategy is developed to manage the cached task results. Building upon this framework, we propose a lightweight and time-efficient algorithm that makes real-time decisions for the long-term optimization problem. Extensive simulation results validate the effectiveness of our approach."
2501.14387,"To support the growing demand for data-intensive and low-latency IoT applications, Multi-Access Edge Computing (MEC) is emerging as an effective edge-computing approach enabling the execution of delay-sensitive processing tasks close to end-users. However, most of the existing works on resource allocation and service placement in MEC systems overlook the unique characteristics of new IoT use cases. For instance, many IoT applications require the periodic execution of computing tasks on real-time data streams that originate from devices dispersed over a wide area. Thus, users requesting IoT services are typically distant from the data producers. To fill this gap, the contribution of this work is two-fold. Firstly, we propose a MEC-compliant architectural solution to support the operation of multiple IoT service providers over a common MEC platform deployment, which enables the steering and shaping of IoT data transport within the platform. Secondly, we model the problem of service placement and data management in the proposed MEC-based solution taking into account the dependencies at the data level between IoT services and sensing resources. Our model also considers that caches can be deployed on MEC hosts, to allow the sharing of the same data between different IoT services with overlapping geographical scope, and provides support for IoT services with heterogeneous QoS requirements, such as different frequencies of periodic task execution. Due to the complexity of the optimisation problem, a heuristic algorithm is proposed using linear relaxation and rounding techniques. Extensive simulation results demonstrate the efficiency of the proposed approach, especially when traffic demands generated by the service requests are not uniform."
2501.14389,"Accurate Probability of Line-of-Sight (PLoS) modeling is important in evaluating the performance of Unmanned Aerial Vehicle (UAV)-based communication systems in urban environments, where real-time communication and low latency are often major requirements. Existing PLoS models often rely on simplified Manhattan grid layouts using International Telecommunication Union (ITU)-defined built-up parameters, which may not reflect the randomness of real cities. Therefore, this paper introduces the Urban LoS Simulator (ULS) to model PLoS for three random city layouts with varying building sizes and shapes constructed using ITU built-up parameters. Based on the ULS simulated data, we obtained the empirical PLoS for four standard urban environments across three different city layouts. Finally, we analyze how well Manhattan grid-based models replicate PLoS results from random and real-world layouts, providing insights into their applicability for time-critical communication systems in urban IoT networks."
2501.14411,"Path Loss (PL) is vital to evaluate the performance of Unmanned Aerial Vehicles (UAVs) as Aerial Base Stations (ABSs), particularly in urban environments with complex propagation due to various obstacles. Accurately modeling PL requires a generalized Probability of Line-of-Sight (PLoS) that can consider multiple obstructions. While the existing PLoS models mostly assume a simplified Manhattan grid with uniform building sizes and spacing, they overlook the real-world variability in building dimensions. Furthermore, such models do not consider other obstacles, such as trees and streetlights, which may also impact the performance, especially in millimeter-wave (mmWave) bands. This paper introduces a Manhattan Random Simulator (MRS) to estimate PLoS for UAV-based communications in urban areas by incorporating irregular building shapes, non-uniform spacing, and additional random obstacles to create a more realistic environment. Lastly, we present the PL differences with and without obstacles for standard urban environments and derive the empirical PL for these environments."
2501.14552,"The IMT-2030 framework provides the vision and conceptual foundation for the next-generation of mobile broadband systems, colloquially known as Sixth-Generation (6G) cellular networks. Academic circles, industry players, and Standard Developing Organizations (SDOs) are already engaged in early standardization discussions for the system, providing key insights for future technical specifications. In this context, a structured thematic synthesis aligned with IMT-2030 is essential to inform the discussions and assist collaboration among 6G stakeholders -- including scholars, professionals, regulators, and SDO officials. This review intends to offer a concise yet informative synthesis of well-established 6G literature, viewed through the IMT-2030 lens, for both specialists and generalists engaged in shaping future standards and advancing 6G research."
2501.14619,"Open Radio Access Network (O-RAN) is transforming the telecommunications landscape by enabling flexible, intelligent, and multi-vendor networks. Central to its architecture are xApps hosted on the Near-Real-Time RAN Intelligent Controller (Near-RT RIC), which optimize network functions in real time. However, the concurrent operation of multiple xApps with conflicting objectives can lead to suboptimal performance. This paper introduces a generalized Conflict Management scheme for Multi-Channel Power Control in O-RAN xApps (COMIX), designed to detect and resolve conflicts between xApps. To demonstrate COMIX, we focus on two Deep Reinforcement Learning (DRL)-based xApps for power control: one maximizes the data rare across UEs, and the other optimizes system-level energy efficiency. COMIX employs a standardized Conflict Mitigation Framework (CMF) for conflict detection and resolution and leverages the Network Digital Twin (NDT) to evaluate the impact of conflicting actions before applying them to the live network. We validate the framework using a realistic multi-channel power control scenario under various conflict resolution policies, demonstrating its effectiveness in balancing antagonistic objectives. Our results highlight significant network energy savings achieved through the conflict management scheme compared to baseline CMF-free methods."
2501.14878,"Vehicular Edge Computing (VEC) is a key research area in autonomous driving. As Intelligent Transportation Systems (ITSs) continue to expand, ground vehicles (GVs) face the challenge of handling huge amounts of sensor data to drive safely. Specifically, due to energy and capacity limitations, GVs will need to offload resource-hungry tasks to external (cloud) computing units for faster processing. In 6th generation (6G) wireless systems, the research community is exploring the concept of Non-Terrestrial Networks (NTNs), where satellites can serve as space edge computing nodes to aggregate, store, and process data from GVs. In this paper we propose new data offloading strategies between a cluster of GVs and satellites in the Low Earth Orbits (LEOs), to optimize the trade-off between coverage and end-to-end delay. For the accuracy of the simulations, we consider real data and orbits from the Starlink constellation, one of the most representative and popular examples of commercial satellite deployments for communication. Our results demonstrate that Starlink satellites can support real-time offloading under certain conditions that depend on the onboard computational capacity of the satellites, the frame rate of the sensors, and the number of GVs."
2501.14967,"Artificial intelligence (AI) has become a pivotal force in reshaping next generation mobile networks. Edge computing holds promise in enabling AI as a service (AIaaS) for prompt decision-making by offloading deep neural network (DNN) inference tasks to the edge. However, current methodologies exhibit limitations in efficiently offloading the tasks, leading to possible resource underutilization and waste of mobile devices' energy. To tackle these issues, in this paper, we study AIaaS at the edge and propose an efficient offloading mechanism for renowned DNN architectures like ResNet and VGG16. We model the inference tasks as directed acyclic graphs and formulate a problem that aims to minimize the devices' energy consumption while adhering to their latency requirements and accounting for servers' capacity. To effectively solve this problem, we utilize a transformer DNN architecture. By training on historical data, we obtain a feasible and near-optimal solution to the problem. Our findings reveal that the proposed transformer model improves energy efficiency compared to established baseline schemes. Notably, when edge computing resources are limited, our model exhibits an 18\% reduction in energy consumption and significantly decreases task failure compared to existing works."
2501.15064,"Traceroutes and geolocation are two essential network measurement tools that aid applications such as network mapping, topology generation, censorship, and Internet path analysis. However, these tools, individually and when combined, have significant limitations that can lead to inaccurate results. Prior research addressed specific issues with traceroutes and geolocation individually, often requiring additional measurements. In this paper, we introduce GeoTrace, a lightweight tool designed to identify, classify, and resolve geolocation anomalies in traceroutes using existing data. GeoTrace leverages the abundant information in traceroutes and geolocation databases to identify anomalous IP addresses with incorrect geolocation. It systematically classifies these anomalies based on underlying causes - such as MPLS effects or interface discrepancies - and refines their geolocation estimates where possible. By correcting these inaccuracies, GeoTrace enhances the reliability of traceroute-based analyses without the need for additional probing. Our work offers a streamlined solution that enhances the accuracy of geolocation in traceroute analysis, paving the way for more reliable measurement studies."
2501.151,"The rapid development of programmable network devices and the widespread use of machine learning (ML) in networking have facilitated efficient research into intelligent data plane (IDP). Offloading ML to programmable data plane (PDP) enables quick analysis and responses to network traffic dynamics, and efficient management of network links. However, PDP hardware pipeline has significant resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in each stage, and lacks support for multiplication, division and floating-point operations. These constraints significantly hinder the development of IDP. This paper presents \quark, a framework that fully offloads convolutional neural network (CNN) inference onto PDP. \quark employs model pruning to simplify the CNN model, and uses quantization to support floating-point operations. Additionally, \quark divides the CNN into smaller units to improve resource utilization on the PDP. We have implemented a testbed prototype of \quark on both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2). Extensive evaluation results demonstrate that \quark achieves 97.3\% accuracy in anomaly detection task while using only 22.7\% of the SRAM resources on the Intel Tofino ASIC switch, completing inference tasks at line rate with an average latency of 42.66$\mu s$."
2501.15164,"Mobile edge computing (MEC) is a promising technology to meet the increasing demands and computing limitations of complex Internet of Things (IoT) devices. However, implementing MEC in urban environments can be challenging due to factors like high device density, complex infrastructure, and limited network coverage. Network congestion and connectivity issues can adversely affect user satisfaction. Hence, in this article, we use unmanned aerial vehicle (UAV)-assisted collaborative MEC architecture to facilitate task offloading of IoT devices in urban environments. We utilize the combined capabilities of UAVs and ground edge servers (ESs) to maximize user satisfaction and thereby also maximize the service provider's (SP) profit. We design IoT task-offloading as joint IoT-UAV-ES association and UAV-network topology optimization problem. Due to NP-hard nature, we break the problem into two subproblems: offload strategy optimization and UAV topology optimization. We develop a Three-sided Matching with Size and Cyclic preference (TMSC) based task offloading algorithm to find stable association between IoTs, UAVs, and ESs to achieve system objective. We also propose a K-means based iterative algorithm to decide the minimum number of UAVs and their positions to provide offloading services to maximum IoTs in the system. Finally, we demonstrate the efficacy of the proposed task offloading scheme over benchmark schemes through simulation-based evaluation. The proposed scheme outperforms by 19%, 12%, and 25% on average in terms of percentage of served IoTs, average user satisfaction, and SP profit, respectively, with 25% lesser UAVs, making it an effective solution to support IoT task requirements in urban environments using UAV-assisted MEC architecture."
2501.15182,"Cost-efficient and low-power sensing nodes enable to monitor various physical environments. Some of these impose extreme operating conditions, subjecting the nodes to excessive heat or rainfall or motion. Rough operating conditions affect the stability of the wireless links the nodes establish and cause a significant amount of packet loss. Adaptive transmission power control (ATPC) enables nodes to adapt to extreme conditions and maintain stable wireless links and often rely on knowledge of the received power as a closed-feedback system to adjust the power of outgoing packets. However, in the presence of a significant packet loss, this knowledge may not reflect the current state of the receiver. In this paper we propose a lightweight n-step predictor which enables transmitters to adapt transmission power in the presence of lost packets. Through extensive practical deployments and testing we demonstrate that the predictor avoids expensive computation and still achieves an average prediction accuracy exceeding 90% with a low-power radio that supports a transmission rate of 250 kbps (CC2538) and 85\% with a low-power radio that supports 50 kbps (CC1200)."
2501.15687,"In this work we study the problem of user association and resource allocation to maximize the proportional fairness of a wireless network with limited backhaul capacity. The optimal solution of this problem requires solving a mixed integer non-linear programming problem which generally cannot be solved in real time. We propose instead to model the problem as a potential game, which decreases dramatically the computational complexity and obtains a user association and resource allocation close to the optimal solution. Additionally, the use of a game-theoretic approach allows an efficient distribution of the computational burden among the computational resources of the network."
2501.15734,"Network slicing aims to enhance flexibility and efficiency in next-generation wireless networks by allocating the right resources to meet the diverse requirements of various applications. Managing these slices with machine learning (ML) algorithms has emerged as a promising approach however explainability has been a challenge. To this end, several Explainable Artificial Intelligence (XAI) frameworks have been proposed to address the opacity in decision-making in many ML methods. In this paper, we propose a Prioritized Value-Decomposition Network (PVDN) as an XAI-driven approach for resource allocation in a multi-agent network slicing system. The PVDN method decomposes the global value function into individual contributions and prioritizes slice outputs, providing an explanation of how resource allocation decisions impact system performance. By incorporating XAI, PVDN offers valuable insights into the decision-making process, enabling network operators to better understand, trust, and optimize slice management strategies. Through simulations, we demonstrate the effectiveness of the PVDN approach with improving the throughput by 67% and 16%, while reducing latency by 35% and 22%, compared to independent and VDN-based resource allocation methods."
2501.15853,"The high energy footprint of 5G base stations, particularly the radio units (RUs), poses a significant environmental and economic challenge. We introduce Kairos, a novel approach to maximize the energy-saving potential of O-RAN's Advanced Sleep Modes (ASMs). Unlike state-of-the-art solutions, which often rely on complex ASM selection algorithms unsuitable for time-constrained base stations and fail to guarantee stringent QoS demands, Kairos offers a simple yet effective joint ASM selection and radio scheduling policy capable of real-time operation. This policy is then optimized using a data-driven algorithm within an xApp, which enables several key innovations: (i) a dimensionality-invariant encoder to handle variable input sizes (e.g., time-varying network slices), (ii) distributional critics to accurately model QoS metrics and ensure constraint satisfaction, and (iii) a single-actor-multiple-critic architecture to effectively manage multiple constraints. Through experimental analysis on a commercial RU and trace-driven simulations, we demonstrate Kairos's potential to achieve energy reductions ranging between 15% and 72% while meeting QoS requirements, offering a practical solution for cost- and energy-efficient 5G networks."
2501.15855,"This paper presents a game theoretic solution for end-to-end channel and power allocation in multihop cognitive radio networks analyzed under the physical interference model. The objective is to find a distributed solution that maximizes the number of flows that can be established in the network. The problem is addressed through three different games: a local flow game which uses complete information about the links of the flow, a potential flow game requiring global network knowledge and a cooperative link game based on partial information regarding the links of the flow. Results show that the proposed link game highly decreases the complexity of the channel and power allocation problem in terms of computational load, reducing the information shared between the links forming each flow with a performance similar to that of the more complex flow games."
2501.15928,"Lyapunov optimization theory has recently emerged as a powerful mathematical framework for solving complex stochastic optimization problems by transforming long-term objectives into a sequence of real-time short-term decisions while ensuring system stability. This theory is particularly valuable in unmanned aerial vehicle (UAV)-based low-altitude economy (LAE) networking scenarios, where it could effectively address inherent challenges of dynamic network conditions, multiple optimization objectives, and stability requirements. Recently, generative artificial intelligence (GenAI) has garnered significant attention for its unprecedented capability to generate diverse digital content. Extending beyond content generation, in this paper, we propose a framework integrating generative diffusion models with reinforcement learning to address Lyapunov optimization problems in UAV-based LAE networking. We begin by introducing the fundamentals of Lyapunov optimization theory and analyzing the limitations of both conventional methods and traditional AI-enabled approaches. We then examine various GenAI models and comprehensively analyze their potential contributions to Lyapunov optimization. Subsequently, we develop a Lyapunov-guided generative diffusion model-based reinforcement learning framework and validate its effectiveness through a UAV-based LAE networking case study. Finally, we outline several directions for future research."
2501.16143,"Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models. In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost. Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment. However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices. Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications. To address this issue, we formulate a multi-objective integer linear programming (ILP) problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity. At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice co-location constraints. Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation. We integrate the proposed approach as a custom plugin of the Kubernetes (K8s) scheduler. Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the benchmark schemes, while ensuring QoS requirements are consistently met."
2501.16236,"The Ethereum network, built on the devp2p protocol stack, was designed to function as a ""world computer"" by supporting decentralized applications through a shared P2P infrastructure. However, the proliferation of blockchain forks has increased network diversity, complicating node discovery and reducing efficiency. Ethereum mainnet nodes cannot easily distinguish between peers from different blockchains until after establishing an expensive TCP connection, encryption, and protocol handshake. This inefficiency is further worsened by client diversity, where differences in software implementations cause protocol incompatibilities and connection failures. This paper introduces a monitoring tool that tracks devp2p message exchanges and client statuses to analyze connection dynamics and protocol variations. Our findings highlight issues such as inefficiencies in node discovery and client incompatibility, including timeouts in Geth during the discovery process. The study emphasizes the need to consider chain and client diversity when assessing the health and performance of the post-merge Ethereum network."
2501.16502,"Open Radio Access Networks (RANs) leverage disaggregated and programmable RAN functions and open interfaces to enable closed-loop, data-driven radio resource management. This is performed through custom intelligent applications on the RAN Intelligent Controllers (RICs), optimizing RAN policy scheduling, network slicing, user session management, and medium access control, among others. In this context, we have proposed dApps as a key extension of the O-RAN architecture into the real-time and user-plane domains. Deployed directly on RAN nodes, dApps access data otherwise unavailable to RICs due to privacy or timing constraints, enabling the execution of control actions within shorter time intervals. In this paper, we propose for the first time a reference architecture for dApps, defining their life cycle from deployment by the Service Management and Orchestration (SMO) to real-time control loop interactions with the RAN nodes where they are hosted. We introduce a new dApp interface, E3, along with an Application Protocol (AP) that supports structured message exchanges and extensible communication for various service models. By bridging E3 with the existing O-RAN E2 interface, we enable dApps, xApps, and rApps to coexist and coordinate. These applications can then collaborate on complex use cases and employ hierarchical control to resolve shared resource conflicts. Finally, we present and open-source a dApp framework based on OpenAirInterface (OAI). We benchmark its performance in two real-time control use cases, i.e., spectrum sharing and positioning in a 5th generation (5G) Next Generation Node Base (gNB) scenario. Our experimental results show that standardized real-time control loops via dApps are feasible, achieving average control latency below 450 microseconds and allowing optimal use of shared spectral resources."
2501.16734,"The growing complexity of network traffic and demand for ultra-low latency communication require smarter packet traffic management. Existing Deep Learning-based queuing approaches struggle with dynamic network scenarios and demand high engineering effort. We propose AQM-LLM, distilling Large Language Models (LLMs) with few-shot learning, contextual understanding, and pattern recognition to improve Active Queue Management (AQM) [RFC 9330] with minimal manual effort. We consider a specific case where AQM is Low Latency, Low Loss, and Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative decoding and reinforcement-based distilling of LLM by tackling congestion prevention in the L4S architecture using Explicit Congestion Notification (ECN) [RFC 9331] and periodic packet dropping. We develop a new open-source experimental platform by executing L4S-AQM on FreeBSD-14, providing interoperable modules to support LLM integration and facilitate IETF recognition through wider testing. Our extensive evaluations show L4S-LLM enhances queue management, prevents congestion, reduces latency, and boosts network performance, showcasing LLMs' adaptability and efficiency in uplifting AQM systems."
2501.16752,"Modern large-scale data centers are known for their engineering complexity, cooling, and oversubscription challenges. To mitigate these issues, this article proposes the implementation of community data centers that are closer to consumers as part of the data center ecosystem. Having a community data center can reduce latency, minimize network burden on Internet Service Providers (ISPs), utilize full computing capability, available during disaster events, and simplify the engineering complexity associated with traditional data centers. In addition to that, this article explores one technical design for such a community data center and the business strategy for operating community data centers."
2501.16805,"Spoofed traffic has been identified as one of the main issues of concern for network hygiene nowadays, as it facilitates Distributed Denial-of-Service (DDoS) attacks by hiding their origin and complicating forensic investigations. Some indicators of poor network hygiene are packets with Bogon or Martian source addresses representing either misconfigurations or spoofed packets. Despite the development of Source Address Validation (SAV) techniques and guidelines such as BCP 38 and BCP 84, Bogons are often overlooked in the filtering practices of network operators. This study uses traceroute measurements from the CAIDA Ark dataset, enriched with historical BGP routing information from RIPE RIS and RouteViews, to investigate the prevalence of Bogon addresses over seven years (2017-2023). Our analysis reveals widespread non-compliance with best practices, with Bogon traffic detected across thousands of ASes. Notably, 82.69%-97.83% of CAIDA Ark vantage points observe paths containing Bogon IPs, primarily RFC1918 addresses. Additionally, 19.70% of all analyzed traceroutes include RFC1918 addresses, while smaller proportions involve RFC6598 (1.50%) and RFC3927 (0.10%) addresses. We identify more than 13,000 unique ASes transiting Bogon traffic, with only 11.64% appearing in more than half of the measurements. Cross-referencing with the Spoofer project and MANRS initiatives shows a concerning gap: 62.67% of ASes that do not filter packets with Bogon sources are marked as non-spoofable, suggesting incomplete SAV implementation. Our contributions include an assessment of network hygiene using the transiting of Bogon packets as a metric, an analysis of the main types of Bogon addresses found in traceroutes, and several proposed recommendations to address the observed gaps, enforcing the need for stronger compliance with best practices to improve global network security."
2501.16842,"Leverage large language model (LLM) to refer the fault is considered to be a potential solution for intelligent network fault diagnosis. However, how to represent network information in a paradigm that can be understood by LLMs has always been a core issue that has puzzled scholars in the field of network intelligence. To address this issue, we propose LLM-based Network Semantic Generation (LNSG) algorithm, which integrates semanticization and symbolization methods to uniformly describe the entire multi-modal network information. Based on the LNSG and LLMs, we present NetSemantic, a plug-and-play, data-independent, network information semantic fault diagnosis framework. It enables rapid adaptation to various network environments and provides efficient fault diagnosis capabilities. Experimental results demonstrate that NetSemantic excels in network fault diagnosis across various complex scenarios in a zero-shot manner."
2501.1689,"This paper presents a game theoretic solution for joint channel allocation and power control in cognitive radio networks analyzed under the physical interference model. The objective is to find a distributed solution that maximizes the network utility, defined with different criteria, with limited information. The problem is addressed through a non-cooperative game based on local information. Although the existence of a pure Nash Equilibrium cannot be assured for this game, simulation results show that it exists with high probability and with a performance similar to that of a potential game, where each player requires overall network information. The obtained results are compared with a centralized heuristic genetic algorithm to show the correctness of the proposals. From this point, utility functions for the local game are modified to restrict the transmitted power to drive the solution to a more cooperative approach. To overcome the convergence limitations of the local game, no-regret learning algorithms are used to perform the joint channel and power allocation. These algorithms provide stable mixed strategies in any scenario with even better global performance. This opens an interesting perspective to develop realistic protocols based on the modeled interactions and increases the adaptability to perform efficient opportunistic spectrum access."
2501.16907,"Open optical networks have been considered to be important for cost-effectively building and operating the networks. Recently, the optical-circuit-switches (OCSes) have attracted industry and academia because of their cost efficiency and higher capacity than traditional electrical packet switches (EPSes) and reconfigurable optical add drop multiplexers (ROADMs). Though the open interfaces and control planes for traditional ROADMs and transponders have been defined by several standard-defining organizations (SDOs), those of OCSes have not. Considering that several OCSes have already been installed in production datacenter networks (DCNs) and several OCS products are on the market, bringing the openness and interoperability into the OCS-based networks has become important. Motivated by this fact, this paper investigates a software-defined networking (SDN) controller for open optical-circuit-switched networks. To this end, we identified the use cases of OCSes and derived the controller requirements for supporting them. We then proposed a multi-vendor (MV) OCS controller framework that satisfies the derived requirements; it was designed to quickly and consistently operate fiber paths upon receiving the operation requests. We validated our controller by implementing it and evaluating its performance on actual MV-OCS networks. It satisfied all the requirements, and fiber paths could be configured within 1.0 second by using our controller."
2501.17014,"Advanced Air Mobility (AAM) is transforming transportation systems by extending them into near-ground airspace, offering innovative solutions to mobility challenges. In this space, electric vertical take-off and landing vehicles (eVTOLs) perform a variety of tasks to improve aviation safety and efficiency, such as collaborative computing and perception. However, eVTOLs face constraints such as compacted shape and restricted onboard computing resources. These limitations necessitate task offloading to nearby high-performance base stations (BSs) for timely processing. Unfortunately, the high mobility of eVTOLs, coupled with their restricted flight airlines and heterogeneous resource management creates significant challenges in dynamic task offloading. To address these issues, this paper introduces a novel network slice-based Low-Altitude Intelligent Network (LAIN) framework for eVTOL tasks. By leveraging advanced network slicing technologies from 5G/6G, the proposed framework dynamically adjusts communication bandwidth, beam alignment, and computing resources to meet fluctuating task demands. Specifically, the framework includes an access pairing method to pre-schedule optimal eVTOL-BS-slice assignments, a pre-assessment algorithm to avoid resource waste, and a deep reinforcement learning-based slice orchestration mechanism to optimize resource allocation and lifecycle management. Simulation results demonstrate that the proposed framework outperforms existing benchmarks in terms of resource allocation efficiency and operational/violation costs across varying eVTOL velocities. This work provides valuable insights into intelligent network slicing for future AAM transportation systems."
2501.17126,"The Cloud-Edge continuum enhances application performance by bringing computation closer to data sources. However, it presents considerable challenges in managing resources and determining service placement, as these tasks require navigating diverse, dynamic environments characterised by fluctuating network conditions. Addressing these challenges calls for tools combining simulation and emulation of Cloud-Edge systems to rigorously assess novel application and resource management strategies. In this paper, we introduce ECLYPSE, a Python-based framework that enables the simulation and emulation of the Cloud-Edge continuum via adaptable resource allocation and service placement models. ECLYPSE features an event-driven architecture for dynamically adapting network configurations and resources. It also supports seamless transitions between simulated and emulated setups. In this work, ECLYPSE capabilities are illustrated over three use cases, showing how the framework supports rapid prototyping across diverse experimental settings."
2501.17127,"P4TG is a hardware-based traffic generator (TG) running on the Intel Tofino 1 ASIC and was programmed using the programming language P4. In its initial version, P4TG could generate up to 10x100 Gb/s of traffic and directly measure rates, packet loss, and other metrics in the data plane. Many researchers and industrial partners requested new features to be incorporated into P4TG since its publication in 2023. With the recently added features, P4TG supports the generation of packets encapsulated with a customizable VLAN, QinQ, VxLAN, MPLS, and SRv6 header. Further, generation of IPv6 traffic is added and P4TG is ported to the Intel Tofino 2 platform enabling a generation capability of up to 10x400 Gb/s. The improvement in user experience focuses on ease of operation. Features like automated ARP replies, improved visualization, report generation, and automated testing based on the IMIX distribution and RFC 2544 are added. Future work on P4TG includes NDP to facilitate IPv6 traffic, and a NETCONF integration to further ease the configuration."
2501.17271,"Data plane programming enables the programmability of network devices with domain-specific programming languages, like P4. One commonly used P4-programmable hardware target is the Intel Tofino switching ASIC. The runtime behavior of an implemented P4 program on Tofino can be configured with shell scripts or a Python library from Barefoot provided with the Tofino. Both are limited in their capabilities and usability. This paper introduces the Rust Barefoot Runtime (RBFRT), a Rust-based control plane library. The RBFRT provides a fast and memory-safe interface to configure the Intel Tofino. We showed that the RBFRT achieves a higher insertion rate for MAT entries and has a shorter response time compared to the Python library."
2501.17331,"This paper addresses the challenge of optimizing handover (HO) performance in non-terrestrial networks (NTNs) to enhance user equipment (UE) effective service time, defined as the active service time excluding HO delays and radio link failure (RLF) periods. Availability is defined as the normalized effective service time which is effected by different HO scenarios: Intra-satellite HO is the HO from one beam to another within the same satellite; inter-satellite HO refers to the HO from one satellite to another where satellites can be connected to the same or different GSs. We investigate the impact of open radio access network (O-RAN) functional splits (FSs) between ground station (GS) and LEO satellites on HO delay and assess how beam configurations affect RLF rates and intra- and inter-satellite HO rates. This work focuses on three O-RAN FSs -- split 7.2x (low layer 1 functions on the satellite), split 2 (layer 1 and layer 2 functions on the satellite), and gNB onboard the satellite -- and two beam configurations (19-beam and 127-beam). In a realistic dynamic LEO satellite constellation where different types of HO scenarios are simulated, we maximize effective service time by tuning the time-to-trigger (TTT) and HO margin (HOM) parameters. Our findings reveal that the gNB onboard the satellite achieves the highest availability, approximately 95.4%, while the split 7.2x exhibits the lowest availability, around 92.8% due to higher intra-satellite HO delays."
2501.17532,"We address the problem of inferring the topology of a wireless network using limited observational data. Specifically, we assume that we can detect when a node is transmitting, but no further information regarding the transmission is available. We propose a novel network estimation procedure grounded in the following abstract problem: estimating the parameters of a finite discrete-time Markov chain by observing, at each time step, which states are visited by multiple ``anonymous'' copies of the chain. We develop a consistent estimator that approximates the transition matrix of the chain in the operator norm, with the number of required samples scaling roughly linearly with the size of the state space. Applying this estimation procedure to wireless networks, our numerical experiments demonstrate that the proposed method accurately infers network topology across a wide range of parameters, consistently outperforming transfer entropy, particularly under conditions of high network congestion."
2501.17684,"The Internet of Batteryless Things revolutionizes sustainable communication as it operates on harvested energy. This harvested energy is dependent on unpredictable environmental conditions; therefore, device operations, including those of its networking stack, must be resilient to power failures. Reactive intermittent computing provides an approach for solving this by notifications of impending power failures, which is implemented by monitoring the harvested energy buffered in a capacitor. However, to use this power-failure notification and guarantee forward progress, systems must break down tasks into atomic transactions that can be predictably finished before the energy runs out. Thus, static program-code analysis must determine the worst-case energy consumption (WCEC) of all transactions. In Wi-Fi-capable devices, drivers are often closed-source, which avoids the determination of WCEC bounds for transactions since static analysis requires all code along with its semantics.In this work, we integrate an energy-aware networking stack with reverse-engineered Wi-Fi drivers to enable full-stack WCEC analysis for physical transmission and reception of packets. Further, we extended a static worst-case analysis tool with a resource-consumption model of our Wi-Fi driver. Our evaluation with the RISC-V-based ESP32-C3 platform gives worst-case bounds with our static analysis approach for the transactions of the full communication stack, therefore showing that Wi-Fi-based reactive intermittent computing is feasible."
2501.17964,"The growing demands of ultra-reliable and low-latency communication (URLLC) in 5G networks necessitate enhanced resilience mechanisms to address user plane failures caused by outages, hardware defects, or software bugs. An important aspect for achieving ultra-reliable communication is the redundant transmission of packets, as also highlighted in 3GPP Release 18. This paper explores leveraging the Packet Replication, Elimination, and Ordering Function (PREOF) to achieve 1+1 path protection within private 5G environments. By extending existing 5G components with mechanisms for packet level redundancy and offloading the reordering mechanism to external servers, the proposed approach ensures minimal packet loss in case of a failure. A conceptual integration of redundant paths and programmable elements is presented, with considerations for deployment in existing 5G infrastructures and the trade-offs of latency versus enhanced traffic engineering. Future work aims to evaluate practical implementations using an open source 5G core, P4-based hardware and offloading technologies like DPDK and eBPF."
2501.18136,We present an optical circuit switch design for programmable integrated photonics (PIPs). Our solution finds the correct and optimal set of matchings that provides all-to-all network connectivity and demonstrates scalability to 32 ports.
2501.18169,"We present a rack-scale compute architecture for ML using multi-accelerator servers connected via chip-to-chip silicon photonic components. Our architecture achieves (1) multi-tenanted resource slicing without fragmentation, (2) 74% faster rack-scale collective communication, and (3) 1.7X speedup in end-to-end ML training throughput."
2501.18332,"The increase in video streaming has presented a challenge of handling stream request effectively, especially over networks that are variable. This paper describes a new adaptive video streaming architecture capable of changing the video quality and buffer size depending on the data and latency of streamed video. For video streaming VLC media player was used where network performance data were obtained through Python scripts with very accurate data rate and latency measurement. The collected data is analyzed using Gemini AI, containing characteristics of the machine learning algorithm that recognizes the best resolution of videos and the buffer sizes. Through the features of real-time monitoring and artificial intelligence decision making, the proposed framework improves the user experience by reducing the occurrence of buffering events while at the same time increasing the video quality. Our findings therefore confirm that the proposed solution based on artificial intelligence increases video quality and flexibility. This study advances knowledge of adaptive streaming and offers an argument about how intelligent datadriven approaches and AI may be useful tools for enhancing the delivery of video in practical environments."
2501.18339,"This paper investigates the impact of reactive jamming on LoRaWAN networks, focusing on showing that LoRaWAN communications can be effectively disrupted with minimal jammer exposure time. The susceptibility of LoRa to jamming is assessed through a theoretical study of how the frame success rate is impacted by only a few jamming symbols. Different jamming approaches are studied, among which repeated-symbol jamming appears to be the most disruptive, with sufficient jamming power. A key contribution of this work is the proposal of a software-defined radio (SDR)-based jamming approach implemented on GNU Radio that generates a controlled number of random symbols, independent of the standard LoRa frame structure. This approach enables precise control over jammer exposure time and provides flexibility in studying the effect of jamming symbols on network performance. The theoretical analysis is validated through experimental results, where the implemented jammer is used to assess the impact of jamming under various configurations. Our findings demonstrate that LoRa-based networks can be disrupted with a minimal number of symbols, emphasizing the need for future research on stealthy communication techniques to counter such jamming attacks."
2501.1884,"Federated testbeds enable collaborative research by providing access to diverse resources, including computing power, storage, and specialized hardware like GPUs, programmable switches and smart Network Interface Cards (NICs). Efficiently sharing these resources across federated institutions is challenging, particularly when resources are scarce and costly. GPUs are crucial for AI and machine learning research, but their high demand and expense make efficient management essential. Similarly, advanced experimentation on programmable data plane requires very expensive programmable switches (e.g., based on P4) and smart NICs.This paper introduces SHARY (SHaring Any Resource made easY), a dynamic reservation system that simplifies resource booking and management in federated environments. We show that SHARY can be adopted for heterogenous resources, thanks to an adaptation layer tailored for the specific resource considered. Indeed, it can be integrated with FIGO (Federated Infrastructure for GPU Orchestration), which enhances GPU availability through a demand-driven sharing model. By enabling real-time resource sharing and a flexible booking system, FIGO improves access to GPUs, reduces costs, and accelerates research progress. SHARY can be also integrated with SUP4RNET platform to reserve the access of P4 switches."
2501.19051,"Elastic computing enables dynamic scaling to meet workload demands, and Remote Direct Memory Access (RDMA) enhances this by providing high-throughput, low-latency network communication. However, integrating RDMA into elastic computing remains a challenge, particularly in control plane operations for RDMA connection setup.This paper revisits the assumptions of prior work on high-performance RDMA for elastic computing, and reveals that extreme microsecond-level control plane optimizations are often unnecessary. By challenging the conventional beliefs on the slowness of user-space RDMA control plane and the difficulty of user-space RDMA resource sharing, we uncover new design opportunities. Our key insight is that user-space RDMA connection setup can be significantly improved with caching, while RDMA resources can be efficiently shared among processes using fork. In light of this, we propose Swift, a simple yet effective solution that co-designs RDMA with a serverless framework to optimize performance for elastic computing. At its very core, Swift handles cold and warm serverless requests by swiftly initializing the RDMA control plane with cache-optimized libibverbs, and manages fork requests by leveraging the RDMA's fork capability. Implemented with OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and 18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared to prior solutions."
2501.19109,"Accurate reliability modeling for ultra-reliable low latency communication (URLLC) and hyper-reliable low latency communication (HRLLC) networks is challenging due to the complex interactions between network layers required to meet stringent requirements. In this paper, we propose such a model. We consider the acknowledged mode of the radio link control (RLC) layer, utilizing separate buffers for transmissions and retransmissions, along with the behavior of physical channels. Our approach leverages the effective capacity (EC) framework, which quantifies the maximum constant arrival rate a time-varying wireless channel can support while meeting statistical quality of service (QoS) constraints. We derive a reliability model that incorporates delay violations, various latency components, and multiple transmission attempts. Our method identifies optimal operating conditions that satisfy URLLC/HRLLC constraints while maintaining near-optimal EC, ensuring the system can handle peak traffic with a guaranteed QoS. Our model reveals critical trade-offs between EC and reliability across various use cases, providing guidance for URLLC/HRLLC network design for service providers and system designers."
2501.19167,"Measurement of available path capacity with high accuracy over high-speed links deployed in cloud and transport networks is vital for performance assessment and traffic engineering. Methods for measuring the available path capacity rely on sending and receiving time stamped probe packets. A requirement for accurate estimates of the available path capacity is the ability to generate probe packets at a desired rate and also time stamping with high precision and accuracy. This is challenging especially for measurement systems deployed using general purpose hardware. To touch upon the challenge this paper describes and evaluates four approaches for sending and receiving probe packets in high-speed networks (10+ Gbps). The evaluation shows that the baseline approach, based on the native UDP socket, is suitable for available path capacity measurements over links with capacities up to 2.5 Gbps. For higher capacities we show that an implementation based on Data Plane Development Kit (DPDK) gives good results up to 10 Gbps."
2501.19194,"Careful parametrization of networking protocols is crucial to maximize the performance of low-power wireless systems and ensure that stringent application requirements can be met. This is a non-trivial task involving thorough characterization on testbeds and requiring expert knowledge. Unfortunately, the community still lacks a tool to facilitate parameter exploration while minimizing the necessary experimentation time on testbeds. Such a tool would be invaluable, as exhaustive parameter searches can be time-prohibitive or unfeasible given the limited availability of testbeds, whereas non-exhaustive unguided searches rarely deliver satisfactory results. In this paper, we present APEX, a framework enabling an automated and informed parameter exploration for low-power wireless protocols and allowing to converge to an optimal parameter set within a limited number of testbed trials. We design APEX using Gaussian processes to effectively handle noisy experimental data and estimate the optimality of a certain parameter combination. After developing a prototype of APEX, we demonstrate its effectiveness by parametrizing two IEEE 802.15.4 protocols for a wide range of application requirements. Our results show that APEX can return the best parameter set with up to 10.6x, 4.5x and 3.25x less testbed trials than traditional solutions based on exhaustive search, greedy approaches, and reinforcement learning, respectively."
2501.19348,"Mobile devices have become essential for capturing human activity, and eXtended Data Records (XDRs) offer rich opportunities for detailed user behavior modeling, which is useful for designing personalized digital services. Previous studies have primarily focused on aggregated mobile traffic and mobility analyses, often neglecting individual-level insights. This paper introduces a novel approach that explores the dependency between traffic and mobility behaviors at the user level. By analyzing 13 individual features that encompass traffic patterns and various mobility aspects, we enhance the understanding of how these behaviors interact. Our advanced user modeling framework integrates traffic and mobility behaviors over time, allowing for fine-grained dependencies while maintaining population heterogeneity through user-specific signatures. Furthermore, we develop a Markov model that infers traffic behavior from mobility and vice versa, prioritizing significant dependencies while addressing privacy concerns. Using a week-long XDR dataset from 1,337,719 users across several provinces in Chile, we validate our approach, demonstrating its robustness and applicability in accurately inferring user behavior and matching mobility and traffic profiles across diverse urban contexts."
2502.00142,"RAN slicing technology is a key aspect of the Open RAN paradigm, allowing simultaneous and independent provision of various services such as ultra-reliable low-latency communications (URLLC), enhanced mobile broadband (eMBB), and massive machine-type communications (mMTC) through virtual networks that share a single radio access infrastructure. Efficient resource allocation is crucial for RAN slicing, as each service has specific quality of service (QoS) requirements, and a balance between different services must be maintained. Although heuristic and reinforcement learning (RL) techniques have been explored to achieve efficient resource allocation, these approaches face notable limitations: heuristic algorithms face complexity issues that limit their effectiveness in large networks, RL solutions are constrained by their dependency on training data and struggle to adapt to new scenarios and environments. This paper proposes a framework that leverages quantum optimization techniques to optimize radio resource blocks allocation in Open RAN slicing for URLLC and eMBB services. We provide a classical problem formulation and the quantum implementation using the constrained quadratic model on Dwave quantum annealing platform, showcasing the potential of quantum optimization techniques to deliver in real-time optimal solutions for optimization problems in 5G and beyond networks."
2502.00242,"As wireless network technology advances towards the sixth generation (6G), increasing network energy consumption has become a critical concern due to the growing demand for diverse services, radio deployments at various frequencies, larger bandwidths, and more antennas. Network operators must manage energy usage not only to reduce operational cost and improve revenue but also to minimize environmental impact by reducing the carbon footprint. The 3rd Generation Partnership Project (3GPP) has introduced several network energy savings (NES) features. However, the implementation details and system-level aspects of these features have not been thoroughly investigated. In this paper, we explore system-level resource optimization for network energy savings in low-traffic scenarios. We introduce multiple NES optimization formulations and strategies, and further analyze their performance using a detailed network digital twin. Our results demonstrate promising NES gains of up to 44%. Additionally, we provide practical considerations for implementing the proposed schemes and examine their impacts on user equipment (UE) operation."
2502.0043,"Over the years, advancements such as increased bandwidth, new modulation and coding schemes, frame aggregation, and the use of multiple antennas have been employed to enhance Wi-Fi performance. Nonetheless, as network density and the demand for low-latency applications increases, contention delays and retransmissions have become obstacles to efficient Wi-Fi deployment in modern scenarios. The introduction of Orthogonal Frequency-Division Multiple Access (OFDMA) in the IEEE 802.11 standard allows simultaneous transmissions to and from multiple users within the same transmission opportunity, thereby reducing the contention. However, the AP must efficiently manage the resource allocation, particularly in uplink scenarios where it lacks prior knowledge of users' buffer statuses, thus making polling a critical bottleneck in networks with a high number of users with sporadic traffic pattern. This paper addresses the polling problem and introduces the A2P algorithm, designed to enable scalable and efficient polling in high-density OFDMA-based time sensitive Wi-Fi networks. Simulation results show that A2P outperforms the alternative schemes by maintaining significantly lower delay and packet loss in dense time-sensitive teleconferencing scenario."
2502.00597,"The interconnection network is a key element in High-Performance Computing (HPC) and Datacenter (DC) systems whose performance depends on several design parameters, such as the topology, the switch architecture, and the routing algorithm. Among the most common topologies in HPC systems, the Fat-Tree offers several shortest-path routes between any pair of end-nodes, which allows multi-path routing schemes to balance traffic flows among the available links, thus reducing congestion probability. However, traffic balance cannot solve by itself some congestion situations that may still degrade network performance. Another approach to reduce congestion is queue-based flow separation, but our previous work shows that multi-path routing may spread congested flows across several queues, thus being counterproductive. In this paper, we propose a set of restrictions to improve alternative routes selection for multi-path routing algorithms in Fat-Tree networks, so that they can be positively combined with queuing schemes."
2502.00603,"In cellular networks, virtualized Radio Access Networks (vRANs) enable replacing traditional specialized hardware at cell sites with software running on commodity servers distributed across edge and remote clouds. However, some vRAN functions (e.g., forward error correction (FEC) decoding) require excessive edge compute resources due to their intensive computational demands and inefficiencies caused by workload fluctuations. This high demand for computational power significantly drives up the costs associated with edge computing, posing a major challenge for deploying 5G/6G vRAN solutions. To address this challenge, we propose Hades, a hierarchical architecture for vRAN that enables the distribution of uplink FEC decoding processing across edge and remote clouds. Hades refactors the vRAN stack and introduces mechanisms that allow controlling and managing the workload over these hierarchical cloud resources. More specifically, Hades splits the traditional non-stop run-to-completion iterative FEC decoding process into latency-critical early decoding iterations, i.e., related to MAC processing and early pre-parsing for content identification, and completion decoding iterations, i.e., decoding tasks with larger decoding delay budgets for final data bits extraction. This partitioning provides Hades the flexibility to utilize the available midhaul (MH) network for offloading the latency tolerant part of decoding to remote cloud instances, while performing time-sensitive decoding at the edge cloud locations for low-delay processing. Hades controls decoding load distribution between the edge and remote clouds, based on the edge decoding capacity and the offload network bandwidth, thus improving the utilization of edge compute."
2502.00616,"The interconnection network is a crucial subsystem in High-Performance Computing clusters and Data-centers, guaranteeing high bandwidth and low latency to the applications' communication operations. Unfortunately, congestion situations may spoil network performance unless the network design applies specific countermeasures. Adaptive routing algorithms are a traditional approach to dealing with congestion since they provide traffic flows with alternative routes that bypass congested areas. However, adaptive routing decisions at switches are typically based on local information without a global network traffic perspective, leading to congestion spreading throughout the network beyond the original congested areas. In this paper, we propose a new efficient congestion management strategy that leverages adaptive routing notifications currently available in some interconnect technologies and efficiently isolates the congesting flows in reserved spaces at switch buffers. The experiment results based on simulations of realistic traffic scenarios show that our proposal removes the congestion impact."
2502.00671,"In this demonstration, we showcase POSMAC1, a platform designed to deploy Decision Tree (DT) and Random Forest (RF) models on the NVIDIA DOCA DPU, equipped with an ARM processor, for real-time network traffic classification. Developed specifically for Augmented Reality (AR) and Cloud Gaming (CG) traffic classification, POSMAC streamlines model evaluation, and generalization while optimizing throughput to closely match line rates."
2502.00715,"Open Radio Access Network (O-RAN) offers an open, programmable architecture for next-generation wireless networks, enabling advanced control through AI-based applications on the near-Real-Time RAN Intelligent Controller (near-RT RIC). However, fully integrated, real-time demonstrations of closed-loop optimization in O-RAN remain scarce. In this paper, we present a complete framework that combines the O-RAN Software Community RIC (OSC RIC) with srsRAN for near-real-time network slicing using Reinforcement Learning (RL). Our system orchestrates resources across diverse slice types (eMBB, URLLC, mMTC) for up to 12 UEs. We incorporate GNU Radio blocks for channel modeling, including Free-Space Path Loss (FSPL), single-tap multipath, AWGN, and Doppler effects, to emulate an urban mobility scenario. Experimental results show that our RL-based xApps dynamically adapt resource allocation and maintain QoS under varying traffic demands, highlighting both the feasibility and challenges of end-to-end AI-driven optimization in a lightweight O-RAN testbed. Our findings establish a baseline for real-time RL-based slicing in a disaggregated 5G framework and underscore the need for further enhancements to support fully simulated PHY digital twins without reliance on commercial software."
2502.00785,"Virtual Reality (VR) technology demands real-time data transmission to deliver an immersive and interactive user experience. This study investigates the implementation of UDP Ethernet communication in VR systems, focusing on its impact on network performance. Experiments were conducted to analyze how factors such as cable length, data rate, and packet processing rate (PPR) influence system performance. A series of tests were performed, and the results were visualized through detailed graphs. The findings reveal how variations in these parameters affect communication speed and stability, providing insights for optimizing VR system design. By leveraging the high-speed, low-overhead advantages of UDP Ethernet, this study may contribute understanding of network performance in VR applications, offering practical guidance for developers and engineers in creating responsive and efficient VR environments."
2502.00787,The ongoing growth of the need for superior Internet services creates great pressure on the ISPs as to the accurate estimation of network upgrade need.
2502.00789,Software Defined Networking or SDN is an architectural approach to managing the network where the control and forwarding are different planes that are controlled through an application interface.
2502.01049,"Existing network simulations often rely on simplistic models that send packets at random intervals, failing to capture the critical role of application-level behaviour. This paper presents a statistical approach that extracts and models application behaviour using probability density functions to generate realistic network simulations. By convolving learned application patterns, the framework produces dynamic, scalable traffic representations that closely mimic real-world networks. The method enables rigorous testing of network monitoring tools and anomaly detection systems by dynamically adjusting application behaviour. It is lightweight, capable of running multiple emulated applications on a single machine, and scalable for analysing large networks where real data collection is impractical. To encourage adoption and further testing, the full code is provided as open-source, allowing researchers and practitioners to replicate and extend the framework for diverse network environments."
2502.01089,"This paper investigates a range of cutting-edge technologies and architectural innovations aimed at simplifying network operations, reducing operational expenditure (OpEx), and enabling the deployment of new service models. The focus is on (i) Proposing novel, more efficient 6G architectures, with both Control and User planes enabling the seamless expansion of services, while addressing long-term 6G network evolution. (ii) Exploring advanced techniques for constrained artificial intelligence (AI) operations, particularly the design of AI agents for real-time learning, optimizing energy consumption, and the allocation of computational resources. (iii) Identifying technologies and architectures that support the orchestration of backend services using serverless computing models across multiple domains, particularly for vertical industries. (iv) Introducing optically-based, ultra-high-speed, low-latency network architectures, with fast optical switching and real-time control, replacing conventional electronic switching to reduce power consumption by an order of magnitude."
2502.01193,"Despite their widespread adoption, cellular networks face growing vulnerabilities due to their inherent complexity and the integration of advanced technologies. One of the major threats in this landscape is Voice over IP (VoIP) to GSM gateways, known as SIMBox devices. These devices use multiple SIM cards to route VoIP traffic through cellular networks, enabling international bypass fraud with losses of up to $3.11 billion annually. Beyond financial impact, SIMBox activity degrades network performance, threatens national security, and facilitates eavesdropping on communications. Existing detection methods for SIMBox activity are hindered by evolving fraud techniques and implementation complexities, limiting their practical adoption in operatorthis http URLpaper addresses the limitations of current detection methods by introducing SigN , a novel approach to identifying SIMBox activity at the cellular edge. The proposed method focuses on detecting remote SIM card association, a technique used by SIMBox appliances to mimic human mobility patterns. The method detects latency anomalies between SIMBox and standard devices by analyzing cellular signaling during network attachment. Extensive indoor and outdoor experiments demonstrate that SIMBox devices generate significantly higher attachment latencies, particularly during the authentication phase, where latency is up to 23 times greater than that of standard devices. We attribute part of this overhead to immutable factors such as LTE authentication standards and Internet-based communication protocols. Therefore, our approach offers a robust, scalable, and practical solution to mitigate SIMBox activity risks at the network edge."
2502.01214,"The Dragonfly topology is currently one of the most popular network topologies in high-performance parallel systems. The interconnection networks of many of these systems are built from components based on the InfiniBand specification. However, due to some constraints in this specification, the available versions of the InfiniBand network controller (OpenSM) do not include routing engines based on some popular deadlock-free routing algorithms proposed theoretically for Dragonflies, such as the one proposed by Kim and Dally based on Virtual-Channel shifting. In this paper we propose a straightforward method to integrate this routing algorithm in OpenSM as a routing engine, explaining in detail the configuration required to support it. We also provide experiment results, obtained both from a real InfiniBand-based cluster and from simulation, to validate the new routing engine and to compare its performance and requirements against other routing engines currently available in OpenSM."
2502.0129,"This paper addresses the use of Multipath Transmission Control Protocol (MPTCP) in a single Radio Access Technology (RAT) network. Different from other studies where multiple RATs are explored by the MPTCP, a situation that cannot be always guaranteed, due to lack of coverage for example, in this work we assess and evaluate the capability of MPTCP to operate over a single RAT environment. With a vehicular network as use case, we show how the IEEE 802.11p interface is shared among the multiple logical links created between the On-Board Unit (OBU) and the several Road Side Units (RSUs) in its range, supporting the different MPTCP subflows. The results, obtained through experimentation with real vehicular networking hardware, show that MPTCP allows for seamless handovers, ensuring continuous, stable and efficient communication in highly mobile environments."
2502.01478,"Digital agriculture technologies rely on sensors, drones, robots, and autonomous farm equipment to improve farm yields and incorporate sustainability practices. However, the adoption of such technologies is severely limited by the lack of broadband connectivity in rural areas. We argue that farming applications do not require permanent always-on connectivity. Instead, farming activity and digital agriculture applications follow seasonal rhythms of agriculture. Therefore, the need for connectivity is highly localized in time and space. We introduce BYON, a new connectivity model for high bandwidth agricultural applications that relies on emerging connectivity solutions like citizens broadband radio service (CBRS) and satellite networks. BYON creates an agile connectivity solution that can be moved along a farm to create spatio-temporal connectivity bubbles. BYON incorporates a new gateway design that reacts to the presence of crops and optimizes coverage in agricultural settings. We evaluate BYON in a production farm and demonstrate its benefits."
2502.01537,"Vehicular ad hoc networks (VANETs) provide the communications required to deploy Intelligent Transportation Systems (ITS). In the current state of the art in this field there is a lack of studies on real outdoor experiments to validate thenew VANETs protocols and applications proposed by designers. In this work we have addressed the definition of a testbed in order to study the performance of the Vehicular Data Transfer Protocol (VDTP) in a real urban VANET. The VDTP protocol has been tested by employing six different parameter settings: one defined by human experts and five automatically optimized by means of metaheuristic algorithms (PSO, DE, GA, ES, and SA). As a result, we have been able to confirm the performance improvements when optimized VDTP configurations are used, validating the results previously obtained through simulation."
2502.01807,"Virtual Network Embedding (VNE) is a technique for mapping virtual networks onto a physical network infrastructure, enabling multiple virtual networks to coexist on a shared physical network. Previous works focused on implementing centralized VNE algorithms, which suffer from lack of scalability and robustness. This project aims to implement a decentralized virtual network embedding algorithm that addresses the challenges of network virtualization, such as scalability, single point of failure, and DoS attacks. The proposed approach involves selecting L leaders from the physical nodes and embedding a virtual network request (VNR) in the local network of each leader using a simple algorithm like BFS. The algorithm then uses a leader-election mechanism for determining the node with the lowest cost and highest revenue and propagates the embedding to other leaders. By utilizing decentralization, we improve the scalability and robustness of the solution. Additionally, we evaluate the effectiveness of our fully decentralized algorithm by comparing it with existing approaches. Our algorithm performs $12\%$ better in terms of acceptance rate and improves the revenue-to-cost ratio by roughly $21\%$ to compared approaches."
2502.01826,"Synthesizing radio-frequency (RF) data given the transmitter and receiver positions, e.g., received signal strength indicator (RSSI), is critical for wireless networking and sensing applications, such as indoor localization. However, it remains challenging due to complex propagation interactions, including reflection, diffraction, and scattering. State-of-the-art neural radiance field (NeRF)-based methods achieve high-fidelity RF data synthesis but are limited by long training times and high inference latency. We introduce GSRF, a framework that extends 3D Gaussian Splatting (3DGS) from the optical domain to the RF domain, enabling efficient RF data synthesis. GSRF realizes this adaptation through three key innovations: First, it introduces complex-valued 3D Gaussians with a hybrid Fourier-Legendre basis to model directional and phase-dependent radiance. Second, it employs orthographic splatting for efficient ray-Gaussian intersection identification. Third, it incorporates a complex-valued ray tracing algorithm, executed on RF-customized CUDA kernels and grounded in wavefront propagation principles, to synthesize RF data in real time. Evaluated across various RF technologies, GSRF preserves high-fidelity RF data synthesis while achieving significant improvements in training efficiency, shorter training time, and reduced inference latency."
2502.01843,"Join-the-shortest queue (JSQ) and its variants have often been used in solving load balancing problems. The aim of such policies is to minimize the average system occupation, e.g., the customer's system time. In this work we extend the traditional load balancing setting to include constraints that may be imposed, e.g., due to the communication network. We cast the problem into the framework of constrained MDPs, enabling the consideration of both action-dependent constraints, such as, e.g, bandwidth limitation, and state-dependent constraints, such as, e.g., minimum queue utilization. Unlike the state-of-the-art approaches, our load-balancing policies, in particular JSED-$k$ and JSSQ, are both provably safe and yet strive to minimize the system occupancy. Their performance is tested with extensive numerical results under various system settings."
2502.01964,"Generating and distributing remote entangled pairs (EPs) is a primary function of quantum networks, as entanglement is the fundamental resource for key quantum network applications. A critical performance metric for quantum networks is the time-to-serve (TTS) for users' EP requests, which is the time to distribute EPs between the requested nodes. Minimizing the TTS is essential given the limited qubit coherence time. In this paper, we study the Adaptive Continuous entanglement generation Protocol (ACP), which enables quantum network nodes to continuously generate EPs with their neighbors, while adaptively selecting the neighbors to optimize TTS. Meanwhile, entanglement purification is used to mitigate decoherence in pre-generated EPs prior to the arrival of user requests. We extend the SeQUeNCe simulator to fully implement ACP and conduct extensive simulations across various network scales. Our results show that ACP reduces TTS by up to 94% and increases entanglement fidelity by up to 0.05."
2502.02087,"The introduction of silicon chipsets with the capability of processing incoming optical packet traffic, creates a new generation of packet-optical nodes, the whiteboxes. Their inherent functionality of carrying pluggable coherent transceiver modules, extends their scope in the field of transport optical networks. Also, their hybrid nature improves the overall efficiency since higher layer functionality is now performed at wire speed. This is feasible by embedding in the computational logic, apart from the typical packet inspection routines and security features, Machine Learning-based traffic engineering as well. In this work, a topology based on multiple packet-optical nodes residing at the edges (ingress and egress) of a ROADM network, is evaluated according to the average laser frequency configuration time. This is achieved by exploiting telemetry analytics which are collected by the CMIS driver of the pluggable transceivers, for allocating efficient frequency slots to the source and destination of connectivity requests traversing through the ROADM network. This pair of nodes is supervised by the packet SDN controller which is part of the control plane of the optical transport network. This controller receives telemetry feedback from the whiteboxes and uses it to execute efficient ML techniques locally, for finding efficient frequency slots for the incoming transport requests. Next, it applies them to request's edge nodes. The decrease of the average laser configuration time is achieved in the evaluated topology, improving the overall efficiency of the control plane."
2502.02141,"To achieve ubiquitous connectivity of the sixth generation communication, the space-air-ground integrated network (SAGIN) is a popular topic. However, the dynamic nodes in SAGIN such as satellites and unmanned aerial vehicles, may be fragile and out of operation, which can potentially cause service failure. Therefore, the research on service recovery in SAGIN under situations of resource failure is critical. In order to facilitate the flexible resource utilization of SAGIN, the network function virtualization technology (NFV) is proposed to be employed. Firstly, the task management is transformed into the deployment of service function chains (SFCs). Then, we design an NFV-based SFC recovery model in SAGIN in the face of resource failure, so that tasks can quickly select alternative resources to complete deployments. Moreover, the problem of SFC recovery is formulated to minimize the total time consumption for all completed SFCs. Since it is an NP-hard integer linear programming problem, we propose the efficient recovery algorithm based on the matching game. Finally, via various simulations, the effectiveness of the proposed algorithm and its advantages are verified, where the total time consumption is optimized by about 25%, compared with other benchmark methods."
2502.0217,"Mobility performance has been a key focus in cellular networks up to 5G. To enhance handover (HO) performance, 3GPP introduced Conditional Handover (CHO) and Layer 1/Layer 2 Triggered Mobility (LTM) mechanisms in 5G. While these reactive HO strategies address the trade-off between HO failures (HOF) and ping-pong effects, they often result in inefficient radio resource utilization due to additional HO preparations. To overcome these challenges, this article proposes a proactive HO framework for mobility management in O-RAN, leveraging user-cell link predictions to identify the optimal target cell for HO. We explore various categories of Graph Neural Networks (GNNs) for link prediction and analyze the complexity of applying them to the mobility management domain. Two GNN models are compared using a real-world dataset, with experimental results demonstrating their ability to capture the dynamic and graph-structured nature of cellular networks. Finally, we present key insights from our study and outline future steps to enable the integration of GNN-based link prediction for mobility management in O-RAN networks."
2502.02253,"The maturity and commercial roll-out of 5G networks and its deployment for private networks makes 5G a key enabler for various vertical industries and applications, including robotics. Providing ultra-low latency, high data rates, and ubiquitous coverage and wireless connectivity, 5G fully unlocks the potential of robot autonomy and boosts emerging robotic applications, particularly in the domain of autonomous mobile robots. Ensuring seamless, efficient, and reliable navigation and operation of robots within a 5G network requires a clear understanding of the expected network quality in the deployment environment. However, obtaining real-time insights into network conditions, particularly in highly dynamic environments, presents a significant and practical challenge. In this paper, we present a novel framework for building a Network Digital Twin (NDT) using real-time data collected by robots. This framework provides a comprehensive solution for monitoring, controlling, and optimizing robotic operations in dynamic network environments. We develop a pipeline integrating robotic data into the NDT, demonstrating its evolution with real-world robotic traces. We evaluate its performances in radio-aware navigation use case, highlighting its potential to enhance energy efficiency and reliability for 5Genabled robotic operations."
2502.02564,"At the edge, there is a high level of similarity in computing. One approach that has been proposed to enhance the efficiency of edge computing is computation reuse, which eliminates redundant computations. Edge computing is integrated with the ICN architecture, capitalizing on its inherent intelligence to facilitate computation reuse and reduce redundancies in computing operations. In many past works, ICN's ability to enable computation reuse through caching has been limited. In this context, a new approach is proposed that considers computation requests with similar input data, which yield identical results, as equivalent. This method facilitates computation reuse through caching in ICN. The use of approximate results to reduce redundant computations without requiring high accuracy in input matching is provided. This concept is termed the Similarity Index, which effectively considers images to be similar despite minor changes in the angle of photography. The Similarity Index is determined through an algorithm known as HNSW and utilizes the SIFT descriptor to identify similar data. This approach helps reduce user latency times by providing quick access to results. The evaluation, simulated using the ndnSIM tool, showed an 86% improvement in completion time compared to scenarios without computation reuse, whereas previous works reported only a 70% improvement. To strengthen this method, an analytical model for computing request transfer considering computation reuse in ICN-based edge computing is provided. To assess the accuracy of the model, several evaluations have been conducted in the simulator by varying the parameters, resulting in a maximum error percentage of approximately 16%."
2502.02842,"5G networks support various advanced applications through network slicing, network function virtualization (NFV), and edge computing, ensuring low latency and service isolation. However, private 5G networks relying on open-source tools still face challenges in maturity and integration with edge/cloud platforms, compromising proper slice isolation. This study investigates resource allocation mechanisms to address this issue, conducting experiments in a hospital scenario with medical video conferencing. The results show that CPU limitations improve the performance of prioritized slices, while memory restrictions have minimal impact. The generated data and scripts have been made publicly available for future research and machine learning applications."
2502.02874,"Machine Learning (ML) has proven to be a promising solution to provide novel scalable and efficient fault management solutions in modern 5G-and-beyond communication networks. In the context of microwave networks, ML-based solutions have received significant attention. However, current solutions can only be applied to monolithic scenarios in which a single entity (e.g., an operator) manages the entire network. As current network architectures move towards disaggregated communication platforms in which multiple operators and vendors collaborate to achieve cost-efficient and reliable network management, new ML-based approaches for fault management must tackle the challenges of sharing business-critical information due to potential conflicts of interest. In this study, we explore the application of Federated Learning in disaggregated microwave networks for failure-cause identification using a real microwave hardware failure dataset. In particular, we investigate the application of two Vertical Federated Learning (VFL), namely using Split Neural Networks (SplitNNs) and Federated Learning based on Gradient Boosting Decision Trees (FedTree), on different multi-vendor deployment scenarios, and we compare them to a centralized scenario where data is managed by a single entity. Our experimental results show that VFL-based scenarios can achieve F1-Scores consistently within at most a 1% gap with respect to a centralized scenario, regardless of the deployment strategies or model types, while also ensuring minimal leakage of sensitive-data."
2502.02877,"While federated learning (FL) eliminates the transmission of raw data over a network, it is still vulnerable to privacy breaches from the communicated model parameters. Differential privacy (DP) is often employed to address such issues. However, the impact of DP on FL in multi-tier networks -- where hierarchical aggregations couple noise injection decisions at different tiers, and trust models are heterogeneous across subnetworks -- is not well understood. To fill this gap, we develop \underline{M}ulti-Tier \underline{F}ederated Learning with \underline{M}ulti-Tier \underline{D}ifferential \underline{P}rivacy ({\tt M$^2$FDP}), a DP-enhanced FL methodology for jointly optimizing privacy and performance over such networks. One of the key principles of {\tt M$^2$FDP} is to adapt DP noise injection across the established edge/fog computing hierarchy (e.g., edge devices, intermediate nodes, and other tiers up to cloud servers) according to the trust models in different subnetworks. We conduct a comprehensive analysis of the convergence behavior of {\tt M$^2$FDP} under non-convex problem settings, revealing conditions on parameter tuning under which the training process converges sublinearly to a finite stationarity gap that depends on the network hierarchy, trust model, and target privacy level. We show how these relationships can be employed to develop an adaptive control algorithm for {\tt M$^2$FDP} that tunes properties of local model training to minimize energy, latency, and the stationarity gap while meeting desired convergence and privacy criterion. Subsequent numerical evaluations demonstrate that {\tt M$^2$FDP} obtains substantial improvements in these metrics over baselines for different privacy budgets and system configurations."
2502.02886,"Mobile Edge Computing (MEC) and Open Radio Access Networks (ORAN) are transformative technologies in the development of next-generation wireless communication systems. MEC pushes computational resources closer to end-users, enabling low latency and efficient processing, while ORAN promotes interoperability and openness in radio networks, thereby fostering innovation. This paper explores recent advancements in these two domains, with a particular focus on how Artificial Intelligence (AI) and Machine Learning (ML) techniques are being utilized to solve complex wireless challenges. In MEC, Deep Reinforcement Learning (DRL) is leveraged for optimizing computation offloading, ensuring energy-efficient solutions, and meeting Quality of Service (QoS) requirements. In ORAN, AI/ML is used to develop intelligent xApps for network slicing, scheduling, and online training to enhance network adaptability. This reading report provides an in-depth analysis of multiple key papers, discusses the methodologies employed, and highlights the impact of these technologies in improving network efficiency and scalability."
2502.02889,"The integration of Artificial Intelligence (AI) and Machine Learning (ML) in next-generation wireless communication systems has become a cornerstone for advancing intelligent, adaptive, and scalable networks. This reading report examines key innovations in dynamic spectrum sensing (DSS), beginning with the foundational DeepSense framework, which uses convolutional neural networks (CNNs) and spectrogram-based analysis for real-time wideband spectrum monitoring. Building on this groundwork, it highlights advancements such as DeepSweep and Wideband Signal Stitching, which address the challenges of scalability, latency, and dataset diversity through parallel processing, semantic segmentation, and robust data augmentation strategies. The report then explores Open Radio Access Networks (ORAN), focusing on AI/ML-driven enhancements for UAV experimentation, digital twin-based optimization, network slicing, and self-healing xApp development. By bridging AI-based DSS methodologies with ORAN's open, vendor-neutral architecture, these studies underscore the potential of software-defined, intelligent infrastructures in enabling efficient, resilient, and self-optimizing networks for 5G/6G ecosystems. Through this synthesis, the report highlights AI's transformative role in shaping the future of wireless communication and autonomous systems."
2502.0304,"The increasing demands for sustainable and efficient manufacturing systems have driven the integration of Internet of Things (IoT) technologies into smart manufacturing. This study investigates IoT-enabled systems designed to enhance energy efficiency and resource optimization in the manufacturing sector, focusing on a multi-layered architecture integrating sensors, edge computing, and cloud platforms. MATLAB Simulink was utilized for modeling and simulation, replicating typical manufacturing conditions to evaluate energy consumption, machine uptime, and resource usage. The results demonstrate an 18% reduction in energy consumption, a 22% decrease in machine downtime, and a 15% improvement in resource utilization. Comparative analyses highlight the superiority of the proposed framework in addressing operational inefficiencies and aligning with sustainability goals. The study underscores the potential of IoT in transforming traditional manufacturing into interconnected, intelligent systems, offering practical implications for industrial stakeholders aiming to optimize operations while adhering to global sustainability standards. Future work will focus on addressing identified challenges such as high deployment costs and data security concerns, aiming to facilitate the broader adoption of IoT in industrial applications.Keywords: IoT (Internet of Things), Smart Manufacturing, Energy Efficiency, Resource Optimization, Manufacturing"
2502.0313,"One of the current trends related to data centers is providing it with renewable energy sources. This paper suggests an analysis technique for a model uses solar panels energy to power a data center consists of 100 traditional servers, physical infrastructures, 5 backup batteries. The analysis passes through three phases: Initially, the power consumption model of the data center is proposed to show the variation in traffic and total energy consumed. Then, a solar system model is designed according to the power needed. At the last phase, the desired battery capacity is chosen as (10000Ah-48V) to accommodate the solar energy production."
2502.0325,"The rapid deployment of low earth orbit (LEO) satellite constellations has drawn attention to the potential of nonterrestrial networks (NTN) in providing global communication services. Telecom operators are attempting to collaborate with satellite network providers to develop mobile satellite networks, which serve as an effective supplement to terrestrial networks. However, current mobile satellite network architectures still employ the single-anchor design of terrestrial mobile networks, leading to severely circuitous routing for users and significantly impacting their service experience. To reduce unnecessary latency caused by circuitous routing and provide users with low-latency global internet services, this paper presents SkyOctopus, an advanced multi-anchor mobile satellite network architecture. SkyOctopus innovatively deploys traffic classifiers on satellites to enable connections between users and multiple anchor points distributed globally. It guarantees optimal anchor point selection for each user's target server by monitoring multiple end-to-end paths. We build a prototype of SkyOctopus using enhanced Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink, Kuiper, and OneWeb. We conducted extensive experiments, and the results demonstrate that, compared to standard 5G NTN and two other existing schemes, SkyOctopus can reduce end-to-end latency by up to 53\%."
2502.03377,"As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization algorithm, significantly improves the global system energy efficiency and surpasses the popular MARL and other conventional schemes."
2502.03416,"The 5G New Radio (NR) standard introduces new frequency bands allocated in Frequency Range 2 (FR2) to support enhanced Mobile Broadband (eMBB) in congested environments and enables new use cases such as Ultra-Reliable Low Latency Communication (URLLC). The 3GPP introduced 256QAM support for FR2 frequency bands to further enhance downlink capacity. However, sustaining 256QAM on FR2 in practical environments is challenging due to strong path loss and susceptibility to distortion. While 256QAM can improve theoretical throughput by 33%, compared to 64QAM, and is widely adopted in FR1, its real-world impact when utilized in FR2 is questionable, given the significant path loss and distortions experienced in the FR2 range. Additionally, using higher modulation correlates to higher BLER, increased instability, and retransmission. Moreover, 256QAM also utilizes a different MCS table defining the modulation and code rate at different Channel Quality Indexes (CQI), affecting the UE's link adaptation behavior. This paper investigates the real-world performance of 256QAM utilization on FR2 bands in two countries, across three RAN manufacturers, and in both NSA (EN-DC) and SA (NR-DC) configurations, under various scenarios, including open-air plazas, city centers, footbridges, train station platforms, and stationary environments. The results show that 256QAM provides a reasonable throughput gain when stationary but marginal improvements when there is UE mobility while increasing the probability of NACK responses, increasing BLER, and the number of retransmissions. Finally, MATLAB simulations are run to validate the findings as well as explore the effect of the recently introduced 1024QAM on FR2."
2502.03441,"The objective of this study is to propose a self-powered wireless network solution that utilizes strategically deployed wireless sensor nodes within buildings for environmental data collection, while integrating advanced security measures and sustainable power management strategies."
2502.03677,"Mesoscale IoT applications, such as P2P energy trade and real-time industrial control systems, demand high throughput and low latency, with a secondary emphasis on energy efficiency as they rely on grid power or large-capacity batteries. MARS, a multi-radio architecture, leverages ML to instantaneously select the optimal radio for transmission, outperforming the single-radio systems. However, MARS encounters a significant issue with cost sensitivity, where high-cost errors account for 40% throughput loss. Current cost-sensitive ML algorithms assign a misclassification cost for each class but not for each data sample. In MARS, each data sample has different costs, making it tedious to employ existing cost-sensitive ML algorithms. First, we address this issue by developing COMNETS, an ML-based radio selector using oblique trees optimized by Tree Alternating Optimization (TAO). TAO incorporates sample-specific misclassification costs to avert high-cost errors and achieves a 50% reduction in the decision tree size, making it more suitable for resource-constrained IoT devices. Second, we prove the stability property of TAO and leverage it to understand the critical factors affecting the radio-selection problem. Finally, our real-world evaluation of COMNETS at two different locations shows an average throughput gain of 20.83%, 17.39% than MARS."
2502.03885,"Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism. However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs (e.g., TPUv4) take a middle-ground approach, but the fault explosion radius remains large.We propose InfiniteHBD, a transceiver-centric HBD architecture that integrates connectivity and dynamic switching at the transceiver level by embedding Optical Circuit Switching (OCS) within each transceiver. It enables reconfigurable point-to-multipoint communication and scalable variable-size ring topologies. InfiniteHBD achieves datacenter-scale scalability without cost explosion, fault isolation at the node level, and full bandwidth utilization for healthy GPUs. Key innovations include a Silicon Photonic-based OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology, and an HBD-DCN orchestration algorithm. The evaluation demonstrates that InfiniteHBD reduces cost to 31% of NVL-72, achieves a near-zero GPU waste ratio (over 10x lower than NVL-72 and TPUv4), maintains near-zero cross-ToR traffic under 7% node fault ratio, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs/node)."
2502.03899,"Network slicing has emerged as a key network technology, providing network operators with the means to offer virtual networks to vertical users over a single physical network infrastructure. Recent research has resulted mainly in techniques for managing and deploying network slices, but the implementation of network slices on a real physical transport network infrastructure has received much less attention. Standardization bodies, such as the Internet Engineering Task Force (IETF), have provided some implementation recommendations. Still, there is a lack of mechanisms to implement network slices capable of handling traffic bursts while simultaneously meeting the Quality of Service (QoS) requirements of the traffic flows associated with the slices. In this paper, we propose a novel fine-grained resource control mechanism to implement transport network slices that meet traffic QoS requirements while both accepting limited traffic bursts, and enabling efficient bandwidth sharing within and across slices. The mechanism is executed at the edge of the transport network. The proposed model aligns with current standards on network slicing and has been tested on an experimental platform. Using this platform, we have conducted an extensive experimental campaign that demonstrates that our proposal can effectively control traffic bursts generated within the network slices while maximizing bandwidth utilization across the network."
2502.03909,"Anomaly-based Network Intrusion Detection Systems (NIDS) require correctly labelled, representative and diverse datasets for an accurate evaluation and development. However, several widely used datasets do not include labels which are fine-grained enough and, together with small sample sizes, can lead to overfitting issues that also remain undetected when using test data. Additionally, the cybersecurity sector is evolving fast, and new attack mechanisms require the continuous creation of up-to-date datasets. To address these limitations, we developed a modular traffic generator that can simulate a wide variety of benign and malicious traffic. It incorporates multiple protocols, variability through randomization techniques and can produce attacks along corresponding benign traffic, as it occurs in real-world scenarios. Using the traffic generator, we create a dataset capturing over 12 million samples with 82 flow-level features and 21 fine-grained labels. Additionally, we include several web attack types which are often underrepresented in other datasets."
2502.04236,"This paper presents the $\underline{\textbf{saf}}$e sub$\underline{\textbf{flo}}$w (Saflo) eBPF-based multipath TCP (MPTCP) scheduler, designed to mitigate traffic analysis attacks in cellular networks. Traffic analysis attacks, which exploit vulnerabilities in Downlink Control Information (DCI) messages, remain a significant security threat in LTE/5G networks. To counter such threats, the Saflo scheduler employs multipath communication combined with additional security-related tasks. Specifically, it utilizes eBPF tools to operate in both kernel and user spaces. In the kernel space, the eBPF scheduler performs multipath scheduling while excluding paths disabled by the user-space programs. The user-space programs conduct security-related computations and machine learning-based attack detection, determining whether each path should be enabled or disabled. This approach offloads computationally intensive tasks to user-space programs, enabling timely multipath scheduling in kernel space. The Saflo scheduler was evaluated in a private LTE/5G testbed. The results demonstrated that it significantly reduces the accuracy of video identification and user identification attacks in cellular networks while maintaining reasonable network performance for users."
2502.04933,"Conventional 5G network management mechanisms, that operate in isolated silos across different network segments, will experience significant limitations in handling the unprecedented hyper-complexity and massive scale of the sixth generation (6G). Holistic intelligence and end-to-end automation are, thus, positioned as key enablers of forthcoming 6G networks. The Large Language Model (LLM) technology, a major breakthrough in the Generative Artificial Intelligence (AI) field, enjoys robust human-like language processing, advanced contextual reasoning and multi-modal capabilities. These features foster a holistic understanding of network behavior and an autonomous decision-making. This paper investigates four possible architectural designs for integrated LLM and 6G networks, detailing the inherent technical intricacies, the merits and the limitations of each design. As an internal functional building block of future 6G networks, the LLM will natively benefit from their improved design-driven security policies from the early design and specification stages. An illustrative scenario of slicing conflicts is used to prove the effectiveness of our architectural framework in autonomously dealing with complicated network anomalies. We finally conclude the paper with an overview of the key challenges and the relevant research trends for enabling Mobile Networkspecialized LLMs. This study is intended to provide Mobile Network Operators (MNOs) with a comprehensive guidance in their paths towards embracing the LLM technology."
2502.04937,"The development and operation of smart cities relyheavily on large-scale Internet-of-Things (IoT) networks and sensor infrastructures that continuously monitor various aspects of urban environments. These networks generate vast amounts of data, posing challenges related to bandwidth usage, energy consumption, and system scalability. This paper introduces a novel sensing paradigm called Data-driven Modality Fusion (DMF), designed to enhance the efficiency of smart city IoT network management. By leveraging correlations between timeseries data from different sensing modalities, the proposed DMF approach reduces the number of physical sensors required for monitoring, thereby minimizing energy expenditure, communication bandwidth, and overall deployment costs. The framework relocates computational complexity from the edge devices to the core, ensuring that resource-constrained IoT devices are not burdened with intensive processing tasks. DMF is validated using data from a real-world IoT deployment in Madrid, demonstrating the effectiveness of the proposed system in accurately estimating traffic, environmental, and pollution metrics from a reduced set of sensors. The proposed solution offers a scalable, efficient mechanism for managing urban IoT networks, while addressing issues of sensor failure and privacy concerns."
2502.05116,"In this paper, we investigate an accurate synchronization between a physical network and its digital network twin (DNT), which serves as a virtual representation of the physical network. The considered network includes a set of base stations (BSs) that must allocate its limited spectrum resources to serve a set of users while also transmitting its partially observed physical network information to a cloud server to generate the DNT. Since the DNT can predict the physical network status based on its historical status, the BSs may not need to send their physical network information at each time slot, allowing them to conserve spectrum resources to serve the users. However, if the DNT does not receive the physical network information of the BSs over a large time period, the DNT's accuracy in representing the physical network may degrade. To this end, each BS must decide when to send the physical network information to the cloud server to update the DNT, while also determining the spectrum resource allocation policy for both DNT synchronization and serving the users. We formulate this resource allocation task as an optimization problem, aiming to maximize the total data rate of all users while minimizing the asynchronization between the physical network and the DNT. To address this problem, we propose a method based on the GRUs and the value decomposition network (VDN). Simulation results show that our GRU and VDN based algorithm improves the weighted sum of data rates and the similarity between the status of the DNT and the physical network by up to 28.96%, compared to a baseline method combining GRU with the independent Q learning."
2502.05707,"Integrated Access and Backhaul (IAB) has been recently proposed by 3GPP to enable network operators to deploy fifth generation (5G) mobile networks with reduced costs. In this paper, we propose to use IAB to build a dynamic wireless backhaul network capable to provide additional capacity to those Base Stations (BS) experiencing congestion momentarily. As the mobile traffic demand varies across time and space, and the number of slice combinations deployed in a BS can be prohibitively high, we propose to use Deep Reinforcement Learning (DRL) to select, from a set of candidate BSs, the one that can provide backhaul capacity for each of the slices deployed in a congested BS. Our results show that a Double Deep Q-Network (DDQN) agent using a fully connected neural network and the Rectified Linear Unit (ReLU) activation function with only one hidden layer is capable to perform the BS selection task successfully, without any failure during the test phase, after being trained for around 20 episodes."
2502.05708,"We present Generalizable Wireless Radiance Fields (GWRF), a framework for modeling wireless signal propagation at arbitrary 3D transmitter and receiver positions. Unlike previous methods that adapt vanilla Neural Radiance Fields (NeRF) from the optical to the wireless signal domain, requiring extensive per-scene training, GWRF generalizes effectively across scenes. First, a geometry-aware Transformer encoder-based wireless scene representation module incorporates information from geographically proximate transmitters to learn a generalizable wireless radiance field. Second, a neural-driven ray tracing algorithm operates on this field to automatically compute signal reception at the receiver. Experimental results demonstrate that GWRF outperforms existing methods on single scenes and achieves state-of-the-art performance on unseen scenes."
2502.05711,"Physical-Layer Network Coding (PNC) is an effective technique to improve the throughput and latency in wireless networks. However, there are two major challenges for PNC, especially when using higher order modulations: 1) phase synchronization and power control at the paired User Equipments (UEs); and 2) the ambiguity removal of the PNC mapping at the relay node. To address these challenges, in this paper, we apply power control at transmitting UEs and exploit Reconfigurable Intelligent Surfaces (RISs) to synchronize the phase of the transmitted signals and ensure that they arrive at the relay with the same power and phase rotation. Then, we employ modular addition for an unambiguous PNC mapping for M-ary Quadrature Amplitude Modulations (M-QAM). We evaluate the performance of the system in the framework of Orthogonal Frequency Division Multiplexing (OFDM)-PNC for different RIS sizes and modulation orders. Furthermore, we study the sensitivity of PNC systems for Channel Estimation Error (CEE). The results reveal that 1) PNC systems show quite higher sensitivity to CEE compared with RIS-assisted one-way relay channel systems; 2) when the CEE is low, RIS can considerably enhance the Signal-to-Noise Ratio (SNR) of the PNC system, e.g., for a Bit Error Rate (BER) of $10^{-3}$ (without channel coding), increasing the RIS size from one to 256 elements in 28 GHz band leads to 200% improvement in SNR."
2502.05763,"This paper investigates two key performance aspects of the interplay between public DNS resolution services and content delivery networks -- the latency of DNS queries for resolving CDN-accelerated hostnames and the latency between the end-user and the CDN's edge server obtained by the user through a given resolution service. While these important issues have been considered in the past, significant developments, such as the IPv6 finally getting traction, the adoption of the ECS extension to DNS by major DNS resolution services, and the embracing of anycast by some CDNs warrant a reassessment under these new realities. Among the resolution services we consider, We find Google DNS and OpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in terms of DNS latency, and trace the cause to drastically lower cache hit rates. At the same time, we find that Google and OpenDNS have largely closed the gap with ISP resolvers in the quality of CDNs'client-to-edge-server mappings as measured by latency, while the Cloudflare resolver still shows some penalty with Akamai, and Quad9 exhibits a noticeable penalty with three of the four CDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to map clients to servers. Finally, in several locations, we observe IPv6 penalty in the latency of client-to-CDN-edge-server mappings produced by the resolvers. Moreover, this penalty does not rise above typical thresholds employed by the Happy Eyeballs algorithm for falling back to IPv4 communication. Thus, dual-stacked clients in these locations may experience suboptimal performance."
2502.0633,"The emergence of THz (Terahertz) frequency wireless networks holds great potential for advancing various high-demand services, including Industrial Internet of Things (IIoT) applications. These use cases benefit significantly from the ultra-high data rates, low latency, and high spatial resolution offered by THz frequencies. However, a primary well-known challenge of THz networks is their limited coverage range due to high path loss and vulnerability to obstructions. This paper addresses this limitation by proposing two novel multi-hop protocols, Table-Less (TL) and Table-Based (TB), respectively, both avoiding centralized control and/or control plane transmissions. Indeed, both solutions are distributed, simple, and rapidly adaptable to network changes. Simulation results demonstrate the effectiveness of our approaches, as well as revealing interesting trade-offs between TL and TB routing protocols, both in a real IIoT THz network and under static and dynamic conditions."
2502.06508,"The smart metering infrastructure may become one of the key elements in efficiently managing energy in smart cities. At the same time, traditional measurement record collection is performed by manual methods, which raises cost, safety, and accuracy issues. This paper proposes an innovative SMI architecture based on an unmanned aerial vehicle swarm organizing itself for the autonomous data collection in smart metering infrastructure with scalability and cost-effectiveness while minimizing risks. We design an architecture-based comprehensive system with various phases of operation, communication protocols, and robust failure-handling mechanisms to ensure reliable operations. We further perform extensive simulations in maintenance of precise formations during flight, efficient data collection from smart meters, and adaptation to various failure scenarios. Importantly, we analyze the energy consumption of the proposed system in both drone flight operations and network communication. We now propose a battery sizing strategy and provide an estimate of the operational lifetime of the swarm, underlining the feasibility and practicality of our approach. Our results show that UAV swarms have great potential to revolutionize smart metering and to bring a further brick to greener and more resilient smart cities."
2502.06581,"The explosive growth of video data has driven the development of distributed video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling efficient video processing, real-time inference, and privacy-preserving analysis. Among multiple advantages, CETC systems can distribute video processing tasks and enable adaptive analytics across cloud, edge, and terminal devices, leading to breakthroughs in video surveillance, autonomous driving, and smart cities. In this survey, we first analyze fundamental architectural components, including hierarchical, distributed, and hybrid frameworks, alongside edge computing platforms and resource management mechanisms. Building upon these foundations, edge-centric approaches emphasize on-device processing, edge-assisted offloading, and edge intelligence, while cloud-centric methods leverage powerful computational capabilities for complex video understanding and model training. Our investigation also covers hybrid video analytics incorporating adaptive task offloading and resource-aware scheduling techniques that optimize performance across the entire system. Beyond conventional approaches, recent advances in large language models and multimodal integration reveal both opportunities and challenges in platform scalability, data protection, and system reliability. Future directions also encompass explainable systems, efficient processing mechanisms, and advanced video analytics, offering valuable insights for researchers and practitioners in this dynamic field."
2502.06674,"The emergence of the fifth generation (5G) technology has transformed mobile networks into multi-service environments, necessitating efficient network slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition across multiple network domains, each potentially managed by different service providers, poses a significant challenge due to limited visibility into real-time underlying domain conditions. This paper introduces Risk-Aware Iterated Local Search (RAILS), a novel risk model-driven meta-heuristic framework designed to jointly address SLA decomposition and service provider selection in multi-domain networks. By integrating online risk modeling with iterated local search principles, RAILS effectively navigates the complex optimization landscape, utilizing historical feedback from domain controllers. We formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP) problem and prove its NP-hardness. Extensive simulations demonstrate that RAILS achieves near-optimal performance, offering an efficient, real-time solution for adaptive SLA management in modern multi-domain networks."
2502.06701,"The sixth generation of wireless networks envisions intelligent and adaptive environments capable of meeting the demands of emerging applications such as immersive extended reality, advanced healthcare, and the metaverse. However, this vision requires overcoming critical challenges, including the limitations of conventional wireless technologies in mitigating path loss and dynamically adapting to diverse user needs. Among the proposed reconfigurable technologies, pinching antenna systems (PASs) offer a novel way to turn path loss into a programmable parameter by using dielectric waveguides to minimize propagation losses at high frequencies. In this paper, we develop a comprehensive analytical framework that derives closed-form expressions for the outage probability and average rate of PASs while incorporating both free-space path loss and waveguide attenuation under realistic conditions. In addition, we characterize the optimal placement of pinching antennas to maximize performance under waveguide losses. Numerical results show the significant impact of waveguide losses on system performance, especially for longer waveguides, emphasizing the importance of accurate loss modeling. Despite these challenges, PASs consistently outperform conventional systems in terms of reliability and data rate, underscoring their potential to enable high-performance programmable wireless environments."
2502.06743,"In the beyond 5G era, AI/ML empowered realworld digital twins (DTs) will enable diverse network operators to collaboratively optimize their networks, ultimately improving end-user experience. Although centralized AI-based learning techniques have been shown to achieve significant network traffic accuracy, resulting in efficient network operations, they require sharing of sensitive data among operators, leading to privacy and security concerns. Distributed learning, and specifically federated learning (FL), that keeps data isolated at local clients, has emerged as an effective and promising solution for mitigating such concerns. Federated learning poses, however, new challenges in ensuring fairness both in terms of collaborative training contributions from heterogeneous data and in mitigating bias in model predictions with respect to sensitive attributes. To address these challenges, a fair FL framework is proposed for collaborative network traffic prediction and resource allocation. To demonstrate the effectiveness of the proposed approach, noniid and imbalanced federated datasets based on real-word traffic traces are utilized for an elastic optical network. The assumption is that different optical nodes may be managed by different operators. Fairness is evaluated according to the coefficient of variations measure in terms of accuracy across the operators and in terms of quality-of-service across the connections (i.e., reflecting end-user experience). It is shown that fair traffic prediction across the operators result in fairer resource allocations across the connections."
2502.06969,"This paper investigates the challenges and trade-offs associated with implementing Automatic Speech Recognition (ASR) in resource-limited Wireless Sensor Networks (WSNs) for real-time voice communication. We analyze three main architectural approaches: Network Speech Recognition (NSR), Distributed Speech Recognition (DSR), and Embedded Speech Recognition (ESR). Each approach is evaluated based on factors such as bandwidth consumption, processing power requirements, latency, accuracy (Word Error Rate - WER), and adaptability to offline operation. We discuss the advantages and disadvantages of each method, considering the computational and communication limitations of WSN nodes. This comparative study provides insights for selecting the most appropriate ASR implementation strategy based on specific application requirements and resource constraints."
2502.07069,"Providing timely and informative data in integrated terrestrial and non-terrestrial networks is critical as data volume grows while the resources available on devices remain limited. To address this, we adopt a semantics-aware approach to optimize the Version Age of Information (VAoI) in a status update system in which a remote Energy Harvesting (EH) Internet of Things (IoT) device samples data and transmits it to a network of interconnected Low Earth Orbit (LEO) satellites for dissemination and utilization. The optimal update policy is derived through stochastic modeling and optimization of the VAoI across the network. The results indicate that this policy reduces the frequency of updates by skipping stale or irrelevant data, significantly improving energy efficiency."
2502.07495,"Network stream mining is fundamental to many network operations. Sketches, as compact data structures that offer low memory overhead with bounded accuracy, have emerged as a promising solution for network stream mining. Recent studies attempt to optimize sketches using machine learning; however, these approaches face the challenges of lacking adaptivity to dynamic networks and incurring high training costs. In this paper, we propose LLM-Sketch, based on the insight that fields beyond the flow IDs in packet headers can also help infer flow sizes. By using a two-tier data structure and separately recording large and small flows, LLM-Sketch improves accuracy while minimizing memory usage. Furthermore, it leverages fine-tuned large language models (LLMs) to reliably estimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and the results demonstrate that LLM-Sketch outperforms state-of-the-art methods by achieving a $7.5\times$ accuracy improvement."
2502.07901,"Low Earth Orbit (LEO) satellite networks serve as a cornerstone infrastructure for providing ubiquitous connectivity in areas where terrestrial infrastructure is unavailable. With the emergence of Direct-to-Cell (DTC) satellites, these networks can provide direct access to mobile phones and IoT devices without relying on terrestrial base stations, leading to a surge in massive connectivity demands for the serving satellite. To address this issue, group communication is an effective paradigm that enables simultaneous content delivery to multiple users and thus optimizes bandwidth reuse. Although extensive research has been conducted to improve group communication performance, securing this communication without compromising its inherent spectrum efficiency remains a critical challenge. To address this, we introduce StarCast, a secure group encryption scheme for LEO satellite networks. Our solution leverages ciphertext-policy attribute-based encryption (CP-ABE) to implement fine-grained access control by embedding access policies directly within the ciphertext. Unlike standard secure communication approaches that require dedicated per-user channels and significantly deplete limited satellite spectrum resources, StarCast maintains efficient spectrum reuse within user groups while ensuring that only authorized users can access transmitted data. Additionally, it significantly reduces the costly key management overhead associated with conventional encryption schemes."
2502.0801,"Due to the involved massive number of devices, radio frequency (RF) energy harvesting is indispensable to realize the foreseen Internet-of-Everything (IoE) within 6G networks. Analogous to the cellular networks concept, shared energy stations (ESs) are foreseen to supply energy-as-a-service (EaaS) in order to recharge devices that belong to different IoE operators who are offering diverse use cases. Considering the capital expenditure (CAPEX) for ES deployment along with their finite wireless energy transfer (WET) zones, spatial energy gaps are plausible. Furthermore, the ESs deployment cannot cover 100% of the energy-harvesting devices of all coexisting IoE use cases. In this context, we utilize percolation theory to characterize the feasibility of large-scale device-to-device (D2D) connectivity of IoE networks operating under EaaS platforms. Assuming that ESs and IoE devices follow independent Poisson point processes (PPPs), we construct a connectivity graph for the IoE devices that are within the WET zones of ESs. Continuum percolation on the construct graph is utilized to derive necessary and sufficient conditions for large-scale RF-powered D2D connectivity in terms of the required IoE device density and communication range along with the required ESs density and WET zone size. Fixing the IoE network parameters along with the size of WET zones, we obtain the approximate critical value of the ES density that ensures large-scale connectivity using the inner-city and Gilbert disk models. By imitating the bounds and combining the approximations, we construct an approximate expression for the critical ES density function, which is necessary to minimize the EaaS CAPEX under the IoE connectivity constraint."
2502.08019,"With the advent of the 6G era, global connectivity has become a common goal in the evolution of communications, aiming to bring Internet services to more unconnected regions. Additionally, the rise of applications such as the Internet of Everything and remote education also requires global connectivity. Non-terrestrial networks (NTN), particularly low earth orbit (LEO) satellites, play a crucial role in this future vision. Although some literature already analyze the coverage performance using stochastic geometry, the ability of generating large-scale continuous service area is still expected to analyze. Therefore, in this paper, we mainly investigate the necessary conditions of LEO satellite deployment for large-scale continuous service coverage on the earth. Firstly, we apply percolation theory to a closed spherical surface and define the percolation on a sphere for the first time. We introduce the sub-critical and super-critical cases to prove the existence of the phase transition of percolation probability. Then, through stereographic projection, we introduce the tight bounds and closed-form expression of the critical number of LEO satellites on the same constellation. In addition, we also investigate how the altitude and maximum slant range of LEO satellites affect percolation probability, and derive the critical values of them. Based on our findings, we provide useful recommendations for companies planning to deploy LEO satellite networks to enhance connectivity."
2502.08381,"The powerfulness of LLMs indicates that deploying various LLMs with different scales and architectures on end, edge, and cloud to satisfy different requirements and adaptive heterogeneous hardware is the critical way to achieve ubiquitous intelligence for 6G. However, the massive parameter scale of LLMs poses significant challenges in deploying them on edge devices due to high computational and storage demands. Considering that the sparse activation in Mixture of Experts (MoE) is effective on scalable and dynamic allocation of computational and communications resources at the edge, this paper proposes a novel MoE-empowered collaborative deployment framework for edge LLMs, denoted as CoEL. This framework fully leverages the properties of MoE architecture and encompasses four key aspects: Perception, Deployment, Compression, and Updating. Edge servers broadcast their resource status and the specific resource requirements of LLMs to their neighbors. Then, utilizing this data, two sophisticated deployment strategies are proposed for satisfying varying model scales, ensuring that each model is deployed effectively. One for deploying LLMs on a single edge device through intra-device resource collaboration, and another for a distributed deployment across multiple edge devices via inter-device resource collaboration. Furthermore, both the models and the intermediate data are compressed for reducing memory footprint by quantization and reducing the volume of intermediate data by token fusion and pruning. Finally, given the dynamic of network topology, resource status, and user requirements, the deployment strategies are regularly updated to maintain its relevance and effectiveness. This paper also delineates the challenges and potential research directions for the deployment of edge LLMs."
2502.08529,"Cell-free multiple input multiple output (CF-MIMO) systems improve spectral and energy efficiencies using distributed access points (APs) to provide reliable service across an area equivalent to multiple conventional cells. This paper presents a novel design and implementation of a CF-MIMO network leveraging the open radio access network (O-RAN) architecture based testbed to enhance the performance of interference-prone user. The proposed prototype is developed based on open source software components and unlike many other prototypes, our testbed is able to serve commercial 5G user equipment (UE). The RAN intelligent controller (RIC) allows the cell-free (CF) network to access the embedded artificial intelligence and benefit from the network optimisation techniques that O-RAN brings. The testbed includes an intelligent antenna association xApp which determines the antenna group that serves each UE based on the live key performance measurements. The paper demonstrates the deployment and operation of the CF network and the xApp and discusses how the CF networks can benefit from the O-RAN architecture."
2502.08535,"The widespread use of Smart Home devices has attracted significant research interest in understanding their behavior within home networks. Unlike general-purpose computers, these devices exhibit relatively simple and predictable network activity patterns. However, previous studies have primarily focused on normal network conditions, overlooking potential hidden patterns that emerge under challenging conditions. Discovering these hidden flows is crucial for assessing device robustness. This paper addresses this gap by presenting a framework that systematically and automatically reveals these hidden communication patterns. By actively disturbing communication and blocking observed traffic, the framework generates comprehensive profiles structured as behavior trees, uncovering flows that are missed by more shallow methods. This approach was applied to ten real-world devices, identifying 254 unique flows, with over 27% only discovered through this new method. These insights enhance our understanding of device robustness and can be leveraged to improve the accuracy of network security measures."
2502.08576,"Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management."
2502.08787,"Unmanned Aerial Vehicles (UAVs) increasingly enhance the Quality of Service (QoS) in wireless networks due to their flexibility and cost-effectiveness. However, optimizing UAV placement in dynamic, obstacle-prone environments remains a significant research challenge due to their complexity. Reinforcement Learning (RL) offers adaptability and robustness in such environments, proving effective for UAV optimization.This paper introduces RLpos-3, a novel framework that integrates standard RL techniques and simulation libraries with Network Simulator 3 (ns-3) to facilitate the development and evaluation of UAV positioning algorithms. RLpos-3 serves as a supplementary tool for researchers, enabling the implementation, analysis, and benchmarking of UAV positioning strategies across diverse environmental conditions while meeting user traffic demands. To validate its effectiveness, we present use cases demonstrating RLpos-3's performance in optimizing UAV placement under realistic conditions, such as urban and obstacle-rich environments."
2502.08849,"IP Geofeed is a recently proposed informational standard that allows network operators to publish the geographical location of deployed IPv4 and IPv6 prefixes. In this work we study the adoption of IP geofeed, assess deployment of geofeed at Regional Internet Registry and Autonomous System levels, and analyze adherence to RFC 8805 and RFC 9092 in deployed geofeeds. We evaluate the authentication mechanism proposed in RFC 9092 and find that it lacks key features from a security perspective. We propose a novel approach to simplify the authentication of geofeeds and assess its efficiency using different benchmarks. Our findings highlight the challenges in current geofeed adoption and the potential for improving both security and scalability in geofeed validation processes."
2502.09281,"After a decade of research in userspace network stacks, why do new solutions remain inaccessible to most developers? We argue that this is because they ignored (1) the hardware constraints of public cloud NICs (vNICs) and (2) the flexibility required by applications. Concerning the former, state-of-the-art proposals rely on specific NIC features (e.g., flow steering, deep buffers) that are not broadly available in vNICs. As for the latter, most of these stacks enforce a restrictive execution model that does not align well with cloud application requirements.We propose a new userspace network stack, Machnet, built for public cloud VMs. Central to Machnet is a new ''Least Common Denominator'' model, a conceptual NIC with a minimal feature set supported by all kernel-bypass vNICs. The challenge is to build a new solution with performance comparable to existing stacks while relying only on basic features (e.g., no flow steering, no RSS reconfiguration). Machnet uses a microkernel design to provide higher flexibility in application execution compared to a library OS design; we show that microkernels' inter-process communication overhead is negligible on large cloud networks."
2502.09305,"Mobile network operators constantly optimize their networks to ensure superior service quality and coverage. This optimization is crucial for maintaining an optimal user experience and requires extensive data collection and analysis. One of the primary methods for gathering this data is through drive tests, where technical teams use specialized equipment to collect signal information across various regions. However, drive tests are both costly and time-consuming, and they face challenges such as traffic conditions, environmental factors, and limited access to certain areas. These constraints make it difficult to replicate drive tests under similar conditions. In this study, we propose a method that enables operators to predict received signal strength at specific locations using data from other drive test points. By reducing the need for widespread drive tests, this approach allows operators to save time and resources while still obtaining the necessary data to optimize their networks and mitigate the challenges associated with traditional drive tests."
2502.09313,"With the rapid development of delay-sensitive services happened in industrial manufacturing, Internet of Vehicles, and smart logistics, more stringent delay requirements are put forward for the intelligent machine (IM) network. Short packet transmissions are widely adopted to reduce delay in IM networks. However, the delay performance of an IM network has not been sufficiently analyzed. This paper applies queuing theory and stochastic geometry to construct network model and transmission model for downlink communication, respectively, proposes and derives the following three metrics, e.g., the transmission success probability (with delay as the threshold), expected delay, and delay jitter. To accurately characterize the transmission delay with short packets, the finite blocklength capacity is used to measure the channel transmission rate. Simulation results show that the increase of packet length and IM density significantly deteriorates the three metrics. Short packets are needed to improve the three metrics, especially in high IM density scenarios. The outcomes of this paper provide an important theoretical basis for the optimization design and performance improvement of IM networks."
2502.09783,"A science is defined by a set of encyclopedic knowledge related to facts or phenomena following rules or evidenced by experimentally-driven observations. Computer Science and in particular computer networks is a relatively new scientific domain maturing over years and adopting the best practices inherited from more fundamental disciplines. The design of past, present and future networking components and architectures have been assisted, among other methods, by experimentally-driven research and in particular by the deployment of test platforms, usually named as testbeds. However, often experimentally-driven networking research used scattered methodologies, based on ad-hoc, small-sized testbeds, producing hardly repeatable results. We believe that computer networks needs to adopt a more structured methodology, supported by appropriate instruments, to produce credible experimental results supporting radical and incremental innovations. This paper reports lessons learned from the design and operation of test platforms for the scientific community dealing with digital infrastructures. We introduce the SLICES initiative as the outcome of several years of evolution of the concept of a networking test platform transformed into a scientific instrument. We address the challenges, requirements and opportunities that our community is facing to manage the full research-life cycle necessary to support a scientific methodology."
2502.10617,"Including Vulnerable Road User (VRU) in Cooperative Intelligent Transport Systems (C-ITS) framework aims to increase road safety. However, this approach implies a massive increase of network nodes and thus is vulnerable to medium capacity issues, e.g., contention, congestion, resource scheduling. Implementing cluster schemes -- to reduce the number of nodes but represent the same number of VRUs -- is a direct way to address the issue. One of them is suggested by European Telecommunications Standards Institute (ETSI) and consists of nodes (connected pedestrians and cyclists) sending vicarious messages to enable a leader node to cover for a cluster of VRUs. However, the proposed scheme includes negotiation to establish a cluster, and in-cluster communication to maintain it, requiring extra messages of variable sizes and thus does not fully resolve the original medium capacity issues. Furthermore, these exchanges assume network reliability (i.e. a lossless channel and low latency to meet time constraints). We propose a method for VRU Awareness Message (VAM) clustering where 1) all cluster operations are performed without negotiation, 2) cluster leaders do not require sending additional messages or meet deadlines, and 3) assumes a lossy communication channel and offers a mechanism for cluster resilience. Our results show the feasibility of the concept by halving message generations compared to individual messages while keeping the awareness levels (i.e., that VRUs are accounted for)."
2502.10687,"Integrated sensing and communication (ISAC) has garnered substantial research interest owing to its pivotal role in advancing the development of next-generation (6G) wireless networks. However, achieving a performance balance between communication and sensing in the dual-function radar communication (DFRC)-based ISAC system remains a significant challenge. In this paper, a low-altitude intelligent reflecting surface (IRS)-assisted ISAC system is explored, where a base station (BS) supports dual-functional operations, enabling both data transmission for multiple users and sensing for a blocked target, with the channel quality enhanced by an IRS mounted on the unmanned aerial vehicle (UAV). Moreover, we formulate an integrated communication, sensing, and energy efficiency multi-objective optimization problem (CSEMOP), which aims to maximize the communication rate of the users and the sensing rate of the target, while minimizing UAV propulsion energy consumption by jointly optimizing the BS beamforming matrix, IRS phase shifts, the flight velocity and angle of the UAV. Considering the non-convexity, trade-off, and dynamic nature of the formulated CSEMOP, we propose a generative diffusion model-based deep deterministic policy gradient (GDMDDPG) algorithm to solve the problem. Specifically, the diffusion model is incorporated into the actor network of DDPG to improve the action quality, with noise perturbation mechanism for better exploration and recent prioritized experience replay (RPER) sampling mechanism for enhanced training efficiency. Simulation results indicate that the GDMDDPG algorithm delivers superior performance compared to the existing methods."
2502.10731,"As an important component of the sixth generation communication technologies, the space-air-ground integrated network (SAGIN) attracts increasing attentions in recent years. However, due to the mobility and heterogeneity of the components such as satellites and unmanned aerial vehicles in multi-layer SAGIN, the challenges of inefficient resource allocation and management complexity are aggregated. To this end, the network function virtualization technology is introduced and can be implemented via service function chains (SFCs) deployment. However, urgent unexpected tasks may bring conflicts and resource competition during SFC deployment, and how to schedule the SFCs of multiple tasks in SAGIN is a key issue. In this paper, we address the dynamic and complexity of SAGIN by presenting a reconfigurable time extension graph and further propose the dynamic SFC scheduling model. Then, we formulate the SFC scheduling problem to maximize the number of successful deployed SFCs within limited resources and time horizons. Since the problem is in the form of integer linear programming and intractable to solve, we propose the algorithm by incorporating deep reinforcement learning. Finally, simulation results show that the proposed algorithm has better convergence and performance compared to other benchmark algorithms."
2502.10775,"In this paper, we propose a novel cloud-native architecture for collaborative agentic network slicing. Our approach addresses the challenge of managing shared infrastructure, particularly CPU resources, across multiple network slices with heterogeneous requirements. Each network slice is controlled by a dedicated agent operating within a Dockerized environment, ensuring isolation and scalability. The agents dynamically adjust CPU allocations based on real-time traffic demands, optimizing the performance of the overall system. A key innovation of this work is the development of emergent communication among the agents. Through their interactions, the agents autonomously establish a communication protocol that enables them to coordinate more effectively, optimizing resource allocations in response to dynamic traffic demands. Based on synthetic traffic modeled on real-world conditions, accounting for varying load patterns, tests demonstrated the effectiveness of the proposed architecture in handling diverse traffic types, including eMBB, URLLC, and mMTC, by adjusting resource allocations to meet the strict requirements of each slice. Additionally, the cloud-native design enables real-time monitoring and analysis through Prometheus and Grafana, ensuring the system's adaptability and efficiency in dynamic network environments. The agents managed to learn how to maximize the shared infrastructure with a conflict rate of less than 3%."
2502.1086,"Network slicing is one of the most critical 5G pillars. It allows for sharing a 5G infrastructure among different tenants leading to improved service customisation and increased operators' revenues. Concurrently, introducing the Multi-access Edge Computing (MEC) into 5G to support time-critical applications raises the need to integrate this distributed computing infrastructure to the 5G network slicing framework. Indeed, end-to-end latency guarantees require the end-to-end management of slice resources. For this purpose, after discussing the main gaps in the state-of-the-art with regards to such an objective, we propose a novel slicing architecture that enables the management and orchestration of slice segments that span over all the domains of an end-to-end application service, including the MEC. We also show how this general management architecture can be instantiated into a multi-tenant MEC infrastructure. A preliminary implementation of the proposed architecture focusing on the MEC domain is also provided, together with performance tests to validate the feasibility and efficacy of our design approach."
2502.10891,"Underwater communication is essential for both recreational and scientific activities, such as scuba diving. However, existing methods remain highly constrained by environmental challenges and often require specialized hardware, driving research into more accessible underwater communication solutions. While recent acoustic-based communication systems support text messaging on mobile devices, their low data rates severely limit broader applications.We present AquaScope, the first acoustic communication system capable of underwater image transmission on commodity mobile devices. To address the key challenges of underwater environments -- limited bandwidth and high transmission errors -- AquaScope employs and enhances generative image compression to improve compression efficiency, and integrates it with reliability-enhancement techniques at the physical layer to strengthen error resilience. We implemented AquaScope on the Android platform and demonstrated its feasibility for underwater image transmission. Experimental results show that AquaScope enables reliable, low-latency image transmission while preserving perceptual image quality, across various bandwidth-constrained and error-prone underwater conditions."
2502.11021,"Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency."
2502.11298,"Efficient Service Function Chain (SFC) provisioning and Virtual Network Function (VNF) placement are critical for enhancing network performance in modern architectures such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids decision-making in dynamic network environments, its reliance on structured inputs and predefined rules limits adaptability in unforeseen scenarios. Additionally, incorrect actions by a DRL agent may require numerous training iterations to correct, potentially reinforcing suboptimal policies and degrading performance. This paper integrates DRL with Language Models (LMs), specifically Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT, to enhance network management. By feeding final VNF allocations from DRL into the LM, the system can process and respond to queries related to SFCs, DCs, and VNFs, enabling real-time insights into resource utilization, bottleneck detection, and future demand planning. The LMs are fine-tuned to our domain-specific dataset using Low-Rank Adaptation (LoRA). Results show that BERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and higher confidence (0.83 compared to 0.74), though BERT requires approximately 46% more processing time."
2502.11386,"Due to massive computational demands of large generative models, AI-Generated Content (AIGC) can organize collaborative Mobile AIGC Service Providers (MASPs) at network edges to provide ubiquitous and customized content generation for resource-constrained users. However, such a paradigm faces two significant challenges: 1) raw prompts (i.e., the task description from users) often lead to poor generation quality due to users' lack of experience with specific AIGC models, and 2) static service provisioning fails to efficiently utilize computational and communication resources given the heterogeneity of AIGC tasks. To address these challenges, we propose an intelligent mobile AIGC service scheme. Firstly, we develop an interactive prompt engineering mechanism that leverages a Large Language Model (LLM) to generate customized prompt corpora and employs Inverse Reinforcement Learning (IRL) for policy imitation through small-scale expert demonstrations. Secondly, we formulate a dynamic mobile AIGC service provisioning problem that jointly optimizes the number of inference trials and transmission power allocation. Then, we propose the Diffusion-Enhanced Deep Deterministic Policy Gradient (D3PG) algorithm to solve the problem. By incorporating the diffusion process into Deep Reinforcement Learning (DRL) architecture, the environment exploration capability can be improved, thus adapting to varying mobile AIGC scenarios. Extensive experimental results demonstrate that our prompt engineering approach improves single-round generation success probability by 6.3 times, while D3PG increases the user service experience by 67.8% compared to baseline DRL approaches."
2502.11595,"Industrial cyber-physical systems require dependable network communication with formal end-to-end reliability guarantees. Striving towards this goal, recent efforts aim to advance the integration of 5G into Time-Sensitive Networking (TSN). However, we show that IEEE 802.1Qbv TSN schedulers that are unattuned to 5G packet delay variations may jeopardize any reliability guarantees provided by the 5G system. We demonstrate this on a case where a 99.99% reliability in the inner 5G network diminishes to below 10% when looking at end-to-end communication in TSN. In this paper, we overcome this shortcoming by introducing Full Interleaving Packet Scheduling (FIPS) as a wireless-friendly IEEE 802.1Qbv scheduler. To the best of our knowledge, FIPS is the first to provide formal end-to-end QoS guarantees in wireless TSN. FIPS allows a controlled batching of TSN streams, which improves schedulability in terms of the number of wireless TSN streams by a factor of up to x45. Even in failure cases, FIPS isolates the otherwise cascading QoS violations to the affected streams and protects all other streams. With formal end-to-end reliability, improved schedulability, and fault isolation, FIPS makes a substantial advance towards dependability in wireless TSN."
2502.11983,"Transmission Control Protocol (TCP) continues to be the dominant transport protocol on the Internet. The stability of fluid models has been a key consideration in the design of TCP and the performance evaluation of TCP algorithms. Based on local stability analysis, we formulate some design considerations for a class of TCP algorithms. We begin with deriving sufficient conditions for the local stability of a generalized TCP algorithm in the presence of heterogeneous round-trip delays. Within this generalized model, we consider three specific variants of TCP: TCP Reno, Compound TCP, and Scalable TCP. The sufficient conditions we derive are scalable across network topologies with one, two, and many bottleneck links. We are interested in networks with intermediate and small drop-tail buffers as they offer smaller queuing delays. The small buffer regime is more attractive as the conditions for stability are decentralized. TCP algorithms that follow our design considerations can provide stable operation on any network topology, irrespective of the number of bottleneck links or delays in the network."
2502.12557,"Vehicular clouds (VCs) play a crucial role in the Internet-of-Vehicles (IoV) ecosystem by securing essential computing resources for a wide range of tasks. This paPertackles the intricacies of resource provisioning in dynamic VCs for computation-intensive tasks, represented by undirected graphs for parallel processing over multiple vehicles. We model the dynamics of VCs by considering multiple factors, including varying communication quality among vehicles, fluctuating computing capabilities of vehicles, uncertain contact duration among vehicles, and dynamic data exchange costs between vehicles. Our primary goal is to obtain feasible assignments between task components and nearby vehicles, called templates, in a timely manner with minimized task completion time and data exchange overhead. To achieve this, we propose a hybrid graph task scheduling (P-HTS) methodology that combines offline and online decision-making modes. For the offline mode, we introduce an approach called risk-aware pilot isomorphic subgraph searching (RA-PilotISS), which predicts feasible solutions for task scheduling in advance based on historical information. Then, for the online mode, we propose time-efficient instantaneous isomorphic subgraph searching (TE-InstaISS), serving as a backup approach for quickly identifying new optimal scheduling template when the one identified by RA-PilotISS becomes invalid due to changing conditions. Through comprehensive experiments, we demonstrate the superiority of our proposed hybrid mechanism compared to state-of-the-art methods in terms of various evaluative metrics, e.g., time efficiency such as the delay caused by seeking for possible templates and task completion time, as well as cost function, upon considering different VC scales and graph task topologies."
2502.12804,"The application of reinforcement learning (RL) to dynamic resource allocation in optical networks has been the focus of intense research activity in recent years, with almost 100 peer-reviewed papers. We present a review of progress in the field, and identify significant gaps in benchmarking practices and reproducibility. To determine the strongest benchmark algorithms, we systematically evaluate several heuristics across diverse network topologies. We find that path count and sort criteria for path selection significantly affect the benchmark performance. We meticulously recreate the problems from five landmark papers and apply the improved benchmarks. Our comparisons demonstrate that simple heuristics consistently match or outperform the published RL solutions, often with an order of magnitude lower blocking probability. Furthermore, we present empirical lower bounds on network blocking using a novel defragmentation-based method, revealing that potential improvements over the benchmark heuristics are limited to 19-36% increased traffic load for the same blocking performance in our examples. We make our simulation framework and results publicly available to promote reproducible research and standardized evaluationthis https URL."
2502.12834,"In-band network telemetry (INT) is essential to network management due to its real-time visibility. However, because of the rapid increase in network devices and services, it has become crucial to have targeted access to detailed network information in a dynamic network environment. This paper proposes an intelligent network telemetry system called NTP-INT to obtain more fine-grained network information on high-load switches. Specifically, NTP-INT consists of three modules: network traffic prediction module, network pruning module, and probe path planning module. Firstly, the network traffic prediction module adopts a Multi-Temporal Graph Neural Network (MTGNN) to predict future network traffic and identify high-load switches. Then, we design the network pruning algorithm to generate a subnetwork covering all high-load switches to reduce the complexity of probe path planning. Finally, the probe path planning module uses an attention-mechanism-based deep reinforcement learning (DEL) model to plan efficient probe paths in the network slice. The experimental results demonstrate that NTP-INT can acquire more precise network information on high-load switches while decreasing the control overhead by 50\%."
2502.12875,"Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in modern communication networks,offering flexibility and enhanced coverage for a variety of applica-tions. However, UAV networks pose significant challenges due to their dynamic and distributed nature, particularly when dealing with tasks such as power allocation, channel assignment, caching,and task offloading. Traditional optimization techniques often struggle to handle the complexity and unpredictability of these environments, leading to suboptimal performance. This survey provides a comprehensive examination of how deep reinforcement learning (DRL) can be applied to solve these mathematical optimization problems in UAV communications andthis http URLthan simply introducing DRL methods, the focus is on demonstrating how these methods can be utilized to solve complex mathematical models of the underlying problems. We begin by reviewing the fundamental concepts of DRL, including value-based, policy-based, and actor-critic approaches. Then,we illustrate how DRL algorithms are applied to specific UAV network tasks by discussing from problem formulations to DRL implementation. By framing UAV communication challenges as optimization problems, this survey emphasizes the practical value of DRL in dynamic and uncertain environments. We also explore the strengths of DRL in handling large-scale network scenarios and the ability to continuously adapt to changes in the environment. In addition, future research directions are outlined, highlighting the potential for DRL to further enhance UAV communications and expand its applicability to more complex,multi-agent settings."
2502.13015,"Communication, Navigation, and Surveillance (CNS) is the backbone of the Air Traffic Management (ATM) and Unmanned Aircraft System (UAS) Traffic Management (UTM) systems, ensuring safe and efficient operations of modern and future aviation. Traditionally, the CNS is considered three independent systems: communications, navigation, and surveillance. The current CNS system is fragmented, with limited integration across its three domains. Integrated CNS (ICNS) is a contemporary concept implying that those systems are provisioned through the same technology stack. ICNS is envisioned to improve service quality, spectrum efficiency, communication capacity, navigation predictability, and surveillance capabilities. The 5G technology stack offers higher throughput, lower latency, and massive connectivity compared to many existing communication technologies. This paper presents our 5G ICNS vision and network architecture and discusses how 5G technology can support integrated CNS services using terrestrial and non-terrestrial networks. We also discuss key 5G radio access technologies for delivering integrated CNS services at low altitudes for Innovative Air Mobility (IAM) and Advanced Air Mobility (AAM) operations. Finally, we present relevant challenges and potential research directions for further studies."
2502.13303,"Low-power and low-cost wireless sensor networks enable scalable and affordable sensing and can be deployed in different environments to monitor various physical parameters. In some environments, these networks may have to coexist and interact with other systems which use the same frequency spectrum for communication. This potentially results in cross-technology interference (CTI). Dynamic channel hopping is one of the mechanisms that is currently employed to deal with CTI, but its usefulness depends on the channel selection and occupation timing. In this paper, we experimentally study the impact of CTI (caused by IEEE 802.11 networks) on time synchronization and network join time. Experiment results show that CTI can increase time drift between a child and a parent node by up to $\pm 3$ clock ticks between two synchronization intervals. Likewise, CTI affects new nodes from timely joining a network. In a simple network which does not involve multi-hop communication, the time it takes for nodes to join the network in the absence of CTI is between 40 and 70 ms (83.3\% of the time). In the presence of CTI, 96.82\% of the time, the join time is between 100 and 200 ms. In other words, the join time in the presence of CTI is about five times higher. Interestingly, not only the main spectral lobes, but also the spectral sidelobes of interfering networks impact the performance of low-power networks."
2502.13704,"Comparison between existing, well-established satellite technologies, like the Digital Video Broadcasting (DVB) satellite specifications, and the emerging Third Generation Partnership Project (3GPP) specified 5th Generation New Radio (5G NR) Non-Terrestrial Networks (NTN) is an actively discussed topic in the satellite industry standardization groups. This article presents a thorough performance comparison between DVB Second Generation Satellite Extensions (DVBS2X) and Return Channel via Satellite 2nd Generation (DVBRCS2), and NR NTN in a Geostationary Orbit (GEO) satellite scenario, using system-level simulators (SLS) for evaluation, namely Satellite Network Simulator 3 (SNS3) and ALIX 5G (TN-)NTN SLS, built on the same Network Simulator 3 (ns-3) platform. With the satellite system geometry, beam layout, and link budget aligned to use the 3GPP NTN example parameterization for a fair comparison between DVB and NR NTN, the results show that DVB-S2X consistently achieves higher spectral efficiency than the NR Physical Downlink Shared Channel (PDSCH) on the forward user link. In contrast, on the return link, the NR Physical Uplink Shared Channel (PUSCH) demonstrates better spectral efficiency at the system level. The SLS results incorporate link-level performance, obtained through link-level simulations (LLS) for different modulation and coding schemes (MCS) and waveforms supported by each technology."
2502.13804,"Encrypted traffic classification faces growing challenges as encryption renders traditional deep packet inspection ineffective. This study addresses binary VPN detection, distinguishing VPN-encrypted from non-VPN traffic using wavelet transform-based features across multiple machine learning models. Unlike previous studies focused on application-level classification within encrypted traffic, we specifically evaluate the fundamental task of VPN identification regardless of application type. We analyze the impact of wavelet decomposition levels and dataset filtering on classification performance across significantly imbalanced data, where filtering reduces some traffic categories by up to 95%. Our results demonstrate that Random Forest (RF) achieves superior performance with an F1-score of 99%, maintaining robust accuracy even after significant dataset filtering. Neural Networks (NN) show comparable effectiveness with an F1-score of 98% when trained on wavelet level 12, while Support Vector Machines (SVM) exhibit notable sensitivity to dataset reduction, with F1-scores dropping from 90% to 85% after filtering. Comparing wavelet decomposition at levels 5 and 12, we observe improved classification performance at level 12, particularly for variable traffic types, though the marginal gains may not justify the additional computational overhead. These findings establish RF as the most reliable model for VPN traffic classification while highlighting key performance tradeoffs in feature extraction and preprocessing."
2502.13879,"In light of the ever growing energy needs of the ICT sector, a value that is becoming increasingly important for a mobile network is its power consumption. However, the transition away from legacy network deployments tightly coupled with the underlying hardware and the adoption of the Network Function Virtualization (NFV) paradigm has made more difficult to accurately evaluate their energy and carbon footprint. In this paper, we propose and validate a measurement-based approach to analyze the power consumption of a virtualized 5G core network (5GC) deployment. We design an experimental testbed using commercial off-the-shelf (COTS) hardware and open-source software as a sample architecture simulating an edge computing node and supporting three different virtualization options. We make use of both hardware-based and software-based power meters to investigate the power consumption trends associated with increasing levels of traffic and multiple 5GC deployment types. The results show the feasibility of a real-time power monitoring system and highlight how deployment choices, such as virtualization framework and 5GC software, can significantly impact on the power consumption of the network."
2502.14107,"Low-power and cost-effective IoT sensing nodes enable scalable monitoring of different environments. Some of these environments impose rough and extreme operating conditions, requiring continuous adaptation and reconfiguration of physical and link layer parameters. In this paper, we closely investigate the stability of the wireless links established between nodes deployed on the surface of different water bodies and propose a model to predict the received power. Our model is based on Minimum Mean Square Estimation (MMSE) and relies on the statistics of received power and the motion the nodes experience during communication. One of the drawbacks of MMSE is its reliance on matrix inversion, which is at once computationally expensive and difficult to implement with resource constrained devices. We forgo this stage by estimating model parameters using the gradient-descent approach, which is much simpler to implement. The model achieves a prediction accuracy of 91% even with a small number of iterations."
2502.14399,"Considering device-to-device (D2D) wireless links as a virtual extension of 5G (and beyond) cellular networks to deliver popular contents has been proposed as an interesting approach to reduce energy consumption, congestion, and bandwidth usage at the network edge. In the scenario of multiple users in a region independently requesting some popular content, there is a major potential for energy consumption reduction exploiting D2D communications. In this scenario, we consider the problem of selecting the maximum allowed transmission range (or equivalently the maximum transmit power) for the D2D links that support the content delivery process. We show that, for a given maximum allowed D2D energy consumption, a considerable reduction of the cellular infrastructure energy consumption can be achieved by selecting the maximum D2D transmission range as a function of content class parameters such as popularity and delay-tolerance, compared to a uniform selection across different content classes. Specifically, we provide an analytical model that can be used to estimate the energy consumption (for small delay tolerance) and thus to set the optimal transmission range. We validate the model via simulations and study the energy gain that our approach allows to obtain. Our results show that the proposed approach to the maximum D2D transmission range selection allows a reduction of the overall energy consumption in the range of 30% to 55%, compared to a selection of the maximum D2D transmission range oblivious to popularity and delay tolerance."
2502.14699,"Due to the large data volume and number of distinct elements, space is often the bottleneck of many stream processing systems. The data structures used by these systems often consist of counters whose optimization yields significant memory savings. The challenge lies in balancing the size of the counters: too small, and they overflow; too large, and memory capacity limits their number.In this work, we suggest an efficient encoding scheme that sizes each counter according to its needs. Our approach uses fixed-sized pools of memory (e.g., a single memory word or 64 bits), where each pool manages a small number of counters. We pay special attention to performance and demonstrate considerable improvements for various streaming algorithms and workload characteristics."
2502.14741,"Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work to date has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm. Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR). We re-examine this problem and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6% increased throughput when candidate paths are ordered by number of hops, rather than total length. We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data. We provide details of our methodology and open source all of our code for reproduction. We outperform the previous state-of-the-art RL approach by 2.5% (17.4 Tbps mean additional throughput) and the best heuristic by 1.2% (8.5 Tbps mean additional throughput). This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks."
2502.14784,"This paper studies the radio resource management (RRM) for the uplink (UL) of a cellular system with codebook-based hybrid beamforming. We consider the often neglected but highly practical multi-channel case with fewer radio frequency chains in the base station than user equipment (UEs) in the cell, assuming one RF chain per UE. As for any UL RRM, a per-time slot solution is needed as the allocation of power to subchannels by a UE can only be done once it knows which subchannels it has been allocated. The RRM in this system comprises beam selection, user selection and power allocation, three steps that are intricately coupled and we will show that the order in which they are performed does impact performance and so does the amount of coupling that we take into account. Specifically, we propose 4 online sequential solutions with different orders in which the steps are called and of different complexities, i.e., different levels of coupling between the steps. Our extensive numerical campaign for a mmWave system shows how a well-designed heuristic that takes some level of couplings between the steps can make the performance exceedingly better than a benchmark."
2502.14947,"Real-time interactive Virtual Reality (VR) streaming is a significantly challenging use case for Wi-Fi given its high throughput and low latency requirements, especially considering the constraints imposed by the possible presence of other users and the variability of the available bandwidth. Adaptive BitRate (ABR) algorithms dynamically adjust the encoded bitrate in response to varying network conditions to maintain smooth video playback. In this paper, we present the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR), a configurable algorithm implemented in Air Light VR (ALVR), an open-source VR streaming solution. NeSt-VR effectively adjusts video bitrate based on real-time network metrics, such as frame delivery rate, network latency, and estimated available bandwidth, to guarantee user satisfaction. These metrics are part of a comprehensive set we integrated into ALVR to characterize network performance and support the decision-making process of any ABR algorithm, validated through extensive emulated experiments. NeSt-VR is evaluated in both single- and multi-user scenarios, including tests with network capacity fluctuations, user mobility, and co-channel interference. Our results demonstrate that NeSt-VR successfully manages Wi-Fi capacity fluctuations and enhances interactive VR streaming performance in both controlled experiments at UPF's lab and professional tests at CREW's facilities."
2502.15508,"Energy efficiency and reliability are vital design requirements of recent industrial networking solutions. Increased energy consumption, poor data access rates and unpredictable end-to-end data access latencies are catastrophic when transferring high volumes of critical industrial data in strict temporal deadlines. These requirements might become impossible to meet later on, due to node failures, or excessive degradation of the performance of wireless links. In this paper, we focus on maintaining the network functionality required by the industrial, best effort, low-latency applications after such events, by sacrificing latency guarantees to improve energy consumption and reliability. We avoid continuously recomputing the network configuration centrally, by designing an energy efficient, local and distributed path reconfiguration method. Specifically, given the operational parameters required by the applications, our method locally reconfigures the data distribution paths, when a network node fails. Additionally, our method also regulates the return to an operational state of nodes that have been offline in the past. We compare the performance of our method through simulations to the performance of other state of the art protocols and we demonstrate performance gains in terms of energy consumption, data delivery success rate, and in some cases, end-to-end data access latency. We conclude by providing some emerging key insights which can lead to further performance improvements."
2502.15552,"Starlink has introduced the Flat High Performance (FHP) terminal, specifically designed to support the vehicles and the vessels in motion as well as the high-demand stationary users. The research on FHP terminal throughput analysis remains limited, only a few existing studies evaluate FHP, focusing on the limited parameters and scenarios. This paper evaluates the FHP terminal's performance in Finland, Northern Europe. We examine round-trip time (RTT), uplink, and downlink throughput for both stationary and in-motion use. We measure network efficiency across six geographically diverse servers and get insights of network routing strategies. Our results show that Starlink provides high-speed, low-RTT connectivity, however, the throughput experiences fluctuations with slight degradation when in motion. Additionally, we compare Starlink and terrestrial network RTT and possible routing paths."
2502.15585,"The introduction of 5G networks has significantly advanced communication technology, offering faster speeds, lower latency, and greater capacity. This progress sets the stage for Beyond 5G (B5G) networks, which present new complexity and performance requirements challenges. Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models have been widely used to model the optimization of resource allocation problems in networks. This paper reviews 103 studies on resource allocation strategies in 5G and B5G, focusing specifically on optimization problems modelled as LP, ILP, and MILP. The selected studies are categorized based on network architectures, types of resource allocation problems, and specific objective functions and constraints. The review also discusses solution methods for NP-hard ILP and MILP problems by categorizing the solution methods into different categories. Additionally, emerging trends, such as integrating AI and machine learning with optimization models, are explored, suggesting promising future research directions in network optimization. The paper concludes that LP, ILP, and MILP models have been widely adopted across various network architectures, resource types, objective functions, and constraints and remain critical to optimizing next-generation networks."
2502.15705,"Recent advances in low-cost microcontrollers have enabled innovative smart home applications. However, existing systems typically consist of single-purpose devices that only report sensed data to a controller. Given the potential for residential emergencies, we propose to integrate emergency detection systems into smart home environments. We present an ad-hoc distributed sensor network (DSN) designed to detect five common residential emergencies: fires, gas and water leakages, earthquakes, and intrusions. Our novel approach combines diverse sensors with a voting-based consensus algorithm among multiple nodes, improving accuracy and reliability over traditional alert systems. The consensus algorithm employs a majority rule with weighted votes, allowing adjustments for various scenarios. An experimental evaluation confirms our approach's effectiveness in accurately detecting emergencies while demonstrating reliability in mitigating node failures, ensuring system longevity, and maintaining robust communication. Additionally, our approach significantly reduces power consumption compared to alternatives."
2502.15706,"To accommodate ever-growing traffic, network operators are actively deploying high-degree reconfigurable optical add/drop multiplexers (ROADMs) to build large-capacity optical networks. High-degree ROADM-based optical networks have multiple parallel fibers between ROADM nodes, requiring the adoption of ROADM nodes with a large number of inter-/intra-node components. However, this large number of inter-/intra-node optical components in high-degree ROADM networks increases the likelihood of multiple failures simultaneously, and calls for novel methods for accurate localization of multiple failed components. To the best of our knowledge, this is the first study investigating the problem of multi-failure localization for high-degree ROADM-based optical networks. To solve this problem, we first provide a description of the failures affecting both inter-/intra-node components, and we consider different deployments of optical power monitors (OPMs) to obtain information (i.e., optical power) to be used for automated multi-failure localization. Then, as our main and original contribution, we propose a novel method based on a rules-informed neural network (RINN) for multi-failure localization, which incorporates the benefits of both rules-based reasoning and artificial neural networks (ANN). Through extensive simulations and experimental demonstrations, we show that our proposed RINN algorithm can achieve up to around 20 higher localization accuracy compared to baseline algorithms, incurring only around 4.14 ms of average inference time."
2502.15707,"This paper investigates the efficacy ofthis http URLas a framework addressing the challenges posed bythis http URL, particularly in performance, SEO, and equitable web accessibility. By constructing identical websites and web applications in both frameworks, we aim to evaluate the frameworks' behavior under diverse network conditions and capabilities. Beyond quantitative metrics like First Contentful Paint (FCP) and Time to Interactive (TTI), we incorporate qualitative user feedback to assess real-world usability. Our motivation stems from bridging the digital divide exacerbated by client-side rendering (CSR) frameworks and validating investments in modern technologies for businesses and institutions. Employing a novel LLM-assisted migration workflow, this paper also demonstrates the ease with which developers can transition fromthis http URLtothis http URL. Our results highlightthis http URL's promise of better overall performance, without any degradation in user interaction experience, showcasing its potential to mitigate disparities in web accessibility and foster global network equity, thus highlightingthis http URLas a compelling framework for the future of an inclusive web."
2502.15708,"The web experience in developing regions remains subpar, primarily due to the growing complexity of modern webpages and insufficient optimization by content providers. Users in these regions typically rely on low-end devices and limited bandwidth, which results in a poor user experience as they download and parse webpages bloated with excessive third-party CSS and JavaScript (JS). To address these challenges, we introduce the Mobile Application Markup Language (MAML), a flat layout-based web specification language that reduces computational and data transmission demands, while replacing the excessive bloat from JS with a new scripting language centered on essential (and popular) web functionalities. Last but not least, MAML is backward compatible as it can be transpiled to minimal HTML/JavaScript/CSS and thus work with legacy browsers. We benchmark MAML in terms of page load times and sizes, using a translator which can automatically port any webpage to MAML. When compared to the popular Google AMP, across 100 testing webpages, MAML offers webpage speedups by tens of seconds under challenging network conditions thanks to its significant size reductions. Next, we run a competition involving 25 university students porting 50 of the above webpages to MAML using a web-based editor we developed. This experiment verifies that, with little developer effort, MAML is quite effective in maintaining the visual and functional correctness of the originating webpages."
2502.15712,"The increasing prominence of AI necessitates the deployment of inference platforms for efficient and effective management of AI pipelines and compute resources. As these pipelines grow in complexity, the demand for distributed serving rises and introduces much-dreaded network delays. In this paper, we investigate how the network can instead be a boon to the excessively high resource overheads of AI pipelines. To alleviate these overheads, we discuss how resource-intensive data processing tasks -- a key facet of growing AI pipeline complexity -- are well-matched for the computational characteristics of packet processing pipelines and how they can be offloaded onto SmartNICs. We explore the challenges and opportunities of offloading, and propose a research agenda for integrating network hardware into AI pipelines, unlocking new opportunities for optimization."
2502.15713,"This paper addresses the challenges of selecting relay nodes and coordinating among them in UAV-assisted Internet-of-Vehicles (IoV). The selection of UAV relay nodes in IoV employs mechanisms executed either at centralized servers or decentralized nodes, which have two main limitations: 1) the traceability of the selection mechanism execution and 2) the coordination among the selected UAVs, which is currently offered in a centralized manner and is not coupled with the relay selection. Existing UAV coordination methods often rely on optimization methods, which are not adaptable to different environment complexities, or on centralized deep reinforcement learning, which lacks scalability in multi-UAV settings. Overall, there is a need for a comprehensive framework where relay selection and coordination are coupled and executed in a transparent and trusted manner. This work proposes a framework empowered by reinforcement learning and Blockchain for UAV-assisted IoV networks. It consists of three main components: a two-sided UAV relay selection mechanism for UAV-assisted IoV, a decentralized Multi-Agent Deep Reinforcement Learning (MDRL) model for autonomous UAV coordination, and a Blockchain implementation for transparency and traceability in the interactions between vehicles and UAVs. The relay selection considers the two-sided preferences of vehicles and UAVs based on the Quality-of-UAV (QoU) and the Quality-of-Vehicle (QoV). Upon selection of relay UAVs, the decentralized coordination between them is enabled through an MDRL model trained to control their mobility and maintain the network coverage and connectivity using Proximal Policy Optimization (PPO). The evaluation results demonstrate that the proposed selection and coordination mechanisms improve the stability of the selected relays and maximize the coverage and connectivity achieved by the UAVs."
2502.15727,"This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture for network packet seed generation and enrichment. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds' structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings to dynamically reference to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol's Finite State Machine (FSM), then iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. We then evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground-truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities."
2502.15731,"The rapid evolution of communication networks towards 6G increasingly incorporates advanced AI-driven controls across various network segments to achieve intelligent, zero-touch operation. This paper proposes a comprehensive and modular framework for AI controllers, designed to be highly flexible and adaptable for use across both fiber optical and radio networks. Building on the principles established by the O-RAN Alliance for near-Real-Time RAN Intelligent Controllers (near-RT RICs), our framework extends this AI-driven control into the optical domain. Our approach addresses the critical need for a unified AI control framework across diverse network transport technologies and domains, enabling the development of intelligent, automated, and scalable 6G networks."
2502.15733,"The construction of channel gain map (CGM) is essential for realizing environment-aware wireless communications expected in 6G, for which a fundamental problem is how to predict the channel gains at unknown locations effectively by a finite number of measurements. As using a single prediction model is not effective in complex propagation environments, we propose a subregional learning-based CGM construction scheme, with which the entire map is divided into subregions via data-driven clustering, then individual models are constructed and trained for every subregion. In this way, specific propagation feature in each subregion can be better extracted with finite training data. Moreover, we propose to further improve prediction accuracy by uneven subregion sampling, as well as training data reuse around the subregion boundaries. Simulation results validate the effectiveness of the proposed scheme in CGM construction."
2502.15746,"Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in data-intensive services by dynamically caching frequently requested content on edge servers. This capability is critical for applications such as Augmented Reality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where efficient content caching and accurate popularity prediction are essential for optimizing performance. In this paper, we explore the problem of popularity prediction in MEC by utilizing historical time-series request data of intended files, formulating this problem as a ranking task. To this aim, we propose CacheMamba model by employing Mamba, a state-space model (SSM)-based architecture, to identify the top-K files with the highest likelihood of being requested. We then benchmark the proposed model against a Transformer-based approach, demonstrating its superior performance in terms of cache-hit rate, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and Floating-Point Operations Per Second (FLOPS), particularly when dealing with longer sequences."
2502.15754,"This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing."
2502.15775,"In recent years, the rapid expansion of Internet of Things (IoT) nodes and devices has seamlessly integrated technology into everyday life, amplifying the demand for optimized computing solutions. To meet the critical Quality of Service (QoS) requirements such as reduced latency, efficient bandwidth usage, swift reaction times, scalability, privacy, and security serverless edge computing has emerged as a transformative paradigm. This systematic literature review explores the current landscape of serverless edge computing, analyzing recent studies to uncover the present state of this technology. The review identifies the essential features of serverless edge computing, focusing on architectural designs, QoS metrics, implementation specifics, practical applications, and communication modalities central to this paradigm. Furthermore, we propose a comprehensive taxonomy that categorizes existing research efforts, providing a comparative analysis based on these classifications. The paper concludes with an in depth discussion of open research challenges and highlights promising future directions that hold potential for advancing serverless edge computing research."
2502.15918,"Network slicing is a key technology enabling the flexibility and efficiency of 5G networks, offering customized services for diverse applications. However, existing methods face challenges in adapting to dynamic network environments and lack interpretability in performance models. In this paper, we propose a novel interpretable network slice configuration algorithm (\emph{InSlicing}) in open radio access networks, by integrating Kolmogorov-Arnold Networks (KANs) and hybrid optimization process. On the one hand, we use KANs to approximate and learn the unknown performance function of individual slices, which converts the blackbox optimization problem. On the other hand, we solve the converted problem with a genetic method for global search and incorporate a trust region for gradient-based local refinement. With the extensive evaluation, we show that our proposed algorithm achieves high interpretability while reducing 25+\% operation cost than existing solutions."
2502.15936,"Satellite networks are rapidly evolving, yet most \glspl{ntn} remain isolated from terrestrial orchestration frameworks. Their control architectures are typically monolithic and static, limiting their adaptability to dynamic traffic, topology changes, and mission requirements. These constraints lead to inefficient spectrum use and underutilized network capacity. Although \gls{ai} promises automation, its deployment in orbit is limited by computing, energy, and connectivity limitations.This paper introduces Space-O-RAN, a distributed control architecture that extends Open RAN principles into satellite constellations through hierarchical, closed-loop control. Lightweight \glspl{dapp} operate onboard satellites, enabling real-time functions like scheduling and beam steering without relying on persistent ground access. Cluster-level coordination is managed via \glspl{spaceric}, which leverage low-latency \glspl{isl} for autonomous decisions in orbit. Strategic tasks, including AI training and policy updates, are transferred to terrestrial platforms \glspl{smo} using digital twins and feeder links.A key enabler is the dynamic mapping of the O-RAN interfaces to satellite links, supporting adaptive signaling under varying conditions. Simulations using the Starlink topology validate the latency bounds that inform this architectural split, demonstrating both feasibility and scalability for autonomous satellite RAN operations."
2502.16028,"In precision agriculture and plant science, there is an increasing demand for wireless sensors that are easy to deploy, maintain, and monitor. This paper investigates a novel approach that leverages recent advances in extremely low-power wireless communication and sensing, as well as the rapidly increasing availability of unmanned aerial vehicle (UAV) platforms. By mounting a specialized wireless payload on a UAV, battery-less sensor tags can harvest wireless beacon signals emitted from the drone, dramatically reducing the cost per sensor. These tags can measure environmental information such as temperature and humidity, then encrypt and transmit the data in the range of several meters. An experimental implementation was constructed at AERPAW, an NSF-funded wireless aerial drone research platform. While ground-based tests confirmed reliable sensor operation and data collection, airborne trials encountered wireless interference that impeded successfully detecting tag data. Despite these challenges, our results suggest further refinements could improve reliability and advance precision agriculture and agrarian research."
2502.16198,"6G networks aim to achieve global coverage, massive connectivity, and ultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and Semantic Communication (SemCom) are essential for realizing these goals, yet they introduce considerable complexity in resource orchestration. Drawing inspiration from research in robotics, a viable solution to manage this complexity is the application of Large Language Models (LLMs). Although the use of LLMs in network orchestration has recently gained attention, existing solutions have not sufficiently addressed LLM hallucinations or their adaptation to network dynamics. To address this gap, this paper proposes a framework called Autonomous Reinforcement Coordination (ARC) for a SemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented Generator (RAG) monitors services, users, and resources and processes the collected data, while a Hierarchical Action Planner (HAP) orchestrates resources. ARC decomposes orchestration into two tiers, utilizing LLMs for high-level planning and Reinforcement Learning (RL) agents for low-level decision-making, in alignment with the Mixture of Experts (MoE) concept. The LLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered by contrastive learning, while the RL agents employ replay buffer management for continual learning, thereby achieving efficiency, accuracy, and adaptability. Simulations are provided to demonstrate the effectiveness of ARC, along with a comprehensive discussion on potential future research directions to enhance and upgrade ARC."
2502.16199,"Wireless key generation holds significant promise for establishing cryptographic keys in Next-Gen Internet of Vehicles (IoV) systems. However, existing approaches often face inefficiencies and performance limitations caused by frequent channel probing and ineffective quantization. To address these challenges, this paper introduces LLMKey, a novel key generation system designed to enhance efficiency and security. We identify excessive channel probing and suboptimal quantization as critical bottlenecks in current methods. To mitigate these issues, we propose an innovative large language model (LLM)-based channel probing technique that leverages the capabilities of LLMs to reduce probing rounds while preserving crucial channel information. Instead of conventional quantization, LLMKey adopts a perturbed compressed sensing-based key delivery mechanism, improving both robustness and security. Extensive evaluations are conducted in four real-world scenarios, encompassing V2I (Vehicle-to-Infrastructure) and V2V (Vehicle-to-Vehicle) settings in both urban and rural environments. The results show that LLMKey achieves an average key agreement rate of 98.78\%, highlighting its effectiveness and reliability across diverse conditions."
2502.16228,"With the popularity of cloud computing and data-intensive applications such as machine learning, datacenter networks have become a critical infrastructure for our digital society. Given the explosive growth of datacenter traffic and the slowdown of Moore's law, significant efforts have been made to improve datacenter network performance over the last decade. A particularly innovative solution is reconfigurable datacenter networks (RDCNs): datacenter networks whose topologies dynamically change over time, in either a demand-oblivious or a demand-aware manner. Such dynamic topologies are enabled by recent optical switching technologies and stand in stark contrast to state-of-the-art datacenter network topologies, which are fixed and oblivious to the actual traffic demand. In particular, reconfigurable demand-aware and 'self-adjusting' datacenter networks are motivated empirically by the significant spatial and temporal structures observed in datacenter communication traffic. This paper presents an overview of reconfigurable datacenter networks. In particular, we discuss the motivation for such reconfigurable architectures, review the technological enablers, and present a taxonomy that classifies the design space into two dimensions: static vs. dynamic and demand-oblivious vs. demand-aware. We further present a formal model and discuss related research challenges. Our article comes with complementary video interviews in which three leading experts, Manya Ghobadi, Amin Vahdat, and George Papen, share with us their perspectives on reconfigurable datacenter networks."
2502.16498,"Traditional congestion control algorithms struggle to maintain the consistent and satisfactory data transmission performance over time-varying networking condition. Simultaneously, as video traffic becomes dominant, the loose coupling between the DASH framework and TCP congestion control results in the un-matched bandwidth usage, thereby limiting video streaming performance. To address these issues, this paper proposes a receiver-driven congestion control framework named Nuwa. Nuwa deploys the congestion avoidance phase at the receiver-side, utilizing one-way queueing delay detection to monitor network congestion and setting specific target delays for different applications. Experimental results demonstrate that, in most cases, with appropriate parameter configuration, Nuwa can improve the throughput of TCP flows 4% to 15.4% and reduce average queueing delay by 6.9% to 29.4%. Furthermore, we also introduce the use of reinforcement learning to dynamically adjust Nuwa's key parameter , enhancing Nuwa's adaptability to the unpredictable environment."
2502.16866,"The increasing complexity and scale of modern telecommunications networks demand intelligent automation to enhance efficiency, adaptability, and resilience. Agentic AI has emerged as a key paradigm for intelligent communications and networking, enabling AI-driven agents to perceive, reason, decide, and act within dynamic networking environments. However, effective decision-making in telecom applications, such as network planning, management, and resource allocation, requires integrating retrieval mechanisms that support multi-hop reasoning, historical cross-referencing, and compliance with evolving 3GPP standards. This article presents a forward-looking perspective on generative information retrieval-inspired intelligent communications and networking, emphasizing the role of knowledge acquisition, processing, and retrieval in agentic AI for telecom systems. We first provide a comprehensive review of generative information retrieval strategies, including traditional retrieval, hybrid retrieval, semantic retrieval, knowledge-based retrieval, and agentic contextual retrieval. We then analyze their advantages, limitations, and suitability for various networking scenarios. Next, we present a survey about their applications in communications and networking. Additionally, we introduce an agentic contextual retrieval framework to enhance telecom-specific planning by integrating multi-source retrieval, structured reasoning, and self-reflective validation. Experimental results demonstrate that our framework significantly improves answer accuracy, explanation consistency, and retrieval efficiency compared to traditional and semantic retrieval methods. Finally, we outline future research directions."
2502.1712,"The advancement towards 6G technology leverages improvements in aerial-terrestrial networking, where one of the critical challenges is the efficient allocation of transmit power. Although existing studies have shown commendable performance in addressing this challenge, a revolutionary breakthrough is anticipated to meet the demands and dynamism of 6G. Potential solutions include: 1) semantic communication and orchestration, which transitions the focus from mere transmission of bits to the communication of intended meanings of data and their integration into the network orchestration process; and 2) distributed machine learning techniques to develop adaptable and scalable solutions. In this context, this paper introduces a power allocation framework specifically designed for semantic-aware networks. The framework addresses a scenario involving multiple Unmanned Aerial Vehicles (UAVs) that collaboratively transmit observations over a multi-channel uplink medium to a central server, aiming to maximise observation quality. To tackle this problem, we present the Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) algorithm, which utilizes the data quality of observing areas as reward feedback during the training phase, thereby constituting a semantic-aware learning mechanism. Simulation results substantiate the efficacy and scalability of our approach, demonstrating its superior performance compared to traditional bit-oriented learning and heuristic algorithms."
2502.17167,"The Metaverse holds the potential to revolutionize digital interactions through the establishment of a highly dynamic and immersive virtual realm over wireless communications systems, offering services such as massive twinning and telepresence. This landscape presents novel challenges, particularly efficient management of multiple access to the frequency spectrum, for which numerous adaptive Deep Reinforcement Learning (DRL) approaches have been explored. However, challenges persist in adapting agents to heterogeneous and non-stationary wireless environments. In this paper, we present a novel approach that leverages Continual Learning (CL) to enhance intelligent Medium Access Control (MAC) protocols, featuring an intelligent agent coexisting with legacy User Equipments (UEs) with varying numbers, protocols, and transmission profiles unknown to the agent for the sake of backward compatibility and privacy. We introduce an adaptive Double and Dueling Deep Q-Learning (D3QL)-based MAC protocol, enriched by a symmetry-aware CL mechanism, which maximizes intelligent agent throughput while ensuring fairness. Mathematical analysis validates the efficiency of our proposed scheme, showcasing superiority over conventional DRL-based techniques in terms of throughput, collision rate, and fairness, coupled with real-time responsiveness in highly dynamic scenarios."
2502.17345,"Next-generation touristic services will rely on the advanced mobile networks' high bandwidth and low latency and the Multi-access Edge Computing (MEC) paradigm to provide fully immersive mobile experiences. As an integral part of travel planning systems, recommendation algorithms devise personalized tour itineraries for individual users considering the popularity of a city's Points of Interest (POIs) as well as the tourist preferences and constraints. However, in the context of next-generation touristic services, recommendation algorithms should also consider the applications (e.g., social network, mobile video streaming, mobile augmented reality) the tourist will consume in the POIs and the quality in which the MEC infrastructure will deliver such applications. In this paper, we address the joint problem of recommending personalized tour itineraries for tourists and efficiently allocating MEC resources for advanced touristic applications. We formulate an optimization problem that maximizes the itinerary of individual tourists while optimizing the resource allocation at the network edge. We then propose an exact algorithm that quickly solves the problem optimally, considering instances of realistic size. Using a real-world location-based photo-sharing database, we conduct and present an exploratory analysis to understand preferences and users' visiting patterns. Using this understanding, we propose a methodology to identify user interest in applications. Finally, we evaluate our algorithm using this dataset. Results show that our algorithm outperforms a modified version of a state-of-the-art solution for personalized tour itinerary recommendation, demonstrating gains up to 11% for resource allocation efficiency and 40% for user experience. In addition, our algorithm performs similarly to the modified state-of-the-art solution regarding traditional itinerary recommendation metrics."
2502.1735,"This work explores employing the concept of goal-oriented (GO) semantic communication for real-time monitoring and control. Generally, GO communication advocates for the deep integration of application targets into the network design. We consider CPS and IoT applications where sensors generate a tremendous amount of network traffic toward monitors or controllers. Here, the practical introduction of GO communication must address several challenges. These include stringent timing requirements, challenging network setups, and limited computing and communication capabilities of the devices involved. Moreover, real-life CPS deployments often rely on heterogeneous communication standards prompted by specific hardware. To address these issues, we introduce a middleware design of a GO distributed Transport Layer (TL) framework for control applications. It offers end-to-end performance improvements for diverse setups and transmitting hardware. The proposed TL protocol evaluates the Value of sampled state Updates (VoU) for the application goal. It decides whether to admit or discard the corresponding packets, thus offloading the network. VoU captures the contribution of utilizing the updates at the receiver into the application's performance. We introduce a belief network and the augmentation procedure used by the sensor to predict the evolution of the control process, including possible delays and losses of status updates in the network. The prediction is made either using a control model dynamics or a Long-Short Term Memory neural network approach. We test the performance of the proposed TL in the experimental framework using Industrial IoT Zolertia ReMote sensors. We show that while existing approaches fail to deliver sufficient control performance, our VoU-based TL scheme ensures stability and performs $\sim$$60\%$ better than the naive GO TL we proposed in our previous work."
2502.18238,"This paper addresses the challenge of integrating semantic communication principles into operated networks, traditionally optimized based on network-centric metrics rather than application-specific needs. Operated networks strongly adhere to the principle of ``separation of concerns"", which emphasizes a clear distinction between network operation and application. Despite the initial perceived incompatibility between semantic communication and the principles of operated networks, this paper provides solutions to reconcile them. The foundations of these solutions include the adoption of non-arbitrary semantic representations as a standard encoding for communications, the establishment of a standard interface between the application and network, and a dedicated network control plane. These enable the application to describe the data typology and the nature of the task, and to agree upon a transmission scheme tailored to the supported task. Through three scenarios involving an application transmitting text representations, we illustrate the implementation of the proposal and demonstrate the potential of the approach."
2502.18381,"Assessing wireless coverage is a fundamental task for public network operators and private deployments, whose goal is to guarantee quality of service across the network while minimizing material waste and energy consumption. These maps are usually built through ray tracing techniques and/or channel measurements that can be consequently translated into network Key Performance Indicators (KPIs), such as capacity or throughput. However, next generation networks (e.g., 6G) typically involve beyond communication resources, towards services that require data transmission, but also processing (local and remote) to perform complex decision making in real time, with the best balance between performance, energy consumption, material waste, and privacy. In this paper, we introduce the novel concept of areas of effectiveness, which goes beyond the legacy notion of coverage, towards one that takes into account capability of the network of offering edge Artificial Intelligence (AI)-related computation. We will show that radio coverage is a poor indicator of real system performance, depending on the application and the computing capabilities of network and devices. This opens new challenges in network planning, but also resource orchestration during operation to achieve the specific goal of communication."
2502.18671,"Wireless Sensor Networks have risen as a highly promising technology suitable for precision agriculture implementations, enabling efficient monitoring and control of agricultural processes. In precision agriculture, accurate and synchronized data collection is crucial for effective analysis and decision making. Using principles of information theory, we can define conditions and parameters that influence the efficient transmission and processing of information. Existing technologies have limitations in maintaining consistent time references, handling node failures, and unreliable communication links, leading to inaccurate data readings. Reliable data storage is demanding now-a-days for storing data on local monitoring station as well as in online live server. Sometime internet is not working properly due to congestion and there is frequent packet loss. Current solutions often synchronize records based on database timestamps, leading to record duplication and waste storage. Both databases synchronize each other after internet restoration. By providing synchronization among nodes and data, accuracy and storage will be saved in IoT based WSNs for precision agriculture applications. A prototype Node-MCU internal memory is used as a resource for achieving data synchronization. This proposed work generates record ID from Node MCU EEPROM which helps in records synchronization if there is any packet loss at the local server or at the online server to maintain synchronization accuracy despite unreliable communication links. Experiment shows that for a particular duration Node MCU generated 2364 packets and packet loss at local server was 08 and at online server was 174 packets. Results shows that after synchronization 99.87% packets were synchronized. Using previous technique of timestamp, the redundancy was 70% which reduced to 0% using our proposed technique."
2502.18753,"Reconfigurable Intelligent Surfaces (RISs) pose as a transformative technology to revolutionize the cellular architecture of Next Generation (NextG) Radio Access Networks (RANs). Previous studies have demonstrated the capabilities of RISs in optimizing wireless propagation, achieving high spectral efficiency, and improving resource utilization. At the same time, the transition to softwarized, disaggregated, and virtualized architectures, such as those being standardized by the O-RAN ALLIANCE, enables the vision of a reconfigurable Open RAN. In this work, we aim to integrate these technologies by studying how different resource allocation policies enhance the performance of RIS-assisted Open RANs. We perform a comparative analysis among various network configurations and show how proper network optimization can enhance the performance across the Enhanced Mobile Broadband (eMBB) and Ultra Reliable and Low Latency Communications (URLLC) network slices, achieving up to ~34% throughput improvement. Furthermore, leveraging the capabilities of OpenRAN Gym, we deploy an xApp on Colosseum, the world's largest wireless system emulator with hardware-in-the-loop, to control the Base Station (BS)'s scheduling policy. Experimental results demonstrate that RIS-assisted topologies achieve high resource efficiency and low latency, regardless of the BS's scheduling policy."
2502.19004,"Although multi-tier vehicular Metaverse promises to transform vehicles into essential nodes -- within an interconnected digital ecosystem -- using efficient resource allocation and seamless vehicular twin (VT) migration, this can hardly be achieved by the existing techniques operating in a highly dynamic vehicular environment, since they can hardly balance multi-objective optimization problems such as latency reduction, resource utilization, and user experience (UX). To address these challenges, we introduce a novel multi-tier resource allocation and VT migration framework that integrates Graph Convolutional Networks (GCNs), a hierarchical Stackelberg game-based incentive mechanism, and Multi-Agent Deep Reinforcement Learning (MADRL). The GCN-based model captures both spatial and temporal dependencies within the vehicular network; the Stackelberg game-based incentive mechanism fosters cooperation between vehicles and infrastructure; and the MADRL algorithm jointly optimizes resource allocation and VT migration in real time. By modeling this dynamic and multi-tier vehicular Metaverse as a Markov Decision Process (MDP), we develop a MADRL-based algorithm dubbed the Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MO-MADDPG), which can effectively balances the various conflicting objectives. Extensive simulations validate the effectiveness of this algorithm that is demonstrated to enhance scalability, reliability, and efficiency while considerably improving latency, resource utilization, migration cost, and overall UX by 12.8%, 9.7%, 14.2%, and 16.1%, respectively."
2502.1902,One of the main challenges associated with the implementation of an Open RAN User-Centric Cell-Free network is the appropriate formulation of serving clusters. This problem can be effectively solved using a proposed algorithm that leverages cooperation between xApp and rApp. Simulation results have shown that the proposed solution allows for a 16% increase in the median of the user throughput distribution compared to the state-of-the-art network-centric architecture
2502.19029,"The upcoming generation of mobile telecommunication systems is expected to support new use cases, where the mobile network serves one or more IP subnetworks located behind the User Equipment (UEs). This would create new challenges for the mobile system to efficiently serve such behind-UE subnetworks, as the latter are commonly not visible to the mobile system. In 3GPP, there have been works on Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet), where the 5G System (5GS) is considered as a bridge or a DetNet node. In order to efficiently serve behind-UE IP subnetworks, we foresee the need for a further generalization where the mobile system (5GS and beyond) acts as a set of IP routers with more generic capabilities. In this article, we introduce the concept of Mobile System Router (MS-Router) which aims to provide a reference architectural design to enable the support of IP routing in the next generation of mobile telecommunication systems. The concept models a mobile system as an IP router per User Plane granularity. Each MS-Router implements an IP routing protocol, exchanges routing messages with the external routers and constructs a routing table, enabling the mobile system to dynamically learn the topology of the IP subnetwork behind the UEs and Data Network. The learned topology is translated into User Plane configuration to serve the IP subnetworks in an optimal way. The article also advocates different approaches where routing protocols can be implemented in the mobile system."
2502.19838,"The scarcity of the licensed spectrum is forcing emerging Internet of Things (IoT) networks to operate within the unlicensed spectrum. Yet there has been extensive observation indicating that performance deterioration and significant unfairness would arise, when newly deployed Aloha-based networks coexist with incumbent Carrier Sense Multiple Access (CSMA)-based WiFi networks, especially without proper adjustment of packet transmission times. Therefore, ensuring harmonious cohabitation between Aloha and CSMA networks is of paramount importance. How to properly tune system parameters to guarantee harmonious coexistence between these two networks, nevertheless, remains largely unexplored. To address the above open issue, this paper proposed a novel dual-channel analytical framework to characterize the throughput performance of the cohabitation between slotted Aloha and CSMA networks. To achieve harmonious coexistence, the total throughput of the coexisting network under a given desired throughput proportion is optimized by tuning the packet transmission time of CSMA nodes and transmission probabilities. The optimization results indicate that the packet transmission time of CSMA nodes should be set slightly less than that of Aloha nodes. The proposed framework is further applied to enhance the network throughput and fairness of the cohabitation of LTE Unlicensed and WiFi networks."
2502.19997,"HTTP is frequently used by smartphones and IoT devices to access information and Web services. Nowadays, HTTP is used in three major versions, each introducing significant changes with respect to the previous one. We evaluated the energy consumption of the major versions of the HTTP protocol when used in the communication between energy-constrained devices and cloud-based or edge-based services. Experimental results show that in a machine-to-machine communication scenario, for the considered client devices - a smartphone and a Single Board Computer - and for a number of cloud/edge services and facilities, HTTP/3 frequently requires more energy than the previous versions of the protocol. The focus of our analysis is on machine-to-machine communication, but to obtain a broader view we also considered a client-server interaction pattern that is more browsing-like. In this case, HTTP/3 can be more energy efficient than the other versions."
2502.20203,"A payment channel network is a blockchain-based overlay mechanism that allows parties to transact more efficiently than directly using the blockchain. These networks are composed of payment channels that carry transactions between pairs of users. Due to its design, a payment channel cannot sustain a net flow of money in either direction indefinitely. Therefore, a payment channel network cannot serve transaction requests arbitrarily over a long period of time. We introduce \emph{DEBT control}, a joint routing and flow-control protocol that guides a payment channel network towards an optimal operating state for any steady-state demand. In this protocol, each channel sets a price for routing transactions through it. Transacting users make flow-control and routing decisions by responding to these prices. A channel updates its price based on the net flow of money through it. The protocol is developed by formulating a network utility maximization problem and solving its dual through gradient descent. We provide convergence guarantees for the protocol and also illustrate its behavior through simulations."
2502.2032,"Recent advancements in AI and edge computing have accelerated the development of machine-centric applications (MCAs), such as smart surveillance systems. In these applications, video cameras and sensors offload inference tasks like license plate recognition and vehicle tracking to remote servers due to local computing and energy constraints. However, legacy network solutions, designed primarily for human-centric applications, struggle to reliably support these MCAs, which demand heterogeneous and fluctuating QoS (due to diverse application inference tasks), further challenged by dynamic wireless network conditions and limited spectrum resources. To tackle these challenges, we propose an Application Context-aware Cross-layer Optimization and Resource Design (ACCORD) framework. This innovative framework anticipates the evolving demands of MCAs in real time, quickly adapting to provide customized QoS and optimal performance, even for the most dynamic and unpredictable MCAs. This also leads to improved network resource management and spectrum utilization. ACCORD operates as a closed feedback-loop system between the application client and network and consists of two key components: (1) Building Application Context: It focuses on understanding the specific context of MCA requirements. Contextual factors include device capabilities, user behavior (e.g., mobility speed), and network channel conditions. (2) Cross-layer Network Parameter Configuration: Utilizing a DRL approach, this component leverages the contextual information to optimize network configuration parameters across various layers, including PHY, MAC, and RLC, as well as the application layer, to meet the desired QoS requirement in real-time. Extensive evaluation with the 3GPP-compliant MATLAB 5G toolbox demonstrates the practicality and effectiveness of our proposed ACCORD framework."
2502.20598,One of the primary research interests adhering to next-generation fiber-wireless access networks is human-to-machine/robot (H2M/R) collaborative communications facilitating Industry 5.0. This paper discusses scalable H2M/R communications across large geographical distances that also allow rapid onboarding of new machines/robots as $\sim72\%$ training time is saved through global-local coordinated learning.
2502.20996,"Future wireless networks will need to support diverse applications (such as extended reality), scenarios (such as fully automated industries), and technological advances (such as terahertz communications). Current wireless networks are designed to perform adequately across multiple scenarios so they lack the adaptability needed for specific use cases. Therefore, meeting the stringent requirements of next-generation applications incorporating technology advances and operating in novel scenarios will necessitate wireless specialized networks which we refer to as SpecNets. These networks, equipped with cognitive capabilities, dynamically adapt to the unique demands of each application, e.g., by automatically selecting and configuring network mechanisms. An enabler of SpecNets are the recent advances in artificial intelligence and machine learning (AI/ML), which allow to continuously learn and react to changing requirements and scenarios. By integrating AI/ML functionalities, SpecNets will fully leverage the concept of AI/ML-defined radios (MLDRs) that are able to autonomously establish their own communication protocols by acquiring contextual information and dynamically adapting to it. In this paper, we introduce SpecNets and explain how MLDR interfaces enable this concept. We present three illustrative use cases for wireless local area networks (WLANs): bespoke industrial networks, traffic-aware robust THz links, and coexisting networks. Finally, we showcase SpecNets' benefits in the industrial use case by introducing a lightweight, fast-converging ML agent based on multi-armed bandits (MABs). This agent dynamically optimizes channel access to meet varying performance needs: high throughput, low delay, or fair access. Results demonstrate significant gains over IEEE 802.11, highlighting the system's autonomous adaptability across diverse scenarios."
2502.21039,"In this paper, we propose procedures to address platoon follower dynamics within adaptive beaconing. We implement them in a known adaptive beaconing scheme which is Jerk Beaconing (JB) to improve its safety. We evaluate our proposed approach in terms of safety, string stability and the channel busy ratio (CBR) overhead. The results reveal that our proposal significantly enhances safety without imposing substantial CBR overhead and maintains the string stability of the PATH CACC controller under normal conditions."
2502.2108,"The current development trend of wireless communications aims at coping with the very stringent reliability and latency requirements posed by several emerging Internet of Things (IoT) application scenarios. Since the problem of realizing Ultra Reliable Low-Latency Communications (URLLC) is becoming more and more important, it has attracted the attention of researchers, and new efficient resource allocation algorithms are necessary. In this paper, we consider a challenging scenario where the available spectrum might be fragmented across non-adjacent portions of the band, and channels are differently affected by interference coming from surrounding networks. Furthermore, Channel State Information (CSI) is assumed to be unavailable, thus requiring an allocation of resources based only on topology information and channel statistics. To address this challenge in a dense smart factory scenario where devices periodically transmit their data to a common receiver, we present a novel resource allocation methodology based on a graph-theoretical approach originally designed to allocate mobility resources in on-demand, shared transportation. The proposed methodology is compared with two benchmark allocation strategies, showing its ability of increasing spectral efficiency of as much as 50% with respect to the best performing benchmark. Contrary to what happens in many resource allocation settings, this increase in spectrum efficiency does not come at the expense of fairness, which is also increased as compared to benchmark algorithms."
2502.21117,"Wireless edge networks in smart industrial environments increasingly operate using advanced sensors and autonomous machines interacting with each other and generating huge amounts of data. Those huge amounts of data are bound to make data management (e.g., for processing, storing, computing) a big challenge. Current data management approaches, relying primarily on centralized data storage, might not be able to cope with the scalability and real time requirements of Industry 4.0 environments, while distributed solutions are increasingly being explored. In this paper, we introduce the problem of distributed data access in multi-hop wireless industrial edge deployments, whereby a set of consumer nodes needs to access data stored in a set of data cache nodes, satisfying the industrial data access delay requirements and at the same time maximizing the network lifetime. We prove that the introduced problem is computationally intractable and, after formulating the objective function, we design a two-step algorithm in order to address it. We use an open testbed with real devices for conducting an experimental investigation on the performance of the algorithm. Then, we provide two online improvements, so that the data distribution can dynamically change before the first node in the network runs out of energy. We compare the performance of the methods via simulations for different numbers of network nodes and data consumers, and we show significant lifetime prolongation and increased energy efficiency when employing the method which is using only decentralized low-power wireless communication instead of the method which is using also centralized local area wireless communication."
2502.21121,"The challenging applications envisioned for the future Internet of Things networks are making it urgent to develop fast and scalable resource allocation algorithms able to meet the stringent reliability and latency constraints typical of the Ultra Reliable, Low Latency Communications (URLLC).However, there is an inherent tradeoff between complexity and performance to be addressed: sophisticated resource allocation methods providing optimized spectrum utilization are challenged by the scale of applications and the concomitant stringent latency constraints. Whether non-trivial resource allocation approaches can be successfully applied in large-scale network instances is still an open question that this paper aims to address. More specifically, we consider a scenario in which Channel State Information (CSI) is used to improve spectrum allocation in a radio environment that experiences channel time correlation.Channel correlation allows the usage of CSI for longer time before an update, thus lowering the overhead burden. Following this intuition, we propose a dynamic pilot transmission allocation scheme in order to adaptively tune the CSI age.We systematically analyze the improvement of this approach applied to a sophisticated, recently introduced graph-based resource allocation method that we extend here to account for CSI.The results show that, even in very dense networks and accounting for the higher computational time of the graph-based approach, this algorithm is able to improve spectrum efficiency by over 12% as compared to a greedy heuristic, and that dynamic pilot transmissions allocation can further boost its performance in terms of fairness, while concomitantly further increase spectrum efficiency of 3-5%. \"
2502.21255,"The increasing traffic demand in cellular networks has recently led to the investigation of new strategies to save precious resources like spectrum and energy. Direct device-to-device (D2D) communication becomes a promising solution if the two terminals are located in close proximity. In this case, the D2D communications should coexist with cellular transmissions, so they must be carefully scheduled in order to avoid harmful interference impacts. In this paper, we outline a novel framework encompassing channel allocation, mode selection and power control for D2D communications. Power allocation is done in a distributed and cognitive fashion at the beginning of each time slot, based on local information, while channel/mode selection is performed in a centralized manner only at the beginning of an epoch, a time interval including a series of subsequent time slots. This hybrid approach guarantees an effective tradeoff between overhead and adaptivity. We analyze in depth the distributed power allocation mechanism, and we state a theorem which allows to derive the optimal power allocation strategy and to compute the corresponding throughput. Extensive simulations confirm the benefits granted by our approach, when compared with state-of-the-art distributed schemes, in terms of throughput and fairness."
2503.00154,"Non-Terrestrial Networks (NTNs) are becoming a critical component of modern communication infrastructures, especially with the advent of Low Earth Orbit (LEO) satellite systems. Traditional centralized learning approaches face major challenges in such networks due to high latency, intermittent connectivity and limited bandwidth. Federated Learning (FL) is a promising alternative as it enables decentralized training while maintaining data privacy. However, existing FL models, such as Federated Learning with Multi-Layer Perceptrons (Fed-MLP), can struggle with high computational complexity and poor adaptability to dynamic NTN environments. This paper provides a detailed analysis for Federated Learning with Kolmogorov-Arnold Networks (Fed-KAN), its implementation and performance improvements over traditional FL models in NTN environments for traffic forecasting. The proposed Fed-KAN is a novel approach that utilises the functional approximation capabilities of KANs in a FL framework. We evaluate Fed-KAN compared to Fed-MLP on a traffic dataset of real satellite operator and show a significant reduction in training and test loss. Our results show that Fed-KAN can achieve a 77.39% reduction in average test loss compared to Fed-MLP, highlighting its improved performance and better generalization ability. At the end of the paper, we also discuss some potential applications of Fed-KAN within O-RAN and Fed-KAN usage for split functionalities in NTN architecture."
2503.00259,"With the increasing demand for wireless connectivity, ensuring the efficient coexistence of multiple radio access technologies in shared unlicensed spectrum has become an important issue. This paper focuses on optimizing Medium Access Control (MAC) parameters to enhance the coexistence of 5G New Radio in Unlicensed Spectrum (NR-U) and Wi-Fi networks operating in unlicensed spectrum with multiple priority classes of traffic that may have varying quality-of-service (QoS) requirements. In this context, we tackle the coexistence parameter management problem by introducing a QoS-aware State-Augmented Learnable (QaSAL) framework, designed to improve network performance under various traffic conditions. Our approach augments the state representation with constraint information, enabling dynamic policy adjustments to enforce QoS requirements effectively. Simulation results validate the effectiveness of QaSAL in managing NR-U and Wi-Fi coexistence, demonstrating improved channel access fairness while satisfying a latency constraint for high-priority traffic."
2503.00375,"Edge computing is a promising solution to enable low-latency IoT applications, by shifting computation from remote data centers to local devices, less powerful but closer to the end user devices. However, this creates the challenge on how to best assign clients to edge nodes offering compute capabilities. So far, two antithetical architectures are proposed: centralized resource orchestration or distributed overlay. In this work we explore a third way, called uncoordinated access, which consists in letting every device exploring multiple opportunities, to opportunistically embrace the heterogeneity of network and load conditions towards diverse edge nodes. In particular, our contribution is intended for emerging serverless IoT applications, which do not have a state on the edge nodes executing tasks. We model the proposed system as a set of M/M/1 queues and show that it achieves a smaller jitter delay than single edge node allocation. Furthermore, we compare uncoordinated access with state-of-the-art centralized and distributed alternatives in testbed experiments under more realistic conditions. Based on the results, our proposed approach, which requires a tiny fraction of the complexity of the alternatives in both the device and network components, is very effective in using the network resources, while incurring only a small penalty in terms of increased compute load and high percentiles of delay."
2503.00767,"We are witnessing a new era where problem-solving and cognitive tasks are being increasingly delegated to Large Language Models (LLMs) across diverse domains, ranging from code generation to holiday planning. This trend also creates a demand for the ubiquitous execution of LLM-powered applications in a wide variety of environments in which traditional terrestrial 2D networking infrastructures may prove insufficient. A promising solution in this context is to extend edge computing into a 3D setting to include aerial platforms organized in multiple layers, a paradigm we refer to as air computing, to augment local devices for running LLM and Generative AI (GenAI) applications. This approach alleviates the strain on existing infrastructure while enhancing service efficiency by offloading computational tasks to the corresponding air units such as UAVs. Furthermore, the coordinated deployment of various air units can significantly improve the Quality of Experience (QoE) by ensuring seamless, adaptive, and resilient task execution. In this study, we investigate the synergy between LLM-based applications and air computing, exploring their potential across various use cases. Additionally, we present a disaster response case study demonstrating how the collaborative utilization of LLMs and air computing can significantly improve outcomes in critical situations."
2503.00976,"The increasing proliferation of Internet of Things (IoT) devices has created a growing need for more efficient communication networks, especially in areas where continuous connectivity is unstable or unavailable. Opportunistic networks have emerged as a possible solution in such scenarios, allowing for intermittent and decentralized data sharing. This article presents a novel communication protocol that uses Bluetooth 5 and the libp2p framework to enable decentralized and opportunistic communications among IoT devices. The protocol provides dynamic peer discovery and decentralized management, resulting in a more flexible and robust IoT network infrastructure. The performance of the proposed architecture was evaluated through experiments in both controlled and industrial scenarios, with a particular emphasis on latency and on the impact of the presence of obstacles. The obtained results show that the protocol has the ability to improve data transfer in environments with limited connectivity, making it adequate for both urban and rural areas, as well as for challenging environments such as shipyards. Moreover, the presented findings conclude that the protocol works well in situations with minimal signal obstruction and short distances, like homes, where average latency values of about 8 s have been achieved with no losses. Furthermore, the protocol can also be used in industrial scenarios, even when metal obstacles increase signal attenuation, and over long distances, where average latency values of about 8.5 s have been obtained together with packet losses of less than 5%."
2503.013,"This paper presents a detailed analysis of coverage in a factory environment using realistic 3D map data to evaluate the benefits of Distributed MIMO (D-MIMO) over colocalized approach. Our study emphasizes the importance of network densification in enhancing D-MIMO performance, ensuring that User Equipment (UE) remains within range of multiple Access Points (APs). To assess MIMO capacity, we compare two propagation channel models: ray tracing and stochastic. While ray tracing provides accurate predictions by considering environmental details and consistent correlations within the MIMO response, stochastic models offer a more generalized and efficient approach. The analysis outlines the strengths and limitations of each model when applied to the simulation of the downlink (DL) and uplink (UL) single-user capacity in various D-MIMO deployment scenarios."
2503.01336,"As computational resources are placed at different points in the edge-cloud continuum, not only the responsiveness on the client side is affected, but also the energy spent during communications. We summarize the main approaches used to estimate the energy consumption of smartphones and the main difficulties that are typically encountered. A case study then shows how such approaches can be put into practice. Results show that the edge is favorable in terms of energy consumption, compared to more remote locations."
2503.01374,"QUIC is a new transport protocol combining the reliability and congestion control features of TCP with the security features of TLS. One of the main challenges with QUIC is to guarantee that any of its implementation follows the IETF specification. This challenge is particularly appealing as the specification is written in textual language, and hence may contain ambiguities. In a recent work, McMillan and Zuck proposed a formal representation of part of draft-18 of the IETF specification. They also showed that this representation made it possible to efficiently generate tests to stress four implementations of QUIC. Our first contribution is to complete and extend the formal representation from draft-18 to draft-29. Our second contribution is to test seven implementations of both QUIC client and server. Our last contribution is to show that our tool can highlight ambiguities in the QUIC specification, for which we suggest paths to corrections"
2503.01397,"As 6G networks evolve, inter-provider agreements become crucial for dynamic resource sharing and network slicing across multiple domains, requiring on-demand capacity provisioning while enabling trustworthy interaction among diverse operators. To address these challenges, we propose a blockchain-based Decentralized Application (DApp) on Ethereum that introduces four smart contracts, organized into a Preliminary Agreement Phase and an Enforcement Phase, and measures their gas usage, thereby establishing an open marketplace where service providers can list, lease, and enforce resource sharing. We present an empirical evaluation of how gas price, block size, and transaction count affect transaction processing time on the live Sepolia Ethereum testnet in a realistic setting, focusing on these distinct smart-contract phases with varying computational complexities. We first examine transaction latency as the number of users (batch size) increases, observing median latencies from 12.5 s to 23.9 s in the Preliminary Agreement Phase and 10.9 s to 24.7 s in the Enforcement Phase. Building on these initial measurements, we perform a comprehensive Kruskal-Wallis test (p < 0.001) to compare latency distributions across quintiles of gas price, block size, and transaction count. The post-hoc analyses reveal that high-volume blocks overshadow fee variations when transaction logic is more complex (effect sizes up to 0.43), whereas gas price exerts a stronger influence when the computation is lighter (effect sizes up to 0.36). Overall, 86% of transactions finalize within 30 seconds, underscoring that while designing decentralized applications, there must be a balance between contract complexity and fee strategies. The implementation of this work is publicly accessible online."
2503.0177,"Flow-level simulation is widely used to model large-scale data center networks due to its scalability. Unlike packet-level simulators that model individual packets, flow-level simulators abstract traffic as continuous flows with dynamically assigned transmission rates. While this abstraction enables orders-of-magnitude speedup, it is inaccurate by omitting critical packet-level effects such as queuing, congestion control, and retransmissions.We present m4, an accurate and scalable flow-level simulator that uses machine learning to learn the dynamics of the network of interest. At the core of m4 lies a novel ML architecture that decomposes state transition computations into distinct spatial and temporal components, each represented by a suitable neural network. To efficiently learn the underlying flow-level dynamics, m4 adds dense supervision signals by predicting intermediate network metrics such as remaining flow size and queue length during training. m4 achieves a speedup of up to 104$\times$ over packet-level simulation. Relative to a traditional flow-level simulation, m4 reduces per-flow estimation errors by 45.3% (mean) and 53.0% (p90). For closed-loop applications, m4 accurately predicts network throughput under various congestion control schemes and workloads."
2503.02252,"Driven by the ever-increasing capacity demands, the 50G passive optical network (PON) is maturing gradually. One of the main challenges for the 50G PON is implementing burst-mode digital signal processing (BM-DSP) for the burst upstream signal. In this paper, we demonstrate a real-time BM-DSP for burst reception of 25Gbit/s on-off keying signal to meet the asymmetric-mode 50G PON demand. The real-time BM-DSP includes the BM frequency-domain timing recovery and BM frequency-domain equalizer, which can be fast converged based on the 42ns designed preamble. Meanwhile, the simplified implementations for fast-Fourier-transform, minimum-mean-square-error, and decision-directed least-mean-square-error algorithms decrease the DSP resources by 28.57%, enabling the loading of real-time BM-DSP in the field programmable gate array with the limited DSP resources. The real-time implementation of BM-DSP can guide the design of application-specific integrated circuits for 50G PON."
2503.03003,"Wide-area scaling trends require new approaches to Internet Protocol (IP) lookup, enabled by modern networking chips such as Intel Tofino, AMD Pensando, and Nvidia BlueField, which provide substantial ternary content-addressable memory (TCAM) and static random-access memory (SRAM). However, designing and evaluating scalable algorithms for these chips is challenging due to hardware-level constraints. To address this, we introduce the CRAM (CAM+RAM) lens, a framework that combines a formal model for evaluating algorithms on modern network processors with a set of optimization idioms. We demonstrate the effectiveness of CRAM by designing and evaluating three new IP lookup schemes: RESAIL, BSIC, and MashUp. RESAIL enables Tofino-2 to scale to 2.25 million IPv4 prefixes$\unicode{x2014}$likely sufficient for the next decade$\unicode{x2014}$while a pure TCAM approach supports only 250k prefixes, just 27% of the current global IPv4 routing table. Similarly, BSIC scales to 390k IPv6 prefixes on Tofino-2, supporting 3.2 times as many prefixes as a pure TCAM implementation. In contrast, existing state-of-the-art algorithms, SAIL for IPv4 and Hi-BST for IPv6, scale to considerably smaller sizes on Tofino-2."
2503.03022,"Machine learning has shown promise in network intrusion detection systems, yet its performance often degrades due to concept drift and imbalanced data. These challenges are compounded by the labor-intensive process of labeling network traffic, especially when dealing with evolving and rare attack types, which makes preparing the right data for adaptation difficult. To address these issues, we propose a generative active adaptation framework that minimizes labeling effort while enhancing model robustness. Our approach employs density-aware dataset prior selection to identify the most informative samples for annotation, and leverages deep generative models to conditionally synthesize diverse samples, thereby augmenting the training set and mitigating the effects of concept drift. We evaluate our end-to-end framework \NetGuard on both simulated IDS data and a real-world ISP dataset, demonstrating significant improvements in intrusion detection performance. Our method boosts the overall F1-score from 0.60 (without adaptation) to 0.86. Rare attacks such as Infiltration, Web Attack, and FTP-BruteForce, which originally achieved F1 scores of 0.001, 0.04, and 0.00, improve to 0.30, 0.50, and 0.71, respectively, with generative active adaptation in the CIC-IDS 2018 dataset. Our framework effectively enhances rare attack detection while reducing labeling costs, making it a scalable and practical solution for intrusion detection."
2503.03097,"This paper characterizes and jointly optimizes Age of Information (AoI) and energy efficiency in heterogeneous correlated random access networks, where each sensor adopts a distinct transmission probability and its observations are correlated with those of other sensors. An analytical model is proposed to analyze AoI and energy efficiency for each sensor. Closed-form expressions for long-term average AoI and energy efficiency are derived, explicitly accounting for spatial correlation and state-dependent power consumption. By constraining sensors to adopt the same transmission probability, three unified transmission strategies are derived: the age-optimal strategy (q_A^), the energy-efficiency optimal strategy (q_E^), and the Pareto-optimal strategy (q^), which jointly optimizes AoI and energy efficiency. A bounded exhaustive search with O(1/(n q_epsilon)) complexity guarantees efficient computation of q^. Theoretically, the correlation gain is proven to significantly enhance both metrics under spatial correlation. To exploit sensor heterogeneity, a gradient-based iterative algorithm, Multi-Start Projected Adaptive Moment Estimation (MS-PAdam), is proposed to jointly optimize all sensors' transmission probabilities, efficiently converging to the optimal AoI-energy-efficiency tradeoff. Crucially, MS-PAdam adaptively suppresses transmissions where marginal gains are outweighed by correlated neighbors' contributions, substantially alleviating competition. Numerical results show MS-PAdam outperforms unified strategies, achieving harmonious operation that mitigates AoI/energy degradation in contention-intensive scenarios."
2503.03291,"We propose Goal-Oriented Random Access (GORA), where transmitters jointly optimize what to send and when to access the shared channel to a common access point, considering the ultimate goal of the information transfer at its final destination. This goal is captured by an objective function, which is expressed as a general (not necessarily monotonic) function of the Age of Information. Our findings reveal that, under certain conditions, it may be desirable for transmitters to delay channel access intentionally and, when accessing the channel, transmit aged samples to reach a specific goal at the receiver."
2503.03341,"In this paper, we analyze the delay performance of an ad-hoc dynamic network where random network coding and broadcast are used in combination to distribute the messages. The analysis is comprehensive for that we consider M-to-N broadcast instead of 1-to-N, which allows both different messages and same messages to be transmitted by several sources at the same time. Although the routes between source-destination pairs are subject to change when some nodes have large backlogs, we derive fixed equivalent routes to provide a upper bound of delay. For some special cases, an detour method is also provided to increase the estimation accuracy. Different network topologies are tested in numeric simulation. The results demonstrate the accuracy of our delay performance approximation."
2503.03521,"In this paper we propose and quantitatively evaluate three performance optimization methods that exploit the concept of communication-compute-control co-design by introducing awareness of communication and compute characteristics into the application logic in different ways to improve overall system performance. We have implemented a closed-loop control of a robotic arm over a wireless network where the controller is deployed into an edge cloud environment. When implementing an industrial system that leverages network and cloud technologies, the level of determinism of the control application can be decreased by nature. This means that some imperfections may be introduced into the control system, and the closed-loop control in substance changes to open-loop during disturbances. We aim to improve the performance of these open-loop control periods by applying methods that can compensate for the imperfections statistically or in a guaranteed way. We demonstrate that co-design-based application improvements with minimal dependencies on the underlying technologies can already yield an order of magnitude gain when it comes to the accurate execution of the robot trajectories during the openloop control periods. Furthermore, by combining the proposed methods, the performance improvements add up and can produce up to 45% shorter trajectory executions compared to individual evaluations."
2503.03523,"The lack of a unified mechanism to coordinate and prioritize the actions of different applications can create three types of conflicts (direct, indirect, and implicit). Conflict management in O-RAN refers to the process of identifying and resolving conflicts between network applications. In our paper, we introduce a novel data-driven GCN-based method called GRAPH-based Intelligent xApp Conflict Prediction and Analysis (GRAPHICA) based on Graph Convolutional Network (GCN). It predicts three types of conflicts (direct, indirect, and implicit) and pinpoints the root causes (xApps). GRAPHICA captures the complex and hidden dependencies among the xApps, controlled parameters, and KPIs in O-RAN to predict possible conflicts. Then, it identifies the root causes (xApps) contributing to the predicted conflicts. The proposed method was tested on highly imbalanced synthesized datasets where conflict instances range from 40% to 10%. The model is tested in a setting that simulates real-world scenarios where conflicts are rare to assess its performance. Experimental results demonstrate a high F1-score over 98% for the synthesized datasets with different levels of class imbalance."
2503.0359,"Millimeter wave (mmWave) technology in vehicle-to-everything (V2X) communication offers unprecedented data rates and low latency, but faces significant reliability challenges due to signal blockages and limited range. This paper introduces a novel system for managing dynamic multi-hop mmWave V2X communications in complex blocking environments. We present a system architecture that integrates a mobility digital twin (DT) with the multi-hop routing control plane, providing a comprehensive, real-time view of the network and its surrounding traffic environment. This integration enables the control plane to make informed routing decisions based on rich contextual data about vehicles, infrastructure, and potential signal blockages. Leveraging this DT-enhanced architecture, we propose an advanced routing algorithm that combines high-precision environmental data with trajectory prediction to achieve blockage-aware mmWave multi-hop V2X routing. Our algorithm anticipates network topology changes and adapts topology dynamically to maintain reliable connections. We evaluate our approach through proof-of-concept simulations using a mobility DT of the Nishishinjuku area. Results demonstrate that our DT-enabled routing strategy significantly outperforms conventional methods in maintaining reliable mmWave V2X connections across various traffic scenarios, including fully connected and mixed traffic environments."
2503.03641,"The prevailing wisdom is that more network bandwidth does not matter much and that website performance is primarily limited by network latency. However, as mobile websites become more complex and mobile network performance improves, does this adage continue to hold? To understand the effects of small changes in network bandwidth and latency on website performance, we propose a novel webpage characterization metrics, Critical Path of Improvement (CPI). We compute CPI for 45 websites and analyze it against the network performance of four mobile ISPs in 57 US cities. Our results show that 18% of websites are primarily limited by bandwidth with others limited by bandwidth to some extent. These results show that contrary to accepted wisdom, insufficient bandwidth is a limiting factor in some website/network combinations. We also offer a discussion of approaches website developers and mobile network administrators can follow to understand and mitigate bandwidth limitations to website performance."
2503.03646,"Multi-access edge computing is a technique that combines the use of communication networks and remote computing resources. It allows to perform complex computational tasks for devices with low computing power while maintaining low latencies. However, it is important to effectively allocate the computing tasks to individual nodes. The work will present how the multi-access edge computing system can be integrated into the 5G network, as well as how resources can be distributed between individual nodes to minimize energy consumption. Some new degrees of freedom will be presented, which enable a significant reduction in energy consumption compared to existing solutions for independent optimization of the computation and communication parts.--Wielodostpowe przetwarzanie brzegowe jest technik czc wykorzystanie sieci komunikacyjnych i oddalonych zasobw obliczeniowych. Pozwala wykona zoone zadania obliczeniowe na potrzeby urzdze o niewielkiej mocy obliczeniowej przy zachowaniu niewielkich opnie. Istotne jest jednak efektywne zarzdzanie przydziaem zada obliczeniowych do poszczeglnych wzw. W pracy przedstawiono jak system przetwarzania brzegowego moe by zintegrowany z sieci 5G, a take jak mona rozdzieli zasoby midzy poszczeglne wzy, eby zminimalizowa zuycie energii. Przedstawiony zostanie szereg nowych stopni swobody, ktre umoliwiaj znaczne obnienie zuycia energii w stosunku do istniejcych rozwiza niezalenej optymalizacji czci obliczeniowej i komunikacyjnej."
2503.03761,"Wireless networks in a room are strongly affected by interferences. To alleviate these effects and enhance the performance of the wireless networks, some optimization was carried out. In this work, an analytical study was introduced to determine the optimal number of access points with their positions on the ground floor at the Architecture Engineering department building - University of Mosul. The implementation has been done using a web-based Wi-Fi and IoT design tool called Hamina Network Planner, and a Wi-Fi Network Planning and Site Survey Software called NetSpot. The experimental results show that the simulation values of the available APs are approximately matched with the real time manual values, achieving the best rates of -22dBm and -31dBm respectively. However, the number of currently available Access Points is not sufficient to cover the building area, so that two scenarios were suggested to overcome this issue. In the first scenario, two access points have been added at different positions in the building depending on the Hamina Network Planner, and in the second one, the transmitted power has been increased. The simulation results demonstrate that the overall coverage rate enhanced and can include most of the building area."
2503.03763,"Distributed quantum networks are not merely information conduits but intricate systems that embody the principles of quantum mechanics. In our study, we examine the underlying mechanisms of quantum connectivity within a distributed framework by exploring phenomena such as superposition and entanglement and their influence on information propagation. We investigate how these fundamental quantum effects interact with routing strategies that, while inspired by classical methods, must contend with quantum decoherence and measurement uncertainties. By simulating distributed networks of 10, 20, 50 and 100 nodes, we assess the performance of routing mechanisms through metrics that reflect both quantum fidelity and operational efficiency. Our findings reveal that the quantum coherence inherent in entangled states can enhance routing fidelity under specific conditions, yet also introduce challenges such as increased computational overhead and sensitivity to network scale. This work bridges the gap between the underlying principles of quantum systems and practical routing implementations, offering new insights into the design of robust distributed quantum networks."
2503.03767,"Internet of Vehicles (IoV), as the core of intelligent transportation system, enables comprehensive interconnection between vehicles and their surroundings through multiple communication modes, which is significant for autonomous driving and intelligent traffic management. However, with the emergence of new applications, traditional communication technologies face the problems of scarce spectrum resources and high latency. Semantic communication, which focuses on extracting, transmitting, and recovering some useful semantic information from messages, can reduce redundant data transmission, improve spectrum utilization, and provide innovative solutions to communication challenges in the IoV. This paper systematically reviews state of art of semantic communications in the IoV, elaborates the technical background of IoV and semantic communications, and deeply discusses key technologies of semantic communications in IoV, including semantic information extraction, semantic communication architecture, resource allocation and management, and so on. Through specific case studies, it demonstrates that semantic communications can be effectively employed in the scenarios of traffic environment perception and understanding, intelligent driving decision support, IoV service optimization, and intelligent traffic management. Additionally, it analyzes the current challenges and future research directions. This survey reveals that semantic communications has broad application prospects in IoV, but it is necessary to solve the real existing problems by combining advanced technologies to promote its wide application in IoV and contributing to the development of intelligent transportation system."
2503.04184,"This white paper discusses the role of large-scale AI in the telecommunications industry, with a specific focus on the potential of generative AI to revolutionize network functions and user experiences, especially in the context of 6G systems. It highlights the development and deployment of Large Telecom Models (LTMs), which are tailored AI models designed to address the complex challenges faced by modern telecom networks. The paper covers a wide range of topics, from the architecture and deployment strategies of LTMs to their applications in network management, resource allocation, and optimization. It also explores the regulatory, ethical, and standardization considerations for LTMs, offering insights into their future integration into telecom infrastructure. The goal is to provide a comprehensive roadmap for the adoption of LTMs to enhance scalability, performance, and user-centric innovation in telecom networks."
2503.04637,"Sensing is emerging as a vital future service in next-generation wireless networks, enabling applications such as object localization and activity recognition. The IEEE 802.11bf standard extends Wi-Fi capabilities to incorporate these sensing functionalities. However, coexistence with legacy Wi-Fi in densely populated networks poses challenges, as contention for channels can impair both sensing and communication quality. This paper develops an analytical framework and a system-level simulation in ns-3 to evaluate the coexistence of IEEE 802.11bf and legacy 802.11ax in terms of sensing delay and communication throughput. Forthis purpose, we have developed a dedicated ns-3 module forIEEE 802.11bf, which is made publicly available as open-source. We provide the first coexistence analysis between IEEE 802.11bfand IEEE 802.11ax, supported by link-level simulation in ns-3to assess the impact on sensing delay and network performance. Key parameters, including sensing intervals, access categories, network densities, and antenna configurations, are systematically analyzed to understand their influence on the sensing delay and aggregated network throughput. The evaluation is further extended to a realistic indoor office environment modeled after the 3GPP TR 38.901 standard. Our findings reveal key trade-offs between sensing intervals and throughput and the need for balanced sensing parameters to ensure effective coexistence in Wi-Fi networks."
2503.05324,"Training large language models (LLMs), and other large machine learning models, involves repeated communication of large volumes of data across a data center network. The communication patterns induced by these training process exhibit high regularity and persistence, giving rise to significant opportunities for optimizing the manner in which flows are routed across the network. We present an algorithmic framework for \textit{quantifying} network-wide efficiency in the context of training LLMs (and other large-scale ML models), and for periodically \textit{optimizing} routing with respect to this global metric."
2503.05429,"Cross-Technology Interference (CTI) poses challenges for the performance and robustness of wireless networks. There are opportunities for better cooperation if the spectral occupation and technology of the interference can be detected. Namely, this information can help the Orthogonal Frequency Division Multiple Access (OFDMA) scheduler in IEEE 802.11ax (Wi-Fi 6) to efficiently allocate resources to multiple users inthe frequency domain. This work shows that a single Channel State Information (CSI) snapshot, which is used for packet demodulation in the receiver, is enough to detect and classify the type of CTI on low-cost Wi-Fi 6 hardware. We show the classification accuracy of a small Convolutional Neural Network (CNN) for different Signal-to-Noise Ratio (SNR) and Signal-to-Interference Ratio (SIR) with simulated data, as well as using a wired and over-the-air test with a professional wireless connectivity tester, while running the inference on the low-cost device. Furthermore, we use openwifi, a full-stack Wi-Fi transceiver running on software-defined radio (SDR) available in the w-iLab.t testbed, as Access Point (AP) to implement a CTI-aware multi-user OFDMA scheduler when the clients send CTI detection feedback to the AP. We show experimentally that it can fully mitigate the 35% throughput loss caused by CTI when the AP applies the appropriate scheduling."
2503.0548,"The advance towards 6G networks comes with the promise of unprecedented performance in sensing and communication capabilities. The feat of achieving those, while satisfying the ever-growing demands placed on wireless networks, promises revolutionary advancements in sensing and communication technologies. As 6G aims to cater to the growing demands of wireless network users, the implementation of intelligent and efficient solutions becomes essential. In particular, reconfigurable intelligent surfaces (RISs), also known as Smart Surfaces, are envisioned as a transformative technology for future 6G networks. The performance of RISs when used to augment existing devices is nevertheless largely affected by their precise location. Suboptimal deployments are also costly to correct, negating their low-cost benefits. This paper investigates the topic of optimal RISs diffusion, taking into account the improvement they provide both for the sensing and communication capabilities of the infrastructure while working with other antennas and sensors. We develop a combined metric that takes into account the properties and location of the individual devices to compute the performance of the entire infrastructure. We then use it as a foundation to build a reinforcement learning architecture that solves the RIS deployment problem. Since our metric measures the surface where given localization thresholds are achieved and the communication coverage of the area of interest, the novel framework we provide is able to seamlessly balance sensing and communication, showing its performance gain against reference solutions, where it achieves simultaneously almost the reference performance for communication and the reference performance for localization."
2503.06164,"Wireless rechargeable sensor networks (WRSNs), supported by recent advancements in wireless power transfer (WPT) technology, hold significant potential for extending network lifetime. However, traditional approaches often prioritize scheduling algorithms and network optimization, overlooking the security risks associated with the charging process, which exposes the network to potential attacks. This paper addresses this gap by integrating Software-Defined Networking (SDN) and Digital Twin technologies for the intelligent detection of Denial of Charging (DoC) attacks in WRSNs. First, it leverages the flexibility and intelligent control of SDN, in combination with Digital Twin, to enhance real-time detection and mitigation of DoC attacks. Second, it employs four key metrics to detect such attacks including charging request patterns, energy consumption, behavioral and reputation scores, and charging behavior and efficiency. The numerical results demonstrate the superior performance of the proposed protocol in terms of energy usage efficiency, survival rate, detection rate, and travel distance."
2503.06197,"O-RAN has brought in deployment flexibility and intelligent RAN control for mobile operators through its disaggregated and modular architecture using open interfaces. However, this disaggregation introduces complexities in system integration and network management, as components are often sourced from different vendors. In addition, the operators who are relying on open source and virtualized components -- which are deployed on commodity hardware -- require additional resilient solutions as O-RAN deployments suffer from the risk of failures at multiple levels including infrastructure, platform, and RAN levels. To address these challenges, this paper proposes FALCON, a fault prediction framework for O-RAN, which leverages infrastructure-, platform-, and RAN-level telemetry to predict faults in virtualized O-RAN deployments. By aggregating and analyzing metrics from various components at different levels using AI/ML models, the FALCON framework enables proactive fault management, providing operators with actionable insights to implement timely preventive measures. The FALCON framework, using a Random Forest classifier, outperforms two other classifiers on the predicted telemetry, achieving an average accuracy and F1-score of more than 98%."
2503.06246,"Access to educational materials in remote Amazonian communities is challenged by limited communication infrastructure. This paper proposes a novel delay-tolerant network (DTN) approach for content distribution and compares the Epidemic, MaxProp, and PRoPHETv2 routing protocols using the ONE simulator under dynamically changing educational file sizes. Results show that while Epidemic routing achieves higher delivery rates due to extensive message replication, it also leads to increased resource usage. MaxProp offers a balance between delivery efficiency and resource utilization by prioritizing message delivery based on predefined heuristics but struggles under high congestion and resource constraints. PRoPHETv2, with its probability-based forwarding, uses resources more efficiently but is less effective in dynamic, dense networks. This analysis highlights trade-offs between delivery performance and resource efficiency, guiding protocol selection for specific community needs. In our future work, we aim to explore adaptive buffer management and congestion-aware DTN protocols."
2503.06302,"Digital network twins (DNTs) are virtual representations of physical networks, designed to enable real-time monitoring, simulation, and optimization of network performance. When integrated with machine learning (ML) techniques, particularly federated learning (FL) and reinforcement learning (RL), DNTs emerge as powerful solutions for managing the complexities of network operations. This article presents a comprehensive analysis of the synergy of DNTs, FL, and RL techniques, showcasing their collective potential to address critical challenges in 6G networks. We highlight key technical challenges that need to be addressed, such as ensuring network reliability, achieving joint data-scenario forecasting, and maintaining security in high-risk environments. Additionally, we propose several pipelines that integrate DNT and ML within coherent frameworks to enhance network optimization and security. Case studies demonstrate the practical applications of our proposed pipelines in edge caching and vehicular networks. In edge caching, the pipeline achieves over 80% cache hit rates while balancing base station loads. In autonomous vehicular system, it ensure a 100% no-collision rate, showcasing its reliability in safety-critical scenarios. By exploring these synergies, we offer insights into the future of intelligent and adaptive network systems that automate decision-making and problem-solving."
2503.06443,"Federated learning (FL) emerges as a promising approach to empower vehicular networks, composed by intelligent connected vehicles equipped with advanced sensing, computing, and communication capabilities. While previous studies have explored the application of FL in vehicular networks, they have largely overlooked the intricate challenges arising from the mobility of vehicles and resource constraints. In this paper, we propose a framework of mobility-aware decentralized federated learning (MDFL) for vehicular networks. In this framework, nearby vehicles train an FL model collaboratively, yet in a decentralized manner. We formulate a local iteration and leader selection joint optimization problem (LSOP) to improve the training efficiency of MDFL. For problem solving, we first reformulate LSOP as a decentralized partially observable Markov decision process (Dec-POMDP), and then develop an effective optimization algorithm based on multi-agent proximal policy optimization (MAPPO) to solve Dec-POMDP. Finally, we verify the performance of the proposed algorithm by comparing it with other algorithms."
2503.06468,"Federated learning (FL) is a promising paradigm that can enable collaborative model training between vehicles while protecting data privacy, thereby significantly improving the performance of intelligent transportation systems (ITSs). In vehicular networks, due to mobility, resource constraints, and the concurrent execution of multiple training tasks, how to allocate limited resources effectively to achieve optimal model training of multiple tasks is an extremely challenging issue. In this paper, we propose a mobility-aware multi-task decentralized federated learning (MMFL) framework for vehicular networks. By this framework, we address task scheduling, subcarrier allocation, and leader selection, as a joint optimization problem, termed as TSLP. For the case with a single FL task, we derive the convergence bound of model training. For general cases, we first model TSLP as a resource allocation game, and prove the existence of a Nash equilibrium (NE). Then, based on this proof, we reformulate the game as a decentralized partially observable Markov decision process (DEC-POMDP), and develop an algorithm based on heterogeneous-agent proximal policy optimization (HAPPO) to solve DEC-POMDP. Finally, numerical results are used to demonstrate the effectiveness of the proposed algorithm."
2503.06521,"As one of the most promising hotspots in the 6G era, space remote sensing information networks play a key and irreplaceable role in areas such as emergency response and scientific research, and are expected to foster remote sensing data processing into the next generation of killer applications. However, due to the inability to deploy ground communication stations at scale and the limited data transmission window, the traditional model for transmitting space data back to ground stations faces significant challenges in terms of timeliness. To address this problem, we focus on the emerging paradigm of on-orbit space data processing, taking the first step toward building a space-based computing infrastructure network. Specifically, we propose a hierarchical space-based computing network architecture that integrates the space-based cloud constellation system, the remote sensing constellation system, the network operation control center, the orchestration data center and the user access portal, offering a detailed description of their functionalities. Next, we analyze three scientific challenges: the characterization and virtualization of multidimensional heterogeneous resources, the efficient orchestration of multidimensional heterogeneous resources for tasks with varying priorities, and the rapid sharing of multidimensional heterogeneous resources to address burst tasks or system failures. Finally, we discuss key technologies to address the aforementioned challenges and highlight promising research priorities for the future."
2503.06883,"Semantic communication has emerged as a transformative paradigm in next-generation communication systems, leveraging advanced artificial intelligence (AI) models to extract and transmit semantic representations for efficient information exchange. Nevertheless, the presence of unpredictable semantic noise, such as ambiguity and distortions in transmitted representations, often undermines the reliability of received information. Conventional approaches primarily adopt adversarial training with noise injection to mitigate the adverse effects of noise. However, such methods exhibit limited adaptability to varying noise levels and impose additional computational overhead during model training. To address these challenges, this paper proposes Noise-Resilient \textbf{Se}mantic Communication with \textbf{Hi}gh-and-\textbf{Lo}w Frequency Decomposition (Se-HiLo) for image transmission. The proposed Se-HiLo incorporates a Finite Scalar Quantization (FSQ) based noise-resilient module, which bypasses adversarial training by enforcing encoded representations within predefined spaces to enhance noise resilience. While FSQ improves robustness, it compromise representational diversity. To alleviate this trade-off, we adopt a transformer-based high-and-low frequency decomposition module that decouples image representations into high-and-low frequency components, mapping them into separate FSQ representation spaces to preserve representational diversity. Extensive experiments demonstrate that Se-HiLo achieves superior noise resilience and ensures accurate semantic communication across diverse noise environments."
2503.0742,"The concept of AI-RAN as specified by the AI-RAN alliance is geared to explore a converged 6G platform that can support management, orchestration, and deployment of both AI and RAN workloads. This concept is central to the development of a 6G architecture that aims to exploit the accelerated compute capabilities for supporting both real-time signal processing and offloading of Generative AI (GenAI) workloads. However, both the architectural framework required to support this vision and the dynamic resource allocation strategy are still in their infancy. The O-RAN architecture intrinsically allows cloud-native disaggregated implementation. Consequently, we explore a framework that can allow orchestration of AI-and-RAN workloads by expanding the Near Real-Time RAN Intelligent Controller (NRT-RIC) within O-RAN. The framework incorporates a monitoring xApp that tracks RAN KPIs and exposes radio analytics to the proposed E2E orchestrator via a recently introduced Y1 interface. The orchestrator implements a Soft Actor-Critic (SAC) reinforcement learning algorithm to dynamically allocate critical computing resources, e.g., Multi-Instance GPUs (MIGs), between latency-sensitive RAN network functions and computationally intensive AI workloads on shared RAN infrastructure. The proposed framework provides insight on how the traditional RAN architecture can be evolved to inherently support emerging GenAI workloads. Our framework prioritizes the real-time requirements of RAN workloads while maintaining efficient resource sharing for AI applications. The simulation results demonstrate the benefits of the proposed framework, as it meets nearly 99% of the requests for RAN workload while effectively supporting AI workloads and achieving 100% utilization of the RAN infrastructure resources in a dynamic environment."
2503.07433,"Network optimization remains fundamental in wireless communications, with Artificial Intelligence (AI)-based solutions gaining widespread adoption. As Sixth-Generation (6G) communication networks pursue full-scenario coverage, optimization in complex extreme environments presents unprecedented challenges. The dynamic nature of these environments, combined with physical constraints, makes it difficult for AI solutions such as Deep Reinforcement Learning (DRL) to obtain effective reward feedback for the training process. However, many existing DRL-based network optimization studies overlook this challenge through idealized environment settings. Inspired by the powerful capabilities of Generative AI (GenAI), especially diffusion models, in capturing complex latent distributions, we introduce a novel Diffusion Reasoning-based Reward Shaping Scheme (DRESS) to achieve robust network optimization. By conditioning on observed environmental states and executed actions, DRESS leverages diffusion models' multi-step denoising process as a form of deep reasoning, progressively refining latent representations to generate meaningful auxiliary reward signals that capture patterns of network systems. Moreover, DRESS is designed for seamless integration with any DRL framework, allowing DRESS-aided DRL (DRESSed-DRL) to enable stable and efficient DRL training even under extreme network environments. Experimental results demonstrate that DRESSed-DRL achieves about 1.5x times faster convergence than its original version in sparse-reward wireless environments and significant performance improvements in multiple general DRL benchmark environments compared to baseline methods. The code of DRESS is available atthis https URL."
2503.0767,"Future wireless networks aim to deliver high data rates and lower power consumption while ensuring seamless connectivity, necessitating robust optimization. Large language models (LLMs) have been deployed for generalized optimization scenarios. To take advantage of generative AI (GAI) models, we propose retrieval augmented generation (RAG) for multi-sensor wireless environment perception. Utilizing domain-specific prompt engineering, we apply RAG to efficiently harness multimodal data inputs from sensors in a wireless environment. Key pre-processing pipelines including image-to-text conversion, object detection, and distance calculations for multimodal RAG input from multi-sensor data are proposed to obtain a unified vector database crucial for optimizing LLMs in global wireless tasks. Our evaluation, conducted with OpenAI's GPT and Google's Gemini models, demonstrates an 8%, 8%, 10%, 7%, and 12% improvement in relevancy, faithfulness, completeness, similarity, and accuracy, respectively, compared to conventional LLM-based designs. Furthermore, our RAG-based LLM framework with vectorized databases is computationally efficient, providing real-time convergence under latency constraints."
2503.07893,"Time-Sensitive Networking (TSN) supports multiple traffic types with diverse timing requirements, such as hard real-time (HRT), soft real-time (SRT), and Best Effort (BE) within a single network. To provide varying Quality of Service (QoS) for these traffic types, TSN incorporates different scheduling and shaping mechanisms. However, assigning traffic types to the proper scheduler or shaper, known as Traffic-Type Assignment (TTA), is a known NP-hard problem. Relying solely on domain expertise to make these design decisions can be inefficient, especially in complex network scenarios. In this paper, we present a proof-of-concept highlighting the advantages of a learning-based approach to the TTA problem. We formulate an optimization model for TTA in TSN and develop a Proximal Policy Optimization (PPO) based Deep Reinforcement Learning (DRL) model, called ``TTASelector'', to assign traffic types to TSN flows efficiently. Using synthetic and realistic test cases, our evaluation shows that TTASelector assigns a higher number of traffic types to HRT and SRT flows compared to the state-of-the-art Tabu Search-based metaheuristic method."
2503.07935,"Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible deployment in 5G and next-generation wireless networks. The performance of such wireless networks can be improved by developing new navigation and wireless adaptation approaches in digital twins (DTs). However, challenges such as complex propagation conditions and hardware complexities in real-world scenarios introduce a realism gap with the DTs. Moreover, while using real-time full-stack protocols in DTs enables subsequent deployment and testing in a real-world environment, development in DTs requires high computational complexity and involves a long development time. In this paper, to accelerate the development cycle, we develop a measurement-calibrated Matlab-based simulation framework to replicate performance in a full-stack UAV wireless network DT. In particular, we use the DT from the NSF AERPAW platform and compare its reports with those generated by our developed simulation framework in wireless networks with similar settings. In both environments, we observe comparable results in terms of RSRP measurement, hence motivating iterative use of the developed simulation environment with the DT."
2503.07973,"The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end transmission by simultaneously utilizing terrestrial and satellite networks, offering significant benefits in scenarios like emergency response and cross-continental communication. Low Earth Orbit (LEO) satellite networks offer reduced Round Trip Time (RTT) for long-distance data transmission and serve as a crucial backup during terrestrial network failures. Meanwhile, terrestrial networks are characterized by ample bandwidth resources and generally more stable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN is vital for optimizing resource utilization and ensuring efficient data transfer by exploiting the complementary strengths of both networks. However, the inherent challenges of STIN, such as heterogeneity, instability, and handovers, pose difficulties for traditional multipath schedulers, which are typically designed for terrestrial networks. We propose a novel multipath data scheduling approach for STIN, Adaptive Latency Compensation Scheduler (ALCS), to address these issues. ALCS refines transmission latency estimates by incorporating RTT, congestion window size, inflight and queuing packets, and satellite trajectory information. It further employs adaptive mechanisms for latency compensation and proactive handover management. It further employs adaptive mechanisms for latency compensation and proactive handover management. Implemented in the MPTCP Linux Kernel and evaluated in a simulated STIN testbed, ALCS outperforms existing multipath schedulers, delivering faster data transmission and achieving throughput gains of 9.8% to 44.0% compared to benchmark algorithms."
2503.08082,"The interference graph, depicting the intra- and inter-cell interference channel gains, is indispensable for resource allocation in multi-cellthis http URL, there lacks viable methods of interference graph estimation (IGE) for multi-cell multi-numerology (MN) networks. To fill this gap, we propose an efficient power-domain approach to IGE for the resource allocation in multi-cell MN networks. Unlike traditional reference signal-based approaches that consume frequency-time resources, our approach uses power as a new dimension for the estimation of channel gains. By carefully controlling the transmit powers of base stations, our approach is capable of estimating both intra- and inter-cell interference channel gains. As a power-domain approach, it can be seamlessly integrated with the resource allocation such that IGE and resource allocation can be conducted simultaneously using the same frequency-time resources. We derive the necessary conditions for the power-domain IGE and design a practical power control scheme. We formulate a multi-objective joint optimization problem of IGE and resource allocation, propose iterative solutions with proven convergence, and analyze the computational complexity. Our simulation results show that power-domain IGE can accurately estimate strong interference channel gains with low power overhead and is robust to carrier frequency and timing offsets."
2503.08123,"With the advent of 6G systems, emerging hyper-connected ecosystems necessitate agile and adaptive medium access control (MAC) protocols to contend with network dynamics and diverse service requirements. We propose LLM4MAC, a novel framework that harnesses large language models (LLMs) within a reinforcement learning paradigm to drive MAC protocol emergence. By reformulating uplink data transmission scheduling as a semantics-generalized partially observable Markov game (POMG), LLM4MAC encodes network operations in natural language, while proximal policy optimization (PPO) ensures continuous alignment with the evolving network dynamics. A structured identity embedding (SIE) mechanism further enables robust coordination among heterogeneous agents. Extensive simulations demonstrate that on top of a compact LLM, which is purposefully selected to balance performance with resource efficiency, the protocol emerging from LLM4MAC outperforms comparative baselines in throughput and generalization."
2503.08267,"This paper introduces a novel neural network (NN) structure referred to as an ``Auto-hybrid precoder'' (Auto-HP) and an unsupervised deep learning (DL) approach that jointly designs \ac{mmWave} probing beams and hybrid precoding matrix design for mmWave multi-user communication system with minimal training pilots. Our learning-based model capitalizes on prior channel observations to achieve two primary goals: designing a limited set of probing beams and predicting off-grid \ac{RF} beamforming vectors. The Auto-HP framework optimizes the probing beams in an unsupervised manner, concentrating the sensing power on the most promising spatial directions based on the surrounding environment. This is achieved through an innovative neural network architecture that respects \ac{RF} chain constraints and models received signal strength power measurements using complex-valued convolutional layers. Then, the autoencoder is trained to directly produce RF beamforming vectors for hybrid architectures, unconstrained by a predefined codebook, based on few projected received signal strength indicators (RSSIs). Finally, once the RF beamforming vectors for the multi-users are predicted, the baseband (BB) digital precoders are designed accounting for the multi-user interference. The Auto-HP neural network is trained end-to-end (E2E) in an unsupervised learning manner with a customized loss function that aims to maximizes the received signal strength. The adequacy of the Auto-HP NN's bottleneck layer dimension is evaluated from an information theory perspective, ensuring maximum data compression and reliable RF beam predictions."
2503.08353,"The transition to 6G is expected to bring significant advancements, including much higher data rates, enhanced reliability and ultra-low latency compared to previous generations. Although 6G is anticipated to be 100 times more energy efficient, this increased efficiency does not necessarily mean reduced energy consumption or enhanced sustainability. Network sustainability encompasses a broader scope, integrating business viability, environmental sustainability, and social responsibility. This paper explores the sustainability requirements for 6G and proposes Open RAN as a key architectural solution. By enabling network diversification, fostering open and continuous innovation, and integrating AI/ML, Open RAN can promote sustainability in 6G. The paper identifies high energy consumption and e-waste generation as critical sustainability challenges and discusses how Open RAN can address these issues through softwarisation, edge computing, and AI integration."
2503.08466,"With the evolution of 5G networks, optimizing resource allocation has become crucial to meeting the increasing demand for massive connectivity and high throughput. Combining Non-Orthogonal Multiple Access (NOMA) and massive Multi-Input Multi-Output (MIMO) enhances spectral efficiency, power efficiency, and device connectivity. However, deploying MIMO-NOMA in dense networks poses challenges in managing interference and optimizing power allocation while ensuring that the Signal-to-Interference-plus-Noise Ratio (SINR) meets required thresholds. Unlike previous studies that analyze user clustering and power allocation techniques under simplified assumptions, this work provides a comparative evaluation of multiple clustering and allocation strategies under identical spatially correlated network conditions. We focus on maximizing the number of served users under a given Quality of Service (QoS) constraint rather than the conventional sum-rate maximization approach. Additionally, we consider spatial correlation in user grouping, a factor often overlooked despite its importance in mitigating intra-cluster interference. We evaluate clustering algorithms, including user pairing, random clustering, Correlation Iterative Clustering Algorithm (CIA), K-means++-based User Clustering (KUC), and Grey Wolf Optimizer-based clustering (GWO), in a downlink spatially correlated MIMO-NOMA environment. Numerical results demonstrate that the GWO-based clustering algorithm achieves superior energy efficiency while maintaining scalability, whereas CIA effectively maximizes the number of served users. These findings provide valuable insights for designing MIMO-NOMA systems that optimize resource allocation in next-generation wireless networks."
2503.08493,"Multi-connectivity (MC) for aerial users via a set of ground access points offers the potential for highly reliable communication. Within an open radio access network (O-RAN) architecture, edge clouds (ECs) enable MC with low latency for users within their coverage area. However, ensuring seamless service continuity for transitional users-those moving between the coverage areas of neighboring ECs-poses challenges due to centralized processing demands. To address this, we formulate a problem facilitating soft handovers between ECs, ensuring seamless transitions while maintaining service continuity for all users. We propose a hierarchical multi-agent reinforcement learning (HMARL) algorithm to dynamically determine the optimal functional split configuration for transitional and non-transitional users. Simulation results show that the proposed approach outperforms the conventional functional split in terms of the percentage of users maintaining service continuity, with at most 4% optimality gap. Additionally, HMARL achieves better scalability compared to the static baselines."
2503.08599,"The Open Radio Access Network (O-RAN)-compliant solutions often lack crucial details for implementing effective control loops at various time scales. To overcome this, we introduce MAREA, an O-RAN-compliant mathematical framework designed for the allocation of radio resources to multiple ultra-Reliable Low Latency Communication (uRLLC) services. In the near-real-time (RT) control loop, MAREA employs a novel Martingales-based model to determine the guaranteed radio resources for each uRLLC service. Unlike traditional queueing theory approaches, this model ensures that the probability of packet transmission delays exceeding a predefined threshold -- the violation probability -- remains below a target tolerance.Additionally, MAREA uses a real-time control loop to monitor transmission queues and dynamically adjust guaranteed radio resources in response to traffic anomalies. To the best of our knowledge, MAREA is the first O-RAN-compliant solution that leverages Martingales for both near-RT and RT control loops. Simulations demonstrate that MAREA significantly outperforms reference solutions, achieving an average violation probability that is x10 lower."
2503.09093,"With the growing demand for dynamic real-time applications, online admission control for time-critical event-triggered (ET) traffic in Time-Sensitive Networking (TSN) has become a critical challenge. The main issue lies in dynamically allocating bandwidth with real-time guarantees in response to traffic changes while also meeting the requirements for rapid response, scalability, and high resource utilization in online scenarios. To address this challenge, we propose an online admission control method for ET traffic based on the TSN/ATS+CBS (asynchronous traffic shaper and credit-based shaper) architecture. This method provides a flexible framework for real-time guaranteed online admission control, supporting dynamic bandwidth allocation and reclamation at runtime without requiring global reconfiguration, thus improving scalability. Within this framework, we further integrate a novel strategy based on network calculus (NC) theory for efficient and high-utilization bandwidth reallocation. On the one hand, the strategy focuses on adaptively balancing residual bandwidth with deadline awareness to prevent bottleneck egress ports, thereby improving admission capacity. On the other hand, it employs a non-trivial analytical result to reduce the search space, accelerating the solving process. Experimental results from both large-scale synthetic and realistic test cases show that, compared to the state-of-the-art, our method achieves an average 56% increase in admitted flows and an average 92% reduction in admission time. Additionally, it postpones the occurrence of bottleneck egress ports and the first rejection of admission requests, thereby enhancing adaptability."
2503.09145,"Despite the rapid advancements in 5G technology, accurately assessing the energy consumption of its Radio Access Networks (RANs) remains a challenge due to the diverse range of applicable technologies and implementation solutions. Designing a versatile power model for estimating the 5G RANspecific power consumption requires extensive data collection and experimental studies to capture the diverse range of technologies and implementation solutions. The objective is to outline a versatile energy model capable of estimating RAN-specific energy consumption, encompassing both mobile terminals and the physical layer (PHY) of base stations. In this paper, we focus on the computational complexity of the baseband part of the model. The developed (part of the) model is compared with the estimation of the number of cycles (and energy per cycle) used by a specific implementation (here a Matlab code ported on an Intel target), enabling the assessment of the model with the estimation of energy consumed on a real target. The study's results show a good agreement between the model and the implementation, even if some parts need to be refined to take specific algorithms into account. The key contribution is the development of an initial flexible energy model with finer granularity, enabling comparisons of energy use across various applications and contexts, and offering a comprehensive tool for optimizing 5G network energy consumption."
2503.0943,"Mobile networks evolve on a regular basis to meet the requirements of a rapidly changing application ecosystem; hence, a future-proof design is key to getting the most out of their lifecycle. In comparison to other access networks, one major issue with the 5G Radio Access Network (RAN) is that it behaves as a ""fat Layer 2"" entity, resulting in disparities in Internet Protocol (IP) flow traffic control and radio resource allocation. In this article, we propose an innovative design - Integrated User Plane (IUP) - that incorporates User Plane Function (UPF) functionalities into RAN, and we introduce the Integrated Data Flow Control (IDFC) sublayer with a new traffic management pipeline and various programmable rules. To understand its implications for crucial mobility user cases, a detailed analysis of how IUP interacts with Control Plane (CP) network functions is conducted. Finally, our IUP prototype shows benefits including a 50% saving in both latency and overhead, converged IUP and non-Third-Generation Partnership Project (3GPP) networks for seamless connectivity, and real-time UP programmability in both traffic control and resource allocation via the O-RAN framework."
2503.09776,"As quantum networking continues to grow in importance, its study is of interest to an ever wider community and at an increasing scale. However, the development of its physical infrastructure remains burdensome, and services providing third party access are not enough to meet demand. A variety of simulation frameworks provide a method for testing aspects of such systems on commodity hardware, but are predominantly serial and thus unable to scale to larger networks and/or workloads. One effort to address this was focused on parallelising the SeQUeNCe discrete event simulator, though it has yet to be proven to work well across system architectures or at larger scales. Therein lies the contribution of this work - to more deeply examine its scalability using ORNL Frontier. Our results provide new insight into its scalability behaviour, and we examine its strategy and how it may be able to be improved."
2503.09869,"A well-known expression for the saturation throughput of heterogeneous transmitting nodes in a wireless network using p-CSMA, derived from Renewal Theory, implicitly assumes that all transmitting nodes are in range of, and therefore conflicting with, each other. This expression, as well as simple modifications of it, does not correctly capture the saturation throughput values when an arbitrary topology is specified for the conflict graph between transmitting links. For example, we show numerically that calculations based on renewal theory can underestimate throughput by 48-62% for large packet sizes when the conflict graph is represented by a star topology. This is problematic because real-world wireless networks, such as wireless IoT mesh networks, are often deployed over a large area, resulting in non-complete conflict graphs.To address this gap, we present a computational approach based on a novel Markov chain formulation that yields the exact saturation throughput for each node in the general network case for any given set of access probabilities, as well as a more compact expression for the special case where the packet length is twice the slot length. Using our approach, we show how the transmit probabilities could be optimized to maximize weighted utility functions of the saturation throughput values. This would allow a wireless system designer to set transmit probabilities to achieve desired throughput trade-offs in any given deployment."
2503.10885,"Rotation speed is a key metric for many applications, such as calibrating electric motors in a factory, monitoring a car's engine health, detecting faults in electrical appliances, and more. However, existing measurement techniques have several drawbacks, including the need for line-of-sight visibility, intrusive machine modification, fixed sensor deployment, or short sensing ranges. In this paper, we introduce MagTach, a handheld hardware-software system for rotation speed measurement that leverages electromagnetic field sensing to provide highly accurate and convenient readings."
2503.11162,"The integration of the Internet of Things (IoT) in smart agriculture has transformed farming practices by enabling real time monitoring, data-driven decision making, and automation. However, ensuring reliable connectivity in diverse agricultural environments remains a critical challenge. This paper analyzes the performance trade offs between Low Power Wide Area Networks (LPWAN), specifically LoRaWAN, NBIoT, and Sigfox and cellular networks (4G and 5G) in agricultural applications. Beyond a comprehensive literature review, this study evaluates hybrid LPWAN and 5G architectures that integrate the strengths of both network types to enhance cost-efficiency and connectivity reliability. Using real-world case studies, the findings demonstrate that hybrid LPWAN and 5G models can reduce connectivity costs by up to 30% while significantly improving network reliability in remote agricultural settings. This work provides actionable recommendations for selecting optimal IoT connectivity solutions based on agricultural requirements and proposes future research directions to further optimize IoT infrastructure in smart farming."
2503.11449,"As the Metaverse envisions deeply immersive and pervasive connectivity in 6G networks, Integrated Access and Backhaul (IAB) emerges as a critical enabler to meet the demanding requirements of massive and immersive communications. IAB networks offer a scalable solution for expanding broadband coverage in urban environments. However, optimizing IAB node deployment to ensure reliable coverage while minimizing costs remains challenging due to location constraints and the dynamic nature of cities. Existing heuristic methods, such as Greedy Algorithms, have been employed to address these optimization problems. This work presents a novel Deep Reinforcement Learning ( DRL) approach for IAB network planning, tailored to future 6G scenarios that seek to support ultra-high data rates and dense device connectivity required by immersive Metaverse applications. We utilize Deep Q-Network (DQN) with action elimination and integrate DQN, Double Deep Q-Network ( DDQN), and Dueling DQN architectures to effectively manage large state and action spaces. Simulations with various initial donor configurations demonstrate the effectiveness of our DRL approach, with Dueling DQN reducing node count by an average of 12.3% compared to traditional heuristics. The study underscores how advanced DRL techniques can address complex network planning challenges in 6G-enabled Metaverse contexts, providing an efficient and adaptive solution for IAB deployment in diverse urban environments."
2503.1149,"Bluetooth Low Energy (BLE) backscatter is a promising candidate for battery-free Internet of Things (IoT) applications. Unlike existing commodity-level BLE backscatter systems that only enable one-shot communication through BLE advertising packets, we propose PassiveBLE, a backscatter system that can establish authentic and fully compatible BLE connections on data channels. The key enabling techniques include (i) a synchronization circuit that can wake up tags and activate backscatter communications with symbol-level accuracy to facilitate BLE data packet generation; (ii) a distributed coding scheme that offloads the major encoding and processing burdens from tags to the excitation source while achieving high throughput; (iii) a BLE connection scheduler to enable fully compatible BLE connection interactions, including connection establishment, maintenance and termination for multiple backscatter tags. We prototype PassiveBLE tags with off-the-shelf components and also convert the circuits and control logic into ASIC design sketch, whose power consumptions are 491 uW and 9.9 uW, respectively. Experimental results demonstrate that PassiveBLE achieves a success rate of over 99.9% in establishing commodity BLE connections. PassiveBLE also achieves commodity-compatible BLE communication with a high goodput of up to 974 kbps in LE 2M PHY mode and 532 kbps in LE 1M PHY mode, which is about 63.3 times higher than the previous commodity-level BLE backscatter system in the same mode."
2503.11566,"Conflict Mitigation (CM) in Open Radio Access Network (O-RAN) is a topic that is gaining importance as commercial O-RAN deployments become more complex. Although research on CM is already covered in terms of simulated network scenarios, it lacks validation using real-world deployment and Over The Air (OTA) Radio Frequency (RF) transmission. Our objective is to conduct the first assessment of the Conflict Mitigation Framework (CMF) for O-RAN using a real-world testbed and OTA RF transmission. This paper presents results of an experiment using a dedicated testbed built in an O-RAN Open Test and Integration Center (OTIC) to confirm the validity of one of the Conflict Resolution (CR) schemes proposed by existing research. The results show that the implemented conflict detection and resolution mechanisms allow a significant improvement in network operation stability by reducing the variability of the measured Downlink (DL) throughput by 78%."
2503.11649,"Video-conferencing applications face an unwavering surge in traffic, stressing their underlying infrastructure in unprecedented ways. This paper rethinks the key building block for conferencing infrastructures -- selective forwarding units (SFUs). SFUs relay and adapt media streams between participants and, today, run in software on general-purpose servers. Our main insight, discerned from dissecting the operation of production SFU servers, is that SFUs largely mimic traditional packet-processing operations such as dropping and forwarding. Guided by this, we present Scallop, an SDN-inspired SFU that decouples video-conferencing applications into a hardware-based data plane for latency-sensitive and frequent media operations, and a software control plane for the (infrequent) remaining tasks, such as analyzing feedback signals. Our Tofino-based implementation fully supports WebRTC and delivers 7-210 times improved scaling over a 32-core commodity server, while reaping performance improvements by cutting forwarding-induced latency by 26 times."
2503.11933,"With the advent of 6G, Open Radio Access Network (O-RAN) architectures are evolving to support intelligent, adaptive, and automated network orchestration. This paper proposes a novel Edge AI and Network Service Orchestration framework that leverages Large Language Model (LLM) agents deployed as O-RAN rApps. The proposed LLM-agent-powered system enables interactive and intuitive orchestration by translating the user's use case description into deployable AI services and corresponding network configurations. The LLM agent automates multiple tasks, including AI model selection from repositories (e.g., Hugging Face), service deployment, network adaptation, and real-time monitoring via xApps. We implement a prototype using open-source O-RAN projects (OpenAirInterface and FlexRIC) to demonstrate the feasibility and functionality of our framework. Our demonstration showcases the end-to-end flow of AI service orchestration, from user interaction to network adaptation, ensuring Quality of Service (QoS) compliance. This work highlights the potential of integrating LLM-driven automation into 6G O-RAN ecosystems, paving the way for more accessible and efficient edge AI ecosystems."
2503.1205,"Vehicular Ad Hoc Networks (VANETs) are a cornerstone of intelligent transportation systems, facilitating real-time communication between vehicles and infrastructure. However, the dynamic nature of VANETs introduces significant challenges in routing, especially in minimizing communication delay while ensuring route stability. This paper proposes a hierarchical evolutionary optimization framework for delay-constrained routing in vehicular networks. Leveraging multi-objective optimization, the framework balances delay and stability objectives and incorporates adaptive mechanisms like incremental route adjustments and LSTM-based predictive modeling. Simulation results confirm that the proposed framework maintains low delay and high stability, adapting effectively to frequent topology changes in dynamic vehicular environments."
2503.12177,"This study presents an end-to-end wireless digital twin platform constructed using open-source software and open data to enhance the evaluation of mobile communication systems. The proposed open wireless digital twin (OWDT) integrates OpenAirInterface (OAI) for Fifth-Generation New Radio (5G NR) protocol stack emulation and NVIDIA Sionna RT for high-resolution ray-tracing-based radio propagation modeling. This integration enables the realistic emulation of 5G wireless communication in mobility scenarios on a CPU-based Linux system, leveraging real-world building data to bridge the gap between theoretical simulations and real-world deployments. The platform also incorporates OAI FlexRIC, which is an implementation aligned with the O-RAN near-real-time RAN Intelligent Controller (near-RT RIC), to dynamically monitor key performance indicators (KPIs). Through extensive evaluation in urban environments, this study demonstrated the validity of the emulation framework, revealing its capability to replicate real-world communication dynamics with high fidelity. The results underscore the potential of the OWDT to accelerate wireless system development, reduce experimental costs, and optimize network configurations."
2503.122,"Shipping container based modular architectures provide design flexibility in data centers with building blocks to expand the network as and when needed. In this paper, high capacity Modular Data Center (MDC) network architecture with Rich Inter Connections named MODRIC is proposed. MODRIC is a cost-effective switch-centric network design which allows building a flexible MDC network with commodity switches. It uses an inter-container connectivity similar to the structure of generalized hypercube in order to provide high inter-container bandwidth. Further, a hybrid Clos topology is used to build the container network. MODRIC is highly suitable for cost effectively building mega data centers requiring high throughput capacity and resilience against failures. This paper presents the proposed architecture, discusses its relevant properties, and proposes suitable addressing, routing and network construction schemes. The paper also presents comparative studies on its cost and performance with existing network topologies."
2503.12255,"The Internet of Things (IoT) has enabled diverse devices to communicate over the Internet, yet the fragmentation of IoT systems limits seamless data sharing and coordinated management. We have recently introduced SensorsConnect, a unified framework to enable seamless content and sensor data sharing in collaborative IoT systems, inspired by how the World Wide Web (WWW) enabled a shared and accessible space for information among humans. This paper presents the IoT Agentic Search Engine (IoT-ASE), a real-time search engine tailored for IoT environments. IoT-ASE leverages Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) techniques to address the challenge of searching vast, real-time IoT data, enabling it to handle complex queries and deliver accurate, contextually relevant results. We implemented a use-case scenario in Toronto to demonstrate how IoT-ASE can improve service quality recommendations by leveraging real-time IoT data. Our evaluation shows that IoT-ASE achieves a 92\% accuracy in retrieving intent-based services and produces responses that are concise, relevant, and context-aware, outperforming generalized responses from systems like Gemini. These findings highlight the potential IoT-ASE to make real-time IoT data accessible and support effective, real-time decision-making."
2503.12437,"Encoder, decoder and knowledge base are three major components for semantic communication. Recent advances have achieved significant progress in the encoder-decoder design. However, there remains a considerable gap in the construction and utilization of knowledge base, which plays important roles in establishing consensus among communication participants through knowledge transferring and sharing. Current knowledge base designs typically involve complex structures, which lead to significant computational overheads and heavy reliance on manually annotated datasets, making it difficult to adapt to existing encoder-decoder models. Hence, without knowledge transferring and sharing within the network results in poor generalization of encoder-decoder. This necessitates model training for specific tasks and datasets, significantly limiting the scalability of semantic communication systems to larger networks. To address these challenges, we propose an innovative Contrastive Representations Learning based Semantic Communication Framework (CRLSC). In CRLSC, the server-side pre-trained large model utilizes large-scale public datasets to construct shared knowledge base. Local-side encoders in terminal devices conduct training guided by shared knowledge base. These trained encoders can then build private knowledge bases from private datasets and fine-tune decoders for specific tasks. This simple and effective approach can facilitate the knowledge transferring across large-scale heterogeneous networks."
2503.12753,"Deep reinforcement learning (DRL)-based slicing policies have shown significant success in simulated environments but face challenges in physical systems such as open radio access networks (O-RANs) due to simulation-to-reality gaps. These policies often lack safety guarantees to ensure compliance with service level agreements (SLAs), such as the strict latency requirements of immersive applications. As a result, a deployed DRL slicing agent may make resource allocation (RA) decisions that degrade system performance, particularly in previously unseen scenarios. Real-world immersive applications require maintaining SLA constraints throughout deployment to prevent risky DRL exploration. In this paper, we propose SafeSlice to address both the cumulative (trajectory-wise) and instantaneous (state-wise) latency constraints of O-RAN slices. We incorporate the cumulative constraints by designing a sigmoid-based risk-sensitive reward function that reflects the slices' latency requirements. Moreover, we build a supervised learning cost model as part of a safety layer that projects the slicing agent's RA actions to the nearest safe actions, fulfilling instantaneous constraints. We conduct an exhaustive experiment that supports multiple services, including real virtual reality (VR) gaming traffic, to investigate the performance of SafeSlice under extreme and changing deployment conditions. SafeSlice achieves reductions of up to 83.23% in average cumulative latency, 93.24% in instantaneous latency violations, and 22.13% in resource consumption compared to the baselines. The results also indicate SafeSlice's robustness to changing the threshold configurations of latency constraints, a vital deployment scenario that will be realized by the O-RAN paradigm to empower mobile network operators (MNOs)."
2503.12871,"The vision of autonomous systems is becoming increasingly important in many application areas, where the aim is to replace humans with agents. These include autonomous vehicles and other agents' applications in business processes and problem-solving. For networks, the increasing scale and operation and management (O&M) complexity drive the need for autonomous networks (AN). The technical objective of AN is to ensure trustworthy O&M without human intervention for higher efficiency and lower operating costs. However, realizing AN seems more difficult than autonomous vehicles. It encounters challenges of networks' structural and functional complexity, which operate as distributed dynamic systems governed by various technical and economic constraints. A key problem lies in formulating a rigorous development methodology that facilitates a seamless transition from traditional networks to AN. Central to this methodology is the definition of a reference architecture for network agents, which specifies the required functionalities for their realization, regardless of implementation choices. This article proposes a reference architecture characterizing main functional features, illustrating its application with network use cases. It shows how artificial intelligence components can be used to implement the required functionality and its coordination. The latter is achieved through the management and generation of shared domain-specific knowledge stored in long-term memory, ensuring the overall consistency of decisions and their execution. The article concludes with a discussion of architecture specialization for building network layer agents. It also identifies the main technical challenges ahead, such as satisfying essential requirements at development or runtime, as well as the issue of coordinating agents to achieve collective intelligence in meeting overall network goals."
2503.12878,"Application deployment in cloud environment is dominated by Kubernetes-orchestrated microservices. Provides a secure environment, networking, storage, isolation, scheduling, and many other abstractions that can be easily extended to meet our needs. Time-Sensitive Applications (TSAs) have special requirements for compute and network. Deploying TSAs in Kubernetes is challenging because the networking implemented by Container Network Interface (CNI) plugins is not aware of the traffic characteristic required by Time-Sensitive Network. Even if a network interface supports TSN features (e.g.: Scheduled Traffic) and a modified CNI plugin is aware of this interface, the pod network isolation built on top of Linux deletes the metadata required for TSN protocols to work with.We propose TSN metadata proxy, a simple architecture that allows any TSA microservice to use the TSN capabilities of the physical NIC, without any modification. This architecture is tightly integrated with the Kubernetes networking model, works with popular CNI plugins, and supports services such as ClusterIP, NodePort, or LoadBalancer without additional configuration. Unlike former proposals, this architecture does not require either bypassing the Linux kernel network stack, direct access to the physical NIC, escalated privileges for the TSA microservice, or even modification of the TSA."
2503.13402,"The move toward open Sixth-Generation (6G) networks necessitates a novel approach to full-stack simulation environments for evaluating complex technology developments before prototyping and real-world implementation. This paper introduces an innovative approach\footnote{A lightweight, mock version of the code is available on GitHub at that combines a multi-agent framework with the Network Simulator 3 (ns-3) to automate and optimize the generation, debugging, execution, and analysis of complex 5G network scenarios. Our framework orchestrates a suite of specialized agents -- namely, the Simulation Generation Agent, Test Designer Agent, Test Executor Agent, and Result Interpretation Agent -- using advanced LangChain coordination. The Simulation Generation Agent employs a structured chain-of-thought (CoT) reasoning process, leveraging LLMs and retrieval-augmented generation (RAG) to translate natural language simulation specifications into precise ns-3 scripts. Concurrently, the Test Designer Agent generates comprehensive automated test suites by integrating knowledge retrieval techniques with dynamic test case synthesis. The Test Executor Agent dynamically deploys and runs simulations, managing dependencies and parsing detailed performance metrics. At the same time, the Result Interpretation Agent utilizes LLM-driven analysis to extract actionable insights from the simulation outputs. By integrating external resources such as library documentation and ns-3 testing frameworks, our experimental approach can enhance simulation accuracy and adaptability, reducing reliance on extensive programming expertise. A detailed case study using the ns-3 5G-LENA module validates the effectiveness of the proposed approach. The code generation process converges in an average of 1.8 iterations, has a syntax error rate of 17.0%, a mean response time of 7.3 seconds, and receives a human evaluation score of 7.5."
2503.1345,"The evolution of Internet Protocol Television (IPTV) has transformed the landscape of digital broadcasting by leveraging high-speed internet connectivity to deliver high-quality multimedia content. IPTV provides a dynamic and interactive television experience through managed networks, ensuring superior Quality of Service (QoS) compared to open-network Internet TV. This study explores the technical infrastructure of IPTV, including its network architecture, data compression techniques, and the role of protocols such as IGMP and RTSP. It also examines security challenges, including encryption, digital rights management (DRM), and authentication mechanisms that safeguard IPTV services from unauthorized access and piracy. Moreover, the paper analyzes the distinctions between IPTV and open-network Internet TV, highlighting their respective advantages and limitations in terms of service control, bandwidth optimization, and content security. The integration of artificial intelligence (AI) and machine learning (ML) in IPTV enhances personalized content recommendations and predictive analytics, leading to improved user engagement and efficient network management. Additionally, emerging technologies such as 5G and cloud-based IPTV services are explored for their potential to further revolutionize the industry. While IPTV presents a robust alternative to traditional broadcasting, challenges such as bandwidth constraints, cybersecurity threats, and regulatory compliance remain significant. The study concludes that IPTV's future success will depend on advancements in network infrastructure, AI-driven optimizations, and strategic regulatory adaptations. As IPTV continues to evolve, hybrid models integrating IPTV and open-network streaming services are expected to enhance content accessibility, security, and overall user experience."
2503.13455,"Non-terrestrial networks (NTNs) are anticipated to be indispensable in extending coverage and enabling global communication access in next-generation wireless networks. With the extensive deployment of non-terrestrial platforms, evaluating the performance of NTN-enabled communication systems becomes a challenging task. Spherical stochastic geometry (SG) is a recently proposed analytical framework that has garnered increasing attention. Due to its suitability for modeling large-scale dynamic topologies and its ability to provide an analytical framework for interference analysis and low-complexity performance evaluation, spherical SG has been widely applied in NTN performance analysis. This paper surveys the modeling and analysis of NTN networks based on spherical SG. We begin by introducing the spherical SG framework, detailing its history and development. Next, we categorize existing spherical SG models into three types based on orbital modeling methods and provide algorithm implementations for common models. Furthermore, we investigate the accuracy and necessity of spherical modeling through case studies. On the topology level, concepts such as association strategy, central angle, zenith angle, contact angle, and availability probability are introduced, with simple derivations provided. On the channel level, we detail the modeling of large-scale fading, small-scale fading, and beam gain for different channel links. Finally, we discuss several advanced topics that have not been fully explored but have strong motivation and research potential, and we predict future research directions."
2503.13494,"Mobile Edge Computing (MEC) offers low-latency and high-bandwidth support for Internet-of-Vehicles (IoV) applications. However, due to high vehicle mobility and finite communication coverage of base stations, it is hard to maintain uninterrupted and high-quality services without proper service migration among MEC servers. Existing solutions commonly rely on prior knowledge and rarely consider efficient resource allocation during the service migration process, making it hard to reach optimal performance in dynamic IoV environments. To address these important challenges, we propose SR-CL, a novel mobility-aware seamless Service migration and Resource allocation framework via Convex-optimization-enabled deep reinforcement Learning in multi-edge IoV systems. First, we decouple the Mixed Integer Nonlinear Programming (MINLP) problem of service migration and resource allocation into two sub-problems. Next, we design a new actor-critic-based asynchronous-update deep reinforcement learning method to handle service migration, where the delayed-update actor makes migration decisions and the one-step-update critic evaluates the decisions to guide the policy update. Notably, we theoretically derive the optimal resource allocation with convex optimization for each MEC server, thereby further improving system performance. Using the real-world datasets of vehicle trajectories and testbed, extensive experiments are conducted to verify the effectiveness of the proposed SR-CL. Compared to benchmark methods, the SR-CL achieves superior convergence and delay performance under various scenarios."
2503.13515,"Streaming analytics are essential in a large range of applications, including databases, networking, and machine learning. To optimize performance, practitioners are increasingly offloading such analytics to network nodes such as switches. However, resources such as fast SRAM memory available at switches are limited, not uniform, and may serve other functionalities as well (e.g., firewall). Moreover, resource availability can also change over time due to the dynamic demands of in-network applications.In this paper, we propose a new approach to disaggregating data structures over time and space, leveraging any residual resource available at network nodes. We focus on sketches, which are fundamental for summarizing data for streaming analytics while providing beneficial space-accuracy tradeoffs. Our idea is to break sketches into multiple `fragments' that are placed at different network nodes. The fragments cover different time periods and are of varying sizes, and are combined to form a network-wide view of the underlying traffic. We apply our solution to three popular sketches (namely, Count Sketch, Count-Min Sketch, and UnivMon) and demonstrate we can achieve approximately a 75% memory size reduction for the same error for many queries, or a near order-of-magnitude error reduction if memory is kept unchanged."
2503.13726,"This paper addresses the critical challenge posed by the increasing energy consumption in mobile networks, particularly with the advent of Sixth Generation (6G) technologies. We propose an adaptive network management framework that leverages the Open Radio Access Network (O-RAN) architecture to enhance network adaptability and energy efficiency. By utilizing O-RAN's open interfaces and intelligent controllers, our approach implements dynamic resource management strategies that respond to fluctuating user demands while maintaining the quality of service. We design and implement O-RAN-compliant applications to validate our framework, demonstrating significant improvements in energy efficiency without compromising network performance. Our study offers a comprehensive guide for utilizing O-RAN's open architecture to achieve sustainable and energy-efficient 6G networks, aligning with global efforts to reduce the environmental impact of mobile communication systems."
2503.13753,"In this paper, we study the problem of compact routing schemes in weighted undirected and directed graphs.\textit{For weighted undirected graphs}, more than a decade ago, Chechik [PODC'13] presented a $\approx3.68k$-stretch compact routing scheme that uses $\tilde{O}(n^{1/k}\log{D})$ local storage, where $D$ is the normalized diameter, for every $k>1$. We present a $\approx 2.64k$-stretch compact routing scheme that uses $\tilde{O}(n^{1/k})$ local storage \textit{on average} in each vertex. This is the first compact routing scheme that uses total local storage of $\tilde{O}(n^{1+1/k})$ while achieving a $c \cdot k$ stretch, for a constant $c < 3$.In real-world network protocols, messages are usually transformed as part of a communication session between two parties. Therefore, more than two decades ago, Thorup and Zwick [SPAA'01] considered compact routing schemes that establish a communication session using a handshake. In their handshake-based compact routing scheme, the handshake is routed along a $(4k-5)$-stretch path, and the rest of the communication session is routed along an optimal $(2k-1)$-stretch path. It is straightforward to improve the $(4k-5)$-stretch of the handshake to $\approx3.68k$-stretch using the compact routing scheme of Chechik [PODC'13]. We improve the handshake stretch to the optimal $(2k-1)$, by borrowing the concept of roundtrip routing from directed graphs to \textit{undirected} graphs.\textit{For weighted directed graphs}, more than two decades ago, Roditty, Thorup, and Zwick [SODA'02 and TALG'08] presented a $(4k+\eps)$-stretch compact roundtrip routing scheme that uses $\tilde{O}(n^{1/k})$ local storage for every $k\ge 3$. For $k=3$, this gives a $(12+\eps)$-roundtrip stretch using $\tilde{O}(n^{1/3})$ local storage. We improve the stretch by developing a $7$-roundtrip stretch routing scheme with $\tilde{O}(n^{1/3})$ local storage."
2503.13808,"The rapid advancement of internet technology has led to a surge in data transmission, making network traffic classification crucial for security and management. However, there are significant deficiencies in its efficiency for handling multiattribute analysis and its ability to expand model knowledge, making it difficult to adapt to the ever-changing network environment and complex identification requirements. To address this issue, we proposed the SNAKE (Sustainable Network Analysis with Knowledge Exploration) system, which adopts a multi-gated mixture of experts architecture to construct a multi-functional traffic classification model. The system analyzes traffic attributes at different levels through multiple expert sub-models, providing predictions for these attributes via gating and a final Tower network. Additionally, through an intelligent gating configuration, the system enables extremely fast model integration and evolution across various knowledge expansion scenarios. Its excellent compatibility allows it to continuously evolve into a multi-functional largescale model in the field of traffic analysis. Our experimental results demonstrate that the SNAKE system exhibits remarkable scalability when faced with incremental challenges in diverse traffic classification tasks. Currently, we have integrated multiple models into the system, enabling it to classify a wide range of attributes, such as encapsulation usage, application types and numerous malicious behaviors. We believe that SNAKE can pioneeringly create a sustainable and multifunctional large-scale model in the field of network traffic analysis after continuous expansion."
2503.14057,"Bitcoin burn addresses are addresses where bitcoins can be sent but never retrieved, resulting in the permanent loss of those coins. Given Bitcoin's fixed supply of 21 million coins, understanding the usage and the amount of bitcoins lost in burn addresses is crucial for evaluating their economic impact. However, identifying burn addresses is challenging due to the lack of standardized format or convention. In this paper, we propose a novel methodology for the automatic detection of burn addresses using a multi-layer perceptron model trained on a manually classified dataset of 196,088 regular addresses and 2,082 burn addresses. Our model identified 7,905 true burn addresses from a pool of 1,283,997,050 addresses with only 1,767 false positive. We determined that 3,197.61 bitcoins have been permanently lost, representing only 0.016% of the total supply, yet 295 million USD on November 2024. More than 99% of the lost bitcoins are concentrated in just three addresses. This skewness highlights diverse uses of burn addresses, including token creation via proof-of-burn, storage of plain text messages, or storage of images using the OLGA Stamps protocol."
2503.14063,"In a circuit-switched network, traffic can be characterized by several factors that define how communication resources are allocated and utilized during a connection. The amount of traffic basically determines how frequently connection requests arrive, how long the setup connection remains active, and the bandwidth used. The Poisson Arrival Process models traffic arrival events at random intervals. It assumes that events happen independently of one another. This model is ideal for simulating traffic in networks where arrivals happen independently and randomly, such as the start of phone calls, data requests, or packet transmissions. The Poisson Arrival Process and uniformly choosing source and destination pair is been used most commonly by researchers to generate traffic in a network to test various promising routing and spectrum assignment algorithms. It checks the algorithm in uniformly loaded conditions and estimate its baseline performance. In real real-world scenario, a bunch of network nodes can start experiencing heavy data traffic compared to the rest of the network. This can lead to latency issues, or even outages if the network is not optimized to handle the load at these nodes which are also called hotspots. In other terms, hotspot in a network is an area or set of nodes within the network that have a higher likelihood of being involved in communication or data transmission compared to other areas. In this paper, we have tried to find what are the various factors involved in increasing the blocking probability in hotspot traffic scenarios. We have also compared the results with the uniform traffic load conditions in same topology."
2503.14248,"Flying Networks (FNs) have emerged as a promising solution to provide on-demand wireless connectivity when network coverage is insufficient or the communications infrastructure is compromised, such as in disaster management scenarios. Despite extensive research on Unmanned Aerial Vehicle (UAV) positioning and radio resource allocation, the challenge of ensuring reliable traffic relay through backhaul links in predictive FNs remains unexplored.This work proposes Simulated Annealing for predictive FNs (SAFnet), an innovative algorithm that optimizes network performance under positioning constraints, limited bandwidth and minimum rate requirements. Our algorithm uniquely leverages prior knowledge of the first-tier node trajectories to assign bandwidth and dynamically adjust the position of the second-tier flying relay. Building upon Simulated Annealing, our approach enhances this well-known AI algorithm with penalty functions, achieving performance levels comparable to exhaustive search while significantly reducing computational complexity."
2503.14271,"In this paper, we present Kairos, a model predictive control (MPC)-based adaptive bitrate (ABR) scheme that integrates streaming-aware throughput predictions to enhance video streaming quality. Kairos features an attention-based throughput predictor with buffer-aware uncertainty control, improving prediction accuracy and adaptability to network conditions. Specifically, we introduce a multi-time attention network to handle the irregularly sampled sequences in streaming data, creating uniformly spaced latent representations. Additionally, we design a separate prediction network that estimates future throughput at multiple percentiles and incorporates a buffer-aware uncertainty adjustment module. This module dynamically selects the appropriate throughput percentile based on the buffer size, enhancing robustness to varying network conditions. Lastly, to mitigate QoE smoothness penalties caused by predictors focused solely on accuracy, we introduce a smoothness regularizer. By embedding streaming-aware characteristics, such as sampling irregularity, buffer occupancy, and smoothness, into the throughput predictor design, Kairos significantly improves bitrate decision-making within the MPC framework. Extensive trace-driven and real-world experiments demonstrate that Kairos outperforms state-of-the-art ABR schemes, achieving an average QoE improvement of 1.52% to 7.28% under various network conditions."
2503.14351,"Load Balancing (LB) is a routing strategy that increases performance by distributing traffic over multiple outgoing paths. In this work, we introduce a novel methodology to detect the influence of LB on anycast routing, which can be used by operators to detect networks that experience anycast site flipping, where traffic from a single client reaches multiple anycast sites. We use our methodology to measure the effects of LB-behavior on anycast routing at a global scale, covering both IPv4 and IPv6. Our results show that LB-induced anycast site flipping is widespread. The results also show our method can detect LB implementations on the global Internet, including detection and classification of Points-of-Presence (PoP) and egress selection techniques deployed by hypergiants, cloud providers, and network operators. We observe LB-induced site flipping directs distinct flows to different anycast sites with significant latency inflation. In cases with two paths between an anycast instance and a load-balanced destination, we observe an average RTT difference of 30 ms with 8% of load-balanced destinations seeing RTT differences of over 100 ms. Being able to detect these cases can help anycast operators significantly improve their service for affected clients."
2503.14647,"Across large language model (LLM) applications, we observe an emerging trend for reusing KV caches to save the prefill delays of processing repeated input texts in different LLM inputs. This has led to a broad design space, including colocating stored KV caches with (or close to) GPUs to various KV cache compression. However, a key question remains unanswered: can these delay reductions also be economically favorable? Specifically, we ask whether a developer can use public cloud services to store precomputed KV caches and reuse them to save delay without incurring more costs in terms of compute, storage, and network. To answer this question, we propose an validated analytical model for the cloud cost (in compute, storage, and network) of storing and reusing KV caches based on various workload parameters, such as reuse frequency, generated text lengths, model sizes, etc. Preliminary results show that KV cache reusing is able to save both delay and cloud cost across a range of workloads with long context. And we call more efforts on building more economical context augmented LLM by KV cache reusing."
2503.14959,"Lightweight Tunnels (LWTs) in the Linux kernel enable efficient per-route tunneling and are widely used by protocols such as In Situ Operations, Administration, and Maintenance (IOAM), Segment Routing over IPv6 (SRv6), and Routing Protocol for Low-Power and Lossy Networks (RPL). However, a performance issue was detected in their implementations, where a double-reallocation of socket buffers occurs under specific conditions, leading to significant throughput degradation. This paper investigates the root cause of the issue, which depends on the architecture of the Central Processing Unit (CPU) and the Network Interface Card (NIC). We propose a patch for the Linux kernel to fix this problem, replacing the double-reallocation with a single, efficient one. Performance evaluation demonstrates that the patch eliminates the inefficiency, improving forwarding rates by up to 28.8% for affected protocols."
2503.15101,"The emergence of the Non-Terrestrial Network (NTN) concept in the last years has revolutionized the space industry. This novel network architecture composed of aircraft and spacecraft is currently being standardized by the 3GPP. This standardization process follows dedicated phases in which experimentation of the technology is needed. Although some missions have been conducted to demonstrate specific and service-centric technologies, a open flexible in-orbit infrastructure is demanded to support this standardization process. This work presents the 6GStarLab mission, which aims to address this gap. Specifically, this mission envisions to provide a 6U CubeSat as the main in-orbit infrastructure in which multiple technology validations can be uploaded. The concept of this mission is depicted. Additionally, this work presents the details of the satellite platform and the payload. This last one is designed to enable the experimentation in multiple radio-frequency bands (i.e. UHF, S-, X-, and Ka-bands) and an optical terminal. The launch of the satellite is scheduled for Q2 2025, and it will contribute to the standardization of future NTN architectures."
2503.15173,"We consider a novel routing protocol suitable for ad-hoc networks with dynamically changing topologies, such as DECT 2020 NR (NR+) systems, which often lead to missing links between the nodes and thus, incomplete or inefficient routes. A key point of the proposed protocol is the combination of network discovery and matrix completion techniques, which allow the nodes to establish communication paths efficiently and reliably. Additionally, multihop localization is performed to estimate the location of the nodes without needing to broadcast each node's geographical position, thus preserving privacy during the routing process and enabling nodes in the network to independently find potentially missing paths in a decentralized manner instead of flooding the whole network. Simulation results illustrate the good performance of the proposed technique in terms of the average number of hops of the obtained routes in different scenarios, with different network densities and amounts of incompleteness."
2503.15228,"In 3GPP New Radio (NR) Vehicle-to-Everything (V2X), the new standard for next-generation vehicular networks, vehicles can autonomously select sidelink resources for data transmission, which permits network operations without cellular coverage. However, standalone resource allocation is uncoordinated, and is complicated by the high mobility of the nodes that may introduce unforeseen channel collisions (e.g., when a transmitting vehicle changes path) or free up resources (e.g., when a vehicle moves outside of the communication area). Moreover, unscheduled resource allocation is prone to the hidden node and exposed node problems, which are particularly critical considering directional transmissions. In this paper, we implement and demonstrate a new channel access scheme for NR V2X in Frequency Range 2 (FR2), i.e., at millimeter wave (mmWave) frequencies, based on directional and beamformed transmissions along with Sidelink Control Information (SCI) to select resources for transmission. We prove via simulation that this approach can reduce the probability of collision for resource allocation, compared to a baseline solution that does not configure SCI transmissions."
2503.15249,"Analyzing violations of forwarding properties is a classic networking problem. However, existing work is either tailored to the steady state -- and not to transient states during iBGP convergence -- or does analyze transient violations but with inaccurate proxies, like control-plane convergence, or without precise control over the different impact factors.We address this gap with a measurement framework that controllably and accurately measures transient violation times in realistic network deployments. The framework relies on a programmable switch to flexibly emulate diverse topologies and gain traffic visibility at all links -- enabling accurately inferring violation times of any forwarding property. Using the framework, we analyze 50 network scenarios on a topology with 12 real routers, and show how factors like the network configuration and BGP event affect transient violation times. Further, we shed light on less-known aspects of BGP convergence, including that transient violations can start before the trigger event, or that keeping a backup route advertised at all times can increase violation times."
2503.15297,"With the emergence of new application areas such as cyber-physical systems and human-in-the-loop applications ensuring a specific level of end-to-end network latency with high reliability (e.g., 99.9%) is becoming increasingly critical. To align wireless links with these reliability requirements, it is essential to analyze and control network latency in terms of its full probability distribution. However, in a wireless link, the distribution may vary over time, making this task particularly challenging. We propose predicting the latency distribution using state-of-the-art data-driven techniques that leverage historical network information. Our approach tokenizes network state information and processes it using temporal deep-learning architectures-namely LSTM and Transformer models-to capture both short- and long-term delay dependencies. These models output parameters for a chosen parametric density via a mixture density network with Gaussian mixtures, yielding multi-step probabilistic forecasts of future delays. To validate our proposed approach, we implemented and tested these methods using a time-synchronized, SDR-based OpenAirInterface 5G testbed to collect and preprocess network-delay data. Our experiments show that the Transformer model achieves lower negative log-likelihood and mean absolute error than both LSTM and feed-forward baselines in challenging scenarios, while also providing insights into model complexity and training/inference overhead. This framework enables more informed decision-making for adaptive scheduling and resource allocation, paving the way toward enhanced QoS in evolving 5G and 6G networks."
2503.15429,"Within 3GPP, the campus network architecture has evolved as a deployment option for industries and can be provisioned using network slicing over already installed 5G public network infrastructure. In campus networks, the ultra-reliable low latency communication (URLLC) service category is of major interest for applications with strict latency and high-reliability requirements. One way to achieve high reliability in a shared infrastructure is through resource isolation, whereby network slicing can be optimized to adequately reserve computation and transmission capacity. This paper proposes an approach for vertical slicing the radio access network (RAN) to enable the deployment of multiple and isolated campus networks to accommodate URLLC services. To this end, we model RAN function placement as a mixed integer linear programming problem with URLLC-related constraints. We demonstrate that our approach can find optimal solutions in real-world scenarios. Furthermore, unlike existing solutions, our model considers the user traffic flow from a known source node on the network's edge to an unknown \textit{a priori} destination node. This flexibility could be explored in industrial campus networks by allowing dynamic placement of user plane functions (UPFs) to serve the URLLC."
2503.15764,"The promising potential of AI and network convergence in improving networking performance and enabling new service capabilities has recently attracted significant interest. Existing network AI solutions, while powerful, are mainly built based on the close-loop and passive learning framework, resulting in major limitations in autonomous solution finding and dynamic environmental adaptation. Agentic AI has recently been introduced as a promising solution to address the above limitations and pave the way for true generally intelligent and beneficial AI systems. The key idea is to create a networking ecosystem to support a diverse range of autonomous and embodied AI agents in fulfilling their goals. In this paper, we focus on the novel challenges and requirements of agentic AI networking. We propose AgentNet, a novel framework for supporting interaction, collaborative learning, and knowledge transfer among AI agents. We introduce a general architectural framework of AgentNet and then propose a generative foundation model (GFM)-based implementation in which multiple GFM-as-agents have been created as an interactive knowledge-base to bootstrap the development of embodied AI agents according to different task requirements and environmental features. We consider two application scenarios, digital-twin-based industrial automation and metaverse-based infotainment system, to describe how to apply AgentNet for supporting efficient task-driven collaboration and interaction among AI agents."
2503.15822,"The digital twin edge network (DITEN) is a significant paradigm in the sixth-generation wireless system (6G) that aims to organize well-developed infrastructures to meet the requirements of evolving application scenarios. However, the impact of the interaction between the long-term DITEN maintenance and detailed digital twin tasks, which often entail privacy considerations, is commonly overlooked in current research. This paper addresses this issue by introducing a problem of digital twin association and historical data allocation for a federated learning (FL) task within DITEN. To achieve this goal, we start by introducing a closed-form function to predict the training accuracy of the FL task, referring to it as the data utility. Subsequently, we carry out comprehensive convergence analyses on the proposed FL methodology. Our objective is to jointly optimize the data utility of the digital twin-empowered FL task and the energy costs incurred by the long-term DITEN maintenance, encompassing FL model training, data synchronization, and twin migration. To tackle the aforementioned challenge, we present an optimization-driven learning algorithm that effectively identifies optimized solutions for the formulated problem. Numerical results demonstrate that our proposed algorithm outperforms various baseline approaches."
2503.16112,"Traditional video compression algorithms exhibit significant quality degradation at extremely low bitrates. Promptus emerges as a new paradigm for video streaming, substantially cutting down the bandwidth essential for video streaming. However, Promptus is computationally intensive and can not run in real-time on mobile devices. This paper presents PromptMobile, an efficient acceleration framework tailored for on-device Promptus. Specifically, we propose (1) a two-stage efficient generation framework to reduce computational cost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant computations by 16.6%, (3) system-level optimizations to further enhance efficiency. The evaluations demonstrate that compared with the original Promptus, PromptMobile achieves a 13.6x increase in image generation speed. Compared with other streaming methods, PromptMobile achives an average LPIPS improvement of 0.016 (compared with H.265), reducing 60% of severely distorted frames (compared to VQGAN)."
2503.16146,"In large-scale UAV swarms, dynamically executing machine learning tasks can pose significant challenges due to network volatility and the heterogeneous resource constraints of each UAV. Traditional approaches often rely on centralized orchestration to partition tasks among nodes. However, these methods struggle with communication bottlenecks, latency, and reliability when the swarm grows or the topology shifts rapidly. To overcome these limitations, we propose a fully distributed, diffusive metric-based approach for split computing in UAV swarms. Our solution introduces a new iterative measure, termed the aggregated gigaflops, capturing each node's own computing capacity along with that of its neighbors without requiring global network knowledge. By forwarding partial inferences intelligently to underutilized nodes, we achieve improved task throughput, lower latency, and enhanced energy efficiency. Further, to handle sudden workload surges and rapidly changing node conditions, we incorporate an early-exit mechanism that can adapt the inference pathway on-the-fly. Extensive simulations demonstrate that our approach significantly outperforms baseline strategies across multiple performance indices, including latency, fairness, and energy consumption. These results highlight the feasibility of large-scale distributed intelligence in UAV swarms and provide a blueprint for deploying robust, scalable ML services in diverse aerial networks."
2503.16834,"Designing high-performance routing protocols for flying ad hoc networks (FANETs) is challenging due to the diversity of applications and the dynamics of network topology. The existing general-purpose routing protocols for ad hoc networks often oversimplify mobility patterns and disregard the unequal importance of nodes, resulting in suboptimal routing decisions that are unsuitable for task-oriented FANETs. To break the bottleneck, in this paper we propose a betweenness centrality based dynamic source routing (BC-DSR) protocol for a flying ad hoc network (FANET) in marching formation. Firstly, we introduce a Gauss-Markov group (GMG) mobility model based on the leader-follower pattern, which accurately captures the temporal and spatial correlations of node movements in the realistic marching formation. Besides, we exploit the concept of BC defined in graph theory to measure the structural unequal importance of relay nodes, i.e., to determine link weights, in the particular marching formation topology. The path of least cost is calculated relying on a weighted directed graph constructed. The ns-3 based simulation results demonstrate that our BCDSR protocol achieves higher packet-delivery ratio and lower average end-to-end latency and routing overhead ratio than representative benchmark protocols used in FANETs, while maintaining a reasonably small network jitter."
2503.16879,"The rotatable reconfigurable intelligent surface (RIS) can enhance mobile edge computing (MEC) performance by optimizing its orientation to improve the gain of received and transmitted signals. This correspondence investigates a rotatable RIS-assisted MEC system, aimed at minimizing energy consumption for multiple moving user equipment (UEs) through the joint design of RIS orientation, discrete phase shift, computation resource allocation, transmitting power and task offloading strategies. Considering the mobility of UEs, this problem is formulated as a sequential decision-making across multiple time slots. To address this challenge, a soft actor-critic (SAC)-based algorithm is proposed to optimize RIS orientation, phase shift and task offloading strategies, while computation resource allocation and transmitting power are determined based on the actions. Numerical results demonstrate that the proposed scheme exhibits superior convergence and performance compared to benchmarks. Additionally, the rotatable RIS scheme reduces total energy consumption by up to 47.3% compared to the fixed RIS, enhancing MEC system performance."
2503.16895,"In this short paper, we propose a technique for AI-based identification of modulation and coding schemes (MCS) in surrounding cellular signals. Based on the created MCS map, we evaluate the performance of indoor localization techniques."
2503.16915,"In this paper, we investigate beamforming design and trajectory optimization for a multi-unmanned aerial vehicle (UAV)-assisted integrated sensing and communication (ISAC) system. The proposed system employs multiple UAVs equipped with dual-functional radar-communication capabilities to simultaneously perform target sensing and provide communication services to users. We formulate a joint optimization problem that aims to maximize the sum rate of users while maintaining target sensing performance through coordinated beamforming and UAV trajectory design. To address this challenging non-convex problem, we develop a block coordinated descent (BCD)-based iterative algorithm that decomposes the original problem into tractable subproblems. Then, the beamforming design problem is addressed using fractional programming, while the UAV trajectory is refined through the deep deterministic policy gradient (DDPG) algorithm. The simulation results demonstrate that the proposed joint optimization approach achieves significant performance improvements in both communication throughput and sensing accuracy compared to conventional, separated designs. We also show that proper coordination of multiple UAVs through optimized trajectories and beamforming patterns can effectively balance the tradeoff between sensing and communication objectives."
2503.16972,"A Decentralized Identifier (DID) empowers an entity to prove control over a unique and self-issued identifier without relying on any identity provider. The public key material for the proof is encoded into an associated DID document (DDO). This is preferable shared via a distributed ledger because it guarantees algorithmically that everyone has access to the latest state of any tamper-proof DDO but only the entities in control of a DID are able to update theirs. Yet, it is possible to grant deputies the authority to update the DDO on behalf of the DID owner. However, the DID specification leaves largely open on how authorizations over a DDO are managed and enforced among multiple deputies. This article investigates what it means to govern a DID and discusses various forms of how a DID can be controlled by potentially more than one entity. It also presents a prototype of a DID-conform identifier management system where a selected set of governance policies are deployed as Smart Contracts. The article highlights the critical role of governance for the trustworthy and flexible deployment of ledger-anchored DIDs across various domains."
2503.17065,We demonstrate end-to-end 5G Open RAN over PON using off-the-shelf open networking hardware and open source RAN software. The implementation of the Cooperative Transport Interface provides timely synchronisation of PON and RAN schedulers.
2503.17079,"We introduce a ML-based architecture for network operators to detect impairments from specific OSaaS users while blind to the users' internal spectrum details. Experimental studies with three OSaaS users demonstrate the model's capability to accurately classify the source of impairments, achieving classification accuracy of 94.2%."
2503.17094,"The gain spectrum of an Erbium-Doped Fiber Amplifier (EDFA) has a complex dependence on channel loading, pump power, and operating mode, making accurate modeling difficult to achieve. Machine Learning (ML) based modeling methods can achieve high accuracy, but they require comprehensive data collection. We present a novel ML-based Semi-Supervised, Self-Normalizing Neural Network (SS-NN) framework to model the wavelength dependent gain of EDFAs using minimal data, which achieve a Mean Absolute Error (MAE) of 0.07/0.08 dB for booster/pre-amplifier gain prediction. We further perform Transfer Learning (TL) using a single additional measurement per target-gain setting to transfer this model among 22 EDFAs in Open Ireland and COSMOS testbeds, which achieves a MAE of less than 0.19 dB even when operated across different amplifier types. We show that the SS-NN model achieves high accuracy for gain spectrum prediction with minimal data requirement when compared with current benchmark methods."
2503.17343,"Low Earth Orbit (LEO) satellite networks, characterized by their high data throughput and low latency, have gained significant interest from both industry and academia. Routing data efficiently within these networks is essential for maintaining a high quality of service. However, current routing strategies, such as bent-pipe and inter-satellite link (ISL) routing, have their unique challenges. The bent-pipe strategy requires a dense deployment of dedicated ground stations, while the ISL-based strategy can negatively impact satellite battery lifespan due to increased traffic load, leading to sustainability issues.In this paper, we propose sustainable collaborative offloading, a framework that orchestrates groups of existing commercial resources like ground stations and 5G base stations for data offloading. This orchestration enhances total capacity, overcoming the limitations of a single resource. We propose the collaborator group set construction algorithm to construct candidate groups and the collaborator selection and total payment algorithm to select offloading targets and determine payments no less than the costs. Extensive real-world-based simulations show that our solution significantly improves energy consumption, satellite service life, and end-to-end latency."
2503.17554,"Programmable data planes enable users to design data plane algorithms for network devices, providing extensive flexibility for network customization. Programming Protocol-Independent Packet Processors (P4) has become the most widely adopted abstraction, programming language, and framework for data plane programming. However, existing simulation platforms lack high-performance support for P4-based networks. This paper introduces P4sim, a high-performance P4-driven simulation framework built on bmv2 and NS4, seamlessly integrated with ns-3. It improves queue modeling, time scheduling, and P4 architecture support, extending compatibility to V1model, PSA, and PNA. P4sim enables efficient packet processing, accurate time tracking, and seamless interaction between P4-enabled hosts and switches. We evaluate the P4sim in terms of performance and queue management and demonstrate its capabilities using two common use cases: Basic Tunneling and Load Balancing. The results highlight the P4sim as a powerful tool for advancing research and education in programmable networks."
2503.17693,"Due to network delays and scalability limitations, clustered ad hoc networks widely adopt Reinforcement Learning (RL) for on-demand resource allocation. Albeit its demonstrated agility, traditional Model-Free RL (MFRL) solutions struggle to tackle the huge action space, which generally explodes exponentially along with the number of resource allocation units, enduring low sampling efficiency and high interaction cost. In contrast to MFRL, Model-Based RL (MBRL) offers an alternative solution to boost sample efficiency and stabilize the training by explicitly leveraging a learned environment model. However, establishing an accurate dynamic model for complex and noisy environments necessitates a careful balance between model accuracy and computational complexity $\&$ stability. To address these issues, we propose a Conditional Diffusion Model Planner (CDMP) for high-dimensional offline resource allocation in clustered ad hoc networks. By leveraging the astonishing generative capability of Diffusion Models (DMs), our approach enables the accurate modeling of high-quality environmental dynamics while leveraging an inverse dynamics model to plan a superior policy. Beyond simply adopting DMs in offline RL, we further incorporate the CDMP algorithm with a theoretically guaranteed, uncertainty-aware penalty metric, which theoretically and empirically manifests itself in mitigating the Out-of-Distribution (OOD)-induced distribution shift issue underlying scarce training data. Extensive experiments also show that our model outperforms MFRL in average reward and Quality of Service (QoS) while demonstrating comparable performance to other MBRL algorithms."
2503.17708,"Given the limited computing capabilities on autonomous vehicles, onboard processing of large volumes of latency-sensitive tasks presents significant challenges. While vehicular edge computing (VEC) has emerged as a solution, offloading data-intensive tasks to roadside servers or other vehicles is hindered by large obstacles like trucks/buses and the surge in service demands during rush hours. To address these challenges, Reconfigurable Intelligent Surface (RIS) can be leveraged to mitigate interference from ground signals and reach more edge servers by elevating RIS adaptively. To this end, we propose RAISE, an optimization framework for RIS placement in multi-server VEC systems. Specifically, RAISE optimizes RIS altitude and tilt angle together with the optimal task assignment to maximize task throughput under deadline constraints. To find a solution, a two-layer optimization approach is proposed, where the inner layer exploits the unimodularity of the task assignment problem to derive the efficient optimal strategy while the outer layer develops a near-optimal hill climbing (HC) algorithm for RIS placement with low complexity. Extensive experiments demonstrate that the proposed RAISE framework consistently outperforms existing benchmarks."
2503.1785,"Although DRL (deep reinforcement learning) has emerged as a powerful tool for making better decisions than existing hand-crafted communication protocols, it faces significant limitations: 1) Selecting the appropriate neural network architecture and setting hyperparameters are crucial for achieving desired performance levels, requiring domain expertise. 2) The decision-making process in DRL models is often opaque, commonly described as a 'black box.' 3) DRL models are data hungry. In response, we propose CP-AgentNet, the first framework designed to use generative agents for developing communication network protocols. This approach addresses these challenges by creating an autonomous system for protocol design, significantly reducing human effort. We developed LLMA (LLM-agents-based multiple access) and CPTCP (CP-Agent-based TCP) for heterogeneous environments. Our comprehensive simulations have demonstrated the efficient coexistence of LLMA and CPTCP with nodes using different types of protocols, as well as enhanced explainability."
2503.17912,"In the evolution of sixth-generation (6G) mobile communication networks, satellite-terrestrial integrated networks emerge as a promising paradigm, characterized by their wide coverage and reliable transmission capabilities. By integrating with cloud-based terrestrial mobile communication networks, the limitations of low Earth orbit (LEO) satellites, such as insufficient onboard computing capabilities and limited inter-satellite link capacity, can be addressed. In addition, to efficiently respond to the diverse integrated tasks of communication, remote sensing, and navigation, LEO constellations need to be capable of autonomous networking. To this end, this article presents a satellite-terrestrial integrated fog network for 6G. Its system architecture and key technologies are introduced to achieve flexible collaboration between fog satellites and terrestrial cloud computing centers. In particular, key techniques with diverse challenges and their corresponding solutions are discussed, including integrated waveform design and resource management based on fog satellite onboard processing, as well as mobility management and native artificial intelligence based on cloud-fog collaboration. Finally, future challenges and open issues are outlined."
2503.17913,"With the burgeoning demand for data-intensive services, satellite-terrestrial networks (STNs) face increasing backhaul link congestion, deteriorating user quality of service (QoS), and escalating power consumption. Cache-aided STNs are acknowledged as a promising paradigm for accelerating content delivery to users and alleviating the load of backhaul links. However, the dynamic nature of low earth orbit (LEO) satellites and the complex interference among satellite beams and terrestrial base stations pose challenges in effectively managing limited edge resources. To address these issues, this paper proposes a method for dynamically scheduling caching and communication resources, aiming to reduce network costs in terms of transmission power consumption and backhaul traffic, while meeting user QoS demands and resource constraints. We formulate a mixed timescale problem to jointly optimize cache placement, LEO satellite beam direction, and cooperative multicast beamforming among satellite beams and base stations. To tackle this intricate problem, we propose a two-stage solution framework, where the primary problem is decoupled into a short-term content delivery subproblem and a long-term cache placement subproblem. The former subproblem is solved by designing an alternating optimization approach with whale optimization and successive convex approximation methods according to the cache placement state, while cache content in STNs is updated using an iterative algorithm that utilizes historical information. Simulation results demonstrate the effectiveness of our proposed algorithms, showcasing their convergence and significantly reducing transmission power consumption and backhaul traffic by up to 52%."
2503.18122,"Multi-access edge computing (MEC) is a promising technology that provides low-latency processing capabilities. To optimize the network performance in a MEC system, an efficient routing path between a user and a MEC host is essential. The network performance is characterized by multiple attributes, including packet-loss probability, latency, and jitter. A user service may require a particular combination of such attributes, complicating the shortest-path computation. This paper introduces Q-Routing for Multi-Objective shortest-path computation (QR-MO), which simultaneously optimizes multiple attributes. We compare the QR-MO's solutions with the optimal solutions provided by the Multi-objective Dijkstra Algorithm (MDA). The result shows the favorable potential of QR-MO. After 100 episodes, QR-MO achieves 100% accuracy in networks with low to moderate average node degrees, regardless of the size, and over 85% accuracy in networks with high average node degrees."
2503.18133,"We address the problem of beam scheduling for downlink transmissions in a single-cell millimeter wave (mmWave) network. The cell contains a mmWave base station (mBS) and its associated users. At the end of each time slot, a packet arrives into the queue of a user at the mBS with a certain probability. A holding cost is incurred for the packets stored in a user's queue at the mBS in every time slot. The number of simultaneous beams that the mBS can form to different users is less than the number of associated users. Also, a cost is incurred whenever a beam is formed from the mBS to a user. In a given time slot, a packet transmitted from the mBS to a user that has been assigned a beam is successfully received (respectively, not received) if the channel quality between the mBS and the user is good (respectively, bad). In every time slot, the mBS needs to assign the available beams to a subset of the users, in order to minimize the long-run expected average cost. This problem can be modeled as a restless multi-armed bandit problem, which is provably hard to solve. We prove the Whittle indexability of the above beam scheduling problem and propose a strategy to compute the Whittle index of each user. In each time slot, our proposed beam scheduling policy assigns beams to the users with the smallest Whittle indices. Using extensive simulations, we show that our proposed Whittle index-based beam scheduling policy significantly outperforms several scheduling policies proposed in prior work in terms of the average cost, average delay, as well as energy efficiency."
2503.18203,"This paper presents the results of reflection characteristics measurements for reconfigurable intelligent surfaces (RIS). The measurements were carried out in a non-ideal environment, i.e. typical for subsequent practical use of RISes. During the experiments, popular matrices implemented in the open-source project OpenSource- RIS were used. The study focused on obtaining two types of reflection characteristics - two- and three-dimensional."
2503.18218,"The rapid evolution of wireless technologies has intensified interest in open and fully programmable radio access networks for whole-stack research, innovation, and evaluation of emerging solutions. Large-scale wireless living labs, such as ARA, equipped with real-world infrastructure play a vital role in this evolution by enabling researchers to prototype and evaluate advanced algorithms for next-generation wireless systems in outdoor and over-the-air environments benefiting from real-world fidelity and end-to-end programmability. However, at the core of this innovation is the performance in terms of coverage and reliability of these wireless living labs. For instance, interfacing power amplifiers and low noise amplifiers with software-defined radios (SDRs) for experimenting outdoors introduces issues in random access procedure-a process crucial in establishing connectivity between user equipment (UE) and the core network in 5G and 6G systems. Therefore, to ensure seamless connectivity and reliable communications in open-source 5G software stacks such as OpenAirInterface (OAI), we propose a slot-based approach to the 5G random access procedure leveraging full downlink (DL) and uplink (UL) slots instead of using special or mixed slots. We highlight how this approach achieves reliable 5G connectivity over 1 mile-the longest communication range that has been achieved so far in real-world settings using open-source 5G software stacks and the Universal Software Radio Peripheral (USRP) SDRs. We also demonstrate that, in a highly obstructed environment such as an industrial setting, we can increase the probability of a successful random access procedure to 90%-100% when we use at least 9 OFDM symbols to transmit msg2 and msg3."
2503.18487,"Malicious traffic detection is a pivotal technology for network security to identify abnormal network traffic and detect network attacks. Large Language Models (LLMs) are trained on a vast corpus of text, have amassed remarkable capabilities of context-understanding and commonsense knowledge. This has opened up a new door for network attacks detection. Researchers have already initiated discussions regarding the application of LLMs on specific cyber-security tasks. Unfortunately, there remains a lack of comprehensive analysis on harnessing LLMs for traffic detection, as well as the opportunities and challenges. In this paper, we focus on unleashing the full potential of Large Language Models (LLMs) in malicious traffic detection. We present a holistic view of the architecture of LLM-powered malicious traffic detection, including the procedures of Pre-training, Fine-tuning, and Detection. Especially, by exploring the knowledge and capabilities of LLM, we identify three distinct roles LLM can act in traffic classification: Classifier, Encoder, and Predictor. For each of them, the modeling paradigm, opportunities and challenges are elaborated. Finally, we present our design on LLM-powered DDoS detection as a case study. The proposed framework attains accurate detection on carpet bombing DDoS by exploiting LLMs' capabilities in contextual mining. The evaluation shows its efficacy, exhibiting a nearly 35% improvement compared to existing systems."
2503.18495,We present a framework to identify and mitigate rogue OOK signals and user-generated power interference in a multi-user Optical-Spectrum-as-a-Service network. Experimental tests on the OpenIreland-testbed achieve up to 89% detection rate within 10 seconds of an interference event.
2503.18706,"Efficient network modeling is essential for resource optimization and network planning in next-generation large-scale complex networks. Traditional approaches, such as queuing theory-based modeling and packet-based simulators, can be inefficient due to the assumption made and the computational expense, respectively. To address these challenges, we propose an innovative energy-efficient dynamic orchestration of Graph Neural Networks (GNN) based model training and inference framework for context-aware network modeling and predictions. We have developed a low-complexity solution framework, QAG, that is a Quantum approximation optimization (QAO) algorithm for Adaptive orchestration of GNN-based network modeling. We leverage the tripartite graph model to represent a multi-application system with many compute nodes. Thereafter, we apply the constrained graph-cutting using QAO to find the feasible energy-efficient configurations of the GNN-based model and deploying them on the available compute nodes to meet the network modeling application requirements. The proposed QAG scheme closely matches the optimum and offers atleast a 50% energy saving while meeting the application requirements with 60% lower churn-rate."
2503.18915,"In this paper, we conduct an in-depth analysis of radio signal propagation characteristics within the urban environment of Poznan (Poland). The study specifically addresses the deployment of a 5th generation (5G NR - New Radio) Radio Access Network (RAN), which comprises 8 strategically positioned Base Stations (BSs). These base stations are configured with either Single Input Single Output (SISO) or Multiple Input Multiple Output (MIMO) antenna technologies, contingent upon the specific requirements of the network cells they serve. A key focus of our research is the integration of 15 reflecting arrays, known as Reconfigurable Intelligent Surfaces (RISs), which were installed throughout the study area. These RISs were deployed at various suspension heights to evaluate their impact on radio signal propagation and coverage. By exploring the influence of these RIS matrices, our research sheds light on their potential to significantly enhance signal quality, particularly in urban environments."
2503.19003,"Periodic messages transfer data from sensors to actuators in cars, planes, and complex production machines. When considering a given routing, the unicast message starts at its source and goes over several dedicated resources to reach its destination. Such unicast message can be represented as a chain of point-to-point communications. Thus, the scheduling of the periodic chains is a principal problem in time-triggered Ethernet, like IEEE 802.1Qbv Time-Sensitive Networks. This paper studies a strongly NP-hard periodic scheduling problem with harmonic periods, task chains, and dedicated resources. We analyze the problem on several levels of generality and complexity and provide the corresponding proofs. We describe a solution methodology to find a feasible schedule that minimizes the chains' degeneracy related to start-to-end latency normalized in the number of periods. We use the local search with the first fit scheduling heuristic, which we warm-start with a constraint programming model. This notably improves the schedulability of instances with up to 100% utilization and thousands (and more) of tasks, with high-quality solutions found in minutes. An efficient constraint programming matheuristic significantly reduces the degeneracy of the found schedules even further. The method is evaluated on sets of industrial-, avionic-, and automotive-inspired instances."
2503.19038,"This paper addresses the crucial need for reliable wireless communication in vehicular networks, particularly vital for the safety and efficacy of (semi-)autonomous driving amid increasing traffic. We explore the use of Reconfigurable Intelligent Surfaces (RISes) mounted on Drone Relay Stations (DRS) to enhance communication reliability. Our study formulates an optimization problem to pinpoint the optimal location and orientation of the DRS, thereby creating an additional propagation path for vehicle-to-everything (V2X) communications. We introduce a heuristic approach that combines trajectory optimization for DRS positioning and a Q-learning scheme for RIS orientation. Our results not only confirm the convergence of the Q-learning algorithm but also demonstrate significant communication improvements achieved by integrating a DRS into V2X networks."
2503.19097,"The rapid increase in networked systems and data transmission requires advanced data compression solutions to optimize bandwidth utilization and enhance network performance. This study introduces a novel byte-level predictive model using Transformer architecture, capable of handling the redundancy and diversity of data types in network traffic as byte sequences. Unlike traditional methods that require separate compressors for different data types, this unified approach sets new benchmarks and simplifies predictive modeling across various data modalities such as video, audio, images, and text, by processing them at the byte level. This is achieved by predicting subsequent byte probability distributions, encoding them into a sparse rank sequence using lossless entropy coding, and significantly reducing both data size and entropy. Experimental results show that our model achieves compression ratios below 50%, while offering models of various sizes tailored for different communication devices. Additionally, we successfully deploy these models on a range of edge devices and servers, demonstrating their practical applicability and effectiveness in real-world network scenarios. This approach significantly enhances data throughput and reduces bandwidth demands, making it particularly valuable in resource-constrained environments like the Internet of Things sensor networks."
2503.19319,"The demand for MEC has increased with the rise of data-intensive applications and 5G networks, while conventional cloud models struggle to satisfy low-latency requirements. While task offloading is crucial for minimizing latency on resource-constrained User Equipment (UE), fully offloading of all tasks to MEC servers may result in overload and possible task drops. Overlooking the effect of number of dropped tasks can significantly undermine system efficiency, as each dropped task results in unfulfilled service demands and reduced reliability, directly impacting user experience and overall network performance. In this paper, we employ task partitioning, enabling partitions of task to be processed locally while assigning the rest to MEC, thus balancing the load and ensuring no task drops. This methodology enhances efficiency via Mixed Integer Linear Programming (MILP) and Cuckoo Search, resulting in effective task assignment and minimum latency. Moreover, we ensure each user's RB allocation stays within the maximum limit while keeping latency low. Experimental results indicate that this strategy surpasses both full offloading and full local processing, providing significant improvements in latency and task completion rates across diverse number of users. In our scenario, MILP task partitioning results in 24% reduction in latency compared to MILP task offloading for the maximum number of users, whereas Cuckoo search task partitioning yields 18% latency reduction in comparison with Cuckoo search task offloading."
2503.1932,"This paper explores the advancement of Vehicular Edge Computing (VEC) as a tailored application of Mobile Edge Computing (MEC) for the automotive industry, addressing the rising demand for real-time processing in connected and autonomous vehicles. VEC brings computational resources closer to vehicles, reducing data processing delays crucial for safety-critical applications such as autonomous driving and intelligent traffic management. However, the challenge lies in managing the high and dynamic task load generated by vehicles' data streams. We focus on enhancing task offloading and scheduling techniques to optimize both communication and computation latencies in VEC networks. Our approach involves implementing task scheduling algorithms, including First-Come, First-Served (FCFS), Shortest Deadline First (SDF), and Particle Swarm Optimization (PSO) for optimization. Additionally, we divide portions of tasks between the MEC servers and vehicles to reduce the number of dropped tasks and improve real-time adaptability. This paper also compares fixed and shared bandwidth scenarios to manage transmission efficiency under varying loads. Our findings indicate that MEC+Local (partitioning) scenario significantly outperforms MEC-only scenario by ensuring the completion of all tasks, resulting in a zero task drop ratio. The MEC-only scenario demonstrates approximately 5.65% better average end-to-end latency compared to the MEC+Local (partitioning) scenario when handling 200 tasks. However, this improvement comes at the cost of dropping a significant number of tasks (109 out of 200). Additionally, allocating shared bandwidth helps to slightly decrease transmission waiting time compared to using fixed bandwidth."
2503.19555,"Deterministic communications are essential for industrial automation, ensuring strict latency requirements and minimal jitter in packet transmission. Modern production lines, specializing in robotics, require higher flexibility and mobility, which drives the integration of Time-Sensitive Networking (TSN) and 5G networks in Industry 4.0. TSN achieves deterministic communications by using mechanisms such as the IEEE 802.1Qbv Time-Aware Shaper (TAS), which schedules packet transmissions within precise cycles, thereby reducing latency, jitter, and congestion. 5G networks complement TSN by providing wireless mobility and supporting ultra-Reliable Low-Latency Communications. However, 5G channel effects such as fast fading, interference, and network-induced latency and jitter can disrupt TSN traffic, potentially compromising deterministic scheduling and performance. This paper presents an empirical analysis of 5G network latency and jitter on IEEE 802.1Qbv performance in a 5G-TSN network. We evaluate the impact of 5G integration on TSN's deterministic scheduling through a testbed combining IEEE 802.1Qbv-enabled switches, TSN translators, and a commercial 5G system. Our results show that, with proper TAS configuration in the TSN switch aligned with the 5G system, jitter can be mitigated, maintaining deterministic performance."
2503.20057,"This paper addresses the deployment of a drone equipped with a reconfigurable intelligent surface (RIS), creating a drone relay station (DRS) to enhance the connectivity of vehicle-to-vehicle (V2V) pairs on the ground. The trajectory of the DRS is optimized to quickly reach the best location for maximizing throughput. Additionally, the presence of an interfering node is considered, and an analytical solution is derived to determine the optimal orientation of the DRS at each time step, minimizing interference to the receiver. Simulation results confirm the effectiveness of the proposed framework."
2503.20256,"Nowadays, the convergence of Mobile Edge Computing (MEC) and vehicular networks has emerged as a vital facilitator for the ever-increasing intelligent onboard applications. This paper introduces a multi-tier task offloading mechanism for MEC-enabled vehicular networks leveraging vehicle-to-everything (V2X) communications. The study focuses on applications with sequential subtasks and explores two tiers of collaboration. In the vehicle tier, we design a needing vehicle (NV)-helping vehicle (HV) matching scheme and inter-vehicle collaborative computation is studied, with joint optimization of task offloading decision, communication, and computation resource allocation to minimize energy consumption and meet latency requirements. In the roadside unit (RSU) tier, collaboration among RSUs is investigated to address multi-access issues of bandwidth and computation resources for multiple vehicles. A two-step method is proposed to solve the subchannel allocation problem. Detailed experiments are conducted to demonstrate the effectiveness of the proposed method and assess the impact of different parameters on system energy consumption."
2503.20554,"IP anycast replicates an address at multiple locations to reduce latency and enhance resilience. Due to anycast's crucial role in the modern Internet, earlier research introduced tools to perform anycast censuses. The first, iGreedy, uses latency measurements from geographically dispersed locations to map anycast deployments. The second, MAnycast2, uses anycast to perform a census of other anycast networks. MAnycast2's advantage is speed and coverage but suffers from problems with accuracy, while iGreedy is highly accurate but slower using author-defined probing rates and costlier. In this paper we address the shortcomings of both systems and present LACeS (Longitudinal Anycast Census System). Taking MAnycast2 as a basis, we completely redesign its measurement pipeline, and add support for distributed probing, additional protocols (DNS over UDP, TCP SYN/ACK, and IPv6) and latency measurements similar to iGreedy. We validate LACeS on an anycast testbed with 32 globally distributed nodes, compare against an external anycast production deployment, extensive latency measurements with RIPE Atlas and cross-check over 60% of detected anycast using operator ground truth that shows LACeS achieves high accuracy. Finally, we provide a longitudinal analysis of anycast, covering 17+ months, showing LACeS achieves high precision. We make continual daily LACeS censuses available to the community and release the source code of the tool under a permissive open source license."
2503.21013,"AllReduce is a technique in distributed computing which saw use in many critical applications of deep learning. Existing methods of AllReduce scheduling oftentimes lack flexibility due to being topology-specific or relying on extensive handcrafted designs that require domain-specific knowledge. In this work, we aim to alleviate this inflexibility by proposing a deep-reinforcement-learning (DRL)-based pipeline that can generate AllReduce scheduling for various network topologies without topology-specific design features. The flow scheduling module of this pipeline consists of two hierarchically-structured DRL policies that work cooperatively to find optimal scheduling. We showcase the performance of our method compared to the baseline methods on three topologies: BCube, DCell, and Jellyfish. Finally, we contributed a Python-based simulation environment simulating AllReduce scheduling on these network topologies."
2503.21289,"Cloud-Edge applications like industrial control systems and connected vehicles demand stringent end-to-end latency guarantees. Among existing data plane candidate solutions for bounded latency networking, the guaranteed Latency-Based Forwarding (gLBF) approach ensures punctual delivery of traffic flows by managing per-hop delays to meet specific latency targets, while not requiring that per-flow states are maintained at each hop. However, as a forwarding plane mechanism, gLBF does not define the control mechanisms for determining feasible forwarding paths and per-hop latency budgets for packets to fulfil end-to-end latency objectives. In this work, we propose such a control mechanism implemented in Prolog that complies with gLBF specifications, called declarative gLBF (dgLBF). The declarative nature of Prolog allows our prototype to be concise (~120 lines of code) and easy to extend. We show how the core dgLBF implementation is extended to add reliability mechanisms, path protection, and fate-sharing avoidance to enhance fault tolerance and robustness. Finally, we evaluate the system's performance through simulative experiments under different network topologies and with increasing traffic load to simulate saturated network conditions, scaling up to 6000 flows. Our results show a quasi-linear degradation in placement times and system resilience under heavy traffic."
2503.213,"The actual railway communication system used in Europe for high-speed trains (HST) is called the GSM-R system, which is a communication system based on 2G infrastructure. This system is meant to be replaced by a new system based on 5G NR infrastructure called the Future Railway Mobile Communication System (FRMCS) by 2030. For the next years, both systems will probably coexist in the same frequency band since the migration from GSM-R to FRMCS is planned to be done progressively until the GSM-R system is completely shut down, mainly due to safety and budget constraints. In this paper, we study the resource allocation for the FRMCS system sharing the same frequency band as the already deployed GSM-R system. We formulate the resource allocation problem as an integer linear problem (ILP), known to bethis http URLsolve it in a reasonable time, we propose a scheduling algorithm, called Intelligent Traffic Scheduling Preemptor (ITSP), that allocates resources for the different FRMCS traffic types considered (critical traffic and performance traffic) in the same frequency band with the GSM-R system. Our algorithm is channel quality Indicator (CQI) aware and uses the preemption mechanism in 5G NR standards to optimize the resource allocation for the FRMCS system without impacting the actual GSM-R resource allocation in the context of the white space concept."
2503.21601,"The use of higher frequencies in mobile communication systems leads to smaller cell sizes, resulting in the deployment of more base stations and an increase in handovers to support user mobility. This can lead to frequent radio link failures and reduced data rates. In this work, we propose a handover optimization method using proximal policy optimization (PPO) to develop an adaptive handover protocol. Our PPO-based agent, implemented in the base stations, is highly adaptive to varying user equipment speeds and outperforms the 3GPP-standardized 5G NR handover procedure in terms of average data rate and radio link failure rate. Additionally, our simulation environment is carefully designed to ensure high accuracy, realistic user movements, and fair benchmarking against the 3GPP handover method."
2503.21623,"Reconfigurable Intelligent Surfaces (RIS) have gained significant attention for some time. Thanks to the possibility of individual steering of each reflecting element of the boards, they are envisaged to impact the propagation environment significantly. In this work, we concentrate on the practical verification of this concept. We present the results of detailed measurements of the reflection characteristics of the RIS boards, which have been conducted intentionally in the real environment. Various potential impacting factors have been considered (impact of azimuth and elevation angle, polarization, number of RIS boards, and distance). Achieved measurement results constituted the basis for conceptual analysis on the practical possibility of creating a codebook (consisting of RIS patterns - codewords) for some applications."
2503.21633,"Wireless sensing and the internet of things (IoT) are nowadays pervasive in 5G and beyond networks, and they are expected to play a crucial role in 6G. However, a centralized optimization of a distributed system is not always possible and cost-efficient. In this paper, we analyze a setting in which two sensors collaboratively update a common server seeking to minimize the age of information (AoI) of the latest sample of a common physical process. We consider a distributed and uncoordinated setting where each sensor lacks information about whether the other decides to update the server. This strategic setting is modeled through game theory (GT) and two games are defined: i) a static game of complete information with an incentive mechanism for cooperation, and ii) a repeated game over a finite horizon where the static game is played at each stage. We perform a mathematical analysis of the static game finding three Nash Equilibria (NEs) in pure strategies and one in mixed strategies. A numerical simulation of the repeated game is also presented and novel and valuable insight into the setting is given thanks to the definition of a new metric, the price of delayed updates (PoDU), which shows that the decentralized solution provides results close to the centralized optimum."
2503.21942,"In this study, we investigate the resource management challenges in next-generation mobile crowdsensing networks with the goal of minimizing task completion latency while ensuring coverage performance, i.e., an essential metric to ensure comprehensive data collection across the monitored area, yet it has been commonly overlooked in existing studies. To this end, we formulate a weighted latency and coverage gap minimization problem via jointly optimizing user selection, subchannel allocation, and sensing task allocation. The formulated minimization problem is a non-convex mixed-integer programming issue. To facilitate the analysis, we decompose the original optimization problem into two subproblems. One focuses on optimizing sensing task and subband allocation under fixed sensing user selection, which is optimally solved by the Hungarian algorithm via problem reformulation. Building upon these findings, we introduce a time-efficient two-sided swapping method to refine the scheduled user set and enhance system performance. Extensive numerical results demonstrate the effectiveness of our proposed approach compared to various benchmark strategies."
2503.22034,"The rise of Network Function Virtualization (NFV) has transformed network infrastructures by replacing fixed hardware with software-based Virtualized Network Functions (VNFs), enabling greater agility, scalability, and cost efficiency. Virtualization increases the distribution of system components and introduces stronger interdependencies. As a result, failures become harder to predict, monitor, and manage compared to traditional monolithic networks. Reliability, i.e. the ability of a system to perform regularly under specified conditions, and availability, i.e. the probability of a system of being ready to use, are critical requirements that must be guaranteed to maintain seamless network operations. Accurate modeling of these aspects is crucial for designing robust, fault-tolerant virtualized systems that can withstand service disruptions. This survey focuses on reliability and availability attributes of virtualized networks from a modeling perspective. After introducing the NFV architecture and basic definitions, we discuss the standardization efforts of the European Telecommunications Standards Institute (ETSI), which provides guidelines and recommendations through a series of standard documents focusing on reliability and availability. Next, we explore several formalisms proposed in the literature for characterizing reliability and availability, with a focus on their application to modeling the failure and repair behavior of virtualized networks through practical examples. Then, we overview numerous references demonstrating how different authors adopt specific methods to characterize reliability and/or availability of virtualized systems. Moreover, we present a selection of the most valuable software tools that support modeling of reliable virtualized networks. Finally, we discuss a set of open problems with the aim to encourage readers to explore further advances in this field."
2503.22095,"Only the chairs can edit The rapid growth of high-bandwidth applications in fifth-generation (5G) networks and beyond has driven a substantial increase in traffic within transport optical networks. While network slicing effectively addresses diverse quality of service (QoS) requirements-including bit rate, latency, and reliability-it also amplifies vulnerabilities to failures, particularly when a single disruption in the optical layer impacts multiple services within the 5G network. To address these challenges, we propose a Fast Disrupted Service Prioritization (FDSP) algorithm that strategically allocates resources to the most critical disrupted services. Specifically, FDSP employs a fast-solving integer linear programming (ILP) model to evaluate three key factors-service priority, bit rate, and remaining holding time-and integrates a physical-layer impairment (PLI)-aware routing and spectrum allocation approach. By leveraging this combined strategy, FDSP minimizes service disruption while optimizing resource utilization. Simulation results on Germany's network demonstrate that our approach significantly enhances the reliability and efficiency of survivable 5G slicing, thereby reducing blocking probability."
2503.22663,"Access to raw network traffic data is essential for many computer networking tasks, from traffic modeling to performance evaluation. Unfortunately, this data is scarce due to high collection costs and governance rules. Previous efforts explore this challenge by generating synthetic network data, but fail to reliably handle multi-flow sessions, struggle to reason about stateful communication in moderate to long-duration network sessions, and lack robust evaluations tied to real-world utility. We propose a new method based on state-space models called NetSSM that generates raw network traffic at the packet-level granularity. Our approach captures interactions between multiple, interleaved flows -- an objective unexplored in prior work -- and effectively reasons about flow-state in sessions to capture traffic characteristics. NetSSM accomplishes this by learning from and producing traces 8x and 78x longer than existing transformer-based approaches. Evaluation results show that our method generates high-fidelity traces that outperform prior efforts in existing benchmarks. We also find that NetSSM's traces have high semantic similarity to real network data regarding compliance with standard protocol requirements and flow and session-level traffic characteristics."
2503.22981,"Many extreme-scale applications require the movement of large quantities of data to, from, and among leadership computing facilities, as well as other scientific facilities and the home institutions of facility users. These applications, particularly when leadership computing facilities are involved, can touch upon edge cases (e.g., terabyte files) that had not been a focus of previous Globus optimization work, which had emphasized rather the movement of many smaller (megabyte to gigabyte) files. We report here on how automated client-driven chunking can be used to accelerate both the movement of large files and the integrity checking operations that have proven to be essential for large data transfers. We present detailed performance studies that provide insights into the benefits of these modifications in a range of file transfer scenarios."
2503.22982,"By provisioning inference offloading services, edge inference drives the rapid growth of AI applications at network edge. However, how to reduce the inference latency remains a significant challenge. To address this issue, we develop a parameter-sharing AI model loading (PartialLoading) framework for multi-user edge inference, which exploits two key insights: 1) the majority of latency arises from loading AI models into server GPU memory, and 2) different AI models can share a significant number of parameters, for which redundant loading should be avoided. Towards this end, we formulate a joint multi-user scheduling and spectrum bandwidth allocation problem to maximize task throughput by exploiting shared parameter blocks across models. The intuition is to judiciously schedule user requests to reuse the shared parameter blocks between consecutively loaded models, thereby reducing model loading time substantially. To facilitate solution finding, we decouple the problem into two sub-problems, i.e., user scheduling and bandwidth allocation, showing that solving them sequentially leads to the solution to the original problem. Due to the NP-hardness of the problem, we first study an important special case called the ""backbone-sharing"" case, and design a dynamic programming-based algorithm to obtain the optimal solution in polynomial time. For the general case, we propose a greedy heuristic to obtain the sub-optimal solution efficiently. Simulation results demonstrate that the proposed framework significantly improves task throughput under deadline constraints compared with user scheduling without exploiting parameter sharing."
2503.23,"As networks advance toward the Sixth Generation (6G), management of high-speed and ubiquitous connectivity poses major challenges in meeting diverse Service Level Agreements (SLAs). The Zero Touch Network (ZTN) framework has been proposed to automate and optimize network management tasks. It ensures SLAs are met effectively even during dynamic network conditions. Though, ZTN literature proposes closed-loop control, methods for implementing such a mechanism remain largely unexplored. This paper proposes a novel two-stage closedloop control for ZTN to optimize the network continuously. First, an XGBoosted Bidirectional Long Short Term Memory (BiLSTM) model is trained to predict the network state (in terms of bandwidth). In the second stage, the Q-learning algorithm selects actions based on the predicted network state to optimize Quality of Service (QoS) parameters. By selecting appropriate actions, it serves the applications perpetually within the available resource limits in a closed loop. Considering the scenario of network congestion, with available bandwidth as state and traffic shaping options as an action for mitigation, results show that the proposed closed-loop mechanism can adjust to changing network conditions. Simulation results show that the proposed mechanism achieves 95% accuracy in matching the actual network state by selecting the appropriate action based on the predicted state."
2503.23132,"With the rapid growth of the low-altitude economy, there is increasing demand for real-time data collection using UAV-assisted wireless sensor networks. This paper investigates the problem of minimizing the age of information (AoI) in UAV-assisted wireless sensor networks by optimizing the UAV flight routing. We formulate the AoI minimization task and propose a large language model (LLM)-assisted UAV routing algorithm (LAURA). LAURA employs an LLM as intelligent crossover operators within an evolutionary optimization framework to efficiently explore the solution space. Simulation results show that LAURA outperforms benchmark methods in reducing the maximum AoI, especially in scenarios with a large number of sensor nodes."
2503.2329,"Vehicle Twins (VTs) as digital representations of vehicles can provide users with immersive experiences in vehicular metaverse applications, e.g., Augmented Reality (AR) navigation and embodied intelligence. VT migration is an effective way that migrates the VT when the locations of physical entities keep changing to maintain seamless immersive VT services. However, an efficient VT migration is challenging due to the rapid movement of vehicles, dynamic workloads of Roadside Units (RSUs), and heterogeneous resources of the RSUs. To achieve efficient migration decisions and a minimum latency for the VT migration, we propose a multi-agent split Deep Reinforcement Learning (DRL) framework combined with spatio-temporal trajectory generation. In this framework, multiple split DRL agents utilize split architecture to efficiently determine VT migration decisions. Furthermore, we propose a spatio-temporal trajectory generation algorithm based on trajectory datasets and road network data to simulate vehicle trajectories, enhancing the generalization of the proposed scheme for managing VT migration in dynamic network environments. Finally, experimental results demonstrate that the proposed scheme not only enhances the Quality of Experience (QoE) by 29% but also reduces the computational parameter count by approximately 25% while maintaining similar performances, enhancing users' immersive experiences in vehicular metaverses."
2503.23366,"The Deficit Round Robin (DRR) scheduler is widely used in network systems for its simplicity and fairness. However, configuring its integer-valued parameters, known as quanta, to meet stringent delay constraints remains a significant challenge. This paper addresses this issue by demonstrating the convexity of the feasible parameter set for a two-flow DRR system under delay constraints. The analysis is then extended to n-flow systems, uncovering key structural properties that guide parameter selection. Additionally, we propose an optimization method to maximize the number of packets served in a round while satisfying delay constraints. The effectiveness of this approach is validated through numerical simulations, providing a practical framework for enhancing DRR scheduling. These findings offer valuable insights into resource allocation strategies for maintaining Quality of Service (QoS) standards in network slicing environments."
2503.23446,"The expansion of sixth-generation (6G) wireless networks into space introduces technical challenges that conventional bit-oriented communication approaches cannot efficiently address, including intermittent connectivity, severe latency, limited bandwidth, and constrained onboard resources. To overcome these limitations, semantic communication has emerged as a transformative paradigm, shifting the communication focus from transmitting raw data to delivering context-aware, missionrelevant information. In this article, we propose a semantic communication architecture explicitly tailored for the 6G Internet of Space (IoS), integrating multi-modal semantic processing, AIdriven semantic encoding and decoding, and adaptive transmission mechanisms optimized for space environments. The effectiveness of our proposed framework is demonstrated through a representative deep-space scenario involving semantic-based monitoring of Mars dust storms. Finally, we outline open research challenges and discuss future directions toward realizing practical semantic-enabled IoS systems."
2503.23669,"In critical situations such as natural disasters, network outages, battlefield communication, or large-scale public events, Unmanned Aerial Vehicles (UAVs) offer a promising approach to maximize wireless coverage for affected users in the shortest possible time. In this paper, we propose a novel framework where multiple UAVs are deployed with the objective to maximize the number of served user equipment (UEs) while ensuring a predefined data rate threshold. UEs are initially clustered using a K-means algorithm, and UAVs are optimally positioned based on the UEs' spatial distribution. To optimize power allocation and mitigate inter-cluster interference, we employ the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm, considering both LOS and NLOS fading. Simulation results demonstrate that our method significantly enhances UEs coverage and outperforms Deep Q-Network (DQN) and equal power distribution methods, improving their UE coverage by up to 2.07 times and 8.84 times, respectively."
2503.23823,"As edge computing gains prominence in Internet of Things (IoTs), smart cities, and autonomous systems, the demand for real-time machine intelligence with low latency and model reliability continues to grow. Federated Learning (FL) addresses these needs by enabling distributed model training without centralizing user data, yet it remains reliant on centralized servers and lacks built-in mechanisms for transparency and trust. Blockchain and Distributed Ledger Technologies (DLTs) can fill this gap by introducing immutability, decentralized coordination, and verifiability into FL workflows. This article presents current standardization efforts from 3GPP, ETSI, ITU-T, IEEE, and O-RAN that steer the integration of FL and blockchain in IoT ecosystems. We then propose a blockchain-based FL framework that replaces the centralized aggregator, incorporates reputation monitoring of IoT devices, and minimizes overhead via selective on-chain storage of model updates. We validate our approach with IOTA Tangle, demonstrating stable throughput and block confirmations, even under increasing FL workloads. Finally, we discuss architectural considerations and future directions for embedding trustworthy and resource-efficient FL in emerging 6G networks and vertical IoT applications. Our results underscore the potential of DLT-enhanced FL to meet stringent trust and energy requirements of next-generation IoT deployments."
2503.23889,"With the developments of the Internet of Vehicles (IoV) from 4G to 5G, vehicle-to-infrastructure (V2I) communications are becoming attractive for vehicle users (VUEs) to obtain diverse cloud service through base stations (BSs). To tackle V2I link deterioration caused by blockage and out-of-coverage cases, multi-hop V2X routing with both vehicle-to-vehicle (V2V) and V2I links needs to be investigated. However, traditional routing reacts to statistical or real-time information, which may suffer link degradation during path switchover in fast-changing vehicular networks. Predictive routing protocols take timely actions by forecasting link connectivity, but they fail to satisfy specific QoS requirements. Low robustness to link failures is also incurred without considering imperfect prediction. To build continual paths between VUEs and BSs for QoS provision of cloud service, a robust predictive routing framework (ROPE) is proposed with three major components: 1) an early warning scheme detects V2I link deterioration in advance via predicting vehicle mobility and link signal strength to facilitate seamless path switchover; 2) a virtual routing mechanism finds top3 paths that have the highest path strength and satisfy the connectivity and hop count constraints based on the prediction results to fulfill QoS requirements of cloud service; 3) a path verification protocol checks availability and quality of the top3 paths shortly before switchover and activates one qualified path for switchover to ensure routing robustness. We implement ROPE in a simulation framework incorporating real-world urban maps, microscopic traffic generation, geometry-based channel modeling, and offline data analysis as well as online inference. Extensive simulations demonstrate the superiority of ROPE over direct V2I communications and a connectivity-based predictive routing protocol under various scenarios."
2503.24081,"While cell-free massive MIMO (CF-mMIMO) offers both uniform and high network-wide throughput in static networks, its performance in a mobile network is not yet fully addressed. In this paper, we evaluate the performance of a mobile CF-mMIMO network under a comprehensive throughput model and show that it suffers from large performance degradation due to the combined effect of channel aging and handover delay. To improve the performance of CF-mMIMO under mobility, we propose a fairness-differentiated handover scheme. Our scheme differentiates the handover policy for different users by their channel conditions compared to a threshold based on Jain's fairness index, in order to prioritize handovers for the poorly served users. We present an extensive evaluation of the mobile throughput performance of our handover scheme with realistic urban network distributions and UE mobility patterns. Our results show that our scheme significantly outperforms the existing literature benchmarks when considering both channel aging and handover delay cost. Importantly, the advantage of UE-centric over network-centric CF-mMIMO, of uniformly good performance over the network, is uniquely preserved under mobility by our handover scheme. We thus show that CF-mMIMO can be a feasible architecture for practical mobile networks."
2503.24148,"Backscatter is a key technology for battery-free sensing in industrial IoT applications. To fully cover numerous tags in the deployment area, one often needs to deploy multiple readers, each of which communicates with tags within its communication range. However, the actual backscattered signals from a tag are likely to reach a reader outside its communication range and cause interference. Conventional TDMA or CSMA based approaches for interference avoidance separate readers' media access in time, leading to limited network throughput. In this paper, we propose TRIDENT, a novel backscatter design that enables interference avoidance via frequency-space division. By incorporating a tunable bandpass filter and multiple terminal loads, a TRIDENT tag can detect its channel condition and adaptively adjust the frequency and the power of its backscattered signals. We further propose a frequency assignment algorithm for the readers. With these designs, all the readers in the network can operate concurrently without being interfered. We implement TRIDENT and evaluate its performance under various settings. The results demonstrate that TRIDENT enhances the network throughput by 3.18x, compared to the TDMA-based scheme."
2503.24203,"Traffic engineering (TE) in large-scale computer networks has become a fundamental yet challenging problem, owing to the swift growth of global-scale cloud wide-area networks or backbone low-Earth-orbit satellite constellations. To address the scalability issue of traditional TE algorithms, learning-based approaches have been proposed, showing potential of significant efficiency improvement over state-of-the-art methods. Nevertheless, the intrinsic limitations of existing learning-based methods hinder their practical application: they are not generalizable across diverse topologies and network conditions, incur excessive training overhead, and do not respect link capacities by default.This paper proposes TELGEN, a novel TE algorithm that learns to solve TE problems efficiently in large-scale networks, while achieving superior generalizability across diverse network conditions. TELGEN is based on the novel idea of transforming the problem of ""predicting the optimal TE solution"" into ""predicting the optimal TE algorithm"", which enables TELGEN to learn and efficiently approximate the end-to-end solving process of classical optimal TE algorithms. The learned algorithm is agnostic to the exact network topology or traffic patterns, and can efficiently solve TE problems given arbitrary inputs and generalize well to unseen topologies and demands.We trained and evaluated TELGEN on random and real-world networks with up to 5000 nodes and 106 links. TELGEN achieved less than 3% optimality gap while ensuring feasibility in all cases, even when the test network had up to 20x more nodes than the largest in training. It also saved up to 84% solving time than classical optimal solver, and could reduce training time per epoch and solving time by 2-4 orders of magnitude than latest learning algorithms on the largest networks."
2503.24214,"We study an edge demand response problem where, based on historical edge workload demands, an edge provider needs to dispatch moving computing units, e.g. truck-carried modular data centers, in response to emerging hotspots within service area. The goal of edge provider is to maximize the expected revenue brought by serving congested users with satisfactory performance, while minimizing the costs of moving units and the potential service-level agreement violation penalty for interrupted services. The challenge is to make robust predictions for future demands, as well as optimized moving unit dispatching decisions. We propose a learning-based, uncertain-aware moving unit scheduling framework, URANUS, to address this problem. Our framework novelly combines Bayesian deep learning and distributionally robust approximation to make predictions that are robust to data, model and distributional uncertainties in deep learning-based prediction models. Based on the robust prediction outputs, we further propose an efficient planning algorithm to optimize moving unit scheduling in an online manner. Simulation experiments show that URANUS can significantly improve robustness in decision making, and achieve superior performance compared to state-of-the-art reinforcement learning, uncertainty-agnostic learning-based methods, and other baselines."
2503.24296,"We consider a decentralized wireless network with several source-destination pairs sharing a limited number of orthogonal frequency bands. Sources learn to adapt their transmissions (specifically, their band selection strategy) over time, in a decentralized manner, without sharing information with each other. Sources can only observe the outcome of their own transmissions (i.e., success or collision), having no prior knowledge of the network size or of the transmission strategy of other sources. The goal of each source is to maximize their own throughput while striving for network-wide fairness. We propose a novel fully decentralized Reinforcement Learning (RL)-based solution that achieves fairness without coordination. The proposed Fair Share RL (FSRL) solution combines: (i) state augmentation with a semi-adaptive time reference; (ii) an architecture that leverages risk control and time difference likelihood; and (iii) a fairness-driven reward structure. We evaluate FSRL in more than 50 network settings with different number of agents, different amounts of available spectrum, in the presence of jammers, and in an ad-hoc setting. Simulation results suggest that, when we compare FSRL with a common baseline RL algorithm from the literature, FSRL can be up to 89.0% fairer (as measured by Jain's fairness index) in stringent settings with several sources and a single frequency band, and 48.1% fairer on average."
2504.00417,"The integration of Open Radio Access Network (O-RAN) principles into 5G networks introduces a paradigm shift in how radio resources are managed and optimized. O-RAN's open architecture enables the deployment of intelligent applications (xApps) that can dynamically adapt to varying network conditions and user demands. In this paper, we present radio resource scheduling schemes -- a possible O-RAN-compliant xApp can be designed. This xApp facilitates the implementation of customized scheduling strategies, tailored to meet the diverse Quality-of-Service (QoS) requirements of emerging 5G use cases, such as enhanced mobile broadband (eMBB), massive machine-type communications (mMTC), and ultra-reliable low-latency communications (URLLC).We have tested the implemented scheduling schemes within an ns-3 simulation environment, integrated with the O-RAN framework. The evaluation includes the implementation of the Max-Throughput (MT) scheduling policy -- which prioritizes resource allocation based on optimal channel conditions, the Proportional-Fair (PF) scheduling policy -- which balances fairness with throughput, and compared with the default Round Robin (RR) scheduler. In addition, the implemented scheduling schemes support dynamic Time Division Duplex (TDD), allowing flexible configuration of Downlink (DL) and Uplink (UL) switching for bidirectional transmissions, ensuring efficient resource utilization across various scenarios. The results demonstrate resource allocation's effectiveness under MT and PF scheduling policies. To assess the efficiency of this resource allocation, we analyzed the Modulation Coding Scheme (MCS), the number of symbols, and Transmission Time Intervals (TTIs) allocated per user, and compared them with the throughput achieved. The analysis revealed a consistent relationship between these factors and the observed throughput."
2504.00489,"The Internet of Things (IoT) revolution demands scalable, energy-efficient communication protocols supporting widespread device deployments. The LoRa technology, coupled with the LoRaWAN protocol, has emerged as a leading Low Power Wide Area Network (LPWAN) solution, traditionally leveraging sub-GHz frequency bands for reliable long-range communication. However, these bands face constraints such as limited data rates and strict duty cycle regulations. Recent advancements have introduced the 2.4 GHz spectrum, offering superior data rates and unrestricted transmission opportunities at the cost of reduced coverage and severe interference. To solve this trade-off, this paper proposes a novel hybrid approach integrating multi-band (i.e., sub-GHz and 2.4 GHz) and multi-hop communication into LoRaWAN, while preserving compatibility with the existing standard. The proposed network architecture retains Gateways (GWs) and End Devices (EDs) operating within the sub-GHz frequency while introducing multi-band Relays (RLs) that act as forwarding nodes for 2.4 GHz EDs. Utilizing our previously developed open-source and standards-compliant simulation framework, we evaluate the network performance of our solution under realistic deployment scenarios. The results demonstrate substantial improvements compared to standard single-band and single-hop LoRaWAN networks, demonstrating the potential of this approach to redefine LPWAN capabilities and bridge the gap between current solutions and next-generation IoT applications."
2504.00555,"This paper presents a multi-contract blockchain framework for inter-provider agreements in 6G networks, emphasizing performance analysis under a realistic Proof-of-Stake (PoS) setting on Ethereum's Sepolia testnet. We begin by quantifying Ethereum Virtual Machine (EVM)-based gas usage for critical operations such as provider registration, service addition, and SLA penalty enforcement, observing that cold writes and deep data structures can each inflate gas consumption by up to 20\%. We then examine block-level dynamics when multiple transactions execute concurrently, revealing that moderate concurrency (e.g., 30--50 simultaneous transactions) can fill blocks to 80--90\% of their gas limit and nearly double finalization times from around 15~seconds to over 30~seconds. Finally, we synthesize these insights into a practical design guide, demonstrating that flattening nested mappings, consolidating storage writes, and selectively timing high-impact transactions can markedly reduce costs and latency spikes. Collectively, our findings underscore the importance of EVM-specific optimizations and transaction scheduling for large-scale decentralized applications in 6G telecom scenarios. The implementation is available online."
2504.00689,"Unmanned aerial vehicle (UAV) assisted communication is a revolutionary technology that has been recently presented as a potential candidate for beyond fifth-generation millimeter wave (mmWave) communications. Although mmWaves can offer a notably high data rate, their high penetration and propagation losses mean that line of sight (LoS) is necessary for effective communication. Due to the presence of obstacles and user mobility, UAV trajectory planning plays a crucial role in improving system performance. In this work, we propose a novel computational geometry-based trajectory planning scheme by considering the user mobility, the priority of the delay sensitive ultra-reliable low-latency communications (URLLC) and the high throughput requirements of the enhanced mobile broadband (eMBB) traffic. Specifically, we use geometric tools like Apollonius circle and minimum enclosing ball of balls to find the optimal position of the UAV that supports uninterrupted connections to the URLLC users and maximizes the aggregate throughput of the eMBB users. Finally, the numerical results demonstrate the benefits of the suggested approach over an existing state of the art benchmark scheme in terms of sum throughput obtained by URLLC and eMBB users."
2504.00912,"The digital transformation driven by Industry 4.0 relies on networks that support diverse traffic types with strict deterministic end-to-end latency and mobility requirements. To meet these requirements, future industrial automation networks will use time-sensitive networking, integrating 5G as wireless access points to connect production lines with time-sensitive networking bridges and the enterprise edge cloud. However, achieving deterministic end-to-end latency remains a challenge, particularly due to the variable packet transmission delay introduced by the 5G system. While time-sensitive networking bridges typically operate with latencies in the range of hundreds of microseconds, 5G systems may experience delays ranging from a few to several hundred milliseconds. This paper investigates the potential of configuring the 5G time division duplex pattern to minimize packet transmission delay in industrial environments. Through empirical measurements using a commercial 5G system, we evaluate different TDD configurations under varying traffic loads, packet sizes and full buffer status report activation. Based on our findings, we provide practical configuration recommendations for satisfying requirements in industrial automation, helping private network providers increase the adoption of 5G."
2504.01104,"The effective management of large amounts of data processed or required by today's cloud or edge computing systems remains a fundamental challenge. This paper focuses on cache management for applications where data objects can be stored in layered representations. In such representations, each additional data layer enhances the ""quality"" of the object's version but comes with an incremental cost of memory space. This layered approach proves beneficial in various scenarios, including the delivery of zoomable maps, video coding, future Virtual Reality gaming, and layered neural network models where additional data layers improve inference accuracy. In systems where users or devices demand different versions of a data object, layered representations offer flexibility for caching policies to achieve improved hit rates.In this paper, we explore the performance of various traditionally studied caching policies, such as Belady, LRU, and LFU, both with and without layering. To this end, we develop an asymptotically accurate analytical model for Layered LRU (LLRU). We study how the performance of LLRU is impacted by factors such as the number of layers, the popularity of different objects and layers, and overheads associated with storing layered representations. For instance, we show that, for LLRU, more layers are not always beneficial and indeed performance depends in subtle ways on the popularity and size profiles of layers."
2504.01374,"The structure of IP addresses observed in Internet traffic plays a critical role for a wide range of networking problems of current interest. For example, modern network telemetry systems that take advantage of existing data plane technologies for line rate traffic monitoring and processing cannot afford to waste precious data plane resources on traffic that comes from ""uninteresting"" regions of the IP address space. However, there is currently no well-established structural model or analysis toolbox that enables a first-principles approach to the specific problem of identifying ""uninteresting"" regions of the address space or the myriad of other networking problems that prominently feature IP addresses.To address this key missing piece, we present in this paper a first-of-its-kind empirically validated physical explanation for why the observed IP address structure in measured Internet traffic is multifractal in nature. Our root cause analysis overcomes key limitations of mostly forgotten findings from ~20 years ago and demonstrates that the Internet processes and mechanisms responsible for how IP addresses are allocated, assigned, and used in today's Internet are consistent with and well modeled by a class of evocative mathematical models called conservative cascades. We complement this root cause analysis with the development of an improved toolbox that is tailor-made for analyzing finite and discrete sets of IP addresses and includes statistical estimators that engender high confidence in the inferences they produce. We illustrate the use of this toolbox in the context of a novel address structure anomaly detection method we designed and conclude with a discussion of a range of challenging open networking problems that are motivated or inspired by our findings."
2504.01414,"Selecting the optimal radio access technology (RAT) during vertical handovers (VHO) in heterogeneous wireless networks (HWNs) is critical. Multi-attribute decision-making (MADM) is the most common approach used for network selection (NS) in HWNs. However, existing MADM-NS methods face two major challenges: the rank reversal problem (RRP), where the relative ranking of alternatives changes unexpectedly, and inefficient handling of user and/or service requirements. These limitations result in suboptimal RAT selection and diminished quality of service, which becomes particularly critical for time-sensitive applications. To address these issues, we introduce in this work a novel weighting assignment technique called BWM-GWO, which integrates the Best-Worst Method (BWM) with the Grey Wolf Optimization (GWO) algorithm through a convex linear combination. The proposed framework achieves a balanced decision-making process by using BWM to compute subjective weights that capture user/service preferences, while employing GWO to derive objective weights aimed at minimizing RRP. The development and validation of this framework establish a digital model for NS in HWNs, marking the initial step toward realizing a digital twin (DT). Experimental results show that integrating the proposed BWM-GWO technique with MADM-NS reduces RRP occurrence by up to 71.3% while significantly improving user and service satisfaction compared to benchmark approaches."
2504.01422,"In the Offline Finding Network(OFN), offline Bluetooth tags broadcast to the surrounding area, the finder devices receiving the broadcast signal and upload location information to the IoT(Internet of Things) cloud servers, thereby achieving offline finding of lost items. This process is essentially a Bluetooth low energy (BLE) neighbor discovery process(NDP). In the process, the variety of Bluetooth scan modes caused by the scan interval and scan window settings affects the discovery latency of finder devices finding the tag broadcast packets. To optimize the experience of searching for lost devices, we propose the CPBIS-mechanism, a certain proportion broadcast-intervals screening mechanism that calculates the most suitable two broadcast intervals and their proportion for offline tags. This reduces discovery latency in the BLE NDP, improves the discovery success rate, further enhances the user experience. To our knowledge, we are the first to propose a comprehensive solution for configuring the broadcast interval parameters of advertisers in BLE NDP, particularly for configurations involving two or more broadcast intervals. We evaluated the results obtained by CPBIS on the nRF52832 chip. The data shows that the CPBIS-mechanism achieves relatively low discovery latencies for multiple scan modes."
2504.0173,"In this paper, we propose a deep incremental framework for efficient RAN management, introducing the Multi-Service-Modal UE (MSMU) system, which enables a single UE to handle eMBB and uRLLC services simultaneously. We formulate an optimization problem integrating traffic demand prediction, route optimization, RAN slicing, service identification, and radio resource management under uncertainty. We decompose it into long-term (L-SP) and short-term (S-SP) subproblems then propose a Transformer model for L-SP optimization, predicting eMBB and uRLLC traffic demands and optimizing routes for RAN slicing. To address non-stationary network traffic with evolving trends and scale variations, we integrate reversible instance normalization (ReVIN) into the forecasting pipeline. For the S-SP, we propose an LSTM model enabling real-time service type identification and resource management, utilizing L-SP predictions. We incorporate continual learning into the S-SP framework to adapt to new service types while preserving prior knowledge. Experimental results demonstrate that our proposed framework achieves up to 46.86% reduction in traffic demand prediction error, 26.70% and 18.79% improvement in PRBs and power estimation, 7.23% higher route selection accuracy, and 7.29% improvement in service identification over the baselines with 95% average accuracy in continual service identification across seven sequential tasks."
2504.01946,"Time-Sensitive Networking enhances Ethernet-based In-Vehicle Networks (IVNs) with real-time capabilities. Different traffic shaping algorithms have been proposed for time-critical communication, of which the Asynchronous Traffic Shaper (ATS) is an upcoming candidate. However, recent research has shown that ATS can introduce unbounded latencies when shaping traffic from non-FIFO systems. This impacts the applicability of ATS in IVNs, as these networks often use redundancy mechanisms, i.e. Frame Replication and Elimination for Reliability (FRER), that can cause non-FIFO behavior. In this paper, we approach the problem of accumulated delays from ATS by analyzing the scenarios that generate latency and by devising placement and configuration methods for ATS schedulers to prevent this behavior. We evaluate our approach in a simulation environment and show how it prevents conditions of unbounded delays. In an IVN simulation case study, we demonstrate the occurrence of unbounded latencies in a realistic scenario and validate the effectiveness of our solutions in avoiding them."
2504.02174,"Network traffic classification is of great importance for network operators in their daily routines, such as analyzing the usage patterns of multimedia applications and optimizing network configurations. Internet service providers (ISPs) that operate high-speed links expect network flow classifiers to accurately classify flows early, using the minimal number of necessary initial packets per flow. These classifiers must also be robust to packet sequence disorders in candidate flows and capable of detecting unseen flow types that are not within the existing classification scope, which are not well achieved by existing methods. In this paper, we develop FastFlow, a time-series flow classification method that accurately classifies network flows as one of the known types or the unknown type, which dynamically selects the minimal number of packets to balance accuracy and efficiency. Toward the objectives, we first develop a flow representation process that converts packet streams at both per-packet and per-slot granularity for precise packet statistics with robustness to packet sequence disorders. Second, we develop a sequential decision-based classification model that leverages LSTM architecture trained with reinforcement learning. Our model makes dynamic decisions on the minimal number of time-series data points per flow for the confident classification as one of the known flow types or an unknown one. We evaluated our method on public datasets and demonstrated its superior performance in early and accurate flow classification. Deployment insights on the classification of over 22.9 million flows across seven application types and 33 content providers in a campus network over one week are discussed, showing that FastFlow requires an average of only 8.37 packets and 0.5 seconds to classify the application type of a flow with over 91% accuracy and over 96% accuracy for the content providers."
2504.02406,"Artificial Intelligence (AI) is expected to play a key role in 6G networks including optimising system management, operation, and evolution. This requires systematic lifecycle management of AI models, ensuring their impact on services and stakeholders is continuously monitored. While current 6G initiatives introduce AI, they often fall short in addressing end-to-end intelligence and crucial aspects like trust, transparency, privacy, and verifiability. Trustworthy AI is vital, especially for critical infrastructures like 6G. This paper introduces the REASON approach for holistically addressing AI's native integration and trustworthiness in future 6G networks. The approach comprises AI Orchestration (AIO) for model lifecycle management, Cognition (COG) for performance evaluation and explanation, and AI Monitoring (AIM) for tracking and feedback. Digital Twin (DT) technology is leveraged to facilitate real-time monitoring and scenario testing, which are essential for AIO, COG, and AIM. We demonstrate this approach through an AI-enabled xAPP use case, leveraging a DT platform to validate, explain, and deploy trustworthy AI models."
2504.02561,"This paper presents a new framework for integrating Digital Twins (DTs) within Internet of battlespace Things (IoBT) coalitions. We introduce a novel three-tier architecture that enables efficient coordination and management of DT models across coalition partners while addressing key challenges in interoperability, security, and resource allocation. The architecture comprises specialized controllers at each tier: Digital Twin Coalition Partner (DTCP) controllers managing individual coalition partners' DT resources, a central Digital Twin Coalition(DTC) controller orchestrating cross-partner coordination, and Digital Twin Coalition Mission (DTCP) controllers handling mission-specific DT interactions. We propose a hybrid approach for DT model placement across edge devices, tactical nodes, and cloud infrastructure, optimizing performance while maintaining security and accessibility. The architecture leverages software-defined networking principles for dynamic resource allocation and slice management, enabling efficient sharing of computational and network resources between DT operations and primary IoBT functions. Our proposed framework aims to provide a robust foundation for deploying and managing Digital Twins in coalition warfare, enhancing situational awareness, decision-making capabilities, and operational effectiveness while ensuring secure and interoperable operations across diverse coalition partners."
2504.02637,"Medium access in 5G systems was tailored to accommodate diverse traffic classes through network resource slicing. 6G wireless systems are expected to be significantly reliant on Artificial Intelligence (AI), leading to data-driven and goal-oriented communication. This leads to augmentation of the design space for Medium Access Control (MAC) protocols, which is the focus of this article. We introduce a taxonomy based on push-based and pull-based communication, which is useful to categorize both the legacy and the AI-driven access schemes. We provide MAC protocol design guidelines for pull- and push-based communication in terms of goal-oriented criteria, such as timing and data relevance. We articulate a framework for co-existence between pull and push-based communications in 6G systems, combining their advantages. We highlight the design principles and main tradeoffs, as well as the architectural considerations for integrating these designs in Open-Radio Access Network (O-RAN) and 6G systems."
2504.02688,"Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings. Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks. However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands. In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \textit{flight time, handover, connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR. We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment. The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms."
2504.02722,"The Physical Internet (PI) envisions an interconnected, modular, and dynamically managed logistics system inspired by the Digital Internet. It enables open-access networks where shipments traverse a hyperconnected system of hubs, adjusting routes based on real-time conditions. A key challenge in scalable and adaptive freight movement is routing determining how shipments navigate the network to balance service levels, consolidation, and adaptability. This paper introduces directional routing, a dynamic approach that flexibly adjusts shipment paths, optimizing efficiency and consolidation using real-time logistics data. Unlike shortest-path routing, which follows fixed routes, directional routing dynamically selects feasible next-hop hubs based on network conditions, consolidation opportunities, and service level constraints. It consists of two phases: area discovery, which identifies candidate hubs, and node selection, which determines the next hub based on real-time parameters. This paper advances the area discovery phase by introducing a Reduced Search Space Breadth-First Search (RSS-BFS) method to systematically identify feasible routing areas while balancing service levels and consolidation. The proposed approach enhances network fluidity, scalability, and adaptability in PI-based logistics, advancing autonomous and sustainable freight movement."
2504.03178,"Random access networks have long been observed to suffer from low throughput if nodes' access strategy is not properly designed. To improve the throughput performance, learning-based approaches, with which each node learns from the observations and experience to determine its own access strategy, have shown immense potential, but are often designed empirically due to the lack of theoretical guidance. As we will demonstrate in this paper, the queueing-theoretical analysis can be leveraged as a powerful tool for optimal design of learning-based access. Specifically, based on a Multi-Armed-Bandit (MAB) framework, two random access schemes, MTOA-L with local rewards and MTOA-G with global rewards, are proposed for throughput optimization. Though both can achieve the maximum throughput of 1, they have different short-term fairness performance. Through identifying the access strategies learned via MTOA-L and MTOA-G and feeding them into the proposed unified queueing-theoretical framework, the throughput-fairness tradeoff of each is characterized and optimized by properly tuning the key parameters. The comparison of the optimal tradeoffs shows that MTOA-G is much superior to MTOA-L especially when the number of nodes is large."
2504.03244,"In this paper, we propose the Adaptive Movement Sampling Physics-Informed Residual Network (AM-PIRN) to address challenges in solving nonlinear option pricing PDE models, where solutions often exhibit significant curvature or shock waves over time. The AM-PIRN architecture is designed to concurrently minimize PDE residuals and achieve high-fidelity option price approximations by dynamically redistributing training points based on evolving PDE residuals, while maintaining a fixed total number of points. To enhance stability and training efficiency, we integrate a ResNet backbone, replacing conventional fully connected neural networks used in Physics-Informed Neural Networks (PINNs). Numerical experiments across nonlinear option pricing models demonstrate that AM-PIRN outperforms PINN, RAM-PINN, and WAM-PINN in both resolving PDE constraints and accurately estimating option prices. The method's advantages are particularly pronounced in complex or multi-dimensional models, where its adaptive sampling and robust architecture effectively mitigate challenges posed by sharp gradients and high nonlinearity."
2504.03355,"Backscatter is an enabling technology for battery-free sensing in today's Artificial Intelligence of Things (AIOT). Building a backscatter-based sensing system, however, is a daunting task, due to two obstacles: the unaffordable power consumption of the microprocessor and the coexistence with the ambient carrier's traffic. In order to address the above issues, in this paper, we present Leggiero, the first-of-its-kind analog WiFi backscatter with payload transparency. Leveraging a specially designed circuit with a varactor diode, this design avoids using a microprocessor to interface between the radio and the sensor, and directly converts the analog sensor signal into the phase of RF (radio frequency) signal. By carefully designing the reference circuit on the tag and precisely locating the extra long training field (LTF) section of a WiFi packet, Leggiero embeds the analog phase value into the channel state information (CSI). A commodity WiFi receiver without hardware modification can simultaneously decode the WiFi and the sensor data. We implement Leggiero design and evaluate its performance under varied settings. The results show that the power consumption of the Leggiero tag (excluding the power of the peripheral sensor module) is 30uW at a sampling rate of 400Hz, which is 4.8* and 4* lower than the state-of-the-art WiFi backscatter schemes. The uplink throughput of Leggiero is suficient to support a variety of sensing applications, while keeping the WiFi carrier's throughput performance unaffected."
2504.03412,"Parallelizing passive Radio Frequency Identification (RFID) reading is an arguably crucial, yet unsolved challenge in modern IoT applications. Existing approaches remain limited to time-division operations and fail to read multiple tags simultaneously. In this paper, we introduce QuinID, the first frequency-division multiple access (FDMA) RFID system to achieve fully parallel reading. We innovatively exploit the frequency selectivity of the tag antenna rather than a conventional digital FDMA, bypassing the power and circuitry constraint of RFID tags. Specifically, we delicately design the frequency-selective antenna based on surface acoustic wave (SAW) components to achieve extreme narrow-band response, so that QuinID tags (i.e., QuinTags) operate exclusively within their designated frequency bands. By carefully designing the matching network and canceling various interference, a customized QuinReader communicates simultaneously with multiple QuinTags across distinct bands. QuinID maintains high compatibility with commercial RFID systems and presents a tag cost of less than 10 cents. We implement a 5-band QuinID system and evaluate its performance under various settings. The results demonstrate a fivefold increase in read rate, reaching up to 5000 reads per second."
2504.03499,"AI/ML-based tools are at the forefront of resource management solutions for communication networks. Deep learning, in particular, is highly effective in facilitating fast and high-performing decision-making whenever representative training data is available to build offline accurate models. Conversely, online learning solutions do not require training and enable adaptive decisions based on runtime observations, alas are often overly conservative. This extensive tutorial proposes the use of optimistic learning (OpL) as a decision engine for resource management frameworks in modern communication systems. When properly designed, such solutions can achieve fast and high-performing decisions -- comparable to offline-trained models -- while preserving the robustness and performance guarantees of the respective online learning approaches. We introduce the fundamental concepts, algorithms and results of OpL, discuss the roots of this theory and present different approaches to defining and achieving optimism. We proceed to showcase how OpL can enhance resource management in communication networks for several key problems such as caching, edge computing, network slicing, and workload assignment in decentralized O-RAN platforms. Finally, we discuss the open challenges that must be addressed to unlock the full potential of this new resource management approach."
2504.03586,"This paper introduces CAMINO, a Cloud-native Autonomous Management and Intent-based Orchestrator designed to address the challenges of scalable, declarative, and cloud-native service management and orchestration. CAMINO leverages a modular architecture, the Configuration-as-Data (CaD) paradigm, and real-time resource monitoring to facilitate zero-touch provisioning across multi-edge infrastructure. By incorporating intent-driven orchestration and observability capabilities, CAMINO enables automated lifecycle management of network functions, ensuring optimized resource utilisation. The proposed solution abstracts complex configurations into high-level intents, offering a scalable approach to orchestrating services in distributed cloud-native infrastructures. This paper details CAMINO's system architecture, implementation, and key benefits, highlighting its effectiveness in cloud-native telecommunications environments."
2504.03686,"One of the key missions of sixth-generation (6G) mobile networks is to deploy large-scale artificial intelligence (AI) models at the network edge to provide remote-inference services for edge devices. The resultant platform, known as edge inference, will support a wide range of Internet-of-Things applications, such as autonomous driving, industrial automation, and augmented reality. Given the mission-critical and time-sensitive nature of these tasks, it is essential to design edge inference systems that are both reliable and capable of meeting stringent end-to-end (E2E) latency constraints. Existing studies, which primarily focus on communication reliability as characterized by channel outage probability, may fail to guarantee E2E performance, specifically in terms of E2E inference accuracy and latency. To address this limitation, we propose a theoretical framework that introduces and mathematically characterizes the inference outage (InfOut) probability, which quantifies the likelihood that the E2E inference accuracy falls below a target threshold. Under an E2E latency constraint, this framework establishes a fundamental tradeoff between communication overhead (i.e., uploading more sensor observations) and inference reliability as quantified by the InfOut probability. To find a tractable way to optimize this tradeoff, we derive accurate surrogate functions for InfOut probability by applying a Gaussian approximation to the distribution of the received discriminant gain. Experimental results demonstrate the superiority of the proposed design over conventional communication-centric approaches in terms of E2E inference reliability."
2504.0369,"We consider multiple transmitters aiming to communicate their source signals (e.g., images) over a multiple access channel (MAC). Conventional communication systems minimize interference by orthogonally allocating resources (time and/or bandwidth) among users, which limits their capacity. We introduce a machine learning (ML)-aided wireless image transmission method that merges compression and channel coding using a multi-view autoencoder, which allows the transmitters to use all the available channel resources simultaneously, resulting in a non-orthogonal multiple access (NOMA) scheme. The receiver must recover all the images from the received superposed signal, while also associating each image with its transmitter. Traditional ML models deal with individual samples, whereas our model allows signals from different users to interfere in order to leverage gains from NOMA under limited bandwidth and power constraints. We introduce a progressive fine-tuning algorithm that doubles the number of users at each iteration, maintaining initial performance with orthogonalized user-specific projections, which is then improved through fine-tuning steps. Remarkably, our method scales up to 16 users and beyond, with only a 0.6% increase in the number of trainable parameters compared to a single-user model, significantly enhancing recovered image quality and outperforming existing NOMA-based methods over a wide range of datasets, metrics, and channel conditions. Our approach paves the way for more efficient and robust multi-user communication systems, leveraging innovative ML components and strategies."
2504.03704,"This paper presents the integration of OPC UA as a communication protocol in a wireless sensor network and the associated companion specifications as a semantic template for an information model. The Cyber Physical Finite Element Sensor Network (CPFEN ) for Shape Measurements, a distributed wireless system, uses IO-Link Wireless for data transmission at the sensor level, OPC UA provides a unified interface for data access, configuration, monitoring, and calibration tailored to the needs of the CPFEN for all level above. This opens up additional possibilities, such as integrated quality assurance or creating a digital twin, while improving scalability."
2504.03708,"Latency remains a critical bottleneck for deploying foundational artificial intelligence (AI) models, such as large language models (LLMs), in customer-facing, real-time applications. While cloud-based inference offers scalability, it frequently introduces delays unacceptable for interactive experiences, such as semantic search, personalized recommendations, or conversational interfaces. Telecommunications operators, historically adept at solving content latency challenges through partnerships with providers like Google and Facebook, now have a unique opportunity to address similar AI latency concerns. This paper presents a technical framework leveraging Telco infrastructure-spanning regional data centers, existing content delivery network (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical ""AI edges"" for caching and partial inference. We explore the architectural feasibility of embedding semantic and vector-based AI inference caches within existing Telco assets, proposing tiered caching strategies and split-inference architectures that significantly reduce latency and compute costs. Additionally, we address technical challenges specific to Telcos, such as cache synchronization, model distribution, privacy, and hardware acceleration considerations. Finally, we discuss viable partnership models between telcos and AI providers, highlighting how this innovative use of telco infrastructure can unlock both improved AI user experience and new revenue streams."
2504.03721,"In the forthcoming 6G era, extend reality (XR) has been regarded as an emerging application for ultra-reliable and low latency communications (URLLC) with new traffic characteristics and more stringent requirements. In addition to the quasi-periodical traffic in XR, burst traffic with both large frame size and random arrivals in some real world low latency communication scenarios has become the leading cause of network congestion or even collapse, and there still lacks an efficient algorithm for the resource scheduling problem under burst traffic with hard latency constraints. We propose a novel hybrid reinforcement learning framework for resource scheduling with hard latency constraints (HRL-RSHLC), which reuses polices from both old policies learned under other similar environments and domain-knowledge-based (DK) policies constructed using expert knowledge to improve the performance. The joint optimization of the policy reuse probabilities and new policy is formulated as an Markov Decision Problem (MDP), which maximizes the hard-latency constrained effective throughput (HLC-ET) of users. We prove that the proposed HRL-RSHLC can converge to KKT points with an arbitrary initial point. Simulations show that HRL-RSHLC can achieve superior performance with faster convergence speed compared to baseline algorithms."
2504.03747,"We present strategies for placing a swarm of mobile relays to provide a bi-directional wireless network that connects fixed (immobile) terminals. Neither terminals nor relays are permitted to transmit into disk-shaped no-transmission zones. We assume a planar environment and that each transmission area is a disk centered at the transmitter. We seek a strongly connected network between all terminals with minimal total cost, where the cost is the sum area of the transmission disks. Results for networks with increasing levels of complexity are provided. The solutions for local networks containing low numbers of relays and terminals are applied to larger networks. For more complex networks, algorithms for a minimum-spanning tree (MST) based procedure are implemented to reduce the solution cost. A procedure to characterize and determine the possible homotopies of a system of terminals and obstacles is described, and used to initialize the evolution of the network under the presented algorithms."
2504.04008,"This paper presents a system for session-level traffic classification on endpoint devices, developed using a Hardware-aware Neural Architecture Search (HW-NAS) framework. HW-NAS optimizes Convolutional Neural Network (CNN) architectures by integrating hardware constraints, ensuring efficient deployment on resource-constrained devices. Tested on the ISCX VPN-nonVPN dataset, the method achieves 97.06% accuracy while reducing parameters by over 200 times and FLOPs by nearly 4 times compared to leading models. The proposed model requires up to 15.5 times less RAM and 26.4 times fewer FLOPs than the most hardware-demanding models. This system enhances compatibility across network architectures and ensures efficient deployment on diverse hardware, making it suitable for applications like firewall policy enforcement and traffic monitoring."
2504.04027,"Rapid growth of data center networks (DCNs) poses significant challenges for large-scale traffic engineering (TE). Existing acceleration strategies, which rely on commercial solvers or deep learning, face scalability issues and struggle with degrading performance or long computational time. Unlike existing algorithms adopting parallel strategies, we propose Sequential Source-Destination Optimization (SSDO), a sequential solver-free algorithm for TE. SSDO decomposes the problem into subproblems, each focused on adjusting the split ratios for a specific source-destination (SD) demand while keeping others fixed. To enhance the efficiency of subproblem optimization, we design a Balanced Binary Search Method (BBSM), which identifies the most balanced split ratios among multiple solutions that minimize Maximum Link Utilization (MLU). SSDO dynamically updates the sequence of SDs based on real-time utilization, which accelerates convergence and enhances solution quality. We evaluate SSDO on Meta DCNs and two wide-area networks. In a Meta topology, SSDO achieves a 65\% and 60\% reduction in normalized MLU compared to TEAL and POP, two state-of-the-art TE acceleration methods, while delivering a $12\times$ speedup over POP. These results demonstrate the superior performance of SSDO in large-scale TE."
2504.04404,"Modern applications increasingly demand ultra-low latency for data processing, often facilitated by host-controlled accelerators like GPUs and FPGAs. However, significant delays result from host involvement in accessing accelerators. To address this limitation, we introduce a novel paradigm we call Offloading through Remote Accelerator Calls (OffRAC), which elevates accelerators to first-class compute resources. OffRAC enables direct calls to FPGA-based accelerators without host involvement. Utilizing the stateless function abstraction of serverless computing, with applications decomposed into simpler stateless functions, offloading promotes efficient acceleration and distribution of computational loads across the network. To realize this proposal, we present a prototype design and implementation of an OffRAC platform for FPGAs that assembles diverse requests from multiple clients into complete accelerator calls with multi-tenancy performance isolation. This design minimizes the implementation complexity for accelerator users while ensuring isolation and programmability. Results show that the OffRAC approach reduces the latency of network calls to accelerators down to approximately 10.5 us, as well as sustaining high application throughput up to 85Gbps, demonstrating scalability and efficiency, making it compelling for the next generation of low-latency applications."
2504.04438,"The continuous expansion of network data presents a pressing challenge for conventional routing algorithms. As the demand escalates, these algorithms are struggling to cope. In this context, reinforcement learning (RL) and multi-agent reinforcement learning (MARL) algorithms emerge as promising solutions. However, the urgency and importance of the problem are clear, as existing RL/MARL-based routing approaches lack effective communication in run time among routers, making it challenging for individual routers to adapt to complex and dynamic changing networks. More importantly, they lack the ability to deal with dynamically changing network topology, especially the addition of the router, due to the non-scalability of their neural networks. This paper proposes a novel dynamic routing algorithm, DRAMA, incorporating emergent communication in multi-agent reinforcement learning. Through emergent communication, routers could learn how to communicate effectively to maximize the optimization objectives. Meanwhile, a new Q-network and graph-based emergent communication are introduced to dynamically adapt to the changing network topology without retraining while ensuring robust performance. Experimental results showcase DRAMA's superior performance over the traditional routing algorithm and other RL/MARL-based algorithms, achieving a higher delivery rate and lower latency in diverse network scenarios, including dynamic network load and topology. Moreover, an ablation experiment validates the prospect of emergent communication in facilitating packet routing."
2504.04586,"Low Earth Orbit (LEO) satellite communication presents a promising solution for delivering Internet access to users in remote regions. Given that video content is expected to dominate network traffic in LEO satellite systems, this study presents a new video-aware mobility management framework specifically designed for such networks. By combining simulation models with real-world datasets, we highlight the critical role of handoff strategies and throughput prediction algorithms in both single-user and multi-user video streaming scenarios. Building on these insights, we introduce a suite of innovative algorithms that jointly determine satellite selection and video bitrate to enhance users' quality of experience (QoE). Initially, we design model predictive control (MPC) and reinforcement learning (RL) based methods for individual users, then extend the approach to manage multiple users sharing a satellite. Notably, we incorporate centralized training with distributed inference in our RL design to develop distributed policies informed by a global view. The effectiveness of our approach is validated through trace-driven simulations and testbed experiments."
2504.04678,"Federated Learning (FL) deployments using IoT devices is an area that is poised to significantly benefit from advances in NextG wireless. In this paper, we deploy a FL application using a 5G-NR Standalone (SA) testbed with open-source and Commercial Off-the-Shelf (COTS) components. The 5G testbed architecture consists of a network of resource-constrained edge devices, namely Raspberry Pi's, and a central server equipped with a Software Defined Radio (SDR) and running O-RAN software. Our testbed allows edge devices to communicate with the server using WiFi and Ethernet, instead of 5G. FL is deployed using the Flower FL framework, for which we developed a comprehensive instrumentation tool to collect and analyze diverse communications and machine learning performance metrics including: model aggregation time, downlink transmission time, training time, and uplink transmission time. Leveraging these measurements, we perform a comparative analysis of the FL application across three network interfaces: 5G, WiFi, and Ethernet. Our experimental results suggest that, on 5G, the uplink model transfer time is a significant factor in convergence time of FL. In particular, we find that the 5G uplink contributes to roughly 23% of the duration of one average communication round when using all edge devices in our testbed. When comparing the uplink time of the 5G testbed, we find that it is 33.3x higher than Ethernet and 17.8x higher than WiFi. Our results also suggest that 5G exacerbates the well-known straggler effect. For reproducibility, we have open-sourced our FL application, instrumentation tools, and testbed configuration."
2504.05187,"Beamforming is a key technology in millimeter-wave (mmWave) communications that improves signal transmission by optimizing directionality and intensity. However, conventional channel estimation methods, such as pilot signals or beam sweeping, often fail to adapt to rapidly changing communication environments. To address this limitation, multimodal sensing-aided beam prediction has gained significant attention, using various sensing data from devices such as LiDAR, radar, GPS, and RGB images to predict user locations or network conditions. Despite its promising potential, the adoption of multimodal sensing-aided beam prediction is hindered by high computational complexity, high costs, and limited datasets. Thus, in this paper, a resource-efficient learning approach is proposed to transfer knowledge from a multimodal network to a monomodal (radar-only) network based on cross-modal relational knowledge distillation (CRKD), while reducing computational overhead and preserving predictive accuracy. To enable multimodal learning with realistic data, a novel multimodal simulation framework is developed while integrating sensor data generated from the autonomous driving simulator CARLA with MATLAB-based mmWave channel modeling, and reflecting real-world conditions. The proposed CRKD achieves its objective by distilling relational information across different feature spaces, which enhances beam prediction performance without relying on expensive sensor data. Simulation results demonstrate that CRKD efficiently distills multimodal knowledge, allowing a radar-only model to achieve $94.62\%$ of the teacher performance. In particular, this is achieved with just $10\%$ of the teacher network's parameters, thereby significantly reducing computational complexity and dependence on multimodal sensor data."
2504.05222,"The rapid evolution towards the sixth-generation (6G) networks demands advanced beamforming techniques to address challenges in dynamic, high-mobility scenarios, such as vehicular communications. Vision-based beam prediction utilizing RGB camera images emerges as a promising solution for accurate and responsive beam selection. However, reliance on visual data introduces unique vulnerabilities, particularly susceptibility to adversarial attacks, thus potentially compromising beam accuracy and overall network reliability. In this paper, we conduct the first systematic exploration of adversarial threats specifically targeting vision-based mmWave beam selection systems. Traditional white-box attacks are impractical in this context because ground-truth beam indices are inaccessible and spatial dynamics are complex. To address this, we propose a novel black-box adversarial attack strategy, termed Spatial Proxy Attack (SPA), which leverages spatial correlations between user positions and beam indices to craft effective perturbations without requiring access to model parameters or labels. To counteract these adversarial vulnerabilities, we formulate an optimization framework aimed at simultaneously enhancing beam selection accuracy under clean conditions and robustness against adversarial perturbations. We introduce a hybrid deep learning architecture integrated with a dedicated Feature Refinement Module (FRM), designed to systematically filter irrelevant, noisy and adversarially perturbed visual features. Evaluations using standard backbone models such as ResNet-50 and MobileNetV2 demonstrate that our proposed method significantly improves performance, achieving up to an +21.07\% gain in Top-K accuracy under clean conditions and a 41.31\% increase in Top-1 adversarial robustness compared to different baseline models."
2504.05793,"Future vehicles are expected to dynamically deploy in-vehicle applications within a Service-Oriented Architecture (SOA). Critical services operate under hard real-time constraints, which Time-Sensitive Networking (TSN) complements on the in-vehicle Ethernet layer. TSN ensures deterministic communication between critical services and its Credit-Based Shaper (CBS) supports dynamic resource reservations. However, the dynamic nature of service deployment challenges network resource configuration, since any new reservation may change the latency of already validated flows. In addition, standard methods of worst-case latency analysis for CBS have been found incorrect, and current TSN stream reservation procedures lack mechanisms to signal application layer Quality-of-Service (QoS) requirements or verify deadlines. In this paper, we propose a QoS negotiation scheme within the automotive SOA that interacts with the TSN network controller to reserve resources while ensuring latency bounds. We comparatively evaluate reservation schemes using worst-case analysis and simulations of a realistic In-Vehicle Network (IVN) for demonstrating their impact on QoS guarantees, resource utilization, and setup times. We find that only a reservation scheme utilizing per-queue delay budgets and network calculus provides valid configurations and guarantees acceptable latency bounds throughout the IVN. The proposed service negotiation mechanism efficiently establishes 450 vehicular network reservations in just 11 ms."
2504.05964,"The increasing complexity of wireless technologies, such as Wi-Fi, presents significant challenges for Rate Adaptation (RA) due to the large configuration space of transmission parameters. While extensive research has been conducted on RA for low-mobility networks, existing solutions fail to adapt in flying networks, where high mobility and dynamic wireless conditions introduce additional uncertainty.We propose Linear Upper Confidence Bound for RA (LinRA), a novel Contextual Bandit-based approach that leverages real-time link context to optimize transmission rates. Designed for predictive flying networks, where future trajectories are known, LinRA proactively adapts to obstacles affecting channel quality. Simulation results demonstrate that LinRA converges $\mathbf{5.2\times}$ faster than state-of-the-art benchmarks and improves throughput by 80\% in Non Line-of-Sight (NLoS) conditions, matching the performance of ideal algorithms. With low time complexity, LinRA is a scalable and efficient RA solution for predictive flying networks."
2504.06173,"Beamforming techniques are considered as essential parts to compensate severe path losses in millimeter-wave (mmWave) communications. In particular, these techniques adopt large antenna arrays and formulate narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over narrow beams for efficient link configuration by traditional standard defined beam selection approaches, which mainly rely on channel state information and beam sweeping through exhaustive searching, imposes computational and communications overheads. And, such resulting overheads limit their potential use in vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communications involving highly dynamic scenarios. In comparison, utilizing out-of-band contextual information, such as sensing data obtained from sensor devices, provides a better alternative to reduce overheads. This paper presents a deep learning-based solution for utilizing the multi-modality sensing data for predicting the optimal beams having sufficient mmWave received powers so that the best V2I and V2V line-of-sight links can be ensured proactively. The proposed solution has been tested on real-world measured mmWave sensing and communication data, and the results show that it can achieve up to 98.19% accuracies while predicting top-13 beams. Correspondingly, when compared to existing been sweeping approach, the beam sweeping searching space and time overheads are greatly shortened roughly by 79.67% and 91.89%, respectively which confirm a promising solution for beamforming in mmWave enabled communications."
2504.06406,"In this paper, we present a new city-scale decentralized mesh network system suited for disaster recovery and emergencies. When wide-area connectivity is unavailable or significantly degraded, our system, MapMesh, enables static access points and mobile devices equipped with Wi-Fi in a city to route packets via each other for intra-city connectivity and to/from any nodes that might have Internet access, e.g., via satellite. The chief contribution of our work is a new routing protocol that scales to millions of nodes, a significant improvement over prior work on wireless mesh and mobile ad hoc networks. Our approach uses detailed information about buildings from widely available maps--data that was unavailable at scale over a decade ago, but is widely available now--to compute paths in a scalable way."
2504.06478,"In this paper, we consider the task of introducing a new wireless data link over a given occupied frequency band using a multi-antenna transmitter and receiver. We design formally a dynamic multiple-input multiple-output (MIMO) wireless link that can coexist in the fixed congested frequency band by (a) optimally avoiding sensed interference in the joint space-time domain, and (b) protecting existing links by minimizing its own transmitted power in the band. In particular, the transmit beam weight vector and time domain pulse code sequence are jointly optimized to minimize the transmit energy per bit per antenna, while maintaining a pre-defined signal-to-interference-plus-noise ratio (SINR) at the output of the joint space-time maximum SINR receiver filter. Extensive numerical studies are carried out to demonstrate the derived algorithmic solution in light and heavily congested band scenarios with non-cooperative co-channel links. We show that the proposed autonomously reconfigurable 4x4 MIMO link outperforms a non-adaptive transceiver and other forms of waveform shaping in terms of the pre-detection SINR performance and the capability to protect ongoing non-cooperative links by not occupying the band with redundant transmissions."
2504.06587,"While unencrypted information inspection in physical layer (e.g., open headers) can provide deep insights for optimizing wireless networks, the state-of-the-art (SOTA) methods heavily depend on full sampling rate (a.k.a Nyquist rate), and high-cost radios, due to terrestrial and non-terrestrial networks densely occupying multiple bands across large bandwidth (e.g., from 4G/5G at 0.4-7 GHz to LEO satellite at 4-40 GHz). To this end, we present SigChord, an efficient physical layer inspection system built on low-cost and sub-Nyquist sampling radios. We first design a deep and rule-based interleaving algorithm based on Transformer network to perform spectrum sensing and signal recovery under sub-Nyquist sampling rate, and second, cascade protocol identifier and decoder based on Transformer neural networks to help physical layer packets analysis. We implement SigChord using software-defined radio platforms, and extensively evaluate it on over-the-air terrestrial and non-terrestrial wireless signals. The experiments demonstrate that SigChord delivers over 99% accuracy in detecting and decoding, while still decreasing 34% sampling rate, compared with the SOTA approaches."
2504.06682,"The transition towards 6G presents unique challenges and opportunities in mobile networks design and standardization. Addressing these challenges requires a robust methodology for analyzing and selecting innovations that can be effectively translated into 3rd Generation Partnership Project (3GPP) contributions. This paper presents a systematic approach to bridging research and standardization, ensuring that cutting-edge advancements extend beyond academia and translate into concrete standardization efforts. The proposed methodology has been applied within the Italian RESTART framework to two ongoing research areas: Morphable Programmable Networks (MPNs) and Network Digital Twins (NDTs), both key enablers of next-generation networks. MPNs enhance dynamic adaptability and resource management, while NDTs enable real-time simulation, predictive analytics, and intelligent decision-making. Their integration into 3GPP Release 20 will be instrumental in shaping a flexible and future-proof mobile ecosystem. These innovations exemplify how research-driven solutions can align with 6G standardization objectives. By applying the proposed methodology, we aim to establish a systematic pathway for transitioning research into impactful 3GPP contributions, ultimately driving the evolution of next-generation networks."
2504.07262,"As air travel demand increases, uninterrupted high-speed internet access becomes essential. However, current satellite-based systems face latency and connectivity challenges. While prior research has focused on terrestrial 5G and geostationary satellites, there is a gap in optimizing Low Earth Orbit (LEO)-based 5G systems for aircraft. This study evaluates the feasibility of deployment strategies and improving signal quality with LEO satellites for seamless in-flight 5G connectivity. Using Matlab and Simulink, we model satellite trajectories, aircraft movement, and handover mechanisms, complemented by ray-tracing techniques for in-cabin signal analysis. Results show that proposed LEO satellite configurations enhance coverage and reduce latency, with sequential handovers minimizing service interruptions. These findings contribute to advancing in-flight 5G networks, improving passenger experience, and supporting real-time global connectivity solutions."
2504.07431,"Emerging services such as augmented reality (AR) and virtual reality (VR) have increased the volume of data transmitted in wireless communication systems, revealing the limitations of traditional Shannon theory. To address these limitations, semantic communication has been proposed as a solution that prioritizes the meaning of messages over the exact transmission of bits. This paper explores semantic communication for text data transmission in end-to-end (E2E) systems through a novel approach called KG-LLM semantic communication, which integrates knowledge graph (KG) extraction and large language model (LLM) coding. In this method, the transmitter first utilizes a KG to extract key entities and relationships from sentences. The extracted information is then encoded using an LLM to obtain the semantic meaning. On the receiver side, messages are decoded using another LLM, while a bidirectional encoder representations from transformers (i.e., BERT) model further refines the reconstructed sentences for improved semantic similarity. The KG-LLM semantic communication method reduces the transmitted text data volume by 30% through KG-based compression and achieves 84\% semantic similarity between the original and received messages. This demonstrates the KG-LLM methods efficiency and robustness in semantic communication systems, outperforming the deep learning-based semantic communication model (DeepSC), which achieves only 63%."
2504.07528,"The explosive growth of user devices and emerging applications is driving unprecedented traffic demands, accompanied by stringent Quality of Service (QoS) requirements. Addressing these challenges necessitates innovative service orchestration methods capable of seamless integration across the edge-cloud continuum. Terrestrial network-based service orchestration methods struggle to deliver timely responses to growing traffic demands or support users with poor or lack of access to terrestrial infrastructure. Exploiting both aerial and terrestrial resources in service composition increases coverage and facilitates the use of full computing and communication potentials. This paper proposes a service placement and composition mechanism for integrated aerial-terrestrial networks over the edge-cloud continuum while considering the dynamic nature of the network. The service function placement and service orchestration are modeled in an optimization framework. Considering the dynamicity, the Aerial Base Station (ABS) trajectory might not be deterministic, and their mobility pattern might not be known as assumed knowledge. Also, service requests can traverse through access nodes due to users' mobility. By incorporating predictive algorithms, including Deep Reinforcement Learning (DRL) approaches, the proposed method predicts ABS locations and service requests. Subsequently, a heuristic isomorphic graph matching approach is proposed to enable efficient, latency-aware service orchestration. Simulation results demonstrate the efficiency of the proposed prediction and service composition schemes in terms of accuracy, cost optimization, scalability, and responsiveness, ensuring timely and reliable service delivery under diverse network conditions."
2504.07829,"Recently, Semantic Communication (SC) has been recognized as a crucial new paradigm in 6G, significantly improving information transmission efficiency. However, the diverse range of service types in 6G networks, such as high-data-volume services like AR/VR/MR and low-data-volume applications requiring high accuracy, such as industrial control and data collection, presents significant challenges to fully replacing the fundamental technologies with SC. Therefore, we design a Hybrid Semantic Communication Ratio Access Network (HSC-RAN) protocol stack demo for 6G systems to achieve compatibility and smooth transition between SC and non-SC. Specifically, we take the Physical Downlink Shared Channel (PDSCH) as an example, to efficiently integrate SC with Orthogonal Frequency Division Multiplexing (OFDM). Furthermore, we introduce a novel Downlink Control Information (DCI) format that jointly supports SC and non-SC, enabling real-time video transmission via SC and text transmission through non-SC. Experimental results demonstrate that our approach allows simultaneous transmission of semantic and non-semantic information while maintaining high-quality reconstruction at the receiver."
2504.07969,"This paper focuses on a typical uplink transmission scenario over multiple-input multiple-output multiple access channel (MIMO-MAC) and thus propose a multi-user learnable CSI fusion semantic communication (MU-LCFSC) framework. It incorporates CSI as the side information into both the semantic encoders and decoders to generate a proper feature mask map in order to produce a more robust attention weight distribution. Especially for the decoding end, a cooperative successive interference cancellation procedure is conducted along with a cooperative mask ratio generator, which flexibly controls the mask elements of feature mask maps. Numerical results verify the superiority of proposed MU-LCFSC compared to DeepJSCC-NOMA over 3 dB in terms of PSNR."
2504.08134,"Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) that integrates quantum computing capabilities into classical edge computing servers that are proximate to mobile devices. To conceptualize the MEQC, we first design an MEQC system, where mobile devices can offload classical and quantum computation tasks to edge servers equipped with classical and quantum computers. We then formulate the hybrid classical-quantum computation offloading problem whose goal is to minimize system cost in terms of latency and energy consumption. To solve the offloading problem efficiently, we propose a hybrid discrete-continuous multi-agent reinforcement learning algorithm to learn long-term sustainable offloading and partitioning strategies. Finally, numerical results demonstrate that the proposed algorithm can reduce the MEQC system cost by up to 30% compared to existing baselines."
2504.08255,"Cloud-based intelligent connected vehicles (CICVs) leverage cloud computing and vehicle-to-everything (V2X) to enable efficient information exchange and cooperative control. However, communication delay is a critical factor in vehicle-cloud interactions, potentially deteriorating the planning and control (PnC) performance of CICVs. To explore whether the new generation of communication technology, 5G, can support the PnC of CICVs, we present CICV5G, a publicly available 5G communication delay dataset for the PnC of CICVs. This dataset offers real-time delay variations across diverse traffic environments, velocity, data transmission frequencies, and network conditions. It contains over 300,000 records, with each record consists of the network performance indicators (e.g., cell ID, reference signal received power, and signal-to-noise ratio) and PnC related data (e.g., position). Based on the CICV5G, we compare the performance of CICVs with that of autonomous vehicles and examine how delay impacts the PnC of CICVs. The object of this dataset is to support research in developing more accurate communication models and to provide a valuable reference for scheme development and network deployment for CICVs. To ensure that the research community can benefit from this work, our dataset and accompanying code are made publicly available."
2504.08314,"Set reconciliation is a fundamental task in distributed systems, particularly in blockchain networks, where it enables synchronization of transaction pools among peers and facilitates block dissemination. Traditional set reconciliation schemes are either statistical, offering success probability as a function of communication overhead and symmetric difference size, or require parametrization and estimation of that size, which can be error-prone. We present CertainSync, a novel reconciliation framework that, to the best of our knowledge, is the first to guarantee successful set reconciliation without any parametrization or estimators. The framework is rateless and adapts to the unknown symmetric difference size. Reconciliation is guaranteed whenever the communication overhead reaches a lower bound derived from the symmetric difference size and universe size. Our framework builds on recent constructions of Invertible Bloom Lookup Tables (IBLTs), ensuring successful element listing as long as the number of elements is bounded. We provide a theoretical analysis proving the certainty of reconciliation for multiple constructions. Our approach is validated by simulations, showing the ability to synchronize sets with efficient communication costs while maintaining guarantees compared to baseline schemes. To further reduce overhead in large universes such as blockchain networks, CertainSync is extended with a universe reduction technique. We compare and validate this extension, UniverseReduceSync, against the basic framework using real Ethereum transaction hash data. Results show a trade-off between lower communication costs and maintaining guarantees, offering a comprehensive solution for diverse reconciliation scenarios."
2504.0836,"New amendments support Wi-Fi access points (APs) and stations (STAs) in next-generation IEEE 802.11 wireless local area networks (WLANs). IEEE 802.11be (Wi-Fi 7) features multi-link operation (MLO) with multi-link device (MLD) hosting multiple interfaces, highlighting enhanced multi-link single-radio (EMLSR) operation. IEEE 802.11bf features Wi-Fi sensing, enabling integrated sensing and communications (ISAC) in Wi-Fi. In this paper, we pioneer an innovative combination of EMLSR operation and ISAC functionality, considering target tracking with ISAC using EMLSR in IEEE 802.11 WLANs. We establish a unique scenario where AP MLD needs to make ISAC decision and STA MLD selection when its interface gains a transmit opportunity (TXOP). Then, we present key design principles: ISAC decision involves the Kalman filter for target state and a developed time-based strategy for sensing/communications determination, while STA MLD selection involves a Cramr-Rao lower bound (CRLB)-based trilateration performance metric along with a developed candidate strategy for UL sensing and involves a developed weighted proportional fairness-aware heuristic strategy for DL communications. We propose novel non-cooperative and cooperative approaches, where each interface leverages its own information and aggregate information across all interfaces, respectively. For proposed non-cooperative and cooperative approaches, simulation results exhibit their tradeoff and superiority about sensing and communications."
2504.08403,"Advances in the Internet of Things are revolutionizing data acquisition, enhancing artificial intelligence and quality of service. Unmanned Aerial Vehicles (UAVs) provide an efficient data-gathering solution across varied environments. This paper addresses challenges in integrating UAVs for large scale data operations, including mobility, multi-hop paths, and optimized multi-source information transfer. We propose a collaborative UAV framework that enables efficient data sharing with minimal communication overhead, featuring adaptive power control and dynamic resource allocation. Formulated as an NP-hard Integer Linear Program, our approach uses heuristic algorithms to optimize routing through UAV hubs. Simulations show promise in terms of computation time (99% speedup) and outcome (down to 14% deviation from the optimal)."
2504.09019,"EU data localization regulations limit data transfers to non-EU countries with the GDPR. However, BGP, DNS and other Internet protocols were not designed to enforce jurisdictional constraints, so implementing data localization is challenging. Despite initial research on the topic, little is known about if or how companies currently operate their server infrastructure to comply with the regulations. We close this knowledge gap by empirically measuring the extent to which servers and routers that process EU requests are located outside of the EU (and a handful of ``adequate'' non-EU countries). The key challenge is that both browser measurements (to infer relevant endpoints) and data-plane measurements (to infer relevant IP addresses) are needed, but no large-scale public infrastructure allows both. We build a novel methodology that combines BrightData (browser) and RIPE Atlas (data-plane) probes, with joint measurements from over 1,000 networks in 19 EU countries. We find that, on average, 2.3% of servers serving users in each EU country are located in non-adequate destination countries (1.4% of known trackers). Our findings suggest that data localization policies are largely being followed by content providers, though there are exceptions."
2504.09187,"The evolution of 6G envisions a wide range of applications and services characterized by highly differentiated and stringent Quality of Service (QoS) requirements. Open Radio Access Network (O-RAN) technology has emerged as a transformative approach that enables intelligent software-defined management of the RAN. A cornerstone of O-RAN is the RAN Intelligent Controller (RIC), which facilitates the deployment of intelligent applications (xApps and rApps) near the radio unit. In this context, QoS management through O-RAN has been explored using network slice and machine learning (ML) techniques. Although prior studies have demonstrated the ability to optimize RAN resource allocation and prioritize slices effectively, they have not considered the critical integration of Service Level Agreements (SLAs) into the ML learning process. This omission can lead to suboptimal resource utilization and, in many cases, service outages when target Key Performance Indicators (KPIs) are not met. This work introduces RSLAQ, an innovative xApp designed to ensure robust QoS management for RAN slicing while incorporating SLAs directly into its operational framework. RSLAQ translates operator policies into actionable configurations, guiding resource distribution and scheduling for RAN slices. Using deep reinforcement learning (DRL), RSLAQ dynamically monitors RAN performance metrics and computes optimal actions, embedding SLA constraints to mitigate conflicts and prevent outages. Extensive system-level simulations validate the efficacy of the proposed solution, demonstrating its ability to optimize resource allocation, improve SLA adherence, and maintain operational reliability (>95%) in challenging scenarios."
2504.092,"In this paper, we investigate the Domain Name System (DNS) over QUIC (DoQ) and propose a non-disruptive extension, which can greatly reduce DoQ's resource consumption. This extension can benefit all DNS clients - especially Internet of Things (IoT) devices. This is important because even resource-constrained IoT devices can generate dozens of DNS requests every hour. DNS is a crucial service that correlates IP addresses and domain names. It is traditionally sent as plain-text, favoring low-latency results over security and privacy. The repercussion of this can be eavesdropping and information leakage about IoT devices. To address these concerns, the newest and most promising solution is DoQ. QUIC offers features similar to TCP and TLS while also supporting early data delivery and stream multiplexing. DoQ's specification requires that DNS exchanges occur over independent streams in a long-lived QUIC connection. Our hypothesis is that due to DNS's typically high transaction volume, managing QUIC streams may be overly resource intensive for IoT devices. Therefore, we have designed and implemented a data delivery mode for DoQ using QUIC datagrams, which we believe to be more preferable than stream-based delivery. To test our theory, we analyzed the memory, CPU, signaling, power, and time of each DoQ delivery mode in a setup generating real queries and network traffic. Our novel datagram-based delivery mode proved to be decisively more resource-friendly with little compromise in terms of functionality or performance. Furthermore, our paper is the first to investigate multiple queries over DoQ, to our knowledge."
2504.09517,"In a multi robot system establishing trust amongst untrusted robots from different organisations while preserving a robot's privacy is a challenge. Recently decentralized technologies such as smart contract and blockchain are being explored for applications in robotics. However, the limited transaction processing and high maintenance cost hinder the widespread adoption of such approaches. Moreover, blockchain transactions be they on public or private permissioned blockchain are publically readable which further fails to preserve the confidentiality of the robot's data and privacy of the robot. In this work, we propose RoboComm a Decentralized Identity based approach for privacy-preserving interaction between robots. With DID a component of Self-Sovereign Identity; robots can authenticate each other independently without relying on any third-party service. Verifiable Credentials enable private data associated with a robot to be stored within the robot's hardware, unlike existing blockchain based approaches where the data has to be on the blockchain. We improve throughput by allowing message exchange over state channels. Being a blockchain backed solution RoboComm provides a trustworthy system without relying on a single party. Moreover, we implement our proposed approach to demonstrate the feasibility of our solution."
2504.09634,"In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure within resource-constrained IoT systems. Despite their growing adoption, limited research has explored the impact of ML-based IDS on key performance metrics, such as CPU load, CPU usage, and energy consumption, particularly under real-time cyber threats. This study bridges that gap through an empirical evaluation of cutting-edge ML-based IDSs deployed at the edge of IoT networks under both benign and attack scenarios. Additionally, we investigate how SDN's centralized control and dynamic resource management influence IDS performance. Our experimental framework compares traditional ML-based IDS with deep learning (DL)-based counterparts, both with and without SDN integration. Results reveal that edge-deployed ML-based IDSs significantly impact system performance during cyber threats, with marked increases in resource consumption. SDN integration further influences these outcomes, emphasizing the need for optimized architectural design. Statistical analysis using ANOVA confirms the significance of our findings. This research provides critical insights into the performance and trade-offs of deploying ML-based IDSs in edge-based IoT systems."
2504.09751,"Ray tracing (RT) simulation is a widely used approach to enable modeling wireless channels in applications such as network digital twins. However, the computational cost to execute ray tracing (RT) is proportional to factors such as the level of detail used in the adopted 3D scenario. This work proposes RT pre-processing algorithms that aim at simplifying the 3D scene without distorting the channel, by reducing the scenario area and/or simplifying object shapes in the scenario. It also proposes a post-processing method that augments a set of RT results to achieve an improved time resolution. These methods enable using RT in applications that use a detailed and photorealistic 3D scenario while generating consistent wireless channels over time. Our simulation results with different urban scenarios scales, in terms of area and object details, demonstrate that it is possible to reduce the simulation time by more than 50% without compromising the accuracy of the multipath RT parameters, such as angles of arrival and departure, delay, phase, and path gain."
2504.09892,"The increasing gap between datacenter traffic volume and the capacity of electrical switches has driven the development of reconfigurable network designs utilizing optical circuit switching. Recent advancements, particularly those featuring periodic fixed-duration reconfigurations, have achieved practical end-to-end delays of just a few microseconds. However, current designs rely on multi-hop routing to enhance utilization, which can lead to a significant reduction in worst-case throughput and added overhead from congestion control and routing complexity. These factors pose significant operational challenges for the large-scale deployment of these technologies.We present Vermilion, a reconfigurable optical interconnect that breaks the throughput barrier of existing periodic reconfigurable networks, without the need for multi-hop routing -- thus eliminating congestion control and simplifying routing to direct communication. Vermilion adopts a traffic-aware approach while retaining the simplicity of periodic fixed-duration reconfigurations, similar to RotorNet. We formally establish throughput bounds for Vermilion, demonstrating that it achieves at least $33\%$ more throughput in the worst-case compared to existing designs. The key innovation of Vermilion is its short traffic-aware periodic schedule, derived using a matrix rounding technique. This schedule is then combined with a traffic-oblivious periodic schedule to efficiently manage any residual traffic. Our evaluation results support our theoretical findings, revealing significant performance gains for datacenter workloads."
2504.10054,"QUIC, a UDP-based transport protocol, addresses several limitations of TCP by offering built-in encryption, stream multiplexing, and improved loss recovery. To extend these benefits to legacy TCP-based applications, this paper explores the implementation and evaluation of a TCP over QUIC tunneling approach. A lightweight, stream-based tunnel is constructed using the Rust-based Quinn library, enabling TCP traffic to traverse QUIC connections transparently. Performance is evaluated under varying network conditions, including packet loss, high latency, and out-of-order delivery. Results indicate that TCP over QUIC maintains significantly higher throughput than native TCP in lossy or unstable environments, with up to a high improvement under 20\% packet loss. However, under ideal network conditions, tunneling introduces modest overhead due to encryption and user-space processing. These findings provide insights into the trade-offs of TCP over QUIC tunneling and its suitability for deployment in dynamic or impaired networks."
2504.10299,"The Internet comprises tens of thousands of autonomous systems (ASes) whose commercial relationships are not publicly announced. The classification of the Type of Relationship (ToR) between ASes has been extensively studied over the past two decades due to its relevance in network routing management and security.This paper presents a new approach to ToR classification, leveraging publicly available BGP data from the Internet Routing Registry (IRR). We show how the IRR can be mined and the results refined to achieve a large and accurate ToR database. Using a ground truth database with hundreds of entries we show that we indeed manage to obtain high accuracy. About two-thirds of our ToRs are new, namely, they were not obtained by previous works, which means that we enrich our ToR knowledge with links that are otherwise missed."
2504.10365,"The libp2p GossipSub protocol leverages a full-message mesh with a lower node degree and a more densely connected metadata-only (gossip) mesh. This combination allows an efficient dissemination of messages in unstructured peer-to-peer (P2P) networks. However, GossipSub needs to consider message size, which is crucial for the efficient operation of many applications, such as handling large Ethereum blocks. This paper proposes modifications to improve GossipSub's performance when transmitting large messages. We evaluate the proposed improvements using the shadow simulator. Our results show that the proposed improvements significantly enhance GossipSub's performance for large message transmissions in sizeable networks."
2504.10688,"This article explores the growing impact of large language models (LLMs) and Generative AI (GenAI) tools on Internet traffic, focusing on their role as a new and significant source of network load. As these AI tools continue to gain importance in applications ranging from virtual assistants to content generation, the volume of traffic they generate is expected to increase massively. These models use the Internet as the global infrastructure for delivering multimedia messages (text, voice, images, video, etc.) to users, by interconnecting users and devices with AI agents typically deployed in the cloud. We believe this represents a new paradigm that will lead to a considerable increase in network traffic, and network operators must be prepared to address the resulting demands. To support this claim, we provide a proof-of-concept and source code for measuring traffic in remote user-agent interactions, estimating the traffic generated per prompt for some of the most popular open-source LLMs in 2025. The average size of each prompt query and response is 7,593 bytes, with a standard deviation of 369 bytes. These numbers are comparable with email and web browsing traffic. However, we envision AI as the next ""killer application"" that will saturate networks with traffic, such as Peer-to-Peer traffic and Video-on-demand dominated in previous decades."
2504.11233,"[...] This paper presents AutoRAN, an automated, intent-driven framework for zero-touch provisioning of open, programmable cellular networks. Leveraging cloud-native principles, AutoRAN employs virtualization, declarative infrastructure-as-code templates, and disaggregated micro-services to abstract physical resources and protocol stacks. Its orchestration engine integrates Language Models (LLMs) to translate high-level intents into machine-readable configurations, enabling closed-loop control via telemetry-driven observability. Implemented on a multi-architecture OpenShift cluster with heterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access Network (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of O-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS core, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines. Experimental results demonstrate that AutoRAN is capable of deploying an end-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput, validating its ability to streamline configuration, accelerate testing, and reduce manual intervention with similar performance than non cloud-based implementations. With its novel LLM-assisted intent translation mechanism, and performance-optimized automation workflow for multi-vendor environments, AutoRAN has the potential of advancing the robustness of next-generation cellular supply chains through reproducible, intent-based provisioning across public and private deployments."
2504.11334,"While semantic communication (SemCom) has recently demonstrated great potential to enhance transmission efficiency and reliability by leveraging machine learning (ML) and knowledge base (KB), there is a lack of mathematical modeling to rigorously characterize SemCom system and quantify the performance gain obtained from ML and KB. In this paper, we develop a mathematical framework for SemCom based on category theory, rigorously modeling the concepts of semantic entities and semantic probability space. Within this framework, we introduce the semantic entropy to quantify the uncertainty of semantic entities. We theoretically prove that semantic entropy can be effectively reduced by exploiting KBs, which capture semantic dependencies. Within the formulated semantic space, semantic entities can be combined according to the required semantic ambiguity, and the combined entities can be encoded based on semantic dependencies obtained from KB. Then, we derive semantic channel capacity modeling, which incorporates the mutual information obtained in KB to accurately measure the transmission efficiency of SemCom. Numerical simulations validate the effectiveness of the proposed framework, showing that SemCom with KB integration outperforms traditional communication in both entropy reduction and coding efficiency."
2504.11565,"This paper presents an analytical framework for evaluating beam misalignment in 3GPP mmWave NR systems implementing analog beamforming. Our approach captures the interaction between user mobility, beam sweeping mechanisms, and deployment configurations, focusing on long-term average performance metrics. Specifically, we model the beam misalignment rates at both the base station (BS) and user equipment (UE) as Poisson processes and derive expressions for the expected misalignment duration, misalignment fraction, and overall beamforming gain. The framework accounts for practical constraints in NR such as Synchronization Signal Blocks (SSB) periodicity, TDD frame structures, and SSB overhead. Through numerical evaluation based on 3GPP mmWave parameters, we identify key trade-offs between beam counts, user mobility, and SSB timing, providing actionable design insights for robust and efficient beam management in future high-frequency networks."
2504.11696,"The next generation of wireless communications seeks to deeply integrate artificial intelligence (AI) with user-centric communication networks, with the goal of developing AI-native networks that more accurately address user requirements. The rapid development of large language models (LLMs) offers significant potential in realizing these goals. However, existing efforts that leverage LLMs for wireless communication often overlook the considerable gap between human natural language and the intricacies of real-world communication systems, thus failing to fully exploit the capabilities of LLMs. To address this gap, we propose a novel LLM-driven paradigm for wireless communication that innovatively incorporates the nature language to structured query language (NL2SQL) tool. Specifically, in this paradigm, user personal requirements is the primary focus. Upon receiving a user request, LLMs first analyze the user intent in terms of relevant communication metrics and system parameters. Subsequently, a structured query language (SQL) statement is generated to retrieve the specific parameter values from a high-performance real-time database. We further utilize LLMs to formulate and solve an optimization problem based on the user request and the retrieved parameters. The solution to this optimization problem then drives adjustments in the communication system to fulfill the user's requirements. To validate the feasibility of the proposed paradigm, we present a prototype system. In this prototype, we consider user-request centric semantic communication (URC-SC) system in which a dynamic semantic representation network at the physical layer adapts its encoding depth to meet user requirements. Additionally, two LLMs are employed to analyze user requests and generate SQL statements, respectively. Simulation results demonstrate the effectiveness."
2504.1221,"Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge. Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents. Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network. In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents. By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance. Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art."
2504.1277,"This paper explores the Quality of Service (QoS) performance of two widely used Software-Defined Networking (SDN) controllers, POX and Ryu, using Mininet for network simulation. SDN, a transformative approach to network architecture, separates the control and data planes, enabling centralized management, improved agility, and cost-effective solutions. The study evaluates key QoS parameters, including throughput, delay, and jitter, to understand the capabilities and limitations of the POX and Ryu controllers in handling traffic under diverse network topologies. The research employs a systematic methodology involving the design of custom network topologies, implementation of OpenFlow rules, and analysis of controller behavior under simulated conditions. Results reveal that while POX offers simplicity and ease of use, making it suitable for smaller-scale applications and experimentation, Ryu provides superior scalability and adaptability for more complex network environments. The findings highlight the strengths and challenges of each controller, providing valuable insights for organizations seeking to optimize SDN deployment. This study contributes to the growing body of knowledge on SDN technologies and their role in building scalable, efficient, and resilient network infrastructures."
2504.1319,"This paper introduces Cellular-X, an LLM-powered agent designed to automate cellular base station (BS) maintenance. Leveraging multimodal LLM and retrieval-augmented generation (RAG) techniques, Cellular-X significantly enhances field engineer efficiency by quickly interpreting user intents, retrieving relevant technical information, and configuring a BS through iterative self-correction. Key features of the demo include automatic customized BS setup, document-based query answering, and voice-controlled configuration reporting and revision. We implemented Cellular-X on a USRP X310 testbed for demonstration. Demo videos and implementation details are available atthis https URL."
2504.13193,"For a single-gateway LoRaWAN network, this study proposed a history-enhanced two-phase actor-critic algorithm with a shared transformer algorithm (HEAT) to improve network performance. HEAT considers uplink parameters and often neglected downlink parameters, and effectively integrates offline and online reinforcement learning, using historical data and real-time interaction to improve model performance. In addition, this study developed an open source LoRaWAN network simulator LoRaWANSim. The simulator considers the demodulator lock effect and supports multi-channel, multi-demodulator and bidirectional communication. Simulation experiments show that compared with the best results of all compared algorithms, HEAT improves the packet success rate and energy efficiency by 15% and 95%, respectively."
2504.13194,"For large-scale multi-gateway LoRaWAN networks, this study proposes a cloud-edge collaborative resource allocation and decision-making method based on edge intelligence, HEAT-LDL (HEAT-Local Distill Lyapunov), which realizes collaborative decision-making between gateways and terminal nodes. HEAT-LDL combines the Actor-Critic architecture and the Lyapunov optimization method to achieve intelligent downlink control and gateway load balancing. When the signal quality is good, the network server uses the HEAT algorithm to schedule the terminal nodes. To improve the efficiency of autonomous decision-making of terminal nodes, HEAT-LDL performs cloud-edge knowledge distillation on the HEAT teacher model on the terminal node side. When the downlink decision instruction is lost, the terminal node uses the student model and the edge decider based on prior knowledge and local history to make collaborative autonomous decisions. Simulation experiments show that compared with the optimal results of all compared algorithms, HEAT-LDL improves the packet success rate and energy efficiency by 20.5% and 88.1%, respectively."
2504.13361,"In a conventional taxi booking system, all taxi operations are mostly done by a decision made by drivers which is hard to implement in unmanned vehicles. To address this challenge, we introduce a taxi booking system which assists autonomous vehicles to pick up customers. The system can allocate an autonomous vehicle (AV) as well as plan service trips for a customer request. We use our own AV to serve a customer who uses a mobile application to make his taxi request. Apart from customer and AV, we build a server to monitor customers and AVs. It also supports inter-communication between a customer and an AV once AV decided to pick up a customer."
2504.13424,"In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth in the number of cells has rendered manual tuning impractical. Existing research tends to overlook critical engineering details in order to simplify handover problems. In this paper, we classify cell handover into three types, and jointly model their mutual influence. To achieve load balancing, we propose a multi-agent-reinforcement-learning (MARL)-based scheme to automatically optimize the parameters. To reduce the agent interaction costs, a distributed training is implemented based on consensus approximation of global average load, and it is shown that the approximation error is bounded. Experimental results show that our proposed scheme outperforms existing benchmarks in balancing load and improving network performance."
2504.13479,"Recently, the rapid development of LEO satellite networks spurs another widespread concern-data processing at satellites. However, achieving efficient computation at LEO satellites in highly dynamic satellite networks is challenging and remains an open problem when considering the constrained computation capability of LEO satellites. For the first time, we propose a novel distributed learning framework named SFL-LEO by combining Federated Learning (FL) with Split Learning (SL) to accommodate the high dynamics of LEO satellite networks and the constrained computation capability of LEO satellites by leveraging the periodical orbit traveling feature. The proposed scheme allows training locally by introducing an asynchronous training strategy, i.e., achieving local update when LEO satellites disconnect with the ground station, to provide much more training space and thus increase the training performance. Meanwhile, it aggregates client-side sub-models at the ground station and then distributes them to LEO satellites by borrowing the idea from the federated learning scheme. Experiment results driven by satellite-ground bandwidth measured in Starlink demonstrate that SFL-LEO provides a similar accuracy performance with the conventional SL scheme because it can perform local training even within the disconnection duration."
2504.13589,"Large Language Models (LLMs) are likely to play a key role in Intent-Based Networking (IBN) as they show remarkable performance in interpreting human language as well as code generation, enabling the translation of high-level intents expressed by humans into low-level network configurations. In this paper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro, ChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their capacity to generate E2E network configurations for radio access networks (RANs) and core networks in 5G/6G mobile networks. We introduce a novel performance metrics, known as FEACI, to quantitatively assess the format (F), explainability (E), accuracy (A), cost (C), and inference time (I) of the generated answer; existing general metrics are unable to capture these features. The results of our study demonstrate that open-source models can achieve comparable or even superior translation performance compared with the closed-source models requiring costly hardware setup and not accessible to all users."
2504.13594,"Software-defined networking (SDN) based low earth orbit (LEO) satellite networks leverage the SDN's benefits of the separation of data plane and control plane, control plane programmability, and centralized control to alleviate the problem of inefficient resource management under traditional network architectures. The most fundamental issue in SDN-based LEO satellite networks is how to place controllers and assign switches. Their outcome directly affects the performance of the network. However, most existing strategies can not sensibly and dynamically adjust the controller location and controller-switch mapping according to the topology variation and traffic undulation of the LEO satellite network meanwhile. In this paper, based on the dynamic placement dynamic assignment scheme, we first formulate the controller placement and switch assignment (CPSA) problem in the LEO satellite networks, which is an integer nonlinear programming problem. Then, a prior population-based genetic algorithm is proposed to solve it. Some individuals of the final generation of the algorithm for the current time slot are used as the prior population of the next time slot, thus stringing together the algorithms of adjacent time slots for successive optimization. Finally, we obtain the near-optimal solution for each time slot. Extensive experiments demonstrate that our algorithm can adapt to the network topology changes and traffic surges, and outperform some existing CPSA strategies in the LEO satellite networks."
2504.14017,"Autonomous driving is a major paradigm shift in transportation, with the potential to enhance safety, optimize traffic congestion, and reduce fuel consumption. Although autonomous vehicles rely on advanced sensors and on-board computing systems to navigate without human control, full awareness of the driving environment also requires a cooperative effort via Vehicle-To-Everything (V2X) communication. Specifically, vehicles send and receive sensor perceptions to/from other vehicles to extend perception beyond their own sensing range. However, transmitting large volumes of data can be challenging for current V2X communication technologies, so data compression represents a crucial solution to reduce the message size and link congestion. In this paper, we present a statistical characterization of automotive data, focusing on LiDAR sensors. Notably, we provide models for the size of both raw and compressed point clouds. The use of statistical traffic models offers several advantages compared to using real data, such as faster simulations, reduced storage requirements, and greater flexibility in the application design. Furthermore, statistical models can be used for understanding traffic patterns and analyzing statistics, which is crucial to design and optimize wireless networks. We validate our statistical models via a Kolmogorov-Smirnoff test implementing a Bootstrap Resampling scheme. Moreover, we show via ns-3 simulations that using statistical models yields comparable results in terms of latency and throughput compared to real data, which also demonstrates the accuracy of the models."
2504.14022,"Organizations are increasingly offloading their workloads to cloud platforms. For workloads with relaxed deadlines, this presents an opportunity to reduce the total carbon footprint of these computations by moving workloads to datacenters with access to low-carbon power. Recently published results have shown that the carbon footprint of the wide-area network (WAN) can be a significant share of the total carbon output of executing the workload itself, and so careful selection of the time and place where these computations are offloaded is critical. In this paper, we propose an approach to geographic workload migration that uses high-fidelity maps of physical Internet infrastructure to better estimate the carbon costs of WAN transfers. Our findings show that space-shifting workloads can achieve much higher carbon savings than time-shifting alone, if accurate estimates of WAN carbon costs are taken into account."
2504.14088,"Network slicing logically partitions the 5G infrastructure to cater to diverse verticals with varying requirements. However, resource sharing exposes the slices to threats and performance degradation, making slice isolation essential. Fully isolating slices is resource-prohibitive, prompting the need for isolation-aware network slicing, where each slice is assigned a tailored isolation level to balance security, usability, and overhead. This paper investigates end-to-end 5G network slicing with resource isolation from the perspective of the infrastructure provider, ensuring compliance with the customers' service-level agreements. We formulate the online 5G isolation-aware network slicing (5G-INS) as a mixed-integer programming problem, modeling realistic slice isolation levels and integrating slice prioritization. To solve 5G-INS, we propose 5Guard, a novel adaptive framework that leverages an ensemble of custom optimization algorithms to achieve the best solution within resource budget and time constraints. Our results show that 5Guard increases profit by up to 10.1% in resource-constrained environments and up to 25.4% in a real-world large-scale network compared to the best-performing individual algorithm. Furthermore, we analyze the trade-offs between isolation levels, their impact on resource utilization, and the effects of slice placement, demonstrating significant advantages over baseline approaches that enforce uniform isolation policies."
2504.14326,"Mobile metaverses have attracted significant attention from both academia and industry, which are envisioned as the next-generation Internet, providing users with immersive and ubiquitous metaverse services through mobile devices. Driven by Large Language Models (LLMs) and Vision-Language Models (VLMs), Artificial Intelligence (AI) agents hold the potential to empower the creation, maintenance, and evolution of mobile metaverses. Currently, AI agents are primarily constructed using cloud-based LLMs and VLMs. However, several challenges hinder their effective implementation, including high service latency and potential sensitive data leakage during perception and processing. In this paper, we develop an edge-cloud collaboration-based federated AI agent construction framework in mobile metaverses. Specifically, Edge Servers (ESs), acting as agent infrastructures, collaboratively create agent modules in a distributed manner. The cloud server then integrates these modules into AI agents and deploys them at the edge, thereby enabling low-latency AI agent services for users. Considering that ESs may exhibit dynamic levels of willingness to participate in federated AI agent construction, we design a two-period dynamic contract model to continuously motivate ESs to participate in agent module creation, effectively addressing the dynamic information asymmetry between the cloud server and the ESs. Furthermore, we propose an Enhanced Diffusion Model-based Soft Actor-Critic (EDMSAC) algorithm to efficiently generate optimal dynamic contracts, in which dynamic structured pruning is applied to DM-based actor networks to enhance denoising efficiency and policy learning performance. Extensive simulations demonstrate the effectiveness and superiority of the EDMSAC algorithm and the proposed contract model."
2504.14411,"The internet is undergoing a historical transformation from the ""Internet of Websites"" to the ""Internet of AgentSites."" While traditional Websites served as the foundation for information hosting and dissemination, a new frontier is emerging where AgentSites serve as the hubs of the internet, where each AgentSite hosts one or more AI agents that receive tasks, address them, and deliver actionable solutions, marking a significant shift in the digital landscape and representing the next generation of online ecosystems. Under this vision, AIOS, the AI Agent Operating System, serves as the server for the development, deployment and execution of AI agents, which is a fundamental infrastructure for the Internet of Agentsites.In this paper, we introduce AIOS Server, a runtime framework to host agents and enable global-scale collaboration among decentralized agents. AIOS Server provides a communication protocol leveraging the Model Context Protocol (MCP) and JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node operates as a server to host and execute agents, while supporting peer-to-peer coordination without reliance on centralized orchestration. Based on AIOS Server, we further present the world's first practically deployed Internet of Agentsites (AIOS-IoA), including AgentHub for agent registration and discovery and AgentChat for interactive communication, atthis https URL. The agent discovery mechanism based on Distributed Hash Tables (DHT) and a Gossip protocol serves as the search engine for the internet of agentsites. This work provides a practical foundation for building the Internet of Agentsites-a new paradigm where autonomous agents become first-class citizens of the web. The implementation is available atthis https URLand is integrated into the AIOS main branch atthis https URL."
2504.1443,"As 6G networks must support diverse applications with heterogeneous quality-of-service requirements, efficient allocation of limited network resources becomes important. This paper addresses the critical challenge of user admission control in 6G networks enhanced by Reconfigurable Intelligent Surfaces (RIS) and Mobile Edge Computing (MEC). We propose an optimization framework that leverages RIS technology to enhance user admission based on spatial characteristics, priority levels, and resource constraints. Our approach first filters users based on angular alignment with RIS reflection directions, then constructs priority queues considering service requirements and arrival times, and finally performs user grouping to maximize RIS resource utilization. The proposed algorithm incorporates a utility function that balances Quality of Service (QoS) performance, RIS utilization, and MEC efficiency in admission decisions. Simulation results demonstrate that our approach significantly improves system performance with RIS-enhanced configurations. For high-priority eURLLC services, our method maintains over 90% admission rates even at maximum load, ensuring mission-critical applications receive guaranteed service quality."
2504.14443,"Satellite-based communication systems are integral to delivering high-speed data services in aviation, particularly for business aviation operations requiring global connectivity. These systems, however, are challenged by a multitude of interdependent factors such as satellite handovers, congestion, flight maneuvers and seasonal trends, making network performance prediction a complex task. No established methodologies currently exist for network performance prediction in avionic communication systems. This paper addresses the gap by proposing machine learning (ML)-based approaches for pre-flight network performance predictions. The proposed models predict performance along a given flight path, taking as input positional and network-related information and outputting the predicted performance for each position. In business aviation, flight crews typically have multiple flight plans to choose from for each city pair, allowing them to select the most optimal option. This approach enables proactive decision-making, such as selecting optimal flight paths prior to departure."
2504.14686,"Mobile network operators (MNOs) manage Radio Access Networks (RANs) with massive amounts of cells over multiple radio generations (2G-5G). To handle such complexity, operations teams rely on monitoring systems, including anomaly detection tools that identify unexpected behaviors. In this paper, we present c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph Neural Networks (GNNs). Our solution captures spatio-temporal variations by analyzing the behavior of individual cells in relation to their local neighborhoods, enabling the detection of anomalies that are independent of external mobility factors. This, in turn, allows focusing on anomalies associated with network issues (e.g., misconfigurations, equipment failures). We evaluate c-ANEMON using real-world data from a large European metropolitan area (7,890 cells; 3 months). First, we show that the GNN model within our solution generalizes effectively to cells from previously unseen areas, suggesting the possibility of using a single model across extensive deployment regions. Then, we analyze the anomalies detected by c-ANEMON through manual inspection and define several categories of long-lasting anomalies (6+ hours). Notably, 45.95% of these anomalies fall into a category that is more likely to require intervention by operations teams."
2504.1472,"Instant Messaging-Based Video Call Applications (IMVCAs) and Video Conferencing Applications (VCAs) have become integral to modern communication. Ensuring a high Quality of Experience (QoE) for users in this context is critical for network operators, as network conditions significantly impact user QoE. However, network operators lack access to end-device QoE metrics due to encrypted traffic. Existing solutions estimate QoE metrics from encrypted traffic traversing the network, with the most advanced approaches leveraging machine learning models. Subsequently, the need for ground truth QoE metrics for training and validation poses a challenge, as not all video applications provide these metrics. To address this challenge, we propose an application-agnostic approach for objective QoE estimation from encrypted traffic. Independent of the video application, we obtained key video QoE metrics, enabling broad applicability to various proprietary IMVCAs and VCAs. To validate our solution, we created a diverse dataset from WhatsApp video sessions under various network conditions, comprising 25,680 seconds of traffic data and QoE metrics. Our evaluation shows high performance across the entire dataset, with 85.2% accuracy for FPS predictions within an error margin of two FPS, and 90.2% accuracy for PIQE-based quality rating classification."
2504.14745,"The optimization of Precoding Matrix Indicators (PMIs) is crucial for enhancing the performance of 5G networks, particularly in dense deployments where inter-cell interference is a significant challenge. Some approaches have leveraged AI/ML techniques for beamforming and beam selection, however, these methods often overlook the multi-objective nature of PMI selection, which requires balancing spectral efficiency (SE) and interference reduction. This paper proposes an interference-aware PMI selection method using an Advantage Actor-Critic (A2C) reinforcement learning model, designed for deployment within an O-RAN framework as an xApp. The proposed model prioritizes user equipment (UE) based on a novel strategy and adjusts PMI values accordingly, with interference management and efficient resource utilization. Experimental results in an O-RAN environment demonstrate the approach's effectiveness in improving network performance metrics, including SE and interference mitigation."
2504.14749,"Energy consumption in mobile communication networks has become a significant challenge due to its direct impact on Capital Expenditure (CAPEX) and Operational Expenditure (OPEX). The introduction of Open RAN (O-RAN) enables telecommunication providers to leverage network intelligence to optimize energy efficiency while maintaining Quality of Service (QoS). One promising approach involves traffic-aware cell shutdown strategies, where underutilized cells are selectively deactivated without compromising overall network performance. However, achieving this balance requires precise traffic steering mechanisms that account for throughput performance, power efficiency, and network interference constraints.This work proposes a reinforcement learning (RL) model based on the Proximal Policy Optimization (PPO) algorithm to optimize traffic steering and energy efficiency. The objective is to maximize energy efficiency and performance gains while strategically shutting down underutilized cells. The proposed RL model learns adaptive policies to make optimal shutdown decisions by considering throughput degradation constraints, interference thresholds, and PRB utilization balance. Experimental validation using TeraVM Viavi RIC tester data demonstrates that our method significantly improves the network's energy efficiency and downlink throughput."
2504.14833,"Traffic classification is crucial for securing Internet of Things (IoT) networks. Deep learning-based methods can autonomously extract latent patterns from massive network traffic, demonstrating significant potential for IoT traffic classification tasks. However, the limited computational and spatial resources of IoT devices pose challenges for deploying more complex deep learning models. Existing methods rely heavily on either flow-level features or raw packet byte features. Flow-level features often require inspecting entire or most of the traffic flow, leading to excessive resource consumption, while raw packet byte features fail to distinguish between headers and payloads, overlooking semantic differences and introducing noise from feature misalignment. Therefore, this paper proposes IoT-AMLHP, an aligned multimodal learning framework for resource-efficient malicious IoT traffic classification. Firstly, the framework constructs a packet-wise header-payload representation by parsing packet headers and payload bytes, resulting in an aligned and standardized multimodal traffic representation that enhances the characterization of heterogeneous IoT traffic. Subsequently, the traffic representation is fed into a resource-efficient neural network comprising a multimodal feature extraction module and a multimodal fusion module. The extraction module employs efficient depthwise separable convolutions to capture multi-scale features from different modalities while maintaining a lightweight architecture. The fusion module adaptively captures complementary features from different modalities and effectively fuses multimodal features."
2504.14959,"As modern networks continue to grow in both scale and complexity, sharing real-world device configurations poses significant privacy risks, especially when adversaries can infer organizational size or resource distribution from topology data. We present NetCloak, a configuration anonymization framework that adaptively injects synthetic routers and hosts into the network graph to obfuscate true scale, while preserving end-to-end forwarding behavior. NetCloak core techniques include: (1) a graph-embedding expansion algorithm that integrates the original topology into a larger reference graph, ensuring added nodes blend seamlessly with real ones; (2) a k-degree mapping anonymity scheme that selectively adds minimal links to guarantee each original node degree is indistinguishable among at least k peers; (3) a mimicry-driven configuration generator that derives command templates from existing devices, preserving command ordering, naming conventions, and routing policies; and (4) a layered repair process combining SMT-based intra-AS route synthesis with iterative inter-AS filter insertion to restore protocol-correct routing under OSPF and BGP. Extensive experiments on real and emulated campus and data-center topologies demonstrate that NetCloak effectively conceals network size, improving topological rationality by over 70% and configuration fidelity by nearly 30% compared to baseline methods, while reducing route-repair overhead by more than 50% under randomized link costs. NetCloak thus enables safe, privacy-preserving configuration sharing at scale."
2504.15039,"We have been investigating clock skew compensation immune to floating-point precision loss by taking into account the discrete nature of clocks in digital communication systems; extending Bresenham's line drawing algorithm, we constructed an incremental error algorithm using only integer addition/subtraction and comparison. Still, bounding the initial value of the clock remains a challenge, which determines the initial condition of the algorithm and thereby its number of iterations. In this letter, we propose a new incremental error algorithm for clock skew compensation, called direct search, which no longer relies on the bounds on the initial value of the clock. The numerical examples demonstrate that the proposed algorithm can significantly reduce the number of iterations in comparison to the prior work while eliminating the effect of floating-point precision loss on clock skew compensation."
2504.15079,"The growth of low-altitude economy (LAE) has driven a rising demand for efficient and secure communication. However, conventional beamforming optimization techniques struggle in the complex LAE environments. In this context, generative artificial intelligence (GenAI) methods provide a promising solution. In this article, we first introduce the core concepts of LAE and the roles of beamforming in advanced communication technologies for LAE. We then examine their interrelation, followed by an analysis of the limitations of conventional beamforming methods. Next, we provide an overview of how GenAI methods enhance the process of beamforming, with a focus on its applications in LAE. Furthermore, we present a case study using a generative diffusion model (GDM)-based algorithm to enhance the performance of aerial collaborative beamforming-enabled remote secure communications in LAE and simulation results verified the effectiveness of the proposed algorithms. Finally, promising research opportunities are identified."
2504.15577,"This paper addresses the challenge of energy efficiency management faced by intelligent IoT devices in complex application environments. A novel optimization method is proposed, combining Deep Q-Network (DQN) with an edge collaboration mechanism. The method builds a state-action-reward interaction model and introduces edge nodes as intermediaries for state aggregation and policy scheduling. This enables dynamic resource coordination and task allocation among multiple devices. During the modeling process, device status, task load, and network resources are jointly incorporated into the state space. The DQN is used to approximate and learn the optimal scheduling strategy. To enhance the model's ability to perceive inter-device relationships, a collaborative graph structure is introduced to model the multi-device environment and assist in decision optimization. Experiments are conducted using real-world IoT data collected from the FastBee platform. Several comparative and validation tests are performed, including energy efficiency comparisons across different scheduling strategies, robustness analysis under varying task loads, and evaluation of state dimension impacts on policy convergence speed. The results show that the proposed method outperforms existing baseline approaches in terms of average energy consumption, processing latency, and resource utilization. This confirms its effectiveness and practicality in intelligent IoT scenarios."
2504.15774,"This paper aims to improve our understanding of the performance of the Non-Primary Channel Access (NPCA) mechanism, a new feature introduced in IEEE 802.11bn to enhance spectrum utilization in Wi-Fi networks. NPCA enables devices to contend for and transmit on the secondary channel when the primary channel is occupied by transmissions from an Overlapping Basic Service Set (OBSS). We develop a Continuous-Time Markov Chain (CTMC) model that captures the interactions among OBSSs in dense Wireless Local Area Network (WLAN) environments when NPCA is enabled, incorporating new NPCA-specific states and transitions. In addition to the analytical insights offered by the model, we conduct numerical evaluations and simulations to quantify NPCA's impact on throughput and channel access delay across various scenarios. Our results show that NPCA can significantly improve throughput and reduce access delays in favorable conditions for BSSs that support the mechanism. Moreover, NPCA helps mitigate the OBSS performance anomaly, where low-rate OBSS transmissions degrade network performance for all nearby devices. However, we also observe trade-offs: NPCA may increase contention on secondary channels, potentially reducing transmission opportunities for BSSs operating there. Overall, the proposed modeling approach offers a foundation for analyzing, optimizing, and guiding the development of NPCA in next-generation Wi-Fi networks."
2504.16039,"Key performance indicators (KPIs), which can be extracted from the standardized interfaces of network equipment defined by current standards, constitute a primary data source that can be leveraged in the development of non-standardized new equipment, architectures, and computational tools. In next-generation technologies, the demand for data has evolved beyond the conventional log generation or export capabilities provided by existing licensed network monitoring tools. There is now a growing need to collect such data at specific time intervals and with defined granularities. At this stage, the development of real-time KPI extraction methods and enabling their exchange between both standardized/commercialized and non-standardized components or tools has become increasingly critical. This study presents a comprehensive evaluation of three distinct KPI extraction methodologies applied to two commercially available devices. The analysis aims to uncover the strengths, weaknesses, and overall efficacy of these approaches under varying conditions, and highlights the critical insights into the practical capabilities and limitations. The findings serve as a foundational guide for the seamless integration and robust testing of novel technologies and approaches within commercial telecommunication networks. This work aspires to bridge the gap between technological innovation and real-world applicability, fostering enhanced decision-making in network deployment and optimization."
2504.16094,"Neural radiance fields (NeRFs) have recently attracted significant attention in the field of wireless channel prediction, primarily due to their capability for high-fidelity reconstruction of complex wireless measurement environments. However, the ray-tracing component of NeRF-based methods faces challenges in realistically representing wireless scenarios, mainly due to the limited expressive power of multilayer perceptrons (MLPs). To overcome this issue, in this paper, we propose NeRF-APT, an encoder-decoder architecture integrated within a NeRF-based wireless channel prediction framework. Our architecture leverages the strengths of NeRF-like models in learning environmental features and exploits encoder-decoder modules' capabilities for critical information extraction. Additionally, we incorporate an attention mechanism within the skip connections between encoder and decoder layers, significantly enhancing contextual understanding across layers. Extensive experimental evaluations conducted on several realistic and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art approaches in wireless channel prediction."
2504.16322,"The advent of Low-Earth Orbit satellite networks (LSNs), exemplified by initiatives like \emph{Starlink}, \emph{OneWeb} and \emph{Kuiper}, has ushered in a new era of ``Internet from Space"" global connectivity. Recent studies have shown that LSNs are capable of providing unprecedented download capacity and low latency to support Livecast viewing. However, Livecast ingestion still faces significant challenges, such as limited uplink capacity, bandwidth degradation, and the burst of packet loss due to frequent satellite reallocations, which cause previous recovery and adaptive solutions to be inferior under this new scenario. In this paper, we conduct an in-depth measurement study dedicated to understanding the implications of satellite reallocations, which reveals that the network status during reallocations with network anomalies exhibits a different distribution, leading to bimodal behaviors on the overall network performance. Motivated by this finding, we propose BAROC, a framework that can effectively conceal burst packet losses by combining a novel proposed MTP-Informer with bimodal behavior awareness during satellite reallocation. BAROC enhances video QoE on the server side by addressing the above challenges and jointly determining the optimal video encoding and recovery parameters. Our extensive evaluation shows that BAROC outperforms other video delivery recovery approaches, achieving an average PSNR improvement of $1.95$ dB and a maximum of $3.44$ dB, along with enhancements in frame rate and parity packet utilization. Additionally, a comprehensive ablation study is conducted to assess the effectiveness of MTP-Informer and components in BAROC."
2504.16688,"Modeling path loss in indoor LoRaWAN technology deployments is inherently challenging due to structural obstructions, occupant density and activities, and fluctuating environmental conditions. This study proposes a two-stage approach to capture and analyze these complexities using an extensive dataset of 1,328,334 field measurements collected over six months in a single-floor office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First, we implement a multiple linear regression model that includes traditional propagation metrics (distance, structural walls) and an extension with proposed environmental variables (relative humidity, temperature, carbon dioxide, particulate matter, and barometric pressure). Using analysis of variance, we demonstrate that adding these environmental factors can reduce unexplained variance by 42.32 percent. Secondly, we examine residual distributions by fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy, Student's t, and Gaussian Mixture Models (GMMs) with 2 to 5 components. Our results show that a four-component Gaussian Mixture Model captures the residual heterogeneity of indoor signal propagation most accurately, significantly outperforming single-distribution approaches. Given the push toward ultra-reliable, context-aware communications in 6G networks, our analysis shows that environment-aware modeling can substantially improve LoRaWAN network design in dynamic indoor IoT deployments."
2504.16729,"With the rapid development of the Artificial Intelligence of Things (AIoT), mobile edge computing (MEC) becomes an essential technology underpinning AIoT applications. However, multi-angle resource constraints, multi-user task competition, and the complexity of task offloading decisions in dynamic MEC environments present new technical challenges. Therefore, a user-centric deep reinforcement learning (DRL) model splitting inference scheme is proposed to address the problem. This scheme combines model splitting inference technology and designs a UCMS_MADDPG-based offloading algorithm to realize efficient model splitting inference responses in the dynamic MEC environment with multi-angle resource constraints. Specifically, we formulate a joint optimization problem that integrates resource allocation, server selection, and task offloading, aiming to minimize the weighted sum of task execution delay and energy consumption. We also introduce a user-server co-selection algorithm to address the selection issue between users and servers. Furthermore, we design an algorithm centered on user pre-decision to coordinate the outputs of continuous and discrete hybrid decisions, and introduce a priority sampling mechanism based on reward-error trade-off to optimize the experience replay mechanism of the network. Simulation results show that the proposed UCMS_MADDPG-based offloading algorithm demonstrates superior overall performance compared with other benchmark algorithms in dynamic environments."
2504.16848,"With the evolution of Vehicle-to-Everything (V2X) technology and increased deployment of 5G networks and edge computing, Predictive Quality of Service (PQoS) is seen as an enabler for resilient and adaptive V2X communication systems. PQoS incorporates data-driven techniques, such as Machine Learning (ML), to forecast/predict Key Performing Indicators (KPIs) such as throughput, latency, etc. In this paper, we aim to predict downlink throughput in an urban environment using the Berlin V2X cellular dataset. We select features from the ego and lead vehicles to train different ML models to help improve the predicted throughput for the ego vehicle. We identify these features based on an in-depth exploratory data analysis. Results show an improvement in model performance when adding features from the lead vehicle. Moreover, we show that the improvement in model performance is model-agnostic."
2504.16897,"SSL/TLS is a fundamental technology in the network protocol stack that enables encrypted data transmission and authentication of web domains. However, the current model relies on a small number of Certificate Authorities (CAs) to provide and validate certificates, thus creating a highly centralized ecosystem. In this paper, we analyze the degree of centralization of certificate provisioning from CAs in two major political groups: Brazil, Russia, India, China, and South Africa (BRICS) and the European Union (EU). We have found that over 75% of certificates for both BRICS and EU domains originate from CAs based in the United States, indicating possible risks to their digital sovereignty due to the high level of external dependency. This indicates the need for nations within those groups to research alternatives to reduce the high level of dependency on foreign CAs and increase their digital autonomy."
2504.17046,"Deploying multiple controllers in the control panel of software-defined networks increases scalability, availability, and performance, but it also brings challenges, such as controller overload. To address this, load-balancing techniques are employed in software-defined networks. Controller load balancing can be categorized into two main approaches: (1) single-level thresholds and (2) multi-level thresholds. However, previous studies have predominantly relied on single-level thresholds, which result in an imprecise classification of controllers or have assumed uniform controller capacities in multi-level threshold methods. This study explores controller load balancing with a focus on utilizing multi-level thresholds to accurately assess controller status. Switch migration operations are utilized to achieve load balancing, considering factors such as the degree of load imbalance of the target controller and migration efficiency. This includes evaluating the post-migration status of the target controller and the distance between the migrating switch and the target controller to select the appropriate target controller and migrating switch. The proposed scheme reduces controller response time, migration costs, communication overhead, and throughput rate. Results demonstrate that our scheme outperforms others regarding response time and overall performance."
2504.17307,"Fast-evolving machine learning (ML) workloads have increasing requirements for networking. However, host network transport on RDMA NICs is hard to evolve, causing problems for ML workloads. For example, single-path RDMA traffic is prone to flow collisions that severely degrade collective communication performance. We present UCCL, an extensible software transport layer to evolve GPU networking. UCCL decouples the data path and control path of existing RDMA NICs and efficiently runs the control-path transport on host CPUs. This software extensibility brings in transport innovations that cannot be achieved in hardware for ML workloads, e.g., a multipath transport to resolve flow collisions. ML collectives atop UCCL achieve up to 4.5x higher performance compared to existing RDMA NICs."
2504.1731,"In this paper, we introduce an all-optical metro network architecture, called MOON, to serve converged multigranular traffic from fixed, mobile, and edge computing services. Since traffic is characterized by high dynamicity and diverse access requirements, MOON uses network slicing to provide quality of service (QoS) aware wavelength allocation to fulfill the various applications traffic demands. MOON incorporates hybrid optical switching (HOS) combining optical circuit switching (OCS) and optical time slotted switching (OTS) capabilities that appropriately maps different traffic types to them. Specifically, the OCS network slice explicitly serves aggregated traffic of long duration and high volume, while OTS network slice serves short bursty traffic. In order to provide flexibility, separate sets of wavelengths are used for OCS and OTS traffic service, both within a metro-access network (MAN) (intra-MAN) and between different MANs (inter-MAN). We extensively study the required number of wavelengths to efficiently serve OCS and OTS traffic for intra- and inter-MAN communication scenarios, taking into account their specific traffic access requirements in an effort to optimize wavelengths utilization. In our study, we assume nonblocking OCS communication for immediate access; therefore the number of required OCS wavelengths depends only on the number of nodes, while the number of required OTS wavelengths to obtain a desired QoS and latency level is independent from the number for OCS wavelengths. Simulation results show that within an OTS intra-MAN we achieve end-to-end (E2E) latency in submilliseconds scale, suitable for dynamic bursty traffic, while it is an decreasing function of the number of used OTS wavelengths."
2504.1759,"O-RAN (Open-Radio Access Network) offers a flexible, open architecture for next-generation wireless networks. Network slicing within O-RAN allows network operators to create customized virtual networks, each tailored to meet the specific needs of a particular application or service. Efficiently managing these slices is crucial for future 6G networks. O-RAN introduces specialized software applications called xApps that manage different network functions. In network slicing, an xApp can be responsible for managing a separate network slice. To optimize resource allocation across numerous network slices, these xApps must coordinate. Traditional methods where all xApps communicate freely can lead to excessive overhead, hindering network performance. In this paper, we address the issue of xApp conflict mitigation by proposing an innovative Zero-Touch Management (ZTM) solution for radio resource management in O-RAN. Our approach leverages Multi-Agent Reinforcement Learning (MARL) to enable xApps to learn and optimize resource allocation without the need for constant manual intervention. We introduce a Graph Convolutional Network (GCN)-based attention mechanism to streamline communication among xApps, reducing overhead and improving overall system efficiency. Our results compare traditional MARL, where all xApps communicate, against our MARL GCN-based attention method. The findings demonstrate the superiority of our approach, especially as the number of xApps increases, ultimately providing a scalable and efficient solution for optimal network slicing management in O-RAN."
2504.17725,"A Wireless Sensor Network (WSN) is a network that does not rely on a fixed infrastructure and consists of numerous sensors, such as temperature, humidity, GPS, and cameras, equipped with onboard processors that manage and monitor the environment in a specific area. As a result, building a real sensor network testbed for verifying, validating, or experimenting with a newly designed protocol presents considerable challenges in adapting a laboratory scenario due to the significant financial and logistical barriers, such as the need for specialized hardware and large-scale deployments. Additionally, WSN suffers from severe constraints such as restricted power supply, short communication range, limited bandwidth availability, and restricted memory storage. Addressing these challenges, this work presents a flexible testbed solution named STGen that enables researchers to experiment with IoT protocols in a hybrid environment that emulates WSN implementations with the physical Internet through a dedicated physical server named STGen core, which receives sensor traffic and processes it for further actions. The STGen testbed is lightweight in memory usage and easy to deploy. Most importantly, STGen supports large-scale distributed systems, facilitates experimentation with IoT protocols, and enables integration with back-end services for big data analytics and statistical insights. The key feature of STGen is the integration of real-world IoT protocols and their applications with WSN. Its modular and lightweight design makes STGen efficient and enables it to outperform other popular testbeds, such as Gotham and GothX, reducing memory usage by 89\%. While GothX takes approximately 26 minutes to establish a large topology with four VM nodes and 498 Docker nodes, STGen requires only 1.645 seconds to initialize the platform with 500 sensor nodes."
2504.17795,"This dissertation presents three independent novel approaches for distinct scenarios to solve one or more open challenges. The first concern explains the focus on the lifetime of the networks: this dissertation will utilize a fuzzy logic-based clustering protocol with multi-hop transmission for load balancing, energy consumption minimization, and network lifetime prolongation. The protocol forms unequal clusters with cluster head (CH) being selected by fuzzy logic with competition radius. Node distance to the base station, concentration, and residual energy are input variables. The second concern focuses on network stability: we design a type 2 fuzzy logic-based clustering schemes in a multi-hop WSN to reduce energy consumption and improve network scalability. In this clustering scheme, we propose a cluster head (CH) selection strategy where a sensor node is elected as a CH based on type 2 fuzzy logic inputs. To balance the load of CHs we also select their radius size based on the fuzzy logic inputs. Finally, the third concern is focus on the utility of game theory in defensive Wireless Sensor Networks (WSN) from selfish nodes and malicious behavior. Game theory can effectively model WSNs malicious attacks because of their low complexity and scalability. The study, thus, explores different WSN defense strategies from both external attackers and internal nodes acting selfishly or maliciously using the game theory approach. Also, the chapter highlights the general trust model for decision-making using the game theory framework. Besides, the chapter demonstrates the significance of the theory in ensuring WSN security from acute attacks and its role in enhancing trustworthiness in data and cooperation of nodes in various WSN architectures."
2504.17796,"This article presents an analysis of the structural resilience of a fragment of Internet topology against both targeted and random attacks, based on empirical data obtained from the iThena project. Using a processed visualization of the network, a graph representing node connections was generated and subsequently subjected to detailed analysis using centrality metrics and community detection algorithms. Two attack scenarios were carried out: removal of nodes with the highest betweenness centrality and random removal of an equivalent number of nodes. The results indicate that targeted attacks have a significantly more destructive impact on the cohesion and functionality of the network than random disruptions. The article highlights the importance of identifying critical nodes and developing monitoring and protection mechanisms for Internet infrastructure in the context of cybersecurity."
2504.17807,"The rapidly evolving cloud platforms and the escalating complexity of network traffic demand proper network traffic monitoring and anomaly detection to ensure network security and performance. This paper introduces a large language model (LLM)-based network traffic monitoring and anomaly detection system. In addition to existing models such as autoencoders and decision trees, we harness the power of large language models for processing sequence data from network traffic, which allows us a better capture of underlying complex patterns, as well as slight fluctuations in the dataset. We show for a given detection task, the need for a hybrid model that incorporates the attention mechanism of the transformer architecture into a supervised learning framework in order to achieve better accuracy. A pre-trained large language model analyzes and predicts the probable network traffic, and an anomaly detection layer that considers temporality and context is added. Moreover, we present a novel transfer learning-based methodology to enhance the model's effectiveness to quickly adapt to unknown network structures and adversarial conditions without requiring extensive labeled datasets. Actual results show that the designed model outperforms traditional methods in detection accuracy and computational efficiency, effectively identify various network anomalies such as zero-day attacks and traffic congestion pattern, and significantly reduce the false positive rate."
2504.17809,"Monero, a privacy-focused cryptocurrency, employs a decentralized peer-to-peer (P2P) network that plays a critical role in transaction propagation and consensus formation. While much research has explored Monero's privacy transaction mechanisms, its underlying P2P network architecture has remained relatively underexplored. In this study, building on our recent work on Monero network detection, we further investigate the network topology of Monero's P2P structure, which has evolved following recent protocol updates that enhanced security by obscuring peer information. Using k-core decomposition, we confirm that the Monero network exhibits a core-periphery structure, where a tightly interconnected core of supernodes is crucial for maintaining network cohesion, while peripheral nodes rely on these core nodes for connectivity. This structure explains why targeting central nodes does not easily lead to the rapid disintegration of the network's largest connected component while also providing a deeper understanding of the true architecture of Monero's peer protocol."
2504.17818,"In Cognitive Radio Networks (CRNs), secondary users (SUs) must efficiently discover each other across multiple communication channels while avoiding interference from primary users (PUs). Traditional multichannel rendezvous algorithms primarily focus on enabling pairs of SUs to find common channels without explicitly considering the underlying network topology. In this paper, we extend the rendezvous framework to explicitly incorporate network topology, introducing the \emph{multichannel topology discovery problem}. We propose a novel \emph{pseudo-random sweep algorithm with forward replacement}, designed to minimize correlation between consecutive unsuccessful rendezvous attempts, thereby significantly reducing the expected time-to-discovery (ETTD). Additionally, we introduce a \emph{threshold-based stick-together strategy} that dynamically synchronizes user hopping sequences based on partially known information, further enhancing discovery efficiency. Extensive simulation results validate our theoretical analysis, demonstrating that the proposed algorithms substantially outperform conventional (sequential) sweep methods."
2504.17973,An low-latency service scheme is proposed over Passive Optical Network (PON). The Optical Code Division Multiplexing Access (OCDMA) technique is used to define multiple private networks serving as Virtual GE-PON that mimic the service-based VLAN (S-VLAN) in the optical domain.
2504.18029,"The Open Radio Access Network (Open RAN) is an emerging idea -- transforming the traditional Radio Access Networks (RAN) that are monolithic and inflexible into more flexible and innovative. By leveraging open standard interfaces, data collection across all RAN layers becomes feasible, paving the way for the development of energy-efficient Open RAN architectures through Artificial Intelligence / Machine Learning (AI/ML). However, the inherent complexity and black-box nature of AI/ML models used for energy consumption prediction pose challenges in interpreting their underlying factors and relationships. This work presents an integration of eXplainable AI (XAI) to understand the key RAN parameters that contribute to energy consumption. Furthermore, the paper delves into the analysis of RAN parameters -- \emph{airtime}, \emph{goodput}, \emph{throughput}, \emph{buffer status report}, \emph{number of resource blocks}, and many others -- identified by XAI techniques, highlighting their significance in energy consumption."
2504.18031,"Electric Vertical Take-Off and Landing (eVTOL) aircraft, pivotal to Advanced Air Mobility (AAM), are emerging as a transformative transportation paradigm with the potential to redefine urban and regional mobility. While these systems offer unprecedented efficiency in transporting people and goods, they rely heavily on computation capability, safety-critical operations such as real-time navigation, environmental sensing, and trajectory tracking--necessitating robust offboard computational support. A widely adopted solution involves offloading these tasks to terrestrial base stations (BSs) along the flight path. However, air-to-ground connectivity is often constrained by spectrum conflicts with terrestrial users, which poses a significant challenge to maintaining reliable task execution. Cognitive radio (CR) techniques offer promising capabilities for dynamic spectrum access, making them a natural fit for addressing this issue. Existing studies often overlook the time-varying nature of BS resources, such as spectrum availability and CPU cycles, which leads to inaccurate trajectory planning, suboptimal offloading success rates, excessive energy consumption, and operational delays. To address these challenges, we propose a trajectory optimization framework for eVTOL swarms that maximizes task offloading success probability while minimizing both energy consumption and resource competition (e.g., spectrum and CPU cycles) with primary terrestrial users. The proposed algorithm integrates a Multi-Armed Bandit (MAB) model to dynamically estimate BS resource availability and a Monte Carlo Tree Search (MCTS) algorithm to determine optimal offloading decisions, selecting both the BSs and access time windows that align with energy and temporal constraints."
2504.18062,"Despite recent advances in applying large language models (LLMs) and machine learning (ML) techniques to open radio access network (O-RAN), critical challenges remain, such as insufficient cooperation between radio access network (RAN) intelligent controllers (RICs), high computational demands hindering real-time decisions, and the lack of domain-specific finetuning. Therefore, this article introduces the LLM-empowered hierarchical RIC (LLM-hRIC) framework to improve the collaboration between RICs in O-RAN. The LLM-empowered non-real-time RIC (non-RT RIC) acts as a guider, offering a strategic guidance to the near-real-time RIC (near-RT RIC) using global network information. The RL-empowered near-RT RIC acts as an implementer, combining this guidance with local real-time data to make near-RT decisions. We evaluate the feasibility and performance of the LLM-hRIC framework in an integrated access and backhaul (IAB) network setting, and finally, discuss the open challenges of the LLM-hRIC framework for O-RAN."
2504.18432,"As the gap between network and CPU speeds rapidly increases, the CPU-centric network stack proves inadequate due to excessive CPU and memory overhead. While hardware-offloaded network stacks alleviate these issues, they suffer from limited flexibility in both control and data planes. Offloading network stack to off-path SmartNIC seems promising to provide high flexibility; however, throughput remains constrained by inherent SmartNIC architectural limitations.To this end, we design FlexiNS, a SmartNIC-centric network stack with software transport programmability and line-rate packet processing capabilities. To grapple with the limitation of SmartNIC-induced challenges, FlexiNS introduces: (a) a header-only offloading TX path; (b) an unlimited-working-set in-cache processing RX path; (c) a high-performance DMA-only notification pipe; and (d) a programmable offloading engine. We prototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box RDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\times$ higher throughput than the microkernel-based baseline in block storage disaggregation and 1.3$\times$ higher throughput than the hardware-offloaded baseline in KVCache transfer."
2504.18685,"In multicloud environments, where legal obligations, technical constraints and economic interests are at stake, it is of interest to stakeholders to be able to locate cloud data or the cloud instance where data are decrypted for processing, making it particularly vulnerable. This paper proposes an original and practical delay-based approach, called GeoFINDR, to locate a cloud instance, e.g. a Virtual Machine (VM), over the Internet, based on RIPE Atlas landmarks. First, the assumed threat model and assumptions are more realistic than in existing solutions, e.g. VM-scale localization in multicloud environments, a Cloud Service Provider (CSP) lying about the VM's location. Second, the originality of the approach lies in four original ideas: (1) geolocalization is performed from the VM, (2) a Greedy algorithm selects a first set LM_A of distributed audit landmarks in the vicinity of the declared area, (3) a sectorization algorithm identifies a set LM_S of other landmarks with distance delay behavior similar to that of the VM to estimate the sector of the VM, and (4) the estimated location of the VM is calculated as the barycenter position of the LM_S landmarks. An open source tool is published on GitHub and experiments show that localization accuracy can be as high as 22.1km, under unfavorable conditions where the CSP lies about the location of the VM."
2504.18786,"Congestion control algorithms (CCAs) operate in partially observable environments, lacking direct visibility into link capacities, or competing flows. To ensure fair sharing of network resources, CCAs communicate their fair share through observable signals. For instance, Reno's fair share is encoded as $\propto 1/\sqrt{\texttt{loss rate}}$. We call such communication mechanisms \emph{contracts}. We show that the design choice of contracts fixes key steady-state performance metrics, including robustness to errors in congestion signals, fairness, amount of congestion (e.g., delay, loss), and generality (e.g., range of supported link rates). This results in fundamental tradeoffs between these metrics. Using properties of contracts we also identify design pitfalls that lead to starvation (extreme unfairness). We argue that CCA design and analysis should start with contracts to conscientiously pick tradeoffs and avoid pitfalls. We empirically validate our findings and discuss their implications on CCA design and network measurement."
2504.18902,"In the forthcoming era of 6G networks, characterized by unprecedented data rates, ultra-low latency, and extensive connectivity, effective management of Virtualized Network Functions (VNFs) is essential. VNFs are software-based counterparts of traditional hardware devices that facilitate flexible and scalable service provisioning. Service Function Chains (SFCs), structured as ordered sequences of VNFs, are pivotal in orchestrating complex network services. Nevertheless, partitioning SFCs across multi-domain network infrastructures presents substantial challenges due to stringent latency constraints and limited resource availability. Conventional optimization-based methods typically exhibit low scalability, whereas existing data-driven approaches often fail to adequately balance computational efficiency with the capability to effectively account for dependencies inherent in SFCs. To overcome these limitations, we introduce a Transformer-empowered actor-critic framework specifically designed for sequence-aware SFC partitioning. By utilizing the self-attention mechanism, our approach effectively models complex inter-dependencies among VNFs, facilitating coordinated and parallelized decision-making processes. Additionally, we enhance training stability and convergence using $\epsilon$-LoPe exploration strategy as well as Asymptotic Return Normalization. Comprehensive simulation results demonstrate that the proposed methodology outperforms existing state-of-the-art solutions in terms of long-term acceptance rates, resource utilization efficiency, and scalability, while achieving rapid inference. This study not only advances intelligent network orchestration by delivering a scalable and robust solution for SFC partitioning within emerging 6G environments, but also bridging recent advancements in Large Language Models (LLMs) with the optimization of next-generation networks."
2504.19058,"Laminar is the first TCP stack designed for the reconfigurable match-action table (RMT) architecture, widely used in high-speed programmable switches and SmartNICs. Laminar reimagines TCP processing as a pipeline of simple match-action operations, enabling line-rate performance with low latency and minimal energy consumption, while maintaining compatibility with standard TCP and POSIX sockets. Leveraging novel techniques like optimistic concurrency, pseudo segment updates, and bump-in-the-wire processing, Laminar handles the transport logic, including retransmission, reassembly, flow, and congestion control, entirely within the RMT pipeline.We prototype Laminar on an Intel Tofino2 switch, and demonstrate its scalability to terabit speeds, its flexibility, and robustness to network dynamics. Laminar delivers RDMA-equivalent performance, saving up to 16 host CPU cores versus the TAS kernel-bypass TCP stack with short RPC workloads, achieving 1.3$\times$ higher peak throughput at 5$\times$ lower 99.99p tail latency. At scale, Laminar drives nearly $1$Bpps of TCP processing while keeping RPC tail latency near $20\mu s$. For streaming workloads, Laminar achieves $25$Mpps per-core, enough to saturate the line-rate. It significantly benefits real applications: a key-value store on Laminar doubles throughput-per-watt while maintaining a 99.99p tail latency lower than TAS's best case tail latency, and SPDK's NVMe-oTCP reaches RDMA-level efficiency. Demonstrating Laminar's flexibility, we implement TCP stack extensions, including a sequencer API for a linearizable distributed shared log, Timely congestion control, and delayed ACKs. Finally, Laminar generalizes to FPGA SmartNICs, delivering $3\times$ ToNIC's packet rate under equal timing."
2504.1966,"Mixture of Experts (MoE) has emerged as a promising paradigm for scaling model capacity while preserving computational efficiency, particularly in large-scale machine learning architectures such as large language models (LLMs). Recent advances in MoE have facilitated its adoption in wireless networks to address the increasing complexity and heterogeneity of modern communication systems. This paper presents a comprehensive survey of the MoE framework in wireless networks, highlighting its potential in optimizing resource efficiency, improving scalability, and enhancing adaptability across diverse network tasks. We first introduce the fundamental concepts of MoE, including various gating mechanisms and the integration with generative AI (GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive applications of MoE across critical wireless communication scenarios, such as vehicular networks, unmanned aerial vehicles (UAVs), satellite communications, heterogeneous networks, integrated sensing and communication (ISAC), and mobile edge networks. Furthermore, key applications in channel prediction, physical layer signal processing, radio resource management, network optimization, and security are thoroughly examined. Additionally, we present a detailed overview of open-source datasets that are widely used in MoE-based models to support diverse machine learning tasks. Finally, this survey identifies crucial future research directions for MoE, emphasizing the importance of advanced training techniques, resource-aware gating strategies, and deeper integration with emerging 6G technologies."
2504.20246,"Low-latency applications like AR/VR and online gaming need fast, stable connections. New technologies such as V2X, LEO satellites, and 6G bring unique challenges in mobility management. Traditional solutions based on centralized or distributed anchors often fall short in supporting rapid mobility due to inefficient routing, low versatility, and insufficient multi-access support. In this paper, we design a new end-to-end system for tracking multi-connected mobile devices at scale and optimizing performance for latency-sensitive, highly dynamic applications. Our system, based on the locator/ID separation principle, extends to multi-access networks without requiring specialized routers or caching. Using a novel tree embedding-based overlay, we enable fast session setup while allowing endpoints to directly handle mobility between them. Evaluation with real network data shows our solution cuts connection latency to 7.42% inflation over the shortest path, compared to LISP's 359\% due to cache misses. It also significantly reduces location update overhead and disruption time during mobility."
2504.20335,"Caches are fundamental to latency-sensitive systems like Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit phenomenon where multiple requests for an object occur during its fetch from the remote server after a miss significantly inflates user-perceived latency. While recent algorithms acknowledge delayed hits by estimating the resulting aggregate delay, they predominantly focus on its mean value. We identify and demonstrate that such approaches are insufficient, as the real aggregate delay frequently exhibits substantial variance in the true production system, leading to suboptimal latency performance when ignored. Thus, we propose VA-CDH, a variance-aware method to optimize latency for caching with delayed hits. It employs a novel ranking function that explicitly incorporates both the empirically estimated mean and standard deviation of aggregate delay, allowing caching decisions to account for its variation. We derive the analytical distribution of aggregate delay under Poisson arrivals as a theoretical contribution, offering more statistical insight beyond the mean value. Through the simulations conducted on synthetic and real-world datasets, we show that VA-CDH reduces the total latency by 1%-6% approximately compared to state-of-the-art algorithms."
2504.20433,"Fiber to the Room (FTTR) is a next-generation access network designed to deliver high bandwidth, low latency, and room-level optical coverage. This paper presents a comprehensive analysis of the FTTR system architecture and protocol stack, focusing on three key technical aspects: centralized scheduling and control, integrated management and maintenance, and green energy-saving mechanisms. A simplified FTTR architecture based on the convergence of the medium access control (MAC) and physical (PHY) layers is introduced to enhance coordination and scheduling efficiency. An extended remote management scheme, based on the optical network unit management and control interface (OMCI), is described to enable unified control across main fiber units (MFUs) and sub-fiber units (SFUs). Furthermore, a service-aware energy-saving framework is discussed for dynamic power optimization. The paper also explores the integration of artificial intelligence (AI) and passive sensing into FTTR systems to support intelligent scheduling, energy management, and environment-aware optimization. These insights provide technical guidance for the scalable deployment and future evolution of FTTR networks."
2504.20545,"For future large scale robotic moon missions, the availability of infrastructure-less, cheap and low power real-time locating systems (RTLSs) is critical. Traditional RTLS face significant trade-offs between power consumption and localization latency, often requiring anchors to be connected to the power grid or sacrificing speed for energy efficiency. This paper proposes WakeLoc, an on-demand RTLS based on ultra-wideband (UWB), enabling both low-latency and ultra-low power consumption by leveraging UWB wake-up radios (WuRs). In WakeLoc, tags independently start a localization procedure by sending a wake-up call (WuC) to anchors, before performing the actual localization. Distributed tags equipped with WuRs listen to the WuC and use passive listening of the UWB messages to determine their own position. Experimental measurements demonstrate that the localization accuracy in a 2D setup achieves less than 12.9cm error, both for the active and the passive tag. Additional power simulations based on real-world measurements were performed in a realistic environment, showing that anchors can achieve a power consumption as low as 15.53{\mu}W while the RTLS performs one on-demand localization per minute for 5 tags, thus operate up to 5.01 years on a single coin cell battery (690mWh)."
2504.20729,"Flow records, that summarize the characteristics of traffic flows, represent a practical and powerful way to monitor a network. While they already offer significant compression compared to full packet captures, their sheer volume remains daunting, especially for large Internet Service Providers (ISPs). In this paper, we investigate several lossy compression techniques to further reduce storage requirements while preserving the utility of flow records for key tasks, such as predicting the domain name of contacted servers. Our study evaluates scalar quantization, Principal Component Analysis (PCA), and vector quantization, applied to a real-world dataset from an operational campus network. Results reveal that scalar quantization provides the best tradeoff between compression and accuracy. PCA can preserve predictive accuracy but hampers subsequent entropic compression, and while vector quantization shows promise, it struggles with scalability due to the high-dimensional nature of the data. These findings result in practical strategies for optimizing flow record storage in large-scale monitoring scenarios."
2504.20854,"This paper lays the foundation for Genie, a testing framework that captures the impact of real hardware network behavior on ML workload performance, without requiring expensive GPUs. Genie uses CPU-initiated traffic over a hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim simulator to model interaction between the network and the ML workload."
2504.20939,"Semantic communication (SemCom) aims to enhance the resource efficiency of next-generation networks by transmitting the underlying meaning of messages, focusing on information relevant to the end user. Existing literature on SemCom primarily emphasizes learning the encoder and decoder through end-to-end deep learning frameworks, with the objective of minimizing a task-specific semantic loss function. Beyond its influence on the physical and application layer design, semantic variability across users in multi-user systems enables the design of resource allocation schemes that incorporate user-specific semantic requirements. To this end, \emph{a semantic-aware resource allocation} scheme is proposed with the objective of maximizing transmission and semantic reliability, ultimately increasing the number of users whose semantic requirements are met. The resulting resource allocation problem is a non-convex mixed-integer nonlinear program (MINLP), which is known to be NP-hard. To make the problem tractable, it is decomposed into a set of sub-problems, each of which is efficiently solved via geometric programming techniques. Finally, simulations demonstrate that the proposed method improves user satisfaction by up to $17.1\%$ compared to state of the art methods based on quality of experience-aware SemCom methods."
2504.21181,"This paper presents a comprehensive evaluation of network performance in software defined networking (SDN)-based low Earth orbit (LEO) satellite networks, focusing on the Telesat Lightspeed constellation. We propose a green traffic engineering (TE) approach leveraging segment routing IPv6 (SRv6) to enhance energy efficiency. Through simulations, we analyze the impact of SRv6, multi-protocol label switching (MPLS), IPv4, and IPv6 with open shortest path first (OSPF) on key network performance metrics, including peak and average CPU usage, memory consumption, packet delivery rate (PDR), and packet overhead under varying traffic loads. Results show that the proposed green TE approach using SRv6 achieves notable energy efficiency, maintaining lower CPU usage and high PDR compared to traditional protocols. While SRv6 and MPLS introduce slightly higher memory usage and overhead due to their advanced configurations, these trade-offs remain manageable. Our findings highlight SRv6 with green TE as a promising solution for optimizing energy efficiency in LEO satellite networks, contributing to the development of more sustainable and efficient satellite communications."
2504.21311,"This paper investigates covert prompt transmission for secure and efficient large language model (LLM) services over wireless networks. We formulate a latency minimization problem under fidelity and detectability constraints to ensure confidential and covert communication by jointly optimizing the transmit power and prompt compression ratio. To solve this problem, we first propose a prompt compression and encryption (PCAE) framework, performing surprisal-guided compression followed by lightweight permutation-based encryption. Specifically, PCAE employs a locally deployed small language model (SLM) to estimate token-level surprisal scores, selectively retaining semantically critical tokens while discarding redundant ones. This significantly reduces computational overhead and transmission duration. To further enhance covert wireless transmission, we then develop a group-based proximal policy optimization (GPPO) method that samples multiple candidate actions for each state, selecting the optimal one within each group and incorporating a Kullback-Leibler (KL) divergence penalty to improve policy stability and exploration. Simulation results show that PCAE achieves comparable LLM response fidelity to baseline methods while reducing preprocessing latency by over five orders of magnitude, enabling real-time edge deployment. We further validate PCAE effectiveness across diverse LLM backbones, including DeepSeek-32B, Qwen-32B, and their smaller variants. Moreover, GPPO reduces covert transmission latency by up to 38.6\% compared to existing reinforcement learning strategies, with further analysis showing that increased transmit power provides additional latency benefits."
2504.21444,"Immersive communication, including emerging augmented reality, virtual reality, and holographic telepresence, has been identified as a key service for enabling next-generation wireless applications. To align with legacy wireless applications, such as enhanced mobile broadband or ultra-reliable low-latency communication, network slicing has been widely adopted. However, attempting to statistically isolate the above types of wireless applications through different network slices may lead to throughput degradation and increased queue backlog. To address these challenges, we establish a unified QoS-aware framework that supports immersive communication and legacy wireless applications simultaneously. Based on the Lyapunov drift theorem, we transform the original long-term throughput maximization problem into an equivalent short-term throughput maximization weighted by virtual queue length. Moreover, to cope with the challenges introduced by the interaction between large-timescale network slicing and short-timescale resource allocation, we propose an adaptive adversarial slicing (Ad2S) scheme for networks with invarying channel statistics. To track the network channel variations, we also propose a measurement extrapolation-Kalman filter (ME-KF)-based method and refine our scheme into Ad2S-non-stationary refinement (Ad2S-NR). Through extended numerical examples, we demonstrate that our proposed schemes achieve 3.86 Mbps throughput improvement and 63.96% latency reduction with 24.36% convergence time reduction. Within our framework, the trade-off between total throughput and user service experience can be achieved by tuning systematic parameters."
2504.21549,"How to efficiently perform network tomography is a fundamental problem in network management and monitoring. A network tomography task usually consists of applying multiple probing experiments, e.g., across different paths or via different casts (e.g., unicast and multicast). We study how to optimize the network tomography process through online sequential decision-making. From the methodology perspective, we introduce an online probe allocation algorithm that sequentially performs network tomography based on the principles of optimal experimental design and the maximum likelihood estimation. We rigorously analyze the regret of the algorithm under the conditions that i) the optimal allocation is Lipschitz continuous in the parameters being estimated and ii) the parameter estimators satisfy a concentration property. From the application perspective, we present two case studies: a) the classical lossy packet-switched network and b) the quantum bit-flip network. We show that both cases fulfill the two theoretical conditions and provide their corresponding regrets when deploying our proposed online probe allocation algorithm. Besides case studies with theoretical guarantees, we also conduct simulations to compare our proposed algorithm with existing methods and demonstrate our algorithm's effectiveness in a broader range of scenarios. In an experiment on the Roofnet topology, our algorithm improves the estimation accuracy by 13.64% compared with the state-of-the-art baseline."
2504.21571,"To improve sustainability, Internet-of-Things (IoT) is increasingly adopting battery-free devices powered by ambient energy scavenged from the environment. The unpredictable availability of ambient energy leads to device intermittency, bringing critical challenges to device communication and related fundamental operations like data aggregation.We propose FreeBeacon, a novel scheme for efficient communication and data aggregation in battery-free IoT. We argue that the communication challenge between battery-free devices originates from the complete uncertainty of the environment. FreeBeacon is built on the insight that by introducing just a small degree of certainty into the system, the communication problem can be largely simplified. To this end, FreeBeacon first introduces a small number of battery-powered devices as beacons for battery-free devices. Then, FreeBeacon features protocols for battery-free devices to achieve interaction with the beacon and to perform communication efficiently following customized schedules that implement different data aggregation schemes while achieving resilience. We evaluate FreeBeacon with extensive prototype-based experiments and simulation studies. Results show that FreeBeacon can consistently achieve an order of magnitude data aggregation efficiency when compared with the state-of-the-art approaches."
2504.21583,"The rise of the low-altitude economy (LAE) is propelling urban development and emerging industries by integrating advanced technologies to enhance efficiency, safety, and sustainability in low-altitude operations. The widespread adoption of unmanned aerial vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft plays a crucial role in enabling key applications within LAE, such as urban logistics, emergency rescue, and aerial mobility. However, unlike traditional UAV networks, LAE networks encounter increased airspace management demands due to dense flying nodes and potential interference with ground communication systems. In addition, there are heightened and extended security risks in real-time operations, particularly the vulnerability of low-altitude aircraft to cyberattacks from ground-based threats. To address these, this paper first explores related standards and core architecture that support the development of LAE networks. Subsequently, we highlight the integration of technologies such as communication, sensing, computing, positioning, navigation, surveillance, flight control, and airspace management. This synergy of multi-technology drives the advancement of real-world LAE applications, particularly in improving operational efficiency, optimizing airspace usage, and ensuring safety. Finally, we outline future research directions for LAE networks, such as intelligent and adaptive optimization, security and privacy protection, sustainable energy and power management, quantum-driven coordination, generative governance, and three-dimensional (3D) airspace coverage, which collectively underscore the potential of collaborative technologies to advance LAE networks."
2504.21656,"In the future wireless networks, terrestrial, aerial, space, and maritime wireless networks are integrated into a unified network to meet the needs of a fully connected global network. Nowadays, vehicular communication has become one of the challenging applications of wireless networks. In this article, we aim to address the radio resource management in Cellular V2X (C-V2X) networks using Unmanned Aerial Vehicles (UAV) and Non-orthogonal multiple access (NOMA). The goal of this problem is to maximize the spectral efficiency of vehicular users in Cellular Vehicle-to-Everything (C-V2X) networks under a fronthaul constraint. To solve this problem, a two-stage approach is utilized. In the first stage, vehicles in dense area are clustered based on their geographical locations, predicted location of vehicles, and speeds. Then UAVs are deployed to serve the clusters. In the second stage, NOMA groups are formed within each cluster and radio resources are allocated to vehicles based on NOMA groups. An optimization problem is formulated and a suboptimal method is used to solve it. The performance of the proposed method is evaluated through simulations where results demonstrate superiority of proposed method in spectral efficiency, min point, and distance."
2504.21721,"Backpressure (BP) routing and scheduling is a well-established resource allocation method for wireless multi-hop networks, known for its fully distributed operations and proven maximum queue stability. Recent advances in shortest path-biased BP routing (SP-BP) mitigate shortcomings such as slow startup and random walk, but exclusive link-level commodity selection still suffers from the last-packet problem and bandwidth underutilization. Moreover, classic BP routing implicitly assumes single-input-single-output (SISO) transceivers, which can lead to the same packets being scheduled on multiple outgoing links for multiple-input-multiple-output (MIMO) transceivers, causing detouring and looping in MIMO networks. In this paper, we revisit the foundational Lyapunov drift theory underlying BP routing and demonstrate that exclusive commodity selection is unnecessary, and instead propose a Max-Utility link-sharing method. Additionally, we generalize MaxWeight scheduling to MIMO networks by introducing attributed capacity hypergraphs (ACH), an extension of traditional conflict graphs for SISO networks, and by incorporating backlog reassignment into scheduling iterations to prevent redundant packet routing. Numerical evaluations show that our approach substantially mitigates the last-packet problem in state-of-the-art (SOTA) SP-BP under lightweight traffic, and slightly expands the network capacity region for heavier traffic."
2505.00138,"For a given set of transmitters such as cellular base stations or WiFi access points, is it possible to analytically characterize the set of locations that are ""covered"" in the sense that users at these locations experience a certain minimum quality of service? In this paper, we affirmatively answer this question, by providing explicit simple outer bounds and estimates for the coverage manifold. The key geometric elements of our analytical method are the Q cells, defined as the intersections of a small number of disks. The Q cell of a transmitter is an outer bound to the service region of the transmitter, and, in turn, the union of Q cells is an outer bound to the coverage manifold. In infinite networks, connections to the meta distribution of the signal-to-interference ratio allow for a scaling of the Q cells to obtain accurate estimates of the coverage manifold."
2505.00321,"Large artificial intelligence models (LAMs) possess human-like abilities to solve a wide range of real-world problems, exemplifying the potential of experts in various domains and modalities. By leveraging the communication and computation capabilities of geographically dispersed edge devices, edge LAM emerges as an enabling technology to empower the delivery of various real-time intelligent services in 6G. Unlike traditional edge artificial intelligence (AI) that primarily supports a single task using small models, edge LAM is featured by the need of the decomposition and distributed deployment of large models, and the ability to support highly generalized and diverse tasks. However, due to limited communication, computation, and storage resources over wireless networks, the vast number of trainable neurons and the substantial communication overhead pose a formidable hurdle to the practical deployment of edge LAMs. In this paper, we investigate the opportunities and challenges of edge LAMs from the perspectives of model decomposition and resource management. Specifically, we propose collaborative fine-tuning and full-parameter training frameworks, alongside a microservice-assisted inference architecture, to enhance the deployment of edge LAM over wireless networks. Additionally, we investigate the application of edge LAM in air-interface designs, focusing on channel prediction and beamforming. These innovative frameworks and applications offer valuable insights and solutions for advancing 6G technology."
2505.00447,"Wi-Fi networks traditionally use Distributed Coordination Function (DCF) that employs CSMA/CA along with the binary backoff mechanism for channel access. This causes unavoidable contention overheads and does not provide performance guarantees. In this work, we outline some issues that occur with the probabilistic channel access in highly congested scenarios and how those can be mitigated using deterministic scheduling. Towards this, we propose to use Target Wake Time (TWT) - a feature introduced in Wi-Fi 6 as a power-saving mechanism, to improve the performance of Wi-Fi. To gain insights into the workings of the TWT over commercially available off-the-shelf components and to analyze the factors that affect its performance, we carry out various experiments with it over our Wi-Fi 6 testbed. Using these insights and analysis, we formulate and solve an optimization problem to synthesize deterministic schedules and obtain the optimal values of various system parameters. Lastly, we configure our testbed with these optimal parameter values and show that the TWT based deterministic scheduling consistently results in better performance of the TWT-capable clients and overall system performance compared to traditional CSMA/CA based scheduling."
2505.00605,"The development of Open Radio Access Networks (Open RAN), with their disaggregated architectures and virtualization of network functions, has brought considerable flexibility and cost savings to mobile networks. However, these architectural advancements introduce additional latency during the initial attachment procedure of User Equipment (UE), increasing the risk of signaling storms. This paper investigates the latency impact due to disaggregation of the Base-band Unit (BBU) into the Central Unit (CU) and Distributed Unit (DU). Specifically, we model the delays induced due to disaggregation on UE attachment, analyzing the performance under varying load conditions, and sensitivity to processing times. We demonstrate that while both monolithic and Open RAN architectures experience performance degradation under high-load conditions, Open RAN's added overheads can increase its susceptibility to congestion and signaling storms. However, Open RAN's inherent flexibility, enabled by disaggregation and virtualization, allows efficient deployment of resources, faster service deployment, and adaptive congestion control mechanisms to mitigate these risks and enhance overall system resilience. Thereby, we quantify resilience by introducing a new utility function and propose a novel adaptation mechanism to reinforce Open RAN's robustness against signaling storms. Our results show that the proposed adaptive mechanism significantly enhances resilience, achieving improvements of up to 286% over fixed configurations, with resilience scores approaching 0.96 under optimal conditions. While simulation results show that Open RAN disaggregation increases attachment latency and susceptibility to signaling congestion, they also highlight that its architectural flexibility can mitigate these effects, improving resilience under high-load conditions."
2505.00848,"This paper introduces a novel computational approach for offloading sensor data processing tasks to servers in edge networks for better accuracy and makespan. A task is assigned with one of several offloading options, each comprises a server, a route for uploading data to the server, and a service profile that specifies the performance and resource consumption at the server and in the network. This offline offloading and routing problem is formulated as mixed integer programming (MIP), which is non-convex and HP-hard due to the discrete decision variables associated to the offloading options. The novelty of our approach is to transform this non-convex problem into iterative convex optimization by relaxing integer decision variables into continuous space, combining primal-dual optimization for penalizing constraint violations and reweighted $L_1$-minimization for promoting solution sparsity, which achieves better convergence through a smoother path in a continuous search space. Compared to existing greedy heuristics, our approach can achieve a better Pareto frontier in accuracy and latency, scales better to larger problem instances, and can achieve a 7.72--9.17$\times$ reduction in computational overhead of scheduling compared to the optimal solver in hierarchically organized edge networks with 300 nodes and 50--100 tasks."
2505.01185,"LoRaWAN technology's extensive coverage positions it as a strong contender for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor localization remains challenging due to complex environmental conditions, multipath fading, and transient obstructions. This paper proposes a lightweight but robust approach combining adaptive filtering with an extended log-distance, multi-wall path loss and shadowing (PLS) model. Our methodology augments conventional models with critical LoRaWAN parameters (received signal strength indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic environmental indicators (temperature, humidity, carbon dioxide, particulate matter, and barometric pressure). An adaptive Kalman filter reduces RSSI fluctuations, isolating persistent trends from momentary noise. Using a six-month dataset of 1,328,334 field measurements, we evaluate three models: the baseline COST 231 multi-wall model (MWM), the baseline model augmented with environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP (10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation reduces systematic errors by 41.22%, while Kalman filtering significantly enhances robustness under high RSSI volatility by 42.63%, on average across all devices. These findings present an interpretable, efficient solution for precise indoor LoRaWAN localization in dynamically changing environments."
2505.01222,"While UE-centric cell-free massive MIMO (CF-mMIMO) provides high and uniform throughput performance under the assumption of a uniform propagation environment modeled by the log-distance path loss channel model, the performance under a realistic urban propagation environment is not yet fully addressed. In this paper we conduct the first comparative performance study of CF-mMIMO under both the widely assumed log-distance channel model and the realistic urban propagation environment obtained via raytracing using real 3D city layouts and practical AP locations. Our results show that with the raytracing channel model, CF-mMIMO cannot achieve as high and uniform throughput performance as observed with the log-distance channel model, putting into question the attractiveness in practice of CF-mMIMO for real urban deployments."
2505.01689,"The rapid expansion of Internet of Things (IoT) deployments demands wireless protocols that combine high scalability with robust performance. Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) extends LoRaWAN by increasing capacity and resilience through frequency hopping and redundancy. However, current deployments require packet reconstruction at a single gateway, limiting the benefits of LR-FHSS. This paper proposes a macro-diversity reception strategy where multiple gateways collectively receive and combine payload fragments. We develop a stochastic geometry-based analytical model that captures the impact of header repetition, payload fragmentation, and coding redundancy. Closed-form expressions quantify success probabilities under interference, and numerical evaluations demonstrate significant capacity gains over nearest-gateway reception. These results highlight the potential of fragment-level macro-diversity to improve scalability and reliability in future LPWAN deployments."
2505.01796,"The integration of Terrestrial and Non-Terrestrial Networks (TN-NTNs), which was introduced in 5G, is progressing toward a unified and seamless network of networks in Sixth-Generation (6G). This evolution leads to a significant increase in the volume of generated and communicated data, imposing technical and operational requirements accompanied by a higher cost and energy consumption. Efficiently managing the generation and transmission of data in these highly complex unified networks has become essential. In this article, we investigate the semantics-aware information handling problem within unified TN-NTNs, where data communication between the distant TN nodes is enabled via an NTN. To this end, an Internet of Things (IoT) monitoring system is employed, where status updates from a remote IoT device are communicated to a destination monitor via a constellation of Low Earth Orbit (LEO) satellites. We leverage semantic metrics that capture the timeliness, relevance, and utility of information to provide the most informative data for timely and informed decision-making and eventually reduce the volume of transmitted and processed data. The outcome is significantly lower energy consumption, memory, control, and processing requirements (up to 73% lower energy charging demands compared to the state-of-the-art), all without compromising the conveyed information."
2505.01834,"Large Language Models (LLMs) exhibit strong general-purpose reasoning abilities but lack access to wireless environment information due to the absence of native sensory input and domain-specific priors. Previous attempts to apply LLMs in wireless systems either depend on retraining with network-specific data, which compromises language generalization, or rely on manually scripted interfaces, which hinder scalability. To overcome these limitations, we propose a Model Context Protocol (MCP)-based Internet of Experts (IoX) framework that equips LLMs with wireless environment-aware reasoning capabilities. The framework incorporates a set of lightweight expert models, each trained to solve a specific deterministic task in wireless communications, such as detecting a specific wireless attribute, e.g., line-of-sight propagation, Doppler effects, or fading conditions. Through MCP, the LLM can selectively query and interpret expert outputs at inference time, without modifying its own parameters. This architecture enables modular, extensible, and interpretable reasoning over wireless contexts. Evaluated across multiple mainstream LLMs, the proposed wireless environment-aware LLM agents achieve 40%-50% improvements in classification tasks over LLM-only baselines. More broadly, the MCP-based design offers a viable paradigm for future LLMs to inherit structured wireless network management capabilities."
2505.01841,"Intent-driven network management is critical for managing the complexity of 5G and 6G networks. It enables adaptive, on-demand management of the network based on the objectives of the network operators. In this paper, we propose an innovative three-step framework for intent-driven network management based on Generative AI (GenAI) algorithms. First, we fine-tune a Large Language Model (LLM) on a custom dataset using a Quantized Low-Rank Adapter (QLoRA) to enable memory-efficient intent processing within limited computational resources. A Retrieval Augmented Generation (RAG) module is included to support dynamic decision-making. Second, we utilize a transformer architecture for time series forecasting to predict key parameters, such as power consumption, traffic load, and packet drop rate, to facilitate intent validation proactively. Lastly, we introduce a Hierarchical Decision Transformer with Goal Awareness (HDTGA) to optimize the selection and orchestration of network applications and hence, optimize the network. Our intent guidance and processing approach improves BERTScore by 6% and the semantic similarity score by 9% compared to the base LLM model. Again, the proposed predictive intent validation approach can successfully rule out the performance-degrading intents with an average of 88% accuracy. Finally, compared to the baselines, the proposed HDTGA algorithm increases throughput at least by 19.3%, reduces delay by 48.5%, and boosts energy efficiency by 54.9%."
2505.02513,"Inter-provider agreements are central to 6G networks, where administrative domains must securely and dynamically share services. To address the dual need for transparency and confidentiality, we propose a privacy-enabled hybrid blockchain setup using Hyperledger Besu, integrating both public and private transaction workflows. The system enables decentralized service registration, selection, and SLA breach reporting through role-based smart contracts and privacy groups. We design and deploy a proof-of-concept implementation, evaluating performance using end-to-end latency as a key metric within privacy groups. Results show that public interactions maintain stable latency, while private transactions incur additional overhead due to off-chain coordination. The block production rate governed by IBFT 2.0 had limited impact on private transaction latency, due to encryption and peer synchronization. Lessons learned highlight design considerations for smart contract structure, validator management, and scalability patterns suitable for dynamic inter-domain collaboration. Our findings offer practical insights for deploying trustworthy agreement systems in 6G networks using privacy-enabled hybrid blockchains."
2505.02565,"Antifragility of communication systems is defined as measure of benefits gained from the adverse events and variability of its environment. In this paper, we introduce the notion of antifragility in Reconfigurable Intelligent Surface (RIS) assisted communication systems affected by a jamming attack. We analyzed the antifragility of the two hop systems, where the wireless path contains source node, RIS, destination node, and a eavesdropping/jamming node. We propose and analyze the antifragility performance for several jamming models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude shifting. Our paper shows that antifragility throughput can indeed be achieved under certain power thresholds and for various jamming models. In particular, high jamming power combined with low baseline data rates yields an antifragile gain factor of approximately five times. The results confirm that reconfigurable intelligent surfaces, when coupled with an antifragile design philosophy, can convert hostile interference from a liability into a throughput gain."
2505.03016,"The rapid growth of Internet-of-things (IoT) devices, smart vehicles, and other connected objects is driving demand for ubiquitous connectivity and intensive computing capacity. 5G and upcoming 6G networks are crucial to meeting these demands and the fast-evolving services and applications. However, traditional terrestrial networks face limitations in coverage and capacity. Integrated Terrestrial and Non-Terrestrial Networks (ITNTN) are emerging to address these challenges. In essence, ITNTN combines ground-based infrastructure with aerial, space, and water surface networks to provide seamless connectivity and computing resources anytime, anywhere. Given the stringent quality-of-service (QoS) of future services, edge computing will be an inseparable component of ITNTN. Consequently, we dive in this survey into current efforts of integrating cloud/fog/edge computing into ITNTN layers to facilitate stringent QoS services and address the data processing needs of modern applications. Since there have been only limited and partial efforts in integrating computing functionalities within ITNTN, we aim to extend the discussion to the full integration of computing and identifying the challenges and future research directions to achieve it."
2505.03196,"Large Language Models (LLMs) demonstrate strong potential across a variety of tasks in communications and networking due to their advanced reasoning capabilities. However, because different LLMs have different model structures and are trained using distinct corpora and methods, they may offer varying optimization strategies for the same network issues. Moreover, the limitations of an individual LLM's training data, aggravated by the potential maliciousness of its hosting device, can result in responses with low confidence or even bias. To address these challenges, we propose a blockchain-enabled collaborative framework that connects multiple LLMs into a Trustworthy Multi-LLM Network (MultiLLMN). This architecture enables the cooperative evaluation and selection of the most reliable and high-quality responses to complex network optimization problems. Specifically, we begin by reviewing related work and highlighting the limitations of existing LLMs in collaboration and trust, emphasizing the need for trustworthiness in LLM-based systems. We then introduce the workflow and design of the proposed Trustworthy MultiLLMN framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G communication systems and the difficulty of addressing such threats through traditional modeling techniques, we present FBS defense as a case study to empirically validate the effectiveness of our approach. Finally, we outline promising future research directions in this emerging area."
2505.03375,"Wi-Fi sensing is an emerging technology that uses channel state information (CSI) from ambient Wi-Fi signals to monitor human activity without the need for dedicated sensors. Wi-Fi sensing does not only represent a pivotal technology in intelligent Internet of Things (IoT) systems, but it can also provide valuable insights in forensic investigations. However, the high dimensionality of CSI data presents major challenges for storage, transmission, and processing in resource-constrained IoT environments. In this paper, we investigate the impact of lossy compression on the accuracy of Wi-Fi sensing, evaluating both traditional techniques and a deep learning-based approach. Our results reveal that simple, interpretable techniques based on principal component analysis can significantly reduce the CSI data volume while preserving classification performance, making them highly suitable for lightweight IoT forensic scenarios. On the other hand, deep learning models exhibit higher potential in complex applications like activity recognition (achieving compression ratios up to 16000:1 with minimal impact on sensing performance) but require careful tuning and greater computational resources. By considering two different sensing applications, this work demonstrates the feasibility of integrating lossy compression schemes into Wi-Fi sensing pipelines to make intelligent IoT systems more efficient and improve the storage requirements in forensic applications."
2505.03409,"Cardiovascular diseases are a leading cause of fatalities worldwide, often occurring suddenly with limited time for intervention. Current healthcare monitoring systems for cardiac patients rely heavily on hospitalization, which can be impractical for continuous monitoring. This paper presents a novel IoT-based solution for remote, real-time tracking of critical cardiac metrics, addressing the pressing need for accessible and continuous healthcare, particularly for the aging population in Pakistan. The proposed IoT kit measures essential parameters such as body temperature, heart rate (HR), blood pressure (BP), oxygen saturation (SPO2), and electrocardiography (ECG).A key innovation of the system is its integration with a cloud-based application, enabling constant remote monitoring and incorporating an alarm mechanism to alert medical professionals for timely intervention, reducing the risk of catastrophic incidents. The system was tested in a clinical environment with 20 participants, demonstrating results closely aligned with those obtained using standard medical devices. The findings validate the system's potential for reliable remote monitoring, offering a significant step forward in proactive cardiac healthcare management. This novel approach combines IoT technology with cloud-based applications to provide a cost-effective and efficient solution for reducing unexpected fatalities among cardiac patients."
2505.03558,"The teleoperated driving (TD) scenario comes with stringent Quality of Service (QoS) communication constraints, especially in terms of end-to-end (E2E) latency and reliability. In this context, Predictive Quality of Service (PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a powerful tool to estimate QoS degradation and react accordingly. For example, an intelligent agent can be trained to select the optimal compression configuration for automotive data, and reduce the file size whenever QoS conditions deteriorate. However, compression may inevitably compromise data quality, with negative implications for the TD application. An alternative strategy involves operating at the Radio Access Network (RAN) level to optimize radio parameters based on current network conditions, while preserving data quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL) scheduling algorithms, based on Proximal Policy Optimization (PPO), to dynamically and intelligently allocate radio resources to minimize E2E latency in a TD scenario. We evaluate two training paradigms, i.e., decentralized learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in conjunction with two resource allocation strategies, i.e., proportional allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that MAPPO, combined with GA, achieves the best results in terms of latency, especially as the number of vehicles increases."
2505.03908,"Millions of flows are routed concurrently through a modern data-center. These networks are often built as Clos topologies, and flow demands are constrained only by the link capacities at the ingress and egress points. The minimum congestion routing problem seeks to route a set of flows through a data center while minimizing the maximum flow demand on any link. This is easily achieved by splitting flow demands along all available paths. However, arbitrary flow splitting is unrealistic. Instead, network operators rely on heuristics for routing unsplittable flows, the best of which results in a worst-case congestion of $2$ (twice the uniform link capacities). But is $2$ the lowest possible congestion? If not, can an efficient routing algorithm attain congestion below $2$?Guided by these questions, we investigate the minimum congestion routing problem in Clos networks with unsplittable flows. First, we show that for some sets of flows the minimum congestion is at least $\nicefrac{3}{2}$, and that it is $NP$-hard to approximate a minimum congestion routing by a factor less than $\nicefrac{3}{2}$. Second, addressing the motivating questions directly, we present a polynomial-time algorithm that guarantees a congestion of at most $\nicefrac{9}{5}$ for any set of flows, while also providing a $\nicefrac{9}{5}$ approximation of a minimum congestion routing. Last, shifting to the online setting, we demonstrate that no online algorithm (even randomized) can approximate a minimum congestion routing by a factor less than $2$, providing a strict separation between the online and the offline setting."
2505.04068,"Covert Communications (CC) can secure sensitive transmissions in industrial, military, and mission-critical applications within 6G wireless networks. However, traditional optimization methods based on Artificial Noise (AN), power control, and channel manipulation might not adapt to dynamic and adversarial environments due to the high dimensionality, nonlinearity, and stringent real-time covertness requirements. To bridge this gap, we introduce Shadow Wireless Intelligence (SWI), which integrates the reasoning capabilities of Large Language Models (LLMs) with retrieval-augmented generation to enable intelligent decision-making in covert wireless systems. Specifically, we utilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning, combined with real-time retrieval of domain-specific knowledge to improve context accuracy and mitigate hallucinations. Our approach develops a structured CC knowledge base, supports context-aware retrieval, and performs semantic optimization, allowing LLMs to generate and adapt CC strategies in real time. In a case study on optimizing AN power in a full-duplex CC scenario, DeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in the generation of simulation code, outperforming baseline models. These results validate SWI as a robust, interpretable, and adaptive foundation for LLM-driven intelligent covert wireless systems in 6G networks."
2505.04098,"The low-altitude economy (LAE) is a new economic paradigm that leverages low-altitude vehicles (LAVs) to perform diverse missions across diverse areas. To support the operations of LAE, it is essential to establish LAE networks that enable LAV management andthis http URLstudies mainly reuse terrestrial networks to construct LAE networks. However, the limited coverage of terrestrial networks poses challenges for serving LAVs in remote areas. Besides, efficient LAV operations also require support such as localization and navigation, which terrestrial networks designed for communications cannot fully provide. Due to ubiquitous coverage and diverse functions, satellites are a promising technology to support LAVs. Therefore, this article investigates satellite-assisted LAE networking. First, we introduce an overview of LAE and satellites, discussing their features, applications, and architectures. Next, we investigate opportunities for satellites to assist LAE from aspects of communication, control, and computation. As all assistance depends on reliable satellite-LAV communications, we propose a satellite-assisted LAE framework to tackle issues caused by the severe path loss and high dynamics in satellite-assisted LAEthis http URLcase study demonstrates that the distributed MIMO architecture efficiently reduces the required transmission power and extends service duration, while the two-timescale optimization scheme balances the performance and control signaling overheads. Specifically, the proposed framework comprises distributed satellite MIMO, distributed LAV MIMO, and a two-timescale optimization scheme."
2505.04272,"Computation offloading and resource allocation are critical in mobile edge computing (MEC) systems to handle the massive and complex requirements of applications restricted by limited resources. In a multi-user multi-server MEC network, the mobility of terminals causes computing requests to be dynamically distributed in space. At the same time, the non-negligible dependencies among tasks in some specific applications impose temporal correlation constraints on the solution as well, leading the time-adjacent tasks to experience varying resource availability and competition from parallel counterparts. To address such dynamic spatial-temporal characteristics as a challenge in the allocation of communication and computation resources, we formulate a long-term delay-energy trade-off cost minimization problem in the view of jointly optimizing task offloading and resource allocation. We begin by designing a priority evaluation scheme to decouple task dependencies and then develop a grouped Knapsack problem for channel allocation considering the current data load and channel status. Afterward, in order to meet the rapid response needs of MEC systems, we exploit the double duel deep Q network (D3QN) to make offloading decisions and integrate channel allocation results into the reward as part of the dynamic environment feedback in D3QN, constituting the joint optimization of task offloading and channel allocation. Finally, comprehensive simulations demonstrate the performance of the proposed algorithm in the delay-energy trade-off cost and its adaptability for various applications."
2505.04326,"Digital twins (DT) have received significant attention due to their numerous benefits, such as real-time data analytics and cost reduction in production. DT serves as a fundamental component of many applications, encompassing smart manufacturing, intelligent vehicles, and smart cities. By using Machine Learning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently facilitate decision-making and productivity by simulating the status and changes of a physical entity. To handle the massive amount of data brought by DTs, it is challenging to achieve low response latency for data fetching over existing IP-based networks. IP-based networks use host addresses for end-to-end communication, making data distribution between DTs inefficient. Thus, we propose to use DTs in a distributed manner over Named Data Networking (NDN) networks. NDN is data-centric where data is routed based on content names, dynamically adjusting paths to optimize latency. Popular data is cached in network nodes, reducing data transmission and network congestion. Since data is fetched by content names, users and mobile devices can move freely without IP address reassignment. By using in-network caching and adaptive routing, we reckon NDN is an ideal fit for Future G Networks in the context of Digital Twins. We compared DTs in edge scenarios with cloud scenarios over NDN and IP-based networks to validate our insights. Extensive simulation results show that using DT in the edge reduces response latency by 10.2x. This position paper represents an initial investigation into the gap in distributed DTs over NDN, serving as an early-stage study."
2505.04368,"To support large-scale model training, split learning (SL) enables multiple edge devices/servers to share the intensive training workload. However, most existing works on SL focus solely on two-tier model splitting. Moreover, while some recent works have investigated the model splitting and placement problems for multi-hop SL, these solutions fail to overcome the resource idleness issue, resulting in significant network idle time. In this work, we propose a pipelined SL scheme by addressing the joint optimization problem of model splitting and placement (MSP) in multi-hop edge networks. By applying pipeline parallelism to SL, we identify that the MSP problem can be mapped to a problem of minimizing the weighted sum of a bottleneck cost function (min-max) and a linear cost function (min-sum). Based on graph theory, we devise a bottleneck-aware shortest-path algorithm to obtain the optimal solution. Besides, given the MSP outcomes, we also derive the closed-form solution to the micro-batch size in the pipeline. Finally, we develop an alternating optimization algorithm of MSP and micro-batch size to solve the joint optimization problem to minimize the end-to-end training latency. Extensive simulations have demonstrated the significant advantages of our algorithm compared to existing benchmarks without pipeline parallelism."
2505.04879,"The advent of fifth-generation (5G) and Beyond 5G (B5G) networks introduces diverse service requirements, from ultra-low latency to high bandwidth, demanding dynamic monitoring and advanced solutions to ensure Quality of Service (QoS). The transport network - responsible for interconnecting the radio access network and core networks - will increasingly face challenges in efficiently managing complex traffic patterns. The Network Digital Twin (NDT) concept emerges as a promising solution for testing configurations and algorithms in a virtual network before real-world deployment. In this context, this work designs an experimental platform with NDT in a transport network domain, synchronizing with the virtual counterpart and a recommendation system for what-if analysis, enabling intelligent decision-making for dynamic route optimization problems in 5G/B5G scenarios. Our NDT, composed of a Graph Neural Network (GNN), was evaluated across three different network topologies consisting of 8, 16, and 30 nodes. It achieved lower MAPE values for URLLC and eMBB slices, comparing latency predictions with actual latency after the solution implementation. These values indicate high accuracy, demonstrating the solution's effectiveness in generating precise insights into network performance if a particular solution were implemented."
2505.05067,"As intelligent network services continue to diversify, ensuring efficient and adaptive resource allocation in edge networks has become increasingly critical. Yet the wide functional variations across services often give rise to new and unforeseen optimization problems, rendering traditional manual modeling and solver design both time-consuming and inflexible. This limitation reveals a key gap between current methods and human solving - the inability to recognize and understand problem characteristics. It raises the question of whether problem-aware learning can bridge this gap and support effective cross-problem generalization. To answer this question, we propose a problem-aware diffusion (PAD) model, which leverages a problem-aware learning framework to enable cross-problem generalization. By explicitly encoding the mathematical formulations of optimization problems into token-level embeddings, PAD empowers the model to understand and adapt to problem structures. Extensive experiments across six diverse network optimization problems show that PAD generalizes well to unseen problems while significantly improving solution quality and feasibility. Meanwhile, an auxiliary constraint-aware module is designed to enforce solution validity further. The experiments reveal that problem-aware learning is promising for building general-purpose solvers for intelligent network operation and resource management. Our code is open source atthis https URL."
2505.05149,Space Domain Awareness (SDA) system has different major aspects including continues and robust awareness from the network that is crucial for an efficient control over all actors in space. The observability of the space assets on the other hand requires efficient analysis on when and how observed space objects can be controlled. This becomes crucial when real-world spatial dynamics are taken into account as it introduces complexities into the system. The real-world dynamics can reveal the structure of the network including isolated and dominant stations. We propose a Temporal Spectrum Analysis (TSA) scheme that takes into account a set of real-world parameters including actual dynamics of the objects in space to analyze the structure of a ground-space network that inherits temporal spectrum as the key element of design. We study the potential interactions between multiple constellations using TSA and conduct a comprehensive real-world simulations to quantify the structure of the network. Numerical results show how the temporal spectrum of each satellite affects the intra- and inter-constellation network structure including interactions between ground stations and constellations.
2505.05184,"The advancing industrial digitalization enables evolved process control schemes that rely on accurate models learned through data-driven approaches. While they provide high control performance and are robust to smaller deviations, a larger change in process behavior can pose significant challenges, in the worst case even leading to a damaged process plant. Hence, it is important to frequently assess the fit between the model and the actual process behavior. As the number of controlled processes and associated data volumes increase, the need for lightweight and fast reacting assessment solutions also increases. In this paper, we propose CIVIC, an in-network computing-based solution for Continuous In-situ Validation of Industrial Control models. In short, CIVIC monitors relevant process variables and detects different process states through comparison with a priori knowledge about the desired process behavior. This detection can then be leveraged to, e.g., shut down the process or trigger a reconfiguration. We prototype CIVIC on an Intel Tofino-based switch and apply it to a lab-scale water treatment plant. Our results show that we can achieve a high detection accuracy, proving that such monitoring systems are feasible and sensible."
2505.05366,"RDMA is vital for efficient distributed training across datacenters, but millisecond-scale latencies complicate the design of its reliability layer. We show that depending on long-haul link characteristics, such as drop rate, distance and bandwidth, the widely used Selective Repeat algorithm can be inefficient, warranting alternatives like Erasure Coding. To enable such alternatives on existing hardware, we propose SDR-RDMA, a software-defined reliability stack for RDMA. Its core is a lightweight SDR SDK that extends standard point-to-point RDMA semantics -- fundamental to AI networking stacks -- with a receive buffer bitmap. SDR bitmap enables partial message completion to let applications implement custom reliability schemes tailored to specific deployments, while preserving zero-copy RDMA benefits. By offloading the SDR backend to NVIDIA's Data Path Accelerator (DPA), we achieve line-rate performance, enabling efficient inter-datacenter communication and advancing reliability innovation for inter-datacenter training."
2505.05537,"Open Radio Access Network (Open RAN) is a new paradigm to provide fundamental features for supporting next-generation mobile networks. Disaggregation, virtualisation, closed-loop data-driven control, and open interfaces bring flexibility and interoperability to the network deployment. However, these features also create a new surface for security threats. In this paper, we introduce Key Performance Indicators (KPIs) poisoning attack in Near Real-Time control loops as a new form of threat that can have significant effects on the Open RAN functionality. This threat can arise from traffic spoofing on the E2 interface or compromised E2 nodes. The role of KPIs is explored in the use cases of Near Real-Time control loops. Then, the potential impacts of the attack are analysed. An ML-based approach is proposed to detect poisoned KPI values before using them in control loops. Emulations are conducted to generate KPI reports and inject anomalies into the values. A Long Short-Term Memory (LSTM) neural network model is used to detect anomalies. The results show that more amplified injected values are more accessible to detect, and using more report sequences leads to better performance in anomaly detection, with detection rates improving from 62% to 99%."
2505.05996,"Kubernetes Services such as LoadBalancer and NodePort expose applications running on pods within a Kubernetes cluster to external users. While the LoadBalancer Service requires an external load-balancing middleware, its alternative, NodePort Service, adds additional hops on the path between clients and the worker nodes. In this paper, we propose P4Kube, a framework consisting of a P4 data plane program and a Kubernetes plugin. Our solution effectively performs load balancing of requests to the worker nodes of a cluster based on the number of running replicas. In P4Kube, the data packets completely bypass the system's control plane. Unlike the previous work, to update its state, the P4Kube data plane works directly with the Kubernetes control plane without any involvement of the network control plane. Our experiments show up to 50% improvement in the average request time to the cluster compared to conventional approaches."
2505.06025,"Timely and efficient dissemination of service information is critical in compute-first networking systems, where user requests arrive dynamically and computing resources are constrained. In such systems, the access point (AP) plays a key role in forwarding user requests to a server based on its latest received service information. This paper considers a single-source, single-destination system and introduces an Age-and-Value-Aware (AVA) metric that jointly captures both the timeliness and the task relevance of service information. Unlike traditional freshness-based metrics, AVA explicitly incorporates variations in server-side service capacity and AP forwarding decisions, allowing more context-aware update evaluation. Building upon AVA, we propose a reinforcement learning-based update policy that learns to selectively transmit service information updates to the AP. It aims to maximize overall task success while minimizing unnecessary communications. Extensive simulations under diverse user request patterns and varying service capacities demonstrate that AVA reduces the update frequency by over 90% on average compared to baselines, with reductions reaching 98% in certain configurations. Crucially, this reduction is achieved without compromising the accuracy of task execution or the quality of decision making."
2505.06041,"Single Root Input/Output Virtualization (SR-IOV) is a standard technology for forking a single PCI express device and providing it to applications while ensuring performance isolation. It enables container orchestrators to share a limited number of physical network interfaces without incurring significant virtualization overhead. The allocation of virtualized network devices to containers, however, needs to be more configurable based on the bandwidth needs of running applications. Moreover, container orchestrators' network control over the virtualized interfaces is limited by the abilities of SR-IOV. We explore the design considerations for a system with controlled SR-IOV virtualization and present ConRDMA, a novel architecture that enables fine control of RDMA virtualization for containers. Our evaluation shows that ConRDMA enables containers to use RDMA allocated bandwidth more efficiently and to select best-suited nodes to meet their varying communication requirements."
2505.06375,"This paper presents a comprehensive dataset of LoRaWAN technology path loss measurements collected in an indoor office environment, focusing on quantifying the effects of environmental factors on signal propagation. Utilizing a network of six strategically placed LoRaWAN end devices (EDs) and a single indoor gateway (GW) at the University of Siegen, City of Siegen, Germany, we systematically measured signal strength indicators such as the Received Signal Strength Indicator (RSSI) and the Signal-to-Noise Ratio (SNR) under various environmental conditions, including temperature, relative humidity, carbon dioxide (CO$_2$) concentration, barometric pressure, and particulate matter levels (PM$_{2.5}$). Our empirical analysis confirms that transient phenomena such as reflections, scattering, interference, occupancy patterns (induced by environmental parameter variations), and furniture rearrangements can alter signal attenuation by as much as 10.58 dB, highlighting the dynamic nature of indoor propagation. As an example of how this dataset can be utilized, we tested and evaluated a refined Log-Distance Path Loss and Shadowing Model that integrates both structural obstructions (Multiple Walls) and Environmental Parameters (LDPLSM-MW-EP). Compared to a baseline model that considers only Multiple Walls (LDPLSM-MW), the enhanced approach reduced the root mean square error (RMSE) from 10.58 dB to 8.04 dB and increased the coefficient of determination (R$^2$) from 0.6917 to 0.8222. By capturing the extra effects of environmental conditions and occupancy dynamics, this improved model provides valuable insights for optimizing power usage and prolonging device battery life, enhancing network reliability in indoor Internet of Things (IoT) deployments, among other applications. This dataset offers a solid foundation for future research and development in indoor wireless communication."
2505.06678,"Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, further enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to both uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, contract theory is employed to model the information asymmetry, while DRO is utilized to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unity-based teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7\% to 10.74\% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02\% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory. The code and data are publicly available atthis https URL."
2505.06764,"In the rapidly evolving landscape of 5G and B5G (beyond 5G) networks, efficient resource optimization is critical to addressing the escalating demands for high-speed, low-latency, and energy efficient communication. This study explores the integration of Radio Frequency Identification (RFID) technology as a novel approach to enhance resource management in 5G/B5G networks. The motivation behind this research lies in overcoming persistent challenges such as spectrum congestion, high latency, and inefficient load balancing, which impede the performance of traditional resource allocation methods. To achieve this, RFID tags were embedded in critical network components, including user devices, base stations, and Internet of Things (IoT) nodes, enabling the collection of real-time data on device status, location, and resource utilization. RFID readers strategically placed across the network continuously captured this data, which was processed by a centralized controller using a custom-designed optimization algorithm. This algorithm dynamically managed key network resources, including spectrum allocation, load balancing, and energy consumption, ensuring efficient operation under varying network conditions. Simulations were conducted to evaluate the performance of the RFID-based model against traditional 4G dynamic resource allocation techniques. The results demonstrated substantial improvements in key performance metrics."
2505.06789,"The fifth generation of cellular technology (5G) delivers faster speeds, lower latency, and improved network service alongside support for a large number of users and a diverse range of verticals. This brings increased complexity to network control and management, making closed-loop automation essential. In response, the 3rd Generation Partnership Project (3GPP) introduced the Network Data Analytics Function (NWDAF) to streamline network monitoring by collecting, analyzing, and providing insights from network data. While prior research has focused mainly on isolated applications of machine learning within NWDAF, critical aspects such as standardized data collection, analytics integration in closed-loop automation, and end-to-end system evaluation have received limited attention. This work addresses existing gaps by presenting a practical implementation of NWDAF and its integration with leading open-source 5G core network solutions. We develop a 3GPP-compliant User Plane Function (UPF) event exposure service for real-time data collection and an ML model provisioning service integrated with MLflow to support end-to-end machine learning lifecycle management. Additionally, we enhance the Session Management Function (SMF) to consume NWDAF analytics and respond accordingly. Our evaluation demonstrates the solution's scalability, resource efficiency, and effectiveness in enabling closed-loop security management in 5G networks."
2505.06899,"Existing blockchain sharding protocols have focused on eliminating imbalanced workload distributions. However, even with workload balance, disparities in processing capabilities can lead to differential stress among shards, resulting in transaction backlogs in certain shards. Therefore, achieving stress balance among shards in the dynamic and heterogeneous environment presents a significant challenge of blockchain sharding. In this paper, we propose ContribChain, a blockchain sharding protocol that can automatically be aware of node contributions to achieve stress balance. We calculate node contribution values based on the historical behavior to evaluate the performance and security of nodes. Furthermore, we propose node allocation algorithm NACV and account allocation algorithm P-Louvain, which both match shard performance with workload to achieve stress balance. Finally, we conduct extensive experiments to compare our work with state-of-the-art baselines based on real Ethereum transactions. The evaluation results show that P-Louvain reduces allocation execution time by 86% and the cross-shard transaction ratio by 7.5%. Meanwhile, ContribChain improves throughput by 35.8% and reduces the cross-shard transaction ratio by 16%."
2505.07042,"The Congestion Control (CC) module plays a critical role in the Transmission Control Protocol (TCP), ensuring the stability and efficiency of network data transmission. The CC approaches that are commonly used these days employ heuristics-based rules to adjust the sending rate. Due to their heuristics-based nature, these approaches are not only unable to adapt to changing network conditions but are also agnostic to the diverse requirements that different applications often have. Recently, several learning-based CC approaches have been proposed to adapt to changing network conditions. Unfortunately, they are not designed to take application requirements into account. Prior heuristics-based as well as learning-based CC approaches focus on achieving a singular objective, which is often to maximize throughput, even though a lot of applications care more about latency, packet losses, jitter, and different combinations of various network metrics. Motivated by this, we propose a Deep Reinforcement Learning (DRL) based CC framework, namely ASC, which allows any application to specify any arbitrary objectives that the network traffic of that application should achieve and is able to swiftly adapt to the changes in the objectives of the applications as well as to the changes in the network conditions. Our ASC framework further employs a client-server architecture that serves two purposes: 1) it makes ASC highly scalable in terms of the arrival and departure of TCP connections, and 2) it makes ASC very lightweight for the nodes maintaining the TCP connections. We implemented and extensively evaluated ASC in a variety of settings. Our results show that it can not only achieve various objectives but also outperforms prior approaches even in the specific objectives that those approaches were designed to achieve."
2505.07278,"The densification of Wi-Fi deployments means that fully distributed random channel access is no longer sufficient for high and predictable performance. Therefore, the upcoming IEEE 802.11bn amendment introduces multi-access point coordination (MAPC) methods. This paper addresses a variant of MAPC called coordinated spatial reuse (C-SR), where devices transmit simultaneously on the same channel, with the power adjusted to minimize interference. The C-SR scheduling problem is selecting which devices transmit concurrently and with what settings. We provide a theoretical upper bound model, optimized for either throughput or fairness, which finds the best possible transmission schedule using mixed-integer linear programming. Then, a practical, probing-based approach is proposed which uses multi-armed bandits (MABs), a type of reinforcement learning, to solve the C-SR scheduling problem. We validate both classical (flat) MAB and hierarchical MAB (H-MAB) schemes with simulations and in a testbed. Using H-MABs for C-SR improves aggregate throughput over legacy IEEE 802.11 (on average by 80% in random scenarios), without reducing the number of transmission opportunities per station. Finally, our framework is lightweight and ready for implementation in Wi-Fi devices."
2505.0729,"Sixth Generation (6G)-enabled Internet of Vehicles (IoV) facilitates efficient data synchronization through ultra-fast bandwidth and high-density connectivity, enabling the emergence of Vehicle Twins (VTs). As highly accurate replicas of vehicles, VTs can support intelligent vehicular applications for occupants in 6G-enabled IoV. Thanks to the full coverage capability of 6G, resource-constrained vehicles can offload VTs to edge servers, such as roadside units, unmanned aerial vehicles, and satellites, utilizing their computing and storage resources for VT construction and updates. However, communication between vehicles and edge servers with limited coverage is prone to interruptions due to the dynamic mobility of vehicles. Consequently, VTs must be migrated among edge servers to maintain uninterrupted and high-quality services for users. In this paper, we introduce a VT migration framework in 6G-enabled IoV. Specifically, we first propose a Long Short-Term Memory (LSTM)-based Transformer model to accurately predict long-term workloads of edge servers for migration decision-making. Then, we propose a Dynamic Mask Multi-Agent Proximal Policy Optimization (DM-MAPPO) algorithm to identify optimal migration routes in the highly complex environment of 6G-enabled IoV. Finally, we develop a practical platform to validate the effectiveness of the proposed scheme using real datasets. Simulation results demonstrate that the proposed DM-MAPPO algorithm significantly reduces migration latency by 20.82% and packet loss by 75.07% compared with traditional deep reinforcement learning algorithms."
2505.07328,"In contrast to its predecessors, 5G supports a wide range of commercial, industrial, and critical infrastructure scenarios. One key feature of 5G, ultra-reliable low latency communication, is particularly appealing to such scenarios for its real-time capabilities. However, 5G's enhanced security, mostly realized through optional security controls, imposes additional overhead on the network performance, potentially hindering its real-time capabilities. To better assess this impact and guide operators in choosing between different options, we measure the latency overhead of IPsec when applied over the N3 and the service-based interfaces to protect user and control plane data, respectively. Furthermore, we evaluate whether WireGuard constitutes an alternative to reduce this overhead. Our findings show that IPsec, if configured correctly, has minimal latency impact and thus is a prime candidate to secure real-time critical scenarios."
2505.07713,"With the promise of greater decentralization and sustainability, Ethereum transitioned from a Proof-of-Work (PoW) to a Proof-of-Stake (PoS) consensus mechanism. The new consensus protocol introduces novel vulnerabilities that warrant further investigation. The goal of this paper is to investigate the security of Ethereum's PoS system from an Internet routing perspective.To this end, this paper makes two contributions: First, we devise a novel framework for inferring the distribution of validators on the Internet without disturbing the real network. Second, we introduce a class of network-level attacks on Ethereum's PoS system that jointly exploit Internet routing vulnerabilities with the protocol's reward and penalty mechanisms. We describe two representative attacks: StakeBleed, where the attacker triggers an inactivity leak, halting block finality and causing financial losses for all validators; and KnockBlock, where the attacker increases her expected MEV gains by preventing targeted blocks from being included in the chain. We find that both attacks are practical and effective. An attacker executing StakeBleed can inflict losses of almost 300 ETH in just 2 hours by hijacking as few as 30 IP prefixes. An attacker implementing KnockBlock could increase their MEV expected gains by 44.5% while hijacking a single prefix for less than 2 minutes.Our paper serves as a call to action for validators to reinforce their Internet routing infrastructure and for the Ethereum P2P protocol to implement stronger mechanisms to conceal validator locations."
2505.07741,"TCP BBR's behavior has been explained by various theoretical models, and in particular those that describe how it co-exists with other types of flows. However, as new versions of the BBR protocol have emerged, it remains unclear to what extent the high-level behaviors described by these models apply to the newer versions. In this paper, we systematically evaluate the most influential steady-state and fluid models describing BBR's coexistence with loss-based flows over shared bottleneck links. Our experiments, conducted on a new experimental platform (FABRIC), extend previous evaluations to additional network scenarios, enabling comparisons between the two models and include the recently introduced BBRv3. Our findings confirm that the steady-state model accurately captures BBRv1 behavior, especially against single loss-based flows. The fluid model successfully captures several key behaviors of BBRv1 and BBRv2 but shows limitations, in scenarios involving deep buffers, large numbers of flows, or intra-flow fairness. Importantly, we observe clear discrepancies between existing model predictions and BBRv3 behavior, suggesting the need for an updated or entirely new modeling approach for this latest version. We hope these results validate and strengthen the research community's confidence in these models and identify scenarios where they do not apply."
2505.07827,"This paper introduces MACH, a novel approach for optimizing task handover in vehicular computing scenarios. To ensure fast and latency-aware placement of tasks, the decision-making -- where and when should tasks be offloaded -- is carried out decentralized at the Road Side Units (RSUs) who also execute the tasks. By shifting control to the network edge, MACH moves away from the traditional centralized or vehicle-based handover method. Still, it focuses on contextual factors, such as the current RSU load and vehicle trajectories. Thus, MACH improves the overall Quality of Service (QoS) while fairly balancing computational loads between RSUs. To evaluate the effectiveness of our approach, we develop a robust simulation environment composed of real-world traffic data, dynamic network conditions, and different infrastructure capacities. For scenarios that demand low latency and high reliability, our experimental results demonstrate how MACH significantly improves the adaptability and efficiency of vehicular computations. By decentralizing control to the network edge, MACH effectively reduces communication overhead and optimizes resource utilization, offering a robust framework for task handover management."
2505.07834,"We introducethis http URL, a novel domain-specific language (DSL) designed to explicitly regulate interactions between AI models, agents, and web content, addressing critical limitations of the widely adoptedthis http URLstandard. As AI increasingly engages with online materials for tasks such as training, summarization, and content modification, existing regulatory methods lack the necessary granularity and semantic expressiveness to ensure ethical and legal compliance.this http URLextends traditional URL-based access controls by enabling precise element-level regulations and incorporating natural language instructions interpretable by AI systems. To facilitate practical deployment, we provide an integrated development environment with code autocompletion and automatic XML generation. Furthermore, we propose two compliance mechanisms: XML-based programmatic enforcement and natural language prompt integration, and demonstrate their effectiveness through preliminary experiments and case studies. Our approach aims to aid the governance of AI-Internet interactions, promoting responsible AI use in digital ecosystems."
2505.07835,"Twenty-five years ago, the specification of the Intelligent Product was established, envisaging real-time connectivity that not only enables products to gather accurate data about themselves but also allows them to assess and influence their own destiny. Early work by the Auto-ID project focused on creating a single, open-standard repository for storing and retrieving product information, laying a foundation for scalable connectivity. A decade later, the approach was revisited in light of low-cost RFID systems that promised a low-cost link between physical goods and networked information environments. Since then, advances in blockchain, Web3, and artificial intelligence have introduced unprecedented levels of resilience, consensus, and autonomy. By leveraging decentralised identity, blockchain-based product information and history, and intelligent AI-to-AI collaboration, this paper examines these developments and outlines a new specification for the Intelligent Product 3.0, illustrating how decentralised and AI-driven capabilities facilitate seamless interaction between physical AI and everyday products."
2505.07837,"Advanced fifth generation (5G) and beyond (B5G) communication networks have revolutionized wireless technologies, supporting ultra-high data rates, low latency, and massive connectivity. However, they also introduce vulnerabilities, particularly in decentralized Industrial Internet of Things (IIoT) environments. Traditional cryptographic methods struggle with scalability and complexity, leading researchers to explore Artificial Intelligence (AI)-driven physical layer techniques for secure communications. In this context, this paper focuses on the utilization of Machine and Deep Learning (ML/DL) techniques to tackle with the common problem of eavesdropping detection. To this end, a simulated industrial B5G heterogeneous wireless network is used to evaluate the performance of various ML/DL models, including Random Forests (RF), Deep Convolutional Neural Networks (DCNN), and Long Short-Term Memory (LSTM) networks. These models classify users as either legitimate or malicious ones based on channel state information (CSI), position data, and transmission power. According to the presented numerical results, DCNN and RF models achieve a detection accuracy approaching 100\% in identifying eavesdroppers with zero false alarms. In general, this work underlines the great potential of combining AI and Physical Layer Security (PLS) for next-generation wireless networks in order to address evolving security threats."
2505.07841,"With the emergence of large model-based agents, widely adopted transformer-based architectures inevitably produce excessively long token embeddings for transmission, which may result in high bandwidth overhead, increased power consumption and latency. In this letter, we propose a task-oriented multimodal token transmission scheme for efficient multimodal information fusion and utilization. To improve the efficiency of token transmission, we design a two-stage training algotithm, including cross-modal alignment and task-oriented fine-tuning, for large model-based token communication. Meanwhile, token compression is performed using a sliding window pooling operation to save communication resources. To balance the trade-off between latency and model performance caused by compression, we formulate a weighted-sum optimization problem over latency and validation loss. We jointly optimizes bandwidth, power allocation, and token length across users by using an alternating optimization method. Simulation results demonstrate that the proposed algorithm outperforms the baseline under different bandwidth and power budgets. Moreover, the two-stage training algorithm achieves higher accuracy across various signal-to-noise ratios than the method without cross-modal alignment."
2505.07844,"Load Balancing is a fundamental technology for scaling cloud infrastructure. It enables systems to distribute incoming traffic across backend servers using predefined algorithms such as round robin, weighted round robin, least connections, weighted least connections, resource based, weighted response time, source IP hash, and URL hash.This approach has helped software developers, infrastructure engineers, and system administrators address many internet traffic related challenges across modern software architectures ranging from monolithic systems and traditional three tier models to microservices based applications.However, traditional traffic balancing techniques are increasingly becoming inadequate in optimizing distribution times. Existing algorithms are struggling to meet the rising demands of internet traffic, often resulting in degraded user experiences.To proactively address these issues particularly in areas like response time, distribution latency, and system uptime, we need to rethink how load balancing is implemented. Key challenges include traffic management, congestion control, intelligent scheduling, and the ability to determine when and when not to apply load balancing."
2505.07872,"Performance of video streaming, which accounts for most of the traffic in wireless communication, can be significantly improved by caching popular videos at the wireless edge. Determining the cache content that optimizes performance (defined via a revenue function) is thus an important task, and prediction of the future demands based on past history can make this process much more efficient. However, since practical video caching networks involve various parties (e.g., users, isp, and csp) that do not wish to reveal information such as past history to each other, privacy-preserving solutions are required. Motivated by this, we propose a proactive caching method based on users' privacy-preserving multi-slot future demand predictions -- obtained from a trained Transformer -- to optimize revenue. Specifically, we first use a privacy-preserving fl algorithm to train a Transformer to predict multi-slot future demands of the users. However, prediction accuracy is not perfect and decreases the farther into the future the prediction is done. We model the impact of prediction errors invoking the file popularities, based on which we formulate a long-term system revenue optimization to make the cache placement decisions. As the formulated problem is NP-hard, we use a greedy algorithm to efficiently obtain an approximate solution. Simulation results validate that (i) the fl solution achieves results close to the centralized (non-privacy-preserving) solution and (ii) optimization of revenue may provide different solutions than the classical chr criterion."
2505.07877,"General-purpose large language models (LLMs), despite their broad capabilities accrued from open-world data, frequently exhibit suboptimal performance when confronted with the nuanced and specialized demands inherent in real-time telecommunications applications. This investigation addresses this critical limitation through the meticulous fine-tuning of TSLAM-Mini developed by NetoAI, a compact (3.8-billion parameter) causal language model architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen leverages a bespoke dataset comprising 100,000 samples, strategically engineered to address 20 pivotal telecommunications use-cases, encompassing domains such as Network Fundamentals, IP Routing, MPLS, Network Security, Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched with granular insights from venerated network Subject Matter Experts (SMEs) and authoritative RFC documents, thereby capturing high-fidelity representations of real-world network dynamics through simulations inspired by digital twin paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial training efficiency and enabled prospective deployment on resource-constrained hardware. A novel evaluation framework, predicated on a high-capacity LLM (Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to rigorously assess instruction-following fidelity and response quality across the specified telecom use-cases. Empirical results unequivocally demonstrate TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring the profound efficacy of domain-specific datasets and PEFT methodologies for advancing intelligent network management."
2505.07892,"The vision of sixth-generation (6G) wireless networks paves the way for the seamless integration of digital twins into vehicular networks, giving rise to a Vehicular Digital Twin Network (VDTN). The large amount of computing resources as well as the massive amount of spatial-temporal data in Digital Twin (DT) domain can be utilized to enhance the communication and control performance of Internet of Vehicle (IoV) systems. In this article, we first propose the architecture of VDTN, emphasizing key modules that center on functions related to the joint optimization of control and communication. We then delve into the intricacies of the multitimescale decision process inherent in joint optimization in VDTN, specifically investigating the dynamic interplay between control and communication. To facilitate the joint optimization, we define two Value of Information (VoI) concepts rooted in control performance. Subsequently, utilizing VoI as a bridge between control and communication, we introduce a novel joint optimization framework, which involves iterative processing of two Deep Reinforcement Learning (DRL) modules corresponding to control and communication to derive the optimal policy. Finally, we conduct simulations of the proposed framework applied to a platoon scenario to demonstrate its effectiveness in ensu"
2505.07893,"Accurate channel state information (CSI) acquisition for massive multiple-input multiple-output (MIMO) systems is essential for future mobile communication networks. Channel fingerprint (CF), also referred to as channel knowledge map, is a key enabler for intelligent environment-aware communication and can facilitate CSI acquisition. However, due to the cost limitations of practical sensing nodes and test vehicles, the resulting CF is typically coarse-grained, making it insufficient for wireless transceiver design. In this work, we introduce the concept of CF twins and design a conditional generative diffusion model (CGDM) with strong implicit prior learning capabilities as the computational core of the CF twin to establish the connection between coarse- and fine-grained CFs. Specifically, we employ a variational inference technique to derive the evidence lower bound (ELBO) for the log-marginal distribution of the observed fine-grained CF conditioned on the coarse-grained CF, enabling the CGDM to learn the complicated distribution of the target data. During the denoising neural network optimization, the coarse-grained CF is introduced as side information to accurately guide the conditioned generation of the CGDM. To make the proposed CGDM lightweight, we further leverage the additivity of network layers and introduce a one-shot pruning approach along with a multi-objective knowledge distillation technique. Experimental results show that the proposed approach exhibits significant improvement in reconstruction performance compared to the baselines. Additionally, zero-shot testing on reconstruction tasks with different magnification factors further demonstrates the scalability and generalization ability of the proposed approach."
2505.07894,"The paradigm shift from environment-unaware communication to intelligent environment-aware communication is expected to facilitate the acquisition of channel state information for future wireless communications. Channel Fingerprint (CF), as an emerging enabling technology for environment-aware communication, provides channel-related knowledge for potential locations within the target communication area. However, due to the limited availability of practical devices for sensing environmental information and measuring channel-related knowledge, most of the acquired environmental information and CF are coarse-grained, insufficient to guide the design of wireless transmissions. To address this, this paper proposes a deep conditional generative learning approach, namely a customized conditional generative diffusion model (CDiff). The proposed CDiff simultaneously refines environmental information and CF, reconstructing a fine-grained CF that incorporates environmental information, referred to as EnvCF, from its coarse-grained counterpart. Experimental results show that the proposed approach significantly improves the performance of EnvCF construction compared to the baselines."
2505.08032,"Adaptive beam switching in 6G networks is challenged by high frequencies, mobility, and blockage. We propose an Online Learning framework using Deep Reinforcement Learning (DRL) with an enhanced state representation (velocity and blockage history), a GRU architecture, and prioritized experience replay for real-time beam optimization. Validated via Nvidia Sionna under time-correlated blockage, our approach significantly enhances resilience in SNR, throughput, and accuracy compared to a conventional heuristic. Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit (MAB) baseline by leveraging temporal dependencies, achieving lower performance variability. This demonstrates the benefits of memory and prioritized learning for robust 6G beam management, while confirming MAB as a strong baseline."
2505.08046,"Mobile jammers pose a critical threat to 5G networks, particularly in military communications. We propose an intelligent anti-jamming framework that integrates Multiple Signal Classification (MUSIC) for high-resolution Direction-of-Arrival (DoA) estimation, Minimum Variance Distortionless Response (MVDR) beamforming for adaptive interference suppression, and machine learning (ML) to enhance DoA prediction for mobile jammers. Extensive simulations in a realistic highway scenario demonstrate that our hybrid approach achieves an average Signal-to-Noise Ratio (SNR) improvement of 9.58 dB (maximum 11.08 dB) and up to 99.8% DoA estimation accuracy. The framework's computational efficiency and adaptability to dynamic jammer mobility patterns outperform conventional anti-jamming techniques, making it a robust solution for securing 5G communications in contested environments."
2505.08088,"Indoor positioning systems (IPSs) are increasingly vital for location-based services in complex multi-storey environments. This study proposes a novel graph-based approach for floor separation using Wi-Fi fingerprint trajectories, addressing the challenge of vertical localization in indoor settings. We construct a graph where nodes represent Wi-Fi fingerprints, and edges are weighted by signal similarity and contextual transitions. Node2Vec is employed to generate low-dimensional embeddings, which are subsequently clustered using K-means to identify distinct floors. Evaluated on the Huawei University Challenge 2021 dataset, our method outperforms traditional community detection algorithms, achieving an accuracy of 68.97\%, an F1-score of 61.99\%, and an Adjusted Rand Index of 57.19\%. By publicly releasing the preprocessed dataset and implementation code, this work contributes to advancing research in indoor positioning. The proposed approach demonstrates robustness to signal noise and architectural complexities, offering a scalable solution for floor-level localization."
2505.08258,"Indoor position technology has become one of the research highlights in the Internet of Things (IoT), but there is still a lack of universal, low-cost, and high-precision solutions. This paper conducts research on indoor position technology based on location fingerprints and proposes a practical hybrid indoor positioning system. In this experiment, the location fingerprint database is established by using RSS signal in the offline stage, the location algorithm is improved and innovated in the online stage. The weighted k-nearest neighbor algorithm is used for location fingerprint matching and pedestrian dead reckoning technology is used for trajectory tracking. This paper designs and implements an indoor position system that performs the functions of data collection, positioning, and position tracking. Through the test, it is found that it can meet the requirements of indoor positioning."
2505.08328,"Network slicing in 5G/6G Non-Terrestrial Network (NTN) is confronted with mobility and traffic variability. An artificial intelligence (AI)-based digital twin (DT) architecture with deep reinforcement learning (DRL) using Deep deterministic policy gradient (DDPG) is proposed for dynamic optimization of resource allocation. DT virtualizes network states to enable predictive analysis, while DRL changes bandwidth for eMBB slice. Simulations show a 25\% latency reduction compared to static methods, with enhanced resource utilization. This scalable solution supports 5G/6G NTN applications like disaster recovery and urban blockage."
2505.08958,"Entanglement generation in long-distance quantum networks is a difficult process due to resource limitations and the probabilistic nature of entanglement swapping. To maximize success probability, existing quantum routing algorithms employ computationally expensive solutions (e.g., linear programming) to determine which links to entangle and use for end-to-end entanglement generation. Such optimization methods, however, cannot meet the delay requirements of real-world quantum networks, necessitating swift yet efficient real-time optimization models. In this paper, we propose reinforcement learning (RL)-based models to determine which links to entangle and proactively swap to meet connection requests. We show that the proposed RL-based approach is 20x faster compared to linear programming. Moreover, we show that one can take advantage of the longevity of entanglements to (i) cache entangled links for future use and (ii) proactively swap entanglement on high-demand path segments, thereby increasing the likelihood of request success. Through comprehensive simulations, we demonstrate that caching unused entanglements leads to a 10-15% improvement in the performance of state-of-the-art quantum routing algorithms. Complementing caching with proactive entanglement swapping further enhances the request success rate by up to 52.55%."
2505.09222,"Pacing is a key mechanism in modern transport protocols, used to regulate packet transmission timing to minimize traffic burstiness, lower latency, and reduce packet loss. Standardized in 2021, QUIC is a UDP-based protocol designed to improve upon the TCP / TLS stack. While the QUIC protocol recommends pacing, and congestion control algorithms like BBR rely on it, the user-space nature of QUIC introduces unique challenges. These challenges include coarse-grained timers, system call overhead, and OS scheduling delays, all of which complicate precise packet pacing. This paper investigates how pacing is implemented differently across QUIC stacks, including quiche, picoquic, and ngtcp2, and evaluates the impact of system-level features like GSO and Linux qdiscs on pacing. Using a custom measurement framework and a passive optical fiber tap, we establish a baseline with default settings and systematically explore the effects of qdiscs, hardware offloading using the ETF qdisc, and GSO on pacing precision and network performance. We also extend and evaluate a kernel patch to enable pacing of individual packets within GSO buffers, combining batching efficiency with precise pacing. Kernel-assisted and purely user-space pacing approaches are compared. We show that pacing with only user-space timers can work well, as demonstrated by picoquic with BBR. With quiche, we identify FQ as a qdisc well-suited for pacing QUIC traffic, as it is relatively easy to use and offers precise pacing based on packet timestamps. Our findings provide new insights into the trade-offs involved in implementing pacing in QUIC and highlight potential optimizations for real-world applications like video streaming and video calls."
2505.09259,"Space-air-ground integrated network (SAGIN) is envisioned as a key network architecture for achieving ubiquitous coverage in the next-generation communication system. Concurrently, artificial intelligence (AI) plays a pivotal role in managing the complex control of SAGIN, thereby enhancing its automation and flexibility. Despite this, there remains a significant research gap concerning the interaction between AI and SAGIN. In this context, we first present a promising approach for developing a generalized AI model capable of executing multiple tasks simultaneously in SAGIN. Subsequently, we propose a framework that leverages software-defined networking (SDN) and AI technologies to manage the resources and services across the entire SAGIN. Particularly, we demonstrate the real-world applicability of our proposed framework through a comprehensive case study. These works pave the way for the deep integration of SAGIN and AI in future wireless networks."
2505.09339,"Intent-based network (IBN) is a promising solution to automate network operation and management. IBN aims to offer human-tailored network interaction, allowing the network to communicate in a way that aligns with the network users' language, rather than requiring the network users to understand the technical language of the network/devices. Nowadays, different applications interact with the network, each with its own specialized needs and domain language. Creating semantic languages (i.e., ontology-based languages) and associating them with each application to facilitate intent translation lacks technical expertise and is neither practical nor scalable. To tackle the aforementioned problem, we propose a context-aware AI framework that utilizes machine reasoning (MR), retrieval augmented generation (RAG), and generative AI technologies to interpret intents from different applications and generate structured network intents. The proposed framework allows for generalized/domain-specific intent expression and overcomes the drawbacks of large language models (LLMs) and vanilla-RAG framework. The experimental results show that our proposed intent-RAG framework outperforms the LLM and vanilla-RAG framework in intent translation."
2505.09386,"Meteorological disasters such as typhoons, forest fires, and floods can damage the communication infrastructures, which will further disable the communication capabilities of cellular networks. The multi-hop wireless communication based on IoT devices (e.g., rescue robots, UAVs, and mobile devices) becomes an available and rapidly deployable communication approach for search and rescue operations. However, Age of Information (AoI), an emerging network performance metric, has not been comprehensively investigated in this multi-hop model. In this paper, we first construct a UAV-relayed wireless network model and formulate the end-to-end instant AoI. Then we derive the optimal location of the relay UAV to achieve the minimum instant AoI by mathematical analysis. Simulations show that the derived relay location can always guarantee the optimal AoI and outperform other schemes."
2505.09405,"Wormhole attacks can cause serious disruptions to the network topology in disaster rescue opportunity networks.By establishing false Wormhole(WH) links, malicious nodes can mislead legitimate paths in the network, further causing serious consequences such as traffic analysis attacks (i.e., by eavesdropping and monitoring exchanged traffic), denial of service (DoS) or selective packet loss attacks. This paper uses rescue equipment (vehicle-mounted base stations, rescue control centers, etc.) as an effective third-party auditor (TPA), and combines the commonly used Z-Score (Standard Score) data processing method to propose a new detection method based on pure mathematical statistics for detecting wormhole attacks. Finally, we perform a large number of simulations to evaluate the proposed method. Since our proposed strategy does not require auxiliary equipment such as GPS positioning and timers, as a pure data statistical analysis method, it is obviously more economically valuable, feasible, and practical than other strategies in disaster relief."
2505.0944,"Enabling vertical use cases for the sixth generation (6G) wireless networks, such as automated manufacturing, immersive extended reality (XR), and self-driving fleets, will require network designs that meet reliability and latency targets in well-defined service areas. In order to establish a quantifiable design objective, we introduce the novel concept of reliability coverage, defined as the percentage area covered by communication services operating under well-defined reliability and performance targets. Reliability coverage allows us to unify the different network design tasks occurring at different time scales, namely resource orchestration and allocation, resulting in a single framework for dimensioning and optimization in local 6G networks. The two time scales, when considered together, yield remarkably consistent results and allow us to observe how stringent reliability/latency requirements translate into the increased wireless network resource demands."
2505.09753,"In the virtual network embedding problem, the goal is to map embed a set of virtual network instances to a given physical network substrate at minimal cost, while respecting the capacity constraints of the physical network. This NP-hard problem is fundamental to network virtualization, embodying essential properties of resource allocation problems faced by service providers in the edge-to-cloud spectrum. Due to its centrality, this problem and its variants have been extensively studied and remain in the focus of the research community.In this paper, we present a new variant, the virtual network embedding with alternatives problem (VNEAP). This new problem captures the power of a common network virtualization practice, in which virtual network topologies are malleable - embedding of a given virtual network instance can be performed using any of the alternatives from a given set of topology alternatives. We provide two efficient heuristics for VNEAP and show that having multiple virtual network alternatives for the same application is superior to the best results known for the classic formulation. We conclude that capturing the problem domain via VNEAP can facilitate more efficient network virtualization solutions."
2505.09995,"Edge computing, with its low latency, dynamic scalability, and location awareness, along with the convergence of computing and communication paradigms, has been successfully applied in critical domains such as industrial IoT, smart healthcare, smart homes, and public safety. This paper provides a comprehensive survey of open-source edge computing simulators and emulators, presented in our GitHub repository (this https URL), emphasizing the convergence of computing and networking paradigms. By examining more than 40 tools, including CloudSim, NS-3, and others, we identify the strengths and limitations in simulating and emulating edge environments. This survey classifies these tools into three categories: packet-level, application-level, and emulators. Furthermore, we evaluate them across five dimensions, ranging from resource representation to resource utilization. The survey highlights the integration of different computing paradigms, packet processing capabilities, support for edge environments, user-defined metric interfaces, and scenario visualization. The findings aim to guide researchers in selecting appropriate tools for developing and validating advanced computing and networking technologies."
2505.10122,"In unmanned aerial vehicle (UAV)-assisted wake-up radio (WuR)-enabled internet of things (IoT) networks, UAVs can instantly activate the main radios (MRs) of the sensor nodes (SNs) with a wake-up call (WuC) for efficient data collection in mission-driven data collection scenarios. However, the spontaneous response of numerous SNs to the UAV's WuC can lead to significant packet loss and collisions, as WuR does not exhibit its superiority for high-traffic loads. To address this challenge, we propose an innovative receiver-initiated WuR UAV-assisted clustering (RI-WuR-UAC) medium access control (MAC) protocol to achieve low latency and high reliability in ultra-low power consumption applications. We model the proposed protocol using the $M/G/1/2$ queuing framework and derive expressions for key performance metrics, i.e., channel busyness probability, probability of successful clustering, average SN energy consumption, and average transmission delay. The RI-WuR-UAC protocol employs three distinct data flow models, tailored to different network traffic conditions, which perform three MAC mechanisms: channel assessment (CCA) clustering for light traffic loads, backoff plus CCA clustering for dense and heavy traffic, and adaptive clustering for variable traffic loads. Simulation results demonstrate that the RI-WuR-UAC protocol significantly outperforms the benchmark sub-carrier modulation clustering protocol. By varying the network load, we capture the trade-offs among the performance metrics, showcasing the superior efficiency and reliability of the RI-WuR-UAC protocol."
2505.10226,"Visible Light Communication (VLC) provides an energy-efficient wireless solution by using existing LED-based illumination for high-speed data transmissions. Although solar cells offer the advantage of simultaneous energy harvesting and data reception, their broadband nature hinders accurate decoding of color-coded signals like Color Shift Keying (CSK). In this paper, we propose a novel approach exploiting the concept of tandem solar cells, multi-layer devices with partial wavelength selectivity, to capture coarse color information without resorting to energy-limiting color filters. To address the residual spectral overlap, we develop a bidirectional LSTM-based machine learning framework that infers channel characteristics by comparing solar cells' photovoltaic signals with pilot-based anchor data. Our commercial off-the-shelf (COTS) solar prototype achieves robust performance across varying distances and ambient lighting levels, significantly reducing bit error rates compared to conventional channel estimation methods. These findings mark a step toward sustainable, high-performance VLC systems powered by the multi-layer solar technologies."
2505.10537,"The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance."
2505.10816,"Industry 4.0 is transforming manufacturing and logistics by integrating robots into shared human environments, such as factories, warehouses, and healthcare facilities. However, the risk of human-robot collisions, especially in Non-Line-of-Sight (NLoS) scenarios like around corners, remains a critical challenge. Existing solutions, such as vision-based and LiDAR systems, often fail under occlusion, lighting constraints, or privacy concerns, while RF-based systems are limited by range and accuracy.To address these limitations, we propose mmMirror, a novel system leveraging a Van Atta Array-based millimeter-wave (mmWave) reconfigurable intelligent reflecting surface (IRS) for precise, device-free NLoS localization. mmMirror integrates seamlessly with existing frequency-modulated continuous-wave (FMCW) radars and offers: (i) robust NLoS localization with centimeter-level accuracy at ranges up to 3 m, (ii) seamless uplink and downlink communication between radar and IRS, (iii) support for multi-radar and multi-target scenarios via dynamic beam steering, and (iv) reduced scanning latency through adaptive time slot allocation. Implemented using commodity 24 GHz radars and a PCB-based IRS prototype, mmMirror demonstrates its potential in enabling safe human-robot interactions in dynamic and complex environments."
2505.11231,"This article emphasizes the importance of queues associated with the ports of switches in network monitoring. Traditionally, data collection about these queues is done using programmable data planes and telemetry based on INT (In-band Network Telemetry) probes, assuming there is only a single queue per output port. The MM-INT (Multiqueue Multicast - INT)is a solution that utilizes registers to store data from all queues and ports, enabling the efficient collection of monitoring information. The MM-INT avoids probe overload and employs the origin-based routing mechanism and multicast trees for the probes. The results demonstrate significant reductions in the number of probes sent compared to other traditional solutions found in the literature."
2505.11339,"Serverless computing promises enhanced resource efficiency and lower user costs, yet is burdened by a heavyweight, CPU-bound data plane. Prior efforts exploiting shared memory reduce overhead locally but fall short when scaling across nodes. Furthermore, serverless environments can have unpredictable and large-scale multi-tenancy, leading to contention for shared network resources.We present Palladium, a DPU-centric serverless data plane that reduces the CPU burden and enables efficient, zero-copy communication in multi-tenant serverless clouds. Despite the limited general-purpose processing capability of the DPU cores, Palladium strategically exploits the DPU's potential by (1) offloading data transmission to high-performance NIC cores via RDMA, combined with intra-node shared memory to eliminate data copies across nodes, and (2) enabling cross-processor (CPU-DPU) shared memory to eliminate redundant data movement, which overwhelms wimpy DPU cores. At the core of Palladium is the DPU-enabled network engine (DNE) -- a lightweight reverse proxy that isolates RDMA resources from tenant functions, orchestrates inter-node RDMA flows, and enforces fairness under contention.To further reduce CPU involvement, Palladium performs early HTTP/TCP-to-RDMA transport conversion at the cloud ingress, bridging the protocol mismatch before client traffic enters the RDMA fabric, thus avoiding costly protocol translation along the critical path. We show that careful selection of RDMA primitives (i.e., two-sided instead of one-sided) significantly affects the zero-copy data plane.Our preliminary experimental results show that enabling DPU offloading in Palladium improves RPS by 20.9x. The latency is reduced by a factor of 21x in the best case, all the while saving up to 7 CPU cores, and only consuming two wimpy DPU cores."
2505.11973,"Cloud Gaming (CG) research faces challenges due to the unpredictability of game engines and restricted access to commercial platforms and their logs. This creates major obstacles to conducting fair experimentation and evaluation. CGReplay captures and replays player commands and the corresponding video frames in an ordered and synchronized action-reaction loop, ensuring reproducibility. It enables Quality of Experience/Service (QoE/QoS) assessment under varying network conditions and serves as a foundation for broader CG research. The code is publicly available for further development."
2505.11974,"Space-air-ground uniformly integrated network (SAGUIN), which integrates the satellite, aerial, and terrestrial networks into a unified communication architecture, is a promising candidate technology for the next-generation wireless systems. Transmitting on the same frequency band, higher-layer access points (AP), e.g., satellites, provide extensive coverage; meanwhile, it may introduce significant signal propagation delays due to the relatively long distances to the ground users, which can be multiple times longer than the packet durations in task-oriented communications. This phenomena is modeled as a new ``ripple effect'', which introduces spatiotemporally correlated interferences in SAGUIN. This paper studies the task scheduling problem in SAGUIN with ripple effect, and formulates it as a Markov decision process (MDP) to jointly minimize the age of information (AoI) at users and energy consumption at APs. The obtained MDP is challenging due to high dimensionality, partial observations, and dynamic resource constraints caused by ripple effect. To address the challenges of high dimensionality, we reformulate the original problem as a Markov game, where the complexities are managed through interactive decision-making among APs. Meanwhile, to tackle partial observations and the dynamic resource constraints, we adopt a modified multi-agent proximal policy optimization (MAPPO) algorithm, where the actor network filters out irrelevant input states based on AP coverage and its dimensionality can be reduced by more than an order of magnitude. Simulation results reveal that the proposed approach outperforms the benchmarks, significantly reducing users' AoI and APs' energy consumption."
2505.11978,"Despite significant advancements in terrestrial networks, inherent limitations persist in providing reliable coverage to remote areas and maintaining resilience during natural disasters. Multi-tier networks with low Earth orbit (LEO) satellites and high-altitude platforms (HAPs) offer promising solutions, but face challenges from high mobility and dynamic channel conditions that cause unstable connections and frequent handovers. In this paper, we design a three-tier network architecture that integrates LEO satellites, HAPs, and ground terminals with hybrid free-space optical (FSO) and radio frequency (RF) links to maximize coverage while maintaining connectivity reliability. This hybrid approach leverages the high bandwidth of FSO for satellite-to-HAP links and the weather resilience of RF for HAP-to-ground links. We formulate a joint optimization problem to simultaneously balance downlink transmission rate and handover frequency by optimizing network configuration and satellite handover decisions. The problem is highly dynamic and non-convex with time-coupled constraints. To address these challenges, we propose a novel large language model (LLM)-guided truncated quantile critics algorithm with dynamic action masking (LTQC-DAM) that utilizes dynamic action masking to eliminate unnecessary exploration and employs LLMs to adaptively tune hyperparameters. Simulation results demonstrate that the proposed LTQC-DAM algorithm outperforms baseline algorithms in terms of convergence, downlink transmission rate, and handover frequency. We also reveal that compared to other state-of-the-art LLMs, DeepSeek delivers the best performance through gradual, contextually-aware parameter adjustments."
2505.12091,"Future wireless networks must deliver deterministic end-to-end delays for workloads such as smart-factory control loops. On Ethernet these guarantees are delivered by the set of tools within IEEE 802.1 time sensitive networking~(TSN) standards. Credit-based shaper (CBS) is one such tool which enforces bounded latency. Directly porting CBS to 5G/6G New Radio (NR) is non-trivial because NR schedules traffic in discrete-time, modulation-dependent resource allocation, whereas CBS assumes a continuous, fixed-rate link. Existing TSN-over-5G translators map Ethernet priorities to 5G quality of service (QoS) identifiers but leave the radio scheduler unchanged, so deterministic delay is lost within the radio access network (RAN). To address this challenge, we propose a novel slot-native approach that adapts CBS to operate natively in discrete NR slots. We first propose a per-slot credit formulation for each user-equipment ({UE}) queue that debits credit by the granted transport block size~(TBS); we call this discrete-time CBS (CBS-DT). Recognizing that debiting the full {TBS} can unduly penalize transmissions that actually use only part of their grant, we then introduce and analyze {CBS} with Partial Usage ({CBS-PU}). {CBS-PU} scales the credit debit in proportion to the actual bytes dequeued from the downlink queue. The resulting CBS-PU algorithm is shown to maintain bounded credit, preserve long-term rate reservations, and guarantees worst-case delay performance no worse than {CBS-DT}. Simulation results show that slot-level credit gating--particularly CBS-PU--enables NR to export TSN class QoS while maximizing resource utilization."
2505.12132,"The 6G mobile network is the next evolutionary step after 5G, with a prediction of an explosive surge in mobile traffic. It provides ultra-low latency, higher data rates, high device density, and ubiquitous coverage, positively impacting services in various areas. Energy saving is a major concern for new systems in the telecommunications sector because all players are expected to reduce their carbon footprints to contribute to mitigating climate change. Network slicing is a fundamental enabler for 6G/5G mobile networks and various other new systems, such as the Internet of Things (IoT), Internet of Vehicles (IoV), and Industrial IoT (IIoT). However, energy-saving methods embedded in network slicing architectures are still a research gap. This paper discusses how to embed energy-saving methods in network-slicing architectures that are a fundamental enabler for nearly all new innovative systems being deployed worldwide. This paper's main contribution is a proposal to save energy in network slicing. That is achieved by deploying ML-native agents in NS architectures to dynamically orchestrate and optimize resources based on user demands. The SFI2 network slicing reference architecture is the concrete use case scenario in which contrastive learning improves energy saving for resource allocation."
2505.12247,"Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling machines to create content across modalities. As GenAI evolves into autonomous agents capable of reasoning, collaboration, and interaction, they are increasingly deployed on network infrastructures to serve humans automatically. This emerging paradigm, known as the agentic network, presents new optimization challenges due to the demand to incorporate subjective intents of human users expressed in natural language. Traditional generic Deep Reinforcement Learning (DRL) struggles to capture intent semantics and adjust policies dynamically, thus leading to suboptimality. In this paper, we present LAMeTA, a Large AI Model (LAM)-empowered Two-stage Approach for intent-aware agentic network optimization. First, we propose Intent-oriented Knowledge Distillation (IoKD), which efficiently distills intent-understanding capabilities from resource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users. Second, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs with a policy-based DRL framework. In SRL, E-LAMs translate natural language user intents into structured preference vectors that guide both state representation and reward design. The DRL, in turn, optimizes the generative service function chain composition and E-LAM selection based on real-time network conditions, thus optimizing the subjective Quality-of-Experience (QoE). Extensive experiments conducted in an agentic network with 81 agents demonstrate that IoKD reduces mean squared error in intent prediction by up to 22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in maximizing intent-aware QoE."
2505.12336,"Current theoretical studies on IoT-over-LEO satellite systems often rely on unrealistic assumptions, such as infinite terrestrial areas and omnidirectional satellite coverage, leaving significant gaps in theoretical analysis for more realistic operational constraints. These constraints involve finite terrestrial area, limited satellite coverage, Earth curvature effect, integral uplink and downlink analysis, and link-dependent interference. To address these gaps, this paper proposes a novel stochastic geometry based model to rigorously analyze the performance of IoT-over-LEO satellite systems. By adopting a binomial point process (BPP) instead of the conventional Poisson point process (PPP), our model accurately characterizes the geographical distribution of a fixed number of IoT devices in a finite terrestrial region. This modeling framework enables the derivation of distance distribution functions for both the links from the terrestrial IoT devices to the satellites (T-S) and from the satellites to the Earth station (S-ES), while also accounting for limited satellite coverage and Earth curvature effects. To realistically represent channel conditions, the Nakagami fading model is employed for the T-S links to characterize diverse small-scale fading environments, while the shadowed-Rician fading model is used for the S-ES links to capture the combined effects of shadowing and dominant line-of-sight paths. Furthermore, the analysis incorporates uplink and downlink interference, ensuring a comprehensive evaluation of system performance. The accuracy and effectiveness of our theoretical framework are validated through extensive Monte Carlo simulations. These results provide insights into key performance metrics, such as coverage probability and average ergodic rate, for both individual links and the overall system."
2505.12492,"Congestion control (CC) crucially impacts user experience across Internet services like streaming, gaming, AR/VR, and connected cars. Traditionally, CC algorithm design seeks universal control rules that yield high performance across diverse application domains and networks. However, varying service needs and network conditions challenge this approach. We share operational experience with a system that automatically customizes congestion control logic to service needs and network conditions. We discuss design, deployment challenges, and solutions, highlighting performance benefits through case studies in streaming, gaming, connected cars, and more.Our system leverages PCC Vivace, an online-learning based congestion control protocol developed by researchers. Hence, along with insights from customizing congestion control, we also discuss lessons learned and modifications made to adapt PCC Vivace for real-world deployment."
2505.12786,"With the continuous evolution of Large Language Models (LLMs), LLM-based agents have advanced beyond passive chatbots to become autonomous cyber entities capable of performing complex tasks, including web browsing, malicious code and deceptive content generation, and decision-making. By significantly reducing the time, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based agents have led to a phenomenon termed Cyber Threat Inflation, characterized by a significant reduction in attack costs and a tremendous increase in attack scale. To provide actionable defensive insights, in this survey, we focus on the potential cyber threats posed by LLM-based agents across diverse network systems. Firstly, we present the capabilities of LLM-based cyberattack agents, which include executing autonomous attack strategies, comprising scouting, memory, reasoning, and action, and facilitating collaborative operations with other agents or human operators. Building on these capabilities, we examine common cyberattacks initiated by LLM-based agents and compare their effectiveness across different types of networks, including static, mobile, and infrastructure-free paradigms. Moreover, we analyze threat bottlenecks of LLM-based agents across different network infrastructures and review their defense methods. Due to operational imbalances, existing defense methods are inadequate against autonomous cyberattacks. Finally, we outline future research directions and potential defensive strategies for legacy network systems."
2505.13331,"In virtual reality (VR) environments, computational tasks exhibit an elastic nature, meaning they can dynamically adjust based on various user and system constraints. This elasticity is essential for maintaining immersive experiences; however, it also introduces challenges for communication and computing in VR systems. In this paper, we investigate elastic task offloading for multi-user edge-computing-enabled VR systems with multi-connectivity, aiming to maximize the computational energy-efficiency (computational throughput per unit of energy consumed). To balance the induced communication, computation, energy consumption, and quality of experience trade-offs due to the elasticity of VR tasks, we formulate a constrained stochastic computational energy-efficiency optimization problem that integrates the multi-connectivity/multi-user action space and the elastic nature of VR computational tasks. We formulate a centralized phasic policy gradient (CPPG) framework to solve the problem of interest online, using only prior elastic task offloading statistics (energy consumption, response time, and transmission time), and task information (i.e., task size and computational intensity), while observing the induced system performance (energy consumption and latency). We further extend our approach to decentralized learning by formulating an independent phasic policy gradient (IPPG) method and a decentralized shared multi-armed bandit (DSMAB) method. We train our methods with real-world 4G, 5G, and WiGig network traces and 360 video datasets to evaluate their performance in terms of response time, energy efficiency, scalability, and delivered quality of experience. We also provide a comprehensive analysis of task size and its effect on offloading policy and system performance. In particular, we show that CPPG reduces latency by 28% and energy consumption by 78% compared to IPPG."
2505.13801,"The fifth-generation (5G) network faces limitations in supporting emerging applications, such as artificial intelligence (AI), virtual reality (VR) and digital twins. To overcome these confines, sub-Terahertz (sub-THz) and Terahertz (THz) technologies are considered to be key enablers of effective 6G wireless communications, offering higher transmission speeds, longer range and wider bandwidth. Achieving these capabilities requires careful engineering of 6G transceivers, with a focus on efficient power amplifiers (PAs) in the front-end, which play a critical role in effectively amplifying and transmitting signals over long distances. Complimentary metal-oxidesemiconductor (CMOS) technology-based PA in sub-THz suffers severe parasitic and limited maximum frequency, however, this has eventually been solved by different design architectures and scaling down of CMOS technology to break through the frequency limitations. In this article, we reviewed the potentials and capabilities of CMOS technology for designing 6G hardware, identified the state-of-art PA designs in the sub-THz band and then examined as well as compared the designs to identify the suitable design strategies for better performance. The circuit optimisation techniques, such as coupled-line, passive gain boosting method, zero-degree power splitting, load-pull matching, diode and capacitor linearisation for better gain, saturated output power and power added efficiency, are considered for the PA design architectures at different sub-THz bands. Furthermore, these methods are summarised and discussed with their advantages and disadvantages in lieu with their performances. The PA design trends, challenges and future perspectives are also presented and discussed. Therefore, this comprehensive review article will serve as a comparative study and reference for future PA designs for radio frequency integrated circuits (RFIC)."
2505.14085,"Emerging intelligent service scenarios in 6G communication impose stringent requirements for low latency, high reliability, and privacy preservation. Generative large language models (LLMs) are gradually becoming key enablers for the integration of semantic communication and computation. However, due to the limited computational resources of edge devices and the increasing complexity of heterogeneous terminal access, existing centralized inference approaches fail to meet the dual demands of response efficiency and data privacy in edge-side inference tasks. To address these challenges, this paper proposes a novel collaborative inference architecture that integrates cloud-based LLMs with edge-deployed small language models (SLMs), enabling dynamic scheduling and sharing of semantic-level intermediate states, and establishing a unified computation-communication paradigm tailored for 6G networks. Specifically, a key-value (KV) cache reuse mechanism is introduced to enhance the semantic understanding of edge models through contextual guidance from the cloud, while significantly reducing edge-side computational and storage overhead. Furthermore, a cross-node parallel scheduling mechanism is proposed to achieve asynchronous coordination between model state loading and decoding computation, thereby improving edge responsiveness. In addition, we investigate layer alignment and representation compression strategies between heterogeneous models to alleviate the communication burden on the edge. Experimental results demonstrate that the proposed architecture exhibits superior adaptability and scalability in terms of inference latency, system stability, and concurrent processing capacity."
2505.14184,"This paper presents VaN3Twin-the first open-source, full-stack Network Digital Twin (NDT) framework for simulating the coexistence of multiple Vehicle-to-Everything (V2X) communication technologies with accurate physical-layer modeling via ray tracing. VaN3Twin extends the ms-van3t simulator by integrating Sionna Ray Tracer (RT) in the loop, enabling high-fidelity representation of wireless propagation, including diverse Line-of-Sight (LoS) conditions with focus on LoS blockage due to other vehicles' meshes, Doppler effect, and site-dependent effects-e.g., scattering and diffraction. Unlike conventional simulation tools, the proposed framework supports realistic coexistence analysis across DSRC and C-V2X technologies operating over shared spectrum. A dedicated interference tracking module captures cross-technology interference at the time-frequency resource block level and enhances signal-to-interference-plus-noise ratio (SINR) estimation by eliminating artifacts such as the bimodal behavior induced by separate LoS/NLoS propagation models. Compared to field measurements, VaN3Twin reduces application-layer disagreement by 50% in rural and over 70% in urban environments with respect to current state-of-the-art simulation tools, demonstrating its value for scalable and accurate digital twin-based V2X coexistence simulation."
2505.14199,"Since the standardization of IPv6 in 1998, both versions of the Internet Protocol have coexisted in the Internet. Clients usually run algorithms such as Happy Eyeballs, to decide whether to connect to an IPv4 or IPv6 endpoint for dual-stack domains. To identify whether two addresses belong to the same device or service, researchers have proposed different forms of alias resolution techniques. Similarly, one can also form siblings of IPv4 and IPv6 addresses belonging to the same device. Traditionally, all of these approaches have focused on individual IP addresses.In this work, we propose the concept of ""sibling prefixes"", where we extend the definition of an IPv4-IPv6 sibling to two IP prefixe-one IPv4 prefix and its sibling IPv6 prefix. We present a technique based on large-scale DNS resolution data to identify 76k IPv4-IPv6 sibling prefixes. We find sibling prefixes to be relatively stable over time. We present SP-Tuner algorithm to tune the CIDR size of sibling prefixes and improve the perfect match siblings from 52% to 82%. For more than half of sibling prefixes, the organization names for their IPv4 and IPv6 origin ASes are identical, and 60% of all sibling prefixes have at least one of the prefixes with a valid ROV status in RPKI. Furthermore, we identify sibling prefixes in 24 hypergiant and CDN networks. Finally, we plan to regularly publish a list of sibling prefixes to be used by network operators and fellow researchers in dual-stack studies."
2505.14358,"Latency is a key indicator of Internet service performance. Continuously tracking the latency of client requests enables service operators to quickly identify bottlenecks, perform adaptive resource allocation or routing, and mitigate attacks. Passively measuring the response latency at intermediate vantage points is attractive since it provides insight into the experience of real clients without requiring client instrumentation or incurring probing overheads. This paper presents PIRATE, a passive approach to measure response latencies when only the client-to-server traffic is visible, even when transport headers are encrypted. PIRATE estimates the time gap between causal pairs - two requests such that the response to the first triggered the second - as a proxy for the client-side response latency. Our experiments with a realistic web application show that PIRATE can estimate the response latencies measured at the client application layer to within 1 percent. A PIRATE-enhanced layer-4 load balancer (with DSR) cuts tail latencies by 37 percent."
2505.14501,"In mobile network research, the integration of real-world components such as User Equipment (UE) with open-source network infrastructure is essential yet challenging. To address these issues, we introduce open5Gcube, a modular framework designed to integrate popular open-source mobile network projects into a unified management environment. Our publicly available framework allows researchers to flexibly combine different open-source implementations, including different versions, and simplifies experimental setups through containerization and lightweight orchestration. We demonstrate the practical usability of open5Gcube by evaluating its compatibility with various commercial off-the-shelf (COTS) smartphones and modems across multiple mobile generations (2G, 4G, and 5G). The results underline the versatility and reproducibility of our approach, significantly advancing the accessibility of rigorous experimentation in mobile network laboratories."
2505.14509,"This paper investigates the ongoing use of the A5/1 ciphering algorithm within 2G GSM networks. Despite its known vulnerabilities and the gradual phasing out of GSM technology by some operators, GSM security remains relevant due to potential downgrade attacks from 4G/5G networks and its use in IoT applications. We present a comprehensive overview of a historical weakness associated with the A5 family of cryptographic algorithms. Building on this, our main contribution is the design of a measurement approach using low-cost, off-the-shelf hardware to passively monitor Cipher Mode Command messages transmitted by base transceiver stations (BTS). We collected over 500,000 samples at 10 different locations, focusing on the three largest mobile network operators in Germany. Our findings reveal significant variations in algorithm usage among these providers. One operator favors A5/3, while another surprisingly retains a high reliance on the compromised A5/1. The third provider shows a marked preference for A5/3 and A5/4, indicating a shift towards more secure ciphering algorithms in GSM networks."
2505.1454,"5G wireless networks are complex, leveraging layers of scheduling, retransmission, and adaptation mechanisms to maximize their efficiency. But these mechanisms interact to produce significant fluctuations in uplink and downlink capacity and latency. This markedly impacts the performance of real-time applications, such as video-conferencing, which are particularly sensitive to such fluctuations, resulting in lag, stuttering, distorted audio, and low video quality. This paper presents a cross-layer view of 5G networks and their impact on and interaction with video-conferencing applications. We conduct novel, detailed measurements of both Private CBRS and commercial carrier cellular network dynamics, capturing physical- and link-layer events and correlating them with their effects at the network and transport layers, and the video-conferencing application itself. Our two datasets comprise days of low-rate campus-wide Zoom telemetry data, and hours of high-rate, correlated WebRTC-network-5G telemetry data. Based on these data, we trace performance anomalies back to root causes, identifying 24 previously unknown causal event chains that degrade 5G video conferencing. Armed with this knowledge, we build Domino, a tool that automates this process and is user-extensible to future wireless networks and interactive applications."
2505.14574,"Efficient data replication in decentralized storage systems must account for diverse policies, especially in multi-organizational, data-intensive environments. This work proposes PSMOA, a novel Policy Support Multi-objective Optimization Algorithm for decentralized data replication that dynamically adapts to varying organizational requirements such as minimization or maximization of replication time, storage cost, replication based on content popularity, and load balancing while respecting policy constraints. PSMOA outperforms NSGA-II and NSGA-III in both Generational Distance (20.29 vs 148.74 and 67.74) and Inverted Generational Distance (0.78 vs 3.76 and 5.61), indicating better convergence and solution distribution. These results validate PSMOA's novelty in optimizing data replication in multi-organizational environments."
2505.15148,"Centralized Dynamic Spectrum Sharing (DSS) faces challenges like data security, high management costs, and limited scalability. To address these issues, a blockchain-based DSS scheme has been proposed in this paper. First, we utilize the ERC4907 standard to mint Non-Fungible Spectrum Tokens (NFSTs) that serve as unique identifiers for spectrum resources and facilitate renting. Next, we develop a smart contract for NFST auctions, ensuring secure spectrum transactions through the auction process. Lastly, we create a Web3 spectrum auction platform where users can access idle spectrum data and participate in auctions for NFST leases corresponding to the available spectrum. Experimental results demonstrate that our NFST, designed according to the ERC4907 standard, effectively meets users' secure and efficient DSS requirements, making it a feasible solution."
2505.15531,"Caching is crucial for system performance, but the delayed hit phenomenon, where requests queue during lengthy fetches after a cache miss, significantly degrades user-perceived latency in modern high-throughput systems. While prior works address delayed hits by estimating aggregate delay, they universally assume deterministic fetch latencies. This paper tackles the more realistic, yet unexplored, scenario where fetch latencies are stochastic. We present, to our knowledge, the first theoretical analysis of delayed hits under this condition, deriving analytical expressions for both the mean and variance of the aggregate delay assuming exponentially distributed fetch latency. Leveraging these insights, we develop a novel variance-aware ranking function tailored for this stochastic setting to guide cache eviction decisions more effectively. The simulations on synthetic and real-world datasets demonstrate that our proposed algorithm significantly reduces overall latency compared to state-of-the-art delayed-hit strategies, achieving a $3\%-30\%$ reduction on synthetic datasets and approximately $1\%-7\%$ reduction on real-world traces."
2505.15821,"The 3D continuum presents a complex environment that spans the terrestrial, aerial and space domains, with 6Gnetworks serving as a key enabling technology. Current AI approaches for network management rely on monolithic models that fail to capture cross-domain interactions, lack adaptability,and demand prohibitive computational resources. This paper presents a formal model of Compound AI systems, introducing a novel tripartite framework that decomposes complex tasks into specialized, interoperable modules. The proposed modular architecture provides essential capabilities to address the unique challenges of 6G networks in the 3D continuum, where heterogeneous components require coordinated, yet distributed, intelligence. This approach introduces a fundamental trade-off between model and system performance, which must be carefully addressed. Furthermore, we identify key challenges faced by Compound AI systems within 6G networks operating in the 3D continuum, including cross-domain resource orchestration, adaptation to dynamic topologies, and the maintenance of consistent AI service quality across heterogeneous environments."
2505.15827,"Cellular networks are now fundamental infrastructure, powering not just smartphones for daily communication and commerce, but also enabling the expansion of IoT and edge computing through last-mile connectivity. At the core of this infrastructure is the SIM card, which provides essential network authentication and subscriber identification through subscriber cryptographic key and profile information. More recently, the SIM card has evolved from a separate pluggable card, to a card integrated into the board (i.e., soldered onto the board with the same electrical interface) (eSIM), to one that is integrated into the System on Chip (iSIM). However, a fundamental limitation persists across SIM evolution: subscriber identity remains coupled to hardware. eSIM and iSIM technologies, despite enabling remote provisioning, still bind digital identities to specific hardware elements. This makes it complex to support emerging use cases like moving a phone number to a cloud AI service or transferring credentials between different devices while maintaining cellular connectivity. Furthermore, although eSIM and iSIM support multiple profiles (multiple phone numbers or carrier profiles on a single device), all profiles still link back to the same hardware identity. For users seeking to maintain privacy through identity rotation or separation (like having different numbers for different purposes), they are limited by the hardware-bound nature of the security architecture. In this paper, we seek to decouple identity from the device, enhancing privacy and flexibility compared to various SIM designs. By breaking this coupling, we enable scenarios like real identity rotation, integration with virtual assistants, or temporary use of backup phones while maintaining consistent cellular connectivity."
2505.15828,"In this paper, we investigate a quality of experience (QoE)-aware resource allocation problem for reconfigurable intelligent surface (RIS)-assisted digital twin (DT) interaction with uncertain evolution. In the considered system, mobile users are expected to interact with a DT model maintained on a DT server that is deployed on a base station, via effective uplink and downlink channels assisted by an RIS. Our goal is to maximize the sum of all mobile users' joint subjective and objective QoE in DT interactions across various DT scenes, by jointly optimizing phase shift matrix, receive/transmit beamforming matrix, rendering resolution configuration and computing resource allocation. While solving this problem is challenging mainly due to the uncertain evolution of the DT model, which leads to multiple scene-specific problems, and require us to constantly re-solve each of them whenever DT model evolves.To this end, leveraging the dynamic optimization capabilities of decision transformers and the generalization strengths of generative artificial intelligence (GAI), we propose a novel GAI-aided approach, called the prompt-guided decision transformer integrated with zero-forcing optimization (PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO, demonstrating its effectiveness and superiority over counterparts."
2505.15829,"Digital Twin (DT) is a transformative technology poised to revolutionize a wide range of applications. This advancement has led to the emergence of digital twin as a service (DTaaS), enabling users to interact with DT models that accurately reflect the real-time status of their physical counterparts. Quality of DTaaS primarily depends on the freshness of DT data, which can be quantified by the age of information (AoI). The reliance on remote cloud servers solely for DTaaS provisioning presents significant challenges for latency-sensitive applications with strict AoI demands. Edge computing, as a promising paradigm, is expected to enable the AoI-aware provision of real-time DTaaS for users. In this paper, we study the joint optimization of DT model deployment and DT model selection for DTaaS provisioning over edge computing, with the objective of maximizing the quality of DTaaS. To address the uncertainties of DT interactions imposed on DTaaS provisioning, we propose a novel distributionally robust optimization (DRO)-based approach, called Wasserstein DRO (WDRO), where we first reformulate the original problem to a robust optimization problem, with the objective of maximizing the quality of DTaaS under the unforeseen extreme request conditions. Then, we leverage multi-level dual transformations based on Wasserstein distance to derive a robust solution. Simulations are conducted to evaluate the performance of the proposed WDRO, and the results demonstrate its superiority over counterparts."
2505.1583,"Wireless Virtual Reality (VR) is increasingly in demand in Wireless LANs (WLANs). In this paper, a utility function for resource management in wireless VR is proposed. Maximizing the sum rate metric in transmitting VR audio or videos is an important factor for ascertaining low latency in obtaining QoS requirement of users in VR, so forth mmWave frequency band in WLAN technology should be used. This frequency band is presented in IEEE 802.11ad/ay. Resource access method in IEEE 802.11ay standard is MultiUser MIMO (MU-MIMO) with OFDM modulation. Operating at mmWave frequency band is equal to use massive number of antenna to enhance the received power in (Line of Sight) LoS direction by inducing sever propagation with small wavelength. Also for reducing the complexity of hardware in mmWave technology, designers should select some number of connected phase shifters to each antenna element by hybrid beamforming method. Processing delay, transmission delay and queue delay should be considered in acquiring QoS metric in terms of utility function. The optimal closed form expression of the multi-attribute utility function is based on these delays that are calculated by downlink and uplink rates in assistant with hybrid beamforming. Trends of transmission delay and multi-attribute utility function in various Es/N0 values and different scenarios are analyzed. Based on these results, 95.4% accuracy in comparison with ns3 in uplink and downlink channel modeling in IEEE 802.11ay standard's indoor environment has been reported. Also, it is shown that min channel gain consideration can cause reduction in the value of the utility function and incursion in transmission delay in VR."
2505.15835,"Wireless Fidelity (WiFi) based indoor positioning is a widely researched area for determining the position of devices within a wireless network. Accurate indoor location has numerous applications, such as asset tracking and indoor navigation. Despite advances in WiFi localization techniques -- in particular approaches that leverage WiFi telemetry -- their adoption in practice remains limited due to several factors including environmental changes that cause signal fading, multipath effects, interference, which, in turn, impact positioning accuracy. In addition, telemetry data differs depending on the WiFi device vendor, offering distinct features and formats; use case requirements can also vary widely. Currently, there is no unified model to handle all these variations effectively. In this paper, we present WiFiGPT, a Generative Pretrained Transformer (GPT) based system that is able to handle these variations while achieving high localization accuracy. Our experiments with WiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can effectively capture subtle spatial patterns in noisy wireless telemetry, making them reliable regressors. Compared to existing state-of-the-art methods, our method matches and often surpasses conventional approaches for multiple types of telemetry. Achieving sub-meter accuracy for RSSI and FTM and centimeter-level precision for CSI demonstrates the potential of LLM-based localisation to outperform specialized techniques, all without handcrafted signal processing or calibration."
2505.15841,"Visible Light Communication (VLC) combined with Non-Orthogonal Multiple Access (NOMA) offers a promising solution for dense indoor wireless networks. Yet, managing resources effectively is challenged by VLC network dynamic conditions involving user mobility and light dimming. In addition to satisfying Quality of Service (QoS) and network stability requirements. Traditional resource allocation methods and simpler RL approaches struggle to jointly optimize QoS and stability under the dynamic conditions of mobile VLC-NOMA networks. This paper presents MARL frameworks tailored to perform complex joint optimization of resource allocation (NOMA power, user scheduling) and network stability (interference, handovers), considering heterogeneous QoS, user mobility, and dimming in VLC-NOMA systems. Our MARL frameworks capture dynamic channel conditions and diverse user QoS , enabling effective joint optimization. In these frameworks, VLC access points (APs) act as intelligent agents, learning to allocate power and schedule users to satisfy diverse requirements while maintaining network stability by managing interference and minimizing disruptive handovers. We conduct a comparative analysis of two key MARL paradigms: 1) Centralized Training with Decentralized Execution (CTDE) and 2) Centralized Training with Centralized Execution (CTCE). Comprehensive simulations validate the effectiveness of both tailored MARL frameworks and demonstrate an ability to handle complex optimization. The results show key trade-offs, as the CTDE approach achieved approximately 16\% higher for High priority (HP) user QoS satisfaction, while the CTCE approach yielded nearly 7 dB higher average SINR and 12\% lower ping-pong handover ratio, offering valuable insights into the performance differences between these paradigms in complex VLC-NOMA network scenarios."
2505.15847,"In today's world, modern infrastructures are being equipped with information and communication technologies to create large IoT networks.It is essential to monitor these networks to ensure smooth operations by detecting and correcting link failures or abnormal network behaviour proactively, which can otherwise cause interruptions in business operations.This paper presents a novel method for detecting anomalies in wireless links using graph neural networks. The proposed approach involves converting time series data into graphs and training a new graph neural network architecture based on graph attention networks that successfully detects anomalies at the level of individual measurements of the time series data. The model provides competitive results compared to the state of the art while being computationally more efficient with ~171 times fewer trainable parameters."
2505.15848,"We introduce the NVIDIA Sionna Research Kit, a GPU-accelerated research platform for developing and testing AI/ML algorithms in 5G NR cellular networks. Powered by the NVIDIA Jetson AGX Orin, the platform leverages accelerated computing to deliver high throughput and real-time signal processing, while offering the flexibility of a software-defined stack. Built on OpenAirInterface (OAI), it unlocks a broad range of research opportunities. These include developing 5G NR and ORAN compliant algorithms, collecting real-world data for AI/ML training, and rapidly deploying innovative solutions in a very affordable testbed. Additionally, AI/ML hardware acceleration promotes the exploration of use cases in edge computing and AI radio access networks (AI-RAN). To demonstrate the capabilities, we deploy a real-time neural receiver - trained with NVIDIA Sionna and using the NVIDIA TensorRT library for inference - in a 5G NR cellular network using commercial user equipment. The code examples will be made publicly available, enabling researchers to adopt and extend the platform for their own projects."
2505.15854,"The transition from 5G networks to 6G highlights a significant demand for machine learning (ML). Deep learning models, in particular, have seen wide application in mobile networking and communications to support advanced services in emerging wireless environments, such as smart healthcare, smart grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse. The rapid expansion of Internet-of-Things (IoT) devices, many with limited computational capabilities, has accelerated the development of tiny machine learning (TinyML) and resource-efficient ML approaches for cost-effective services. However, the deployment of large-scale machine learning (LargeML) solutions require major computing resources and complex management strategies to support extensive IoT services and ML-generated content applications. Consequently, the integration of TinyML and LargeML is projected as a promising approach for future seamless connectivity and efficient resource management.Although the integration of TinyML and LargeML shows abundant potential, several challenges persist, including performance optimization, practical deployment strategies, effective resource management, and security considerations. In this survey, we review and analyze the latest research aimed at enabling the integration of TinyML and LargeML models for the realization of smart services and applications in future 6G networks and beyond. The paper concludes by outlining critical challenges and identifying future research directions for the holistic integration of TinyML and LargeML in next-generation wireless networks."
2505.15977,"The evolution of 5G wireless technology has revolutionized connectivity, enabling a diverse range of applications. Among these are critical use cases such as real time teleoperation, which demands ultra reliable low latency communications (URLLC) to ensure precise and uninterrupted control, and enhanced mobile broadband (eMBB) services, which cater to data-intensive applications requiring high throughput and bandwidth. In our scenario, there are two queues, one for eMBB users and one for URLLC users. In teleoperation tasks, control commands are received in the URLLC queue, where communication delays occur. The dynamic index (DI) controls the service rate, affecting the telerobotic (URLLC) queue. A separate queue models eMBB data traffic. Both queues are managed through network slicing and application delay constraints, leading to a unified Lagrangian-based Lyapunov optimization for efficient resource allocation. We propose a DRL based hierarchical optimization framework that consists of two levels. At the first level, network optimization dynamically allocates resources for eMBB and URLLC users using a Lagrangian functional and an actor critic network to balance competing objectives. At the second level, control optimization finetunes the best gains for robots, ensuring stability and responsiveness in network conditions. This hierarchical approach enhances both communication and control processes, ensuring efficient resource utilization and optimized performance across the network."
2505.16519,"Over 2.6 billion people remain without access to the Internet in 2025. This phenomenon is especially pronounced in developing regions, where cost and infrastructure limitations are major barriers to connectivity. In response, we design SONIC, a low-cost, scalable data delivery system that builds on existing infrastructures: FM radio for downlink broadcasting, and SMS for personalized uplink. SONIC is motivated by the widespread availability of FM radio and SMS infrastructure in developing regions, along with embedded FM radio tuners in affordable mobile phones. SONIC offers several innovations to effectively transmit Web content over sound over FM radio, in a reliable and compressed form. For example, we transmit pre-rendered webpages and leverage pixel interpolation to recover errors at the receiver. We further modify Android to offer a simpler deployment pipeline, supporting a wide range of devices. We deployed SONIC at an FM radio station in Cameroon for six weeks with 30 participants. Our results demonstrate a sustained downlink throughput of 10 kbps, less than 20% loss for a majority of transmissions with signal strength above -90 dbM, and a strong user engagement across both Web browsing and ChatGPT interactions."
2505.16821,"Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler of the AI-Native Air Interface (AI-AI), where protocol intelligence must scale beyond handcrafted logic. This paper presents, to our knowledge, the first standards-compliant emulation of the Radio Resource Control (RRC) layer using a decoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a multi-vendor corpus of real-world traces spanning both 5G and 4G systems. We treat RRC as a domain-specific language and construct a segmentation-safe, question--answer (Question-and-Answer (QA)) dataset that preserves Abstract Syntax Notation (ASN.1) structure through linearization prior to Byte Pair Encoding (BPE) tokenization. The proposed approach combines parameter-efficient adaptation with schema-bounded prompting to ensure syntactic and procedural fidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance, field-level coverage analysis, and uplink-to-downlink state-machine checks -- alongside semantic similarity and latency profiling across 120 configurations. On 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G sessions, our 8B model achieves a median cosine similarity of 0.97, a 61% relative gain over a zero-shot baseline, while sustaining high conformance rates. These results demonstrate that LAMs, when augmented with protocol-aware reasoning, can directly orchestrate control-plane procedures, laying the foundation for the future Artificial Intelligence (AI)-native Radio Access Network (RAN)."
2505.17337,Large message transmissions in libp2p GossipSub lead to longer than expected network-wide message dissemination times and very high bandwidth utilization. This article identifies key issues responsible for this behavior and proposes modifications to the protocol for transmitting large messages. These modifications preserve the GossipSub resilience and fit well into the current algorithm. The proposed changes are rigorously evaluated for performance using the shadow simulator. Results reveal that the suggested changes reduce bandwidth utilization by up to 61% and message dissemination time by up to 35% under different traffic conditions.
2505.17573,"Real-time traffic monitoring is critical for network operators to ensure performance, security, and visibility, especially as encryption becomes the norm. AI and ML have emerged as powerful tools to create deeper insights from network traffic, but collecting the fine-grained features needed at terabit speeds remains a major bottleneck. We introduce Direct Feature Access (DFA): a high-speed telemetry system that extracts flow features at line rate using P4-programmable data planes, and delivers them directly to GPUs via RDMA and GPUDirect, completely bypassing the ML server's CPU. DFA enables feature enrichment and immediate inference on GPUs, eliminating traditional control plane bottlenecks and dramatically reducing latency. We implement DFA on Intel Tofino switches and NVIDIA A100 GPUs, achieving extraction and delivery of over 31 million feature vectors per second, supporting 524,000 flows within sub-20 ms monitoring periods, on a single port. DFA unlocks scalable, real-time, ML-driven traffic analysis at terabit speeds, pushing the frontier of what is possible for next-generation network monitoring."
2505.1758,"Accurate indoor node localization is critical for practical Wireless Sensor Network (WSN) applications, as Global Positioning System (GPS) fails to provide reliable Line-of-Sight (LoS) conditions in most indoor environments. Real-world localization scenarios often involve unknown obstacles with unpredictable shapes, sizes, quantities, and layouts. These obstacles introduce significant deviations in measured distances between sensor nodes when communication links traverse them, severely compromising localization accuracy. To address this challenge, this paper proposes a robust range-based localization method that strategically identifies and severs obstructed communication paths, leveraging network topology to mitigate obstacle-induced errors. Across diverse obstacle configurations and node densities, the algorithm successfully severed 87% of obstacle-affected paths on average. Under the assumption that Received Signal Strength Indicator (RSSI) provides accurate distance measurements under LoS conditions, the achieved localization accuracy exceeds 99.99%."
2505.17948,"In unmanned aerial vehicle (UAV) assisted millimeter wave (mmWave) communication, appropriate user-UAV association is crucial for improving system performance. In mmWave communication, user throughput largely depends on the line of sight (LoS) connectivity with the UAV, which in turn depends on the mobility pattern of the users. Moreover, different traffic types like enhanced mobile broadband (eMBB) and ultra reliable low latency communication (URLLC) may require different types of LoS connectivity. Existing user-UAV association policies do not consider the user mobility during a time interval and different LoS requirements of different traffic types. In this paper, we consider both of them and develop a user association policy in the presence of building blockages. First, considering a simplified scenario, we have analytically established the LoS area, which is the region where users will experience seamless LoS connectivity for eMBB traffic, and the LoS radius, which is the radius of the largest circle within which the user gets uninterrupted LoS services for URLLC traffic. Then, for a more complex scenario, we present a geometric shadow polygon-based method to compute LoS area and LoS radius. Finally, we associate eMBB and URLLC users, with the UAVs from which they get the maximum average throughput based on LoS area and maximum LoS radius respectively. We show that our approach outperforms the existing discretization based and maximum throughput based approaches."
2505.1829,"We introduce EtherBee, a global dataset integrating detailed Ethereum node metrics, network traffic metadata, and honeypot interaction logs collected from ten geographically diverse vantage points over three months. By correlating node data with granular network sessions and security events, EtherBee provides unique insights into benign and malicious activity, node stability, and network-level threats in the Ethereum peer-to-peer network. A case study shows how client-based optimizations can unintentionally concentrate the network geographically, impacting resilience and censorship resistance. We publicly release EtherBee to promote further investigations into performance, reliability, and security in decentralized networks."
2505.18359,"Standard Power (SP) Wi-Fi 6E in the U.S. is just beginning to be deployed outdoors in the shared but unlicensed 6 GHz band under the control of an Automated Frequency Coordination (AFC) system to protect incumbents, while low-power-indoor (LPI) usage has been steadily increasing over the past 2 years. In this paper, we present the first comprehensive measurements and analyses of a SP Wi-Fi 6E deployment at the University of Notre Dame's football stadium, with 902 access points and a seating capacity of 80,000, coexisting with LPI deployments in adjacent buildings. Measurement campaigns were conducted during and after games, outdoors and indoors to fully characterize the performance of SP Wi-Fi 6E, interactions between SP and LPI and potential for interference to incumbents. Our main conclusions are: (i) in a very short time of about 2 months, the percentage of Wi-Fi 6E client connections is already 14% indicating rapid adoption, (ii) dense SP operation outdoors can negatively impact LPI deployments indoors, depending on building loss, indicating the need to carefully consider hybrid indoor-outdoor sharing deployments, and (iii) spectrum analyzer results indicate an aggregate signal level increase of approximately 10 dB in a Wi-Fi channel during peak usage which could potentially lead to interference since the AFC does not consider aggregate interference when allocating permitted power levels. These results from real-world deployments can inform spectrum policy in other bands where similar sharing mechanisms are being considered, such as 7.125 - 8.4 GHz."
2505.1836,"The 3.55 - 3.7 GHz Citizens Broadband Radio Service (CBRS) band in the U.S., shared with incumbent Navy radars, is witnessing increasing deployments both indoors and outdoors using a shared, licensed model. Among the many use-cases of such private networks is the indoor neutral-host, where cellular customers of Mobile Network Operators (MNOs) can be seamlessly served indoors over CBRS with improved performance, since building loss reduces the indoor signal strength of mid-band 5G cellular signals considerably. In this paper, we present the first detailed measurements and analyses of a real-world deployment of an indoor private network serving as a neutral-host in the CBRS band serving two MNOs. Our findings demonstrate significant advantages: (i) minimal outdoor interference from the CBRS network due to over 22 dB median penetration loss, ensuring compatibility with incumbent users; (ii) substantial indoor performance gains with up to 535$\times$ and 33$\times$ median downlink and uplink throughput improvements, respectively, compared to the worst-performing MNO; (iii) reduced uplink transmit power for user devices (median 12 dB reduction), increasing energy efficiency; and (iv) significant capacity offload from the MNO network (median 233 resource blocks/slot freed in 5G), allowing MNOs to better serve outdoor users. These results highlight the potential of low-power indoor CBRS deployments to improve performance, increase spectrum efficiency, and support coexistence with current and future incumbents, e.g., the 3.1 - 3.45 GHz band being considered for sharing with federal incumbents in the U.S."
2505.18389,"The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates unprecedented opportunities for Intent-Based Networking (IBN) to dynamically optimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...] remains a significant challenge. Current approaches predominantly rely on coarse-grained network slicing, lacking the granularity for dynamic adaptation to individual user conditions and traffic patterns. Despite the existence of a vast body of scheduling algorithms [...], their practical utilization is hindered by implementation heterogeneity, insufficient systematic evaluation in production environments, and the complexity of developing high-performance scheduler implementations.[...] To address these limitations, we propose ALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based RAN), a novel framework leveraging LLMs for automated, intent-driven scheduler design, implementation, and evaluation. ALLSTaR interprets NL intents, automatically generates functional scheduler code from the research literature using OCR and LLMs, and intelligently matches operator intents to the most suitable scheduler(s). Our implementation deploys these schedulers as O-RAN dApps, enabling on-the-fly deployment and testing on a production-grade, 5G-compliant testbed. This approach has enabled the largest-scale OTA experimental comparison of 18 scheduling algorithms automatically synthesized from the academic literature. The resulting performance profiles serve as the input for our Intent-Based Scheduling (IBS) framework, which dynamically selects and deploys appropriate schedulers that optimally satisfy operator intents. We validate our approach through multiple use cases unattainable with current slicing-based optimization techniques, demonstrating fine-grained control based on buffer status, physical layer conditions, and heterogeneous traffic types"
2505.18753,"With the significant advancements in optical computing platforms recently capable of performing various primitive operations, a seamless integration of optical computing into very fabric of optical communication links is envisioned, paving the way for the advent of \textit{optical computing-communication integrated network}, which provides computing services at the ligthpath scale, alongside the traditional high-capacity communication ones. This necessitates a paradigm shift in optical node architecture, moving away from the conventional optical-bypass design that avoids lightpath interference crossing the same node, toward leveraging such interference for computation. Such new computing capability at the optical layer appears to be a good match with the growing needs of geo-distributed machine learning, where the training of large-scale models and datasets spans geographically diverse nodes, and intermediate results require further aggregation/computation to produce the desired outcomes for the destination node. To address this potential use case, an illustrative example is presented, which highlights the merit of providing in-network optical computing services in comparison with the traditional optical-bypass mode in the context of distributed learning scenarios taking place at two source nodes, and partial results are then optically aggregated to the destination. We then formulate the new \textit{routing, wavelength and computing assignment problem} arisen in serving computing requests, which could be considered as an extension of the traditional routing and wavelength assignment, that is used to accommodate the transmission requests. Simulation results performed on the realistic COST239 topology demonstrate the promising spectral efficiency gains achieved through the \textit{optical computing-communication integrated network} compared to the optical-bypass model."
2505.1929,"Software-defined networking (SDN) represents a revolutionary shift in network technology by decoupling the data plane from the control plane.}In this architecture, all network decision-making processes are centralized in a controller, meaning each switch receives routing information from the controller and forwards network packets accordingly. This clearly highlights the crucial role that controllers play in the overall performance of SDN. Ryu is one of the most widely used SDN controllers, known for its ease of use in research due to its support for Python programming. This makes Ryu a suitable option for experimental and academic studies. In this research, we evaluate the performance of the Ryu controller based on various network metrics and across different network topologies. For experimental analysis, we use Mininet, a powerful network emulation tool that enables the creation of diverse network structures and the connection of switches to controllers. To facilitate the experiments, we developed a Python-based script that executes various network scenarios, connects to different controllers, and captures and stores the results. This study not only provides a comprehensive performance evaluation of the Ryu controller but also paves the way for evaluating other SDN controllers in future research."
2505.19305,"With the worldwide growth of remote communication and telepresence, network measurements form a cornerstone of effective performance assessment and diagnostics for Internet users. Most often, users seek for overall connection performance measurement using publicly available tools (also known as `speed tests') that provide an overview of their connection's throughput and latency. However, extracting meaningful insights from these measurements remains a challenging task for a non-technical audience. Interpreting network measurement data often requires considerable domain expertise to account not only for subtle variations of the connection stability and metrics, but even for simpler concepts such as latency under load or packet loss influence towards connection performance. In the absence of proper expertise, common misconceptions can easily arise. To address these issues, researchers should recognize the importance of making network measurements not only more comprehensive but also more accessible for wider audience without deep technical knowledge. A promising direction to achieve this goal involves leveraging recent advancements in large language models (LLMs), which have demonstrated capabilities in conducting an analysis of complex data in other fields, such as laboratory test results interpretation, news summarization, and personal assistance.In this paper, we describe an ongoing effort to apply large language models and historical data to enhance the interpretation of network measurements in real-world environments. We aim to automate the translation of low-level metric data into accessible explanations, allowing non-experts to make more informed decisions regarding network performance and reliability."
2505.19771,"Time-sensitive networks are designed to meet stringent Quality of Service (QoS) requirements for mixed-criticality traffic with diverse performance demands. Ensuring deterministic guarantees for such traffic while reducing deployment costs remains a significant challenge. This paper proposes a cost-efficient partial deployment strategy for Time Sensitive Networking (TSN) devices within legacy Ethernet network. At the core of our approach is the Credit-Based Shaper (CBS), a key TSN scheduling mechanism. Unlike cost-prohibitive full CBS deployment, our approach selectively integrates CBS where it is most needed to enhance performance while reducing costs. Combining Network Calculus for schedulability verification and a heuristic optimization method for CBS configuration and placement, our proposal minimizes deployment costs while improving schedulability for medium-priority traffic and mitigating blocking delays for high-priority traffic. The feasibility and benefits of our approach are validated on a realistic automotive TSN use case with up to 70% of reduction in TSN devices requirements compared to a full deployment."
2505.20239,"Integrating Time-Sensitive Networking (TSN) and 5th Generation (5G) systems is key for providing wireless low-latency services in industry. Despite research efforts, challenges remain. Due to the lack of commercial 5G modems supporting Ethernet-based sessions, tunneling mechanisms must be used to enable Layer 2 connectivity between TSN islands via IP-based 5G modems. Furthermore, harmonizing traffic classification and prioritization between TSN and 5G technologies is crucial for meeting industrial service requirements. In this work, we propose a Virtual Extensible LAN (VxLAN)-based solution to harmonize frame forwarding and Quality of Service (QoS) treatment among 5G and TSN. Our solution supports multiple Virtual Local Area Networks (VLANs) across several production lines. Furthermore, it supports TSN traffic mapping into 5G QoS flows. We use a 5G testbed to validate the effectiveness of the adopted solution. Our results show the average delay introduced by the proposed mechanisms is approximately 100 {\mu}s, which is significantly lower than the typical 5G packet transmission delay. Moreover, our findings demonstrate our solution preserves QoS treatment between the 5G system and TSN, ensuring that the priority of 5G QoS flows aligns with the priorities of industrial traffic flows."
2505.20714,"This paper presents an innovative frequency-embedded 3D Gaussian splatting (3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling, offering an advancement over the existing works limited to single-frequency modeling. Grounded in fundamental physics, we uncover the complex relationship between EM wave propagation behaviors and RF frequencies. Inspired by this, we design an EM feature network with attenuation and radiance modules to learn the complex relationships between RF frequencies and the key properties of each 3D Gaussian, specifically the attenuation factor and RF signal intensity. By training the frequency-embedded 3DGS model, we can efficiently reconstruct RF radiance fields at arbitrary unknown frequencies within a given 3D environment. Finally, we propose a large-scale power angular spectrum (PAS) dataset containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments, and conduct extensive experiments to verify the effectiveness of our method. Our approach achieves an average Structural Similarity Index Measure (SSIM) up to 0.72, and a significant improvement up to 17.8% compared to the current state-of-the-art (SOTA) methods trained on individual test frequencies. Additionally, our method achieves an SSIM of 0.70 without prior training on these frequencies, which represents only a 2.8% performance drop compared to models trained with full PAS data. This demonstrates our model's capability to estimate PAS at unknown frequencies. For related code and datasets, please refer tothis https URL."
2505.20887,"Reconfigurable intelligent surfaces (RISs) have demonstrated an unparalleled ability to reconfigure wireless environments by dynamically controlling the phase, amplitude, and polarization of impinging waves. However, as nearly passive reflective metasurfaces, RISs may not distinguish between desired and interference signals, which can lead to severe spectrum pollution and even affect performance negatively. In particular, in large-scale networks, the signal-to-interference-plus-noise ratio (SINR) at the receiving node can be degraded due to excessive interference reflected from the RIS. To overcome this fundamental limitation, we propose in this paper a trajectory prediction-based dynamical control algorithm (TPC) for anticipating RIS ON-OFF states sequence, integrating a long-short-term-memory (LSTM) scheme to predict user trajectories. In particular, through a codebook-based algorithm, the RIS controller adaptively coordinates the configuration of the RIS elements to maximize the received SINR. Our simulation results demonstrate the superiority of the proposed TPC method over various system settings."
2505.21018,"With the growing demand for high-bandwidth, low-latency applications, Optical Spectrum as a Service (OSaaS) is of interest for flexible bandwidth allocation within Elastic Optical Networks (EONs) and Open Line Systems (OLS). While OSaaS facilitates transparent connectivity and resource sharing among users, it raises concerns over potential network vulnerabilities due to shared fiber access and inter-channel interference, such as fiber non-linearity and amplifier based crosstalk. These challenges are exacerbated in multi-user environments, complicating the identification and localization of service interferences. To reduce system disruptions and system repair costs, it is beneficial to detect and identify such interferences timely. Addressing these challenges, this paper introduces a Machine Learning (ML) based architecture for network operators to detect and attribute interferences to specific OSaaS users while blind to the users' internal spectrum details. Our methodology leverages available coarse power measurements and operator channel performance data, bypassing the need for internal user information of wide-band shared spectra. Experimental studies conducted on a 190 km optical line system in the Open Ireland testbed, with three OSaaS users demonstrate the model's capability to accurately classify the source of interferences, achieving a classification accuracy of 90.3%."
2505.21518,"Neural network-based medium access control (MAC) protocol models (NPMs) improve goodput through site-specific operations but are vulnerable to shifts from their training network environments, such as changes in the number of user equipments (UEs) severely degrade goodput. To enhance resilience against such environmental shifts, we propose three novel semantic MAC protocol frameworks empowered by large language models (LLMs). First, we introduce a token-based protocol model (TPM), where an LLM generates MAC signaling messages. By editing LLM instruction prompts, TPM enables instant adaptation, which can be further enhanced by TextGrad, an LLM-based automated prompt optimizer. TPM inference is fast but coarse due to the lack of real interactions with the changed environment, and computationally intensive due to the large size of the LLM. To improve goodput and computation efficiency, we develop T2NPM, which transfers and augments TPM knowledge into an NPM via knowledge distillation (KD). Integrating TPM and T2NPM, we propose T3NPM, which employs TPM in the early phase and switches to T2NPM later. To optimize this phase switching, we design a novel metric of meta-resilience, which quantifies resilience to unknown target goodput after environmental shifts. Simulations corroborate that T3NPM achieves 20.56% higher meta-resilience than NPM with 19.8x lower computation cost than TPM in FLOPS."
2505.21526,"Bluetooth backscatter systems, as a crucial technology for low-power communication in the Internet of Things (IoT), have witnessed remarkable development in recent years. This article comprehensively analyzes multiple related papers, including the latest advancements in RF-Transformer and B2Loc systems, summarizes their research progress, challenges faced, and classification, and explores the application prospects and future development directions of such systems. Bluetooth backscatter systems have achieved significant results in terms of compatibility with commercial devices, improvement of communication reliability, and increase in throughput. However, they still face challenges in areas such as communication range, anti-interference ability, and hardware costs. In the future, with continuous technological innovation exemplified by breakthroughs in unified hardware abstraction and decimeter-level localization, Bluetooth backscatter systems are expected to play a more significant role in the IoT field."
2505.21529,"Large-scale Internet of Things (IoT) applications, such as asset tracking and remote sensing, demand multi-year battery lifetimes to minimize maintenance and operational costs. Traditional wireless protocols often employ duty cycling, introducing a tradeoff between latency and idle consumption - both unsuitable for event-driven and ultra-low power systems. A promising approach to address these issues is the integration of always-on wake-up radios (WuRs). They provide asynchronous, ultra-low power communication to overcome these constraints.This paper presents WakeMod, an open-source wake-up transceiver module for the 868MHz ISM band. Designed for easy integration and ultra-low power consumption, it leverages the -75dBm sensitive FH101RF WuR. WakeMod achieves a low idle power consumption of 6.9uW while maintaining responsiveness with a sensitivity of -72.6dBm. Reception of a wake-up call is possible from up to 130m of distance with a -2.1dBi antenna, consuming 17.7uJ with a latency below 54.3ms. WakeMod's capabilities have further been demonstrated in an e-ink price tag application, achieving 7.17uW idle consumption and enabling an estimated 8-year battery life with daily updates on a standard CR2032 coin cell. WakeMod offers a practical solution for energy-constrained, long-term IoT deployments, requiring low-latency, and on-demand communication."
2505.2155,"Collaborative agentic AI is projected to transform entire industries by enabling AI-powered agents to autonomously perceive, plan, and act within digital environments. Yet, current solutions in this field are all built in isolation, and we are rapidly heading toward a landscape of fragmented, incompatible ecosystems. In this position paper, we argue that interoperability, achieved by the adoption of minimal standards, is essential to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To this end, we devise a minimal architectural foundation for collaborative agentic AI, named Web of Agents, which is composed of four components: agent-to-agent messaging, interaction interoperability, state management, and agent discovery. Web of Agents adopts existing standards and reuses existing infrastructure where possible. With Web of Agents, we take the first but critical step toward interoperable agentic systems and offer a pragmatic path forward before ecosystem fragmentation becomes the norm."
2505.21553,"Network traffic prediction techniques have attracted much attention since they are valuable for network congestion control and user experience improvement. While existing prediction techniques can achieve favorable performance when there is sufficient training data, it remains a great challenge to make accurate predictions when only a small amount of training data is available. To tackle this problem, we propose a deep learning model, entitled MetaSTNet, based on a multimodal meta-learning framework. It is an end-to-end network architecture that trains the model in a simulator and transfers the meta-knowledge to a real-world environment, which can quickly adapt and obtain accurate predictions on a new task with only a small amount of real-world training data. In addition, we further employ cross conformal prediction to assess the calibrated prediction intervals. Extensive experiments have been conducted on real-world datasets to illustrate the efficiency and effectiveness of MetaSTNet."
2505.21563,"Anomalies are common in network system monitoring. When manifested as network threats to be mitigated, service outages to be prevented, and security risks to be ameliorated, detecting such anomalous network behaviors becomes of great importance. However, the growing scale and complexity of the mobile communication networks, as well as the ever-increasing amount and dimensionality of the network surveillance data, make it extremely difficult to monitor a mobile network and discover abnormal network behaviors. Recent advances in machine learning allow for obtaining near-optimal solutions to complicated decision-making problems with many sources of uncertainty that cannot be accurately characterized by traditional mathematical models. However, most machine learning algorithms are centralized, which renders them inapplicable to a large-scale distributed wireless networks with tens of millions of mobile devices. In this article, we present fog intelligence, a distributed machine learning architecture that enables intelligent wireless network management. It preserves the advantage of both edge processing and centralized cloud computing. In addition, the proposed architecture is scalable, privacy-preserving, and well suited for intelligent management of a distributed wireless network."
2505.21733,"Online data scraping has taken on new dimensions in recent years, as traditional scrapers have been joined by new AI-specific bots. To counteract unwanted scraping, many sites use tools like the Robots Exclusion Protocol (REP), which places a robots$.$txt file at the site root to dictate scraper behavior. Yet, the efficacy of the REP is not well-understood. Anecdotal evidence suggests some bots comply poorly with it, but no rigorous study exists to support (or refute) this claim. To understand the merits and limits of the REP, we conduct the first large-scale study of web scraper compliance with robots$.$txt directives using anonymized web logs from our institution. We analyze the behavior of 130 self-declared bots (and many anonymous ones) over 40 days, using a series of controlled robots$.$txt experiments. We find that bots are less likely to comply with stricter robots$.$txt directives, and that certain categories of bots, including AI search crawlers, rarely check robots$.$txt at all. These findings suggest that relying on robots$.$txt files to prevent unwanted scraping is risky and highlight the need for alternative approaches."
2505.21769,"The QUIC transport protocol represents a significant evolution in web transport technologies, offering improved performance and reduced latency compared to traditional protocols like TCP. Given the growing number of QUIC implementations, understanding their performance, particularly in video streaming contexts, is essential. This paper presents a comprehensive analysis of various QUIC implementations, focusing on their transport-layer congestion control (CC) performance and its impact on HTTP Adaptive Streaming (HAS) in single-server, multi-client environments. Through extensive trace-driven experiments, we explore how different QUIC CCs impact adaptive bitrate (ABR) algorithms in two video streaming scenarios: video-on- demand (VoD) and low-latency live streaming (LLL). Our study aims to shed light on the impact of QUIC CC implementations, queuing strategies, and cooperative versus competitive dynamics of QUIC streams on user QoE under diverse network conditions. Our results demonstrate that identical CC algorithms across different QUIC implementations can lead to significant performance variations, directly impacting the QoE of video streaming sessions. These findings offer valuable insights into the effectiveness of various QUIC implementations and their implications for optimizing QoE, underscoring the need for intelligent cross-layer designs that integrate QUIC CC and ABR schemes to enhance overall streaming performance."
2505.22123,"As immersive eXtended Reality (XR) applications demand substantial network resources, understanding their interaction with 5G networks becomes crucial to improve them. This paper investigates the role of 5G physical-layer monitoring to manage and enhance the remote rendering of XR content dynamically. By observing network metrics directly from the physical layer, we propose a system to adapt streaming parameters such as bitrate, framerate, and resolution in real time based on available network capacity. Using theoretical formulas to estimate maximum data rate, our approach evaluates network resource availability, enabling the renderer to self-adjust media content representation. This is critical for providing consistent and smooth XR experiences to users, especially as network conditions fluctuate. Our findings suggest that physical-layer monitoring offers valuable insights to increase the Quality of Service (QoS) and has the potential to elevate user experience in remote-rendered XR applications."
2505.22132,"The proliferation of Extended Reality (XR) applications, requiring high-quality, low-latency media streaming, has driven the demand for efficient remote rendering solutions. This paper focuses on holographic conferencing in virtual environments and their required uplink and downlink media transmission capabilities. By examining Media over QUIC (MoQ), Real-time Transport Protocol (RTP) over QUIC (RoQ), and Web Real-Time Communication (WebRTC), we assess their latency performance over Wi-Fi and 5G networks. Improvements of approximately 30% in latency and 60% in connection startup are expected in QUIC-based protocols compared to WebRTC. The experimental setup transmits a remote-rendered virtual experience using real-time video streaming protocols to provide the content to the participant. Our findings contribute to understanding the maturity of streaming protocols, particularly within open-source frameworks, and evaluate their suitability in supporting latency-sensitive XR applications. The study highlights specific protocol advantages across varied remote rendering scenarios, informing the design of future XR communication solutions."
2505.22149,"We focus on computation offloading of applications based on convolutional neural network (CNN) from moving devices, such as mobile robots or autonomous vehicles, to MultiAccess Edge Computing (MEC) servers via a mobile network. In order to reduce overall CNN inference time, we design and implement CNN with early exits and splits, allowing a flexible partial or full offloading of CNN inference. Through real-world experiments, we analyze an impact of the CNN inference offloading on the total CNN processing delay, energy consumption, and classification accuracy in a practical road sign recognition task. The results confirm that offloading of CNN with early exits and splits can significantly reduce both total processing delay and energy consumption compared to full local processing while not impairing classification accuracy. Based on the results of real-world experiments, we derive practical models for energy consumption and total processing delay related to offloading of CNN with early exits and splits."
2505.2232,"Recent advances in large language models (LLMs) have opened new possibilities for automated reasoning and decision-making in wireless networks. However, applying LLMs to wireless communications presents challenges such as limited capability in handling complex logic, generalization, and reasoning. Chain-of-Thought (CoT) prompting, which guides LLMs to generate explicit intermediate reasoning steps, has been shown to significantly improve LLM performance on complex tasks. Inspired by this, this paper explores the application potential of CoT-enhanced LLMs in wireless communications. Specifically, we first review the fundamental theory of CoT and summarize various types of CoT. We then survey key CoT and LLM techniques relevant to wireless communication and networking. Moreover, we introduce a multi-layer intent-driven CoT framework that bridges high-level user intent expressed in natural language with concrete wireless control actions. Our proposed framework sequentially parses and clusters intent, selects appropriate CoT reasoning modules via reinforcement learning, then generates interpretable control policies for system configuration. Using the unmanned aerial vehicle (UAV) network as a case study, we demonstrate that the proposed framework significantly outperforms a non-CoT baseline in both communication performance and quality of generated reasoning."
2505.22424,"With the rapid growth of IoT devices and their diverse workloads, container-based microservices deployed at edge nodes have become a lightweight and scalable solution. However, existing microservice scheduling algorithms often assume static resource availability, which is unrealistic when multiple containers are assigned to an edge node. Besides, containers suffer from cold-start inefficiencies during early-stage training in currently popular reinforcement learning (RL) algorithms. In this paper, we propose a hybrid learning framework that combines offline imitation learning (IL) with online Soft Actor-Critic (SAC) optimization to enable a cold-start-aware microservice scheduling with dynamic allocation for computing resources. We first formulate a delay-and-energy-aware scheduling problem and construct a rule-based expert to generate demonstration data for behavior cloning. Then, a GRU-enhanced policy network is designed in the policy network to extract the correlation among multiple decisions by separately encoding slow-evolving node states and fast-changing microservice features, and an action selection mechanism is given to speed up the convergence. Extensive experiments show that our method significantly accelerates convergence and achieves superior final performance. Compared with baselines, our algorithm improves the total objective by $50\%$ and convergence speed by $70\%$, and demonstrates the highest stability and robustness across various edge configurations."
2505.22443,"As sixth-generation (6G) networks continue to evolve, AI-driven solutions are playing a crucial role in enabling more efficient and adaptive resource management in wireless communication. One of the key innovations in 6G is user-centric cell-free massive Multiple-Input Multiple-Output (UC-CFmMIMO), a paradigm that eliminates traditional cell boundaries and enhances network performance by dynamically assigning access points (APs) to users. This approach is particularly well-suited for vehicular networks, offering seamless, homogeneous, ultra-reliable, and low-latency connectivity. However, in dense networks, a key challenge lies in efficiently allocating frequency resources within a limited shared subband spectrum while accounting for frequency selectivity and the dependency of signal propagation on bandwidth. These factors make resource allocation increasingly complex, especially in dynamic environments where maintaining Quality of Service (QoS) is critical. This paper tackles these challenges by proposing a hybrid multi-user allocation strategy that integrates reinforcement learning (RL) and metaheuristic optimization to enhance spectral efficiency (SE), ensure fairness, and mitigate interference within shared subbands. To assess its effectiveness, we compare this hybrid approach with two other methods: the bio-inspired Aquila Optimizer (AO) and Deep Deterministic Policy Gradient (DDPG)-based Actor-Critic Reinforcement Learning (AC-RL). Our evaluation is grounded in real-world patterns and channel characteristics, utilizing the 3GPP-3D channel modeling framework (QuaDRiGa) to capture realistic propagation conditions. The results demonstrate that the proposed hybrid strategy achieves a superior balance among competing objectives, underscoring the role of AI-driven resource allocation in advancing UC-CFmMIMO systems for next-generation wireless networks."
2505.22963,"The upcoming 6G will fundamentally reshape mobile networks beyond communications, unlocking a multitude of applications that were once considered unimaginable. Meanwhile, security and resilience are especially highlighted in the 6G design principles. However, safeguarding 6G networks will be quite challenging due to various known and unknown threats from highly heterogeneous networks and diversified security requirements of distinct use cases, calling for a comprehensive re-design of security architecture. This motivates us to propose ES3A (Entire Smart Service-based Security Architecture), a novel security architecture for 6G networks. Specifically, we first discuss six high-level principles of our ES3A that include hierarchy, flexibility, scalability, resilience, endogeny, and trust and privacy. With these goals in mind, we then introduce three guidelines from a deployment perspective, envisioning our ES3A that offers service-based security, end-to-end protection, and smart security automation for 6G networks. Our architecture consists of three layers and three domains. It relies on a two-stage orchestration mechanism to tailor smart security strategies for customized protection in high-dynamic 6G networks, thereby addressing the aforementioned challenges. Finally, we prototype the proposed ES3A on a real-world radio system based on Software-Defined Radio (SDR). Experiments show the effectiveness of our ES3A. We also provide a case to show the superiority of our architecture."
2505.23249,"In next-generation wireless networks, supporting real-time applications such as augmented reality, autonomous driving, and immersive Metaverse services demands stringent constraints on bandwidth, latency, and reliability. Existing semantic communication (SemCom) approaches typically rely on static models, overlooking dynamic conditions and contextual cues vital for efficient transmission. To address these challenges, we propose CaSemCom, a context-aware SemCom framework that leverages a Large Language Model (LLM)-based gating mechanism and a Mixture of Experts (MoE) architecture to adaptively select and encode only high-impact semantic features across multiple data modalities. Our multimodal, multi-user case study demonstrates that CaSemCom significantly improves reconstructed image fidelity while reducing bandwidth usage, outperforming single-agent deep reinforcement learning (DRL) methods and traditional baselines in convergence speed, semantic accuracy, and retransmission overhead."
2505.23275,"The rapid development of multimodal AI and Large Language Models (LLMs) has greatly enhanced real-time interaction, decision-making, and collaborative tasks. However, in wireless multi-agent scenarios, limited bandwidth poses significant challenges to exchanging semantically rich multimodal information efficiently. Traditional semantic communication methods, though effective, struggle with redundancy and loss of crucial details. To overcome these challenges, we propose a Retrieval-Augmented Multimodal Semantic Communication (RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven semantic refinement tailored for distributed multi-agent environments, enabling efficient exchange of critical multimodal elements through local caching and selective transmission. Our approach dynamically optimizes retrieval using deep reinforcement learning (DRL) to balance semantic fidelity with bandwidth constraints. A comprehensive case study on multi-agent autonomous driving demonstrates that our DRL-based retrieval strategy significantly improves task completion efficiency and reduces communication overhead compared to baseline methods."
2505.23706,"In connected and autonomous vehicles, machine learning for safety message classification has become critical for detecting malicious or anomalous behavior. However, conventional approaches that rely on centralized data collection or purely local training face limitations due to the large scale, high mobility, and heterogeneous data distributions inherent in inter-vehicle networks. To overcome these challenges, this paper explores Distributed Federated Learning (DFL), whereby vehicles collaboratively train deep learning models by exchanging model updates among one-hop neighbors and propagating models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi) Extension Dataset, we show that DFL can significantly improve classification accuracy across all vehicles compared to learning strictly with local data. Notably, vehicles with low individual accuracy see substantial accuracy gains through DFL, illustrating the benefit of knowledge sharing across the network. We further show that local training data size and time-varying network connectivity correlate strongly with the model's overall accuracy. We investigate DFL's resilience and vulnerabilities under attacks in multiple domains, namely wireless jamming and training data poisoning attacks. Our results reveal important insights into the vulnerabilities of DFL when confronted with multi-domain attacks, underlining the need for more robust strategies to secure DFL in vehicular networks."
2505.24051,"With 5G's rapid global uptake, demand for agile private networks has exploded. A defining beyond-5G capability is network slicing. 3GPP specifies three core slice categories, massive Machine-Type Communications (mMTC), enhanced Mobile Broadband (eMBB), and Ultra-Reliable Low-Latency Communications (URLLC), while ETSI's Zero-Touch Network and Service Management (ZSM) targets human-less operation. Yet existing documents do not spell out end-to-end (E2E) management spanning multiple domains and subnet instances. We introduce the Network Slice-as-a-Service Platform (NASP), designed to work across 3GPP and non-3GPP networks. NASP (i) translates business-level slice requests into concrete physical instances and inter-domain interfaces, (ii) employs a hierarchical orchestrator that aligns distributed management functions, and (iii) exposes clean south-bound APIs toward domain controllers. A prototype was built by unifying guidance from 3GPP, ETSI, and O-RAN, identifying overlaps and gaps among them. We tested NASP with two exemplary deployments, 3GPP and non-3GPP, over four scenarios: mMTC, URLLC, 3GPP-Shared, and non-3GPP. The Communication Service Management Function handled all requests, underlining the platform's versatility. Measurements show that core-network configuration dominates slice-creation time (68 %), and session setup in the URLLC slice is 93 % faster than in the Shared slice. Cost analysis for orchestrating five versus ten concurrent slices reveals a 112 % delta between edge and centralized deployments. These results demonstrate that NASP delivers flexible, standards-aligned E2E slicing while uncovering opportunities to reduce latency and operational cost."
2505.2414,"With the rapid growth of Low Earth Orbit (LEO) satellite networks, satellite-IoT systems using the LoRa technique have been increasingly deployed to provide widespread Internet services to low-power and low-cost ground devices. However, the long transmission distance and adverse environments from IoT satellites to ground devices pose a huge challenge to link reliability, as evidenced by the measurement results based on our real-world setup. In this paper, we propose a blind coherent combining design named B2LoRa to boost LoRa transmission performance. The intuition behind B2LoRa is to leverage the repeated broadcasting mechanism inherent in satellite-IoT systems to achieve coherent combining under the low-power and low-cost constraints, where each re-transmission at different times is regarded as the same packet transmitted from different antenna elements within an antenna array. Then, the problem is translated into aligning these packets at a fine granularity despite the time, frequency, and phase offsets between packets in the case of frequent packet loss. To overcome this challenge, we present three designs - joint packet sniffing, frequency shift alignment, and phase drift mitigation to deal with ultra-low SNRs and Doppler shifts featured in satellite-IoT systems, respectively. Finally, experiment results based on our real-world deployments demonstrate the high efficiency of B2LoRa."
2505.24269,"In-network computation represents a transformative approach to addressing the escalating demands of Artificial Intelligence (AI) workloads on network infrastructure. By leveraging the processing capabilities of network devices such as switches, routers, and Network Interface Cards (NICs), this paradigm enables AI computations to be performed directly within the network fabric, significantly reducing latency, enhancing throughput, and optimizing resource utilization. This paper provides a comprehensive analysis of optimizing in-network computation for AI, exploring the evolution of programmable network architectures, such as Software-Defined Networking (SDN) and Programmable Data Planes (PDPs), and their convergence with AI. It examines methodologies for mapping AI models onto resource-constrained network devices, addressing challenges like limited memory and computational capabilities through efficient algorithm design and model compression techniques. The paper also highlights advancements in distributed learning, particularly in-network aggregation, and the potential of federated learning to enhance privacy and scalability. Frameworks like Planter and Quark are discussed for simplifying development, alongside key applications such as intelligent network monitoring, intrusion detection, traffic management, and Edge AI. Future research directions, including runtime programmability, standardized benchmarks, and new applications paradigms, are proposed to advance this rapidly evolving field. This survey underscores the potential of in-network AI to create intelligent, efficient, and responsive networks capable of meeting the demands of next-generation AI applications."
2505.24295,"With increasing density of small cells in modern multi-cell deployments, a given user can have multiple options for its serving cell. The serving cell for each user must be carefully chosen such that the user achieves reasonably high channel quality from it, and the load on each cell is well balanced. It is relatively straightforward to reason about this without slicing, where all users can share a global load balancing criteria set by the network operator. In this paper, we identify the unique challenges that arise when balancing load in a multi-cell setting with 5G slicing, where users are grouped into slices, and each slice has its own optimization criteria, resource quota, and demand distributions, making it hard to even define which cells are overloaded vs underloaded. We address these challenges through our system, RadioWeaver, that co-designs load balancing with dynamic quota allocation for each slice and each cell. RadioWeaver defines a novel global load balancing criteria across slices, that allows it to easily determine which cells are overloaded despite the fact that different slices optimize for different criteria. Our evaluation, using large-scale trace-driven simulations and a small-scale OpenRAN testbed, show how RadioWeaver achieves 16-365% better performance when compared to several baselines."
2505.24552,"The open radio access network (O-RAN) Alliance developed an architecture and specifications for open and disaggregated cellular networks including many elements that are being widely adopted and implemented in both commercial and research networks. In this paper, we develop transaction-based power consumption models of a centralized O-RAN architecture based on commercial hardware and considering the full end-to-end data path from the radio unit to the data center. We focus on recent fanout limitations and early baseband processing requirements related to current implementations of O-RAN and assess the power consumption impact when baseband processing is employed at different centralization points in the network. Additionally, we explore how greater fanout and sharing deeper into the network impact the balance of processing and transmission. Low processing fanout restrictions motivate greater centralization of the processing. At the same time, allowing for more open radio units per open distributed unit will quickly increase the transmission capacity requirements and related energy use."
2505.24663,"Decentralization is a fundamental design element of the Web3 economy. Blockchains and distributed consensus mechanisms are touted as fault-tolerant, attack-resistant, and collusion-proof because they are decentralized. Recent analyses, however, find some blockchains are decentralized, others are centralized, and that there are trends towards both centralization and decentralization in the blockchain economy. Despite the importance and variability of decentralization across blockchains, we still know little about what enables or constrains blockchain decentralization. We hypothesize that the resource flexibility of consensus mechanisms is a key enabler of the sustained decentralization of blockchain networks. We test this hypothesis using three quasi-experimental shocks -- policy-related, infrastructure-related, and technical -- to resources used in consensus. We find strong suggestive evidence that the resource flexibility of consensus mechanisms enables sustained blockchain decentralization and discuss the implications for the design, regulation, and implementation of blockchains."
2505.24675,"Ensuring the trustworthiness and long-term verifiability of scientific data is a foundational challenge in the era of data-intensive, collaborative research. Provenance metadata plays a key role in this context, capturing the origin, transformation, and usage of research artifacts. However, existing solutions often fall short when applied to distributed, multi-institutional settings. This paper introduces a modular, domain-agnostic architecture for provenance tracking in federated environments, leveraging permissioned blockchain infrastructure to guarantee integrity, immutability, and auditability. The system supports decentralized interaction, persistent identifiers for artifact traceability, and a provenance versioning model that preserves the history of updates. Designed to interoperate with diverse scientific domains, the architecture promotes transparency, accountability, and reproducibility across organizational boundaries. Ongoing work focuses on validating the system through a distributed prototype and exploring its performance in collaborative settings."
2505.24806,"The Internet of Multimedia Things (IoMT) represents a significant advancement in the evolution of IoT technologies, focusing on the transmission and management of multimedia streams. As the volume of data continues to surge and the number of connected devices grows exponentially, internet traffic has reached unprecedented levels, resulting in challenges such as server overloads and deteriorating service quality. Traditional computer network architectures were not designed to accommodate this rapid increase in demand, leading to the necessity for innovative solutions. In response, Software-Defined Networks (SDNs) have emerged as a promising framework, offering enhanced management capabilities by decoupling the control layer from the data layer. This study explores the load balancing of servers within software-defined multimedia IoT networks. The Long Short-Term Memory (LSTM) prediction algorithm is employed to accurately estimate server loads and fuzzy systems are integrated to optimize load distribution across servers. The findings from the simulations indicate that the proposed approach enhances the optimization and management of IoT networks, resulting in improved service quality, reduced operational costs, and increased productivity."
2506.00133,"The Internet of Underwater Things (IoUT) faces major challenges such as low bandwidth, high latency, mobility, and limited energy resources. Traditional routing protocols like RPL, which were designed for land-based networks, do not perform well in these underwater conditions. This paper introduces RL-RPL-UA, a new routing protocol that uses reinforcement learning to improve performance in underwater environments. Each node includes a lightweight RL agent that selects the best parent node based on local information such as packet delivery ratio, buffer level, link quality, and remaining energy. RL-RPL-UA keeps full compatibility with standard RPL messages and adds a dynamic objective function to support real-time decision-making. Simulations using Aqua-Sim show that RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per packet by 14.8%, and extends network lifetime by 80 seconds compared to traditional methods. These results suggest that RL-RPL-UA is a promising and energy-efficient routing solution for underwater networks."
2506.00167,"Puncturing is a promising technique in 3GPP to multiplex Enhanced Mobile Broadband (eMBB) and Ultra-Reliable Low Latency Communications (URLLC) traffic on the same 5G New Radio (NR) air interface. The essence of puncturing is to transmit URLLC packets on demand upon their arrival, by preempting the radio resources (or subcarriers) that are already allocated to eMBB traffic. Although it is considered most bandwidth efficient, puncturing URLLC data on eMBB can lead to degradation of eMBB's performance. Most of the state-of-the-art research addressing this problem employ raw eMBB data throughput as performance metric. This is inadequate as, after puncturing, eMBB data may or may not be successfully decoded at its receiver. This paper presents Cyrus+, a deep reinforcement learning (DRL)-based puncturing solution that employs goodput (through feedback from a receiver's decoder), rather than estimated raw throughput, in its design of reward function. Further, Cyrus+ is tailored specifically for the Open RAN (O-RAN) architecture and fully leverages O-RAN's three control loops at different time scales in its design of DRL. In the Non-Real-Time (Non-RT) RAN Intelligent Controller (RIC), Cyrus+ initializes the policy network that will be used in the RT Open Distributed Unit (O-DU). In the Near-RT RIC, Cyrus+ refines the policy based on dynamic network conditions and feedback from the receivers. In the RT O-DU, Cyrus+ generates a puncturing codebook by considering all possible URLLC arrivals. We build a standard-compliant link-level 5G NR simulator to demonstrate the efficacy of Cyrus+. Experimental results show that Cyrus+ outperforms benchmark puncturing algorithms and meets the stringent timing requirement in 5G NR (numerology 3)."
2506.00283,"Low Earth Orbit (LEO) satellite mega-constellations have emerged as a viable access solution for broadband connectivity in underserved areas. In 2024, Starlink, in partnership with T-Mobile, began beta testing an SMS-only Supplemental Coverage from Space (SCS) service. This marks the first large-scale deployment of Direct Satellite-to-Device (DS2D) communications, allowing unmodified smartphones to connect directly to spaceborne base stations. This paper presents the first measurement study of deployed DS2D technologies. Using crowdsourced mobile network data from the U.S. between October 2024 and July 2025, we provide evidence-based insights into the capabilities, limitations, and future evolution of DS2D technologies for extending mobile connectivity. We find a strong correlation between the number of satellites deployed, the number of unique cell identifiers measured, and the volume of measurements, concentrated in accessible areas with poor terrestrial network coverage, such as national parks and sparsely populated counties. Stable physical-layer measurements were observed throughout the period, with a 24-dB lower median RSRP and a 3-dB higher RSRQ compared to terrestrial networks, reflecting the SMS-only usage of the DS2D network during this period. Based on the SINR measurements collected, we estimate the expected performance of the announced DS2D mobile data service to be around 3 Mbps per beam in outdoor conditions. We also discuss strategies to expand this capacity up to 18 Mbps in the future, depending on key regulatory and business decisions, including allowable out-of-band emissions, permitted number of satellites, and availability of spectrum and orbital resources."
2506.00355,"In this letter, we investigate a novel pinching antenna (PA)-aided wireless powered communication network (WPCN), in which multiple PAs are activated along a waveguide to establish robust line-of-sight links with multiple devices. Both time division multiple access (TDMA) and non-orthogonal multiple access (NOMA) protocols are considered in the PA-WPCN. Moreover, some practical considerations, including a proportional power model for the PAs, a waveguide transmission loss model, and a nonlinear energy harvesting model, are incorporated into the PA-WPCN. Furthermore, we formulate a sum-rate maximization problem by jointly optimizing resource allocation and PAs position. To address the challenging problem of the PAs position optimization, we propose a high-performance element-wise (EW) algorithm and a low-complexity stochastic parameter differential evolution (SPDE) algorithm. Numerical results validate the remarkable performance of the proposed PA-WPCN and the effectiveness of our algorithms, indicating that optimal performance is attained when the PA power distribution ratio of approximately 0.55-0.6."
2506.00766,"Location information is a fundamental requirement for unmanned aerial vehicles (UAVs) and other wireless sensor networks (WSNs). However, accurately and efficiently localizing sensor nodes with diverse functionalities remains a significant challenge, particularly in a hardware-constrained environment. To address this issue and enhance the applicability of artificial intelligence (AI), this paper proposes a localization algorithm that does not require additional hardware. Specifically, the angle between a node and the anchor nodes is estimated based on the received signal strength indication (RSSI). A subsequent localization strategy leverages the inferred angular relationships in conjunction with a bounding box. Experimental evaluations in three scenarios with varying number of nodes demonstrate that the proposed method achieves substantial improvements in localization accuracy, reducing the average error by 72.4% compared to the Min-Max and RSSI-based DV-Hop algorithms, respectively."
2506.00822,"The rapid evolution of Industry 4.0 has led to the emergence of smart factories, where multirobot system autonomously operates to enhance productivity, reduce operational costs, and improve system adaptability. However, maintaining reliable and efficient network operations in these dynamic and complex environments requires advanced automation mechanisms. This study presents a zero-touch network platform that integrates a hierarchical Open Radio Access Network (O-RAN) architecture, enabling the seamless incorporation of advanced machine learning algorithms and dynamic management of communication and computational resources, while ensuring uninterrupted connectivity with multirobot system. Leveraging this adaptability, the platform utilizes federated deep reinforcement learning (FedDRL) to enable distributed decision-making across multiple learning agents, facilitating the adaptive parameter reconfiguration of transmitters (i.e., multirobot system) to optimize long-term system throughput and transmission energy efficiency. Simulation results demonstrate that within the proposed O-RAN-enabled zero-touch network platform, FedDRL achieves a 12% increase in system throughput, a 32% improvement in normalized average transmission energy efficiency, and a 28% reduction in average transmission energy consumption compared to baseline methods such as independent DRL."
2506.00834,"Weighted bandwidth allocation is a powerful abstraction that has a wide range of use cases in modern data center networks. However, realizing highly agile and precise weighted bandwidth allocation for large-scale cloud environments is fundamentally challenging. In this paper, we propose Sze, a lightweight decentralized weighted bandwidth allocation system that leverages simple network telemetry features of commodity Ethernet switches. Given the flow weights, Sze can effectively use the telemetry information to compute and enforce the weighted bandwidth allocations without per-flow, topology, or routing knowledge. We demonstrate the effectiveness of Sze through simulations and testbed experiments, improving TPC-H jobs completion time by up to $0.59\times$ and $0.79\times$ on average."
2506.00924,"This paper introduces a dual-layer framework for network operator-side quality of experience (QoE) assessment that integrates both objective network modeling and subjective user perception extracted from live-streaming platforms. On the objective side, we develop a machine learning model trained on mean opinion scores (MOS) computed via the ITU-T P.1203 reference implementation, allowing accurate prediction of user-perceived video quality using only network parameters such as packet loss, delay, jitter, and throughput without reliance on video content or client-side instrumentation. On the subjective side, we present a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback. A large language model is used to assign scalar MOS scores to filtered comments in a deterministic and reproducible manner. To support scalable and interpretable analysis, we construct a labeled dataset of 47,894 live-stream comments, of which about 34,000 are identified as QoE-relevant through multi-layer semantic filtering. Each comment is enriched with simulated Internet Service Provider attribution and temporally aligned using synthetic timestamps in 5-min intervals. The resulting dataset enables operator-level aggregation and time-series analysis of user-perceived quality. A delta MOS metric is proposed to measure each Internet service provider's deviation from platform-wide sentiment, allowing detection of localized degradations even in the absence of direct network telemetry. A controlled outage simulation confirms the framework's effectiveness in identifying service disruptions through comment-based trends alone. The system provides each operator with its own subjective MOS and the global platform average per interval, enabling real-time interpretation of performance deviations and comparison with objective network-based QoE estimates."
2506.01609,"With the rapid development of technology, the number of smart mobile users is increasing, accompanied by growing demands from applications such as virtual/augmented reality (VR/XR), remote surgery, autonomous vehicles, and real-time holographic communications, all of which require high transmission rates and ultra-low latency in 6G and beyond networks (6G+). This poses enormous challenges in efficiently deploying large-scale networks, including network design, planning, troubleshooting, optimization, and maintenance, without affecting the user experience. Network Digital Twin (NDT) has emerged as a potential solution, enabling the creation of a virtual model that reflects the actual network, supporting the simulation of various network designs, applying diverse operating policies, and reproducing complex fault scenarios under real-world conditions. This motivate us for this study, where we provide a comprehensive survey of NDT in the context of 6G+, covering areas such as radio access networks (RAN), transport networks, 5G core networks and beyond (5GCORE+), cloud/edge computing, applications (blockchain, health system, manufacturing, security, and vehicular networks), non-terrestrial networks (NTNs), and quantum networks, from both academic and industrial perspectives. In particular, we are the first to provide an in-depth guide and usage of RAN and 5GCORE+ for NDT. Then, we provide an extensive review of foundation technologies such as transport networks, cloud/edge computing, applications, NTNs, and quantum networks in NDT. Finally, we discuss the key challenges, open issues, and future research directions for NDT in the context of 6G+."
2506.02133,"This research focuses on timestamping methods for profiling network traffic in software-based environments. Accurate timestamping is crucial for evaluating network performance, particularly in Time-Sensitive Networking (TSN). We explore and compare four timestamping techniques within a TSN emulation context, though its findings extend to other network scenarios. The study leverages the Mininet emulator to model TSN networks, defining hosts, bridges, links, and traffic streams. It characterizes bridge latencies and jitter, solves the TSN scheduling problem based on measured parameters, and evaluates the correctness of a deployed schedule for a use case. Key contributions include a methodology for software-based timestamping, solutions for TSN emulation challenges in Linux and Mininet, and experimental insights for optimizing TSN emulation platforms on various system configurations, with and without Intel TCC, either on a high-end workstation or on an industrial PC."
2506.02217,"Vehicle mobility has a significant impact on wireless communication between vehicles (buses) in Public Transportation Systems (PTS). Nevertheless, the transportation literature does not provide satisfactory models for bus movements because they are influenced by a variety of factors (itineraries, timetables, etc.). Custom-made mobility models that take these issues into account require a great deal of effort and may render simulations unfeasible. This article considers a tool (EMMS) that automatically inserts PTS information into a mobility simulator in order to undertake a complete statistical analysis of vehicular density, trip duration, and vehicle-to-vehicle interaction. In light of opportunistic communication services, this analysis is of the utmost importance."
2506.02297,"The fundamental information-theoretic limits of covert, or low probability of detection (LPD), communication have been extensively studied for over a decade, resulting in the square root law (SRL): only $L\sqrt{n}$ covert bits can be reliably transmitted over time-bandwidth product $n$, for constant $L>0$. Transmitting more either results in detection or decoding errors. The SRL imposes significant constraints on hardware realization of provably-secure covert communication. Thus, experimental validation of covert communication is underexplored: to date, only two experimental studies of SRL-based covert communication are available, both focusing on optical channels. Here, we report our initial results demonstrating the provably-secure covert radio-frequency (RF) communication using software-defined radios (SDRs). These validate theoretical predictions, open practical avenues for implementing covert communication systems, as well as raise future research questions."
2506.02785,"Artificial intelligence (AI) has been increasingly applied to the condition monitoring of vehicular equipment, aiming to enhance maintenance strategies, reduce costs, and improve safety. Leveraging the edge computing paradigm, AI-based condition monitoring systems process vast streams of vehicular data to detect anomalies and optimize operational performance. In this work, we introduce a novel vehicle condition monitoring service that enables real-time diagnostics of a diverse set of anomalies while remaining practical for deployment in real-world edge environments. To address mobility challenges, we propose a closed-loop service orchestration framework where service migration across edge nodes is dynamically triggered by network-related metrics. Our approach has been implemented and tested in a real-world race circuit environment equipped with 5G network capabilities under diverse operational conditions. Experimental results demonstrate the effectiveness of our framework in ensuring low-latency AI inference and adaptive service placement, highlighting its potential for intelligent transportation and mobility applications."
2506.03041,"This research presents a novel framework that combines traditional Optical Time-Domain Reflectometer (OTDR) signal analysis with machine learning to localize and classify fiber optic faults in rural broadband infrastructures. The proposed system addresses a critical need in the expansion of middle-mile and last-mile networks, particularly in regions targeted by the U.S. Broadband Equity, Access, and Deployment (BEAD) Program. By enhancing fault diagnosis through a predictive, AI-based model, this work enables proactive network maintenance in low-resource environments. Experimental evaluations using a controlled fiber testbed and synthetic datasets simulating rural network conditions demonstrate that the proposed method significantly improves detection accuracy and reduces false positives compared to conventional thresholding techniques. The solution offers a scalable, field-deployable tool for technicians and ISPs engaged in rural broadband deployment."
2506.03151,"With the increase in global positioning service demands and the requirement for more precise positioning, assisting existing medium and high orbit satellite-enabled positioning systems with low Earth orbit (LEO) satellites has garnered widespread attention. However, providing low computational complexity performance analysis for hybrid LEO/MEO massive satellite constellations remains a challenge. In this article, we introduce for the first time the application of stochastic geometry (SG) framework in satellite-enabled positioning performance analysis and provide an analytical expression for the K-availiability probability and K-localizability probability under bidirectional beam alignment transmissions. The K-localizability probability, defined as the probability that at least K satellites can participate in the positioning process, serves as a prerequisite for positioning. Since the modeling of MEO satellite constellations within the SG framework has not yet been studied, we integrate the advantages of Cox point processes and binomial point processes, proposing a doubly stochastic binomial point process binomial point process for accurate modeling of MEO satellite constellations. Finally, we investigate the impact of constellation configurations and antenna patterns on the localizability performance of LEO, MEO, and hybrid MEO/LEO constellations. We also demonstrate the network performance gains brought to MEO positioning systems by incorporating assistance from LEO satellites."
2506.03166,"The delivery of high-quality, low-latency video streams is critical for remote autonomous vehicle control, where operators must intervene in real time. However, reliable video delivery over Fourth/Fifth-Generation (4G/5G) mobile networks is challenging due to signal variability, mobility-induced handovers, and transient congestion. In this paper, we present a comprehensive blueprint for an integrated video quality monitoring system, tailored to remote autonomous vehicle operation. Our proposed system includes subsystems for data collection onboard the vehicle, video capture and compression, data transmission to edge servers, real-time streaming data management, Artificial Intelligence (AI) model deployment and inference execution, and proactive decision-making based on predicted video quality. The AI models are trained on a hybrid dataset that combines field-trial measurements with synthetic stress segments and covers Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and encoder-only Transformer architectures. As a proof of concept, we benchmark 20 variants from these model classes together with feed-forward Deep Neural Network (DNN) and linear-regression baselines, reporting accuracy and inference latency. Finally, we study the trade-offs between onboard and edge-based inference. We further discuss the use of explainable AI techniques to enhance transparency and accountability during critical remote-control interventions. Our proactive approach to network adaptation and Quality of Experience (QoE) monitoring aims to enhance remote vehicle operation over next-generation wireless networks."
2506.03167,"Semantic communication (SemCom) has emerged as a promising paradigm for 6G wireless systems by transmitting task-relevant information rather than raw bits, yet existing approaches remain vulnerable to dual sources of uncertainty: semantic misinterpretation arising from imperfect feature extraction and transmission-level perturbations from channel noise. Current deep learning based SemCom systems typically employ domain-specific architectures that lack robustness guarantees and fail to generalize across diverse noise conditions, adversarial attacks, and out-of-distribution data. In this paper, a novel and generalized semantic communication framework called WaSeCom is proposed to systematically address uncertainty and enhance robustness. In particular, Wasserstein distributionally robust optimization is employed to provide resilience against semantic misinterpretation and channel perturbations. A rigorous theoretical analysis is performed to establish the robust generalization guarantees of the proposed framework. Experimental results on image and text transmission demonstrate that WaSeCom achieves improved robustness under noise and adversarial perturbations. These results highlight its effectiveness in preserving semantic fidelity across varying wireless conditions."
2506.03196,"Graph-based learning provides a powerful framework for modeling complex relational structures; however, its application within the domain of wireless security remains significantly underexplored. In this work, we introduce the first application of graph-based learning for jamming source localization, addressing the imminent threat of jamming attacks in wireless networks. Unlike geometric optimization techniques that struggle under environmental uncertainties and dense interference, we reformulate the localization as an inductive graph regression task. Our approach integrates structured node representations that encode local and global signal aggregation, ensuring spatial coherence and adaptive signal fusion. To enhance robustness, we incorporate an attention-based \ac{GNN} that adaptively refines neighborhood influence and introduces a confidence-guided estimation mechanism that dynamically balances learned predictions with domain-informed priors. We evaluate our approach under complex \ac{RF} environments with various sampling densities, network topologies, jammer characteristics, and signal propagation conditions, conducting comprehensive ablation studies on graph construction, feature selection, and pooling strategies. Results demonstrate that our novel graph-based learning framework significantly outperforms established localization baselines, particularly in challenging scenarios with sparse and obfuscated signal information. Our code is available atthis https URL."
2506.03203,"With the rise of the Internet of Things (IoT), more sensors are deployed around us, covering a wide range of applications from industry and agriculture to urban environments such as smart cities. Throughout these applications the sensors collect data of various characteristics and support city planners and decision-makers in their work processes, ultimately maximizing the impact of public funds. This paper introduces the design and implementation of a self-sustaining wireless sensor node designed to continuously monitor the utilization of community street workout parks. The proposed sensor node monitors activity by leveraging acceleration data capturing micro-vibrations that propagate through the steel structures of the workout equipment. This allows us to detect activity duration with an average measured error of only 2.8 seconds. The sensor is optimized with an energy-aware, adaptive sampling and transmission algorithm which, in combination with the Long Range Wide Area Network (LoRaWAN), reduces power consumption to just 1.147 mW in normal operation and as low as 0.712 mW in low-power, standby mode allowing 46 days of battery runtime. In addition, the integrated energy-harvesting circuit was tested in the field. By monitoring the battery voltage for multiple days, it was shown that the sensor is capable of operating sustainably year-round without external power sources. To evaluate the sensor effectiveness, we conducted a week-long field test in Zurich, placing sensors at various street workout parks throughout the city. Analysis of the collected data revealed clear patterns in park usage depending on day and location. This dataset is made publicly available through our online dashboard. Finally, we showcase the potential of IoT for city applications in combination with an accessible data interface for decision-makers."
2506.03219,"Human activity recognition (HAR) research often lacks accessible, comprehensive field data. Commercial systems are rarely open source, hard to expand, and limited by issues like node synchronisation, data throughput, unclear sensor placement, complexity, and high cost. As a result, researchers typically use only a few intuitively placed sensors and conduct limited field trials. HARNode overcomes these challenges with a fully open-source hardware and software platform. Each node includes an ESP32-S3 module (AtomS3), a 9-axis IMU (Bosch BMX160), pressure and temperature sensors (Bosch BMP388), a display, and an I2C port. Data is streamed via Wi-Fi, with NTP-based time synchronisation achieving roughly 1 ms accuracy. The system runs for up to 8 hours and is built using off-the-shelf parts, a simple online PCB service, and a compact 3D-printed housing with Velcro straps, enabling flexible and scalable body placement while requiring little hardware knowledge. In a study with ten subjects wearing eleven HARNodes each, setup took under five minutes per person. A random forest classifier distinguished walking from stair-climbing transitions, showing the benefits of sensor-overprovisioning: Seven nodes achieved approx. 98% accuracy, matching the performance of all eleven. These findings confirm HARNode's value as a fast-deploying, scalable tool for field-based HAR research and optimised sensor placement."
2506.03231,"Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available atthis https URL."
2506.03328,"5G/6G sidelink communications addresses the challenge of connecting outer UEs, which are unable to directly access a base station (gNodeB), through inner UEs that act as relays to connect to the gNodeB. The key performance indicators include the achievable rates, the number of outer UEs that can connect to a gNodeB, and the latency experienced by outer UEs in establishing connections. We consider problem of determining the assignment of outer UEs to inner UEs based on the channel, interference, and traffic characteristics. We formulate an optimization problem to maximize a weighted sum rate of UEs, where weights can represent priority, waiting time, and queue length. This optimization accommodates constraints related to channel and interference characteristics that influence the rates at which links can successfully carry assigned traffic. While an exhaustive search can establish an upper bound on achievable rates by this non-convex optimization problem, it becomes impractical for larger number of outer UEs due to scalability issues related to high computational complexity. To address this, we present a greedy algorithm that incrementally selects links to maximize the sum rate, considering already activated links. This algorithm, although effective in achieving high sum rates, may inadvertently overlook some UEs, raising concerns about fairness. To mitigate this, we introduce a fairness-oriented algorithm that adjusts weights based on waiting time or queue length, ensuring that UEs with initially favorable conditions do not unduly disadvantage others over time. We show that this strategy not only improves the average admission ratio of UEs but also ensures a more equitable distribution of service among them, thereby providing a balanced and fair solution to sidelink communications."
2506.03508,"The rapid and substantial fluctuations in wireless network capacity and traffic demand, driven by the emergence of 6G technologies, have exacerbated the issue of traffic-capacity mismatch, raising concerns about wireless network energy consumption. To address this challenge, we propose a model-data dual-driven resource allocation (MDDRA) algorithm aimed at maximizing the integrated relative energy efficiency (IREE) metric under dynamic traffic conditions. Unlike conventional model-driven or data-driven schemes, the proposed MDDRA framework employs a model-driven Lyapunov queue to accumulate long-term historical mismatch information and a data-driven Graph Radial bAsis Fourier (GRAF) network to predict the traffic variations under incomplete data, and hence eliminates the reliance on high-precision models and complete spatial-temporal traffic data. We establish the universal approximation property of the proposed GRAF network and provide convergence and complexity analysis for the MDDRA algorithm. Numerical experiments validate the performance gains achieved through the data-driven and model-driven components. By analyzing IREE and EE curves under diverse traffic conditions, we recommend that network operators shall spend more efforts to balance the traffic demand and the network capacity distribution to ensure the network performance, particularly in scenarios with large speed limits and higher driving visibility."
2506.04404,"This paper proposes FLUC, a modular framework that integrates open-source Large Language Models (LLMs) with Unmanned Aerial Vehicle (UAV) autopilot systems to enable autonomous control in Flying Networks (FNs). FLUC translates high-level natural language commands into executable UAV mission code, bridging the gap between operator intent and UAV behaviour.FLUC is evaluated using three open-source LLMs - Qwen 2.5, Gemma 2, and LLaMA 3.2 - across scenarios involving code generation and mission planning. Results show that Qwen 2.5 excels in multi-step reasoning, Gemma 2 balances accuracy and latency, and LLaMA 3.2 offers faster responses with lower logical coherence. A case study on energy-aware UAV positioning confirms FLUC's ability to interpret structured prompts and autonomously execute domain-specific logic, showing its effectiveness in real-time, mission-driven control."
2506.04514,"The Internet comprises of interconnected, independently managed Autonomous Systems (AS) that rely on the Border Gateway Protocol (BGP) for inter-domain routing. BGP anomalies--such as route leaks and hijacks--can divert traffic through unauthorized or inefficient paths, jeopardizing network reliability and security. Although existing rule-based and machine learning methods can detect these anomalies using structured metrics, they still require experts with in-depth BGP knowledge of, for example, AS relationships and historical incidents, to interpret events and propose remediation. In this paper, we introduce BEAR (BGP Event Analysis and Reporting), a novel framework that leverages large language models (LLMs) to automatically generate comprehensive reports explaining detected BGP anomaly events. BEAR employs a multi-step reasoning process that translates tabular BGP data into detailed textual narratives, enhancing interpretability and analytical precision. To address the limited availability of publicly documented BGP anomalies, we also present a synthetic data generation framework powered by LLMs. Evaluations on both real and synthetic datasets demonstrate that BEAR achieves 100% accuracy, outperforming Chain-of-Thought and in-context learning baselines. This work pioneers an automated approach for explaining BGP anomaly events, offering valuable operational insights for network management."
2506.04593,"Edge caching is an emerging technology that empowers caching units at edge nodes, allowing users to fetch contents of interest that have been pre-cached at the edge nodes. The key to pre-caching is to maximize the cache hit percentage for cached content without compromising users' privacy. In this letter, we propose a federated learning (FL) assisted edge caching scheme based on lightweight architecture denoising diffusion probabilistic model (LDPM). Our simulation results verify that our proposed scheme achieves a higher cache hit percentage compared to existing FL-based methods and baseline methods."
2506.04594,"WiFi networks have achieved remarkable success in enabling seamless communication and data exchange worldwide. The IEEE 802.11be standard, known as WiFi 7, introduces Multi-Link Operation (MLO), a groundbreaking feature that enables devices to establish multiple simultaneous connections across different bands and channels. While MLO promises substantial improvements in network throughput and latency reduction, it presents significant challenges in channel allocation, particularly in dense network environments. Current research has predominantly focused on performance analysis and throughput optimization within static WiFi 7 network configurations. In contrast, this paper addresses the dynamic channel allocation problem in dense WiFi 7 networks with MLO capabilities. We formulate this challenge as a combinatorial optimization problem, leveraging a novel network performance analysis mechanism. Given the inherent lack of prior network information, we model the problem within a Multi-Armed Bandit (MAB) framework to enable online learning of optimal channel allocations. Our proposed Best-Arm Identification-enabled Monte Carlo Tree Search (BAI-MCTS) algorithm includes rigorous theoretical analysis, providing upper bounds for both sample complexity and error probability. To further reduce sample complexity and enhance generalizability across diverse network scenarios, we put forth LLM-BAI-MCTS, an intelligent algorithm for the dynamic channel allocation problem by integrating the Large Language Model (LLM) into the BAI-MCTS algorithm. Numerical results demonstrate that the BAI-MCTS algorithm achieves a convergence rate approximately $50.44\%$ faster than the state-of-the-art algorithms when reaching $98\%$ of the optimal value. Notably, the convergence rate of the LLM-BAI-MCTS algorithm increases by over $63.32\%$ in dense networks."
2506.04768,"Distributed Denial-of-Service (DDoS) attacks represent a cost-effective and potent threat to network stability. While extensively studied in IPv4 networks, DDoS implications in IPv6 remain underexplored. The vast IPv6 address space renders brute-force scanning and amplifier testing for all active addresses impractical. Innovatively, this work investigates AS-level vulnerabilities to reflection amplification attacks in IPv6.One prerequisite for amplification presence is that it is located in a vulnerable autonomous system (AS) without inbound source address validation (ISAV) deployment. Hence, the analysis focuses on two critical aspects: global detection of ISAV deployment and identification of amplifiers within vulnerable ASes. Specifically, we develop a methodology combining ICMP Time Exceeded mechanisms for ISAV detection, employ IPv6 address scanning for amplifier identification, and utilize dual vantage points for amplification verification.Experimental results reveal that 4,460 ASes (61.36% of measured networks) lack ISAV deployment. Through scanning approximately 47M active addresses, we have identified reflection amplifiers in 3,507 ASes. The analysis demonstrates that current IPv6 networks are fertile grounds for reflection amplification attacks, alarming network security."
2506.0486,"Data has become a critical asset in the digital economy, yet it remains underutilized by Mobile Network Operators (MNOs), unlike Over-the-Top (OTT) players that lead global market valuations. To move beyond the commoditization of connectivity and deliver greater value to customers, data analytics emerges as a strategic enabler. Using data efficiently is essential for unlocking new service opportunities, optimizing operational efficiency, and mitigating operational and business risks. Since Release 15, the 3rd Generation Partnership Project (3GPP) has introduced the Network Data Analytics Function (NWDAF) to provide powerful insights and predictions using data collected across mobile networks, supporting both user-centric and network-oriented use cases. However, academic research has largely focused on a limited set of methods and use cases, driven by the availability of datasets, restricting broader exploration. This study analyzes trends and gaps in more than 70 articles and proposes two novel use cases to promote the adoption of NWDAF and explore its potential for monetization."
2506.04974,"Indoor environments present a significant challenge for wireless connectivity, as immense data demand strains traditional solutions. Public Mobile Network Operators (MNOs), utilizing outdoor macro base stations (BSs), suffer from poor signal penetration. Indoor Wi-Fi networks, on the other hand, may face reliability issues due to spectrum contention. Shared spectrum models, particularly the Citizens Broadband Radio Service (CBRS) utilized by private 4G/5G networks, have emerged as a promising alternative to provide reliable indoor service. Moreover, these private networks are equipped with the neutral-host (NH) model, seamlessly offloading indoor MNOs' traffic to the private CBRS network. This paper presents a comprehensive, in-situ performance evaluation of three co-located technologies utilizing mid-bands spectrum (1-6 GHz)--a CBRS-based NH network, public MNO macro networks, and a Wi-Fi 6 network--within a large, big-box retail store characterized by significant building loss. Our analysis demonstrates: (i) the NH network provides superior indoor coverage compared to MNO macro, requiring only six CBRS devices (CBSDs)--versus 65 Access Points (APs) for enterprise Wi-Fi--to achieve full coverage, with a median building loss of 26.6 dB ensuring interference-free coexistence with outdoor federal incumbents; (ii) the NH network achieves substantial indoor throughput gains, with per-channel normalized throughput improvements of 1.44x and 1.62x in downlink (DL), and 4.33x and 13x in uplink (UL), compared to 4G and 5G macro deployments, respectively; (iii) the NH deployment achieves a median indoor aggregated physical (PHY)-layer DL throughput gain of 2.08x over 5G macro deployments indoors, despite utilizing only 40 MHz of aggregated bandwidth compared to 225 MHz for 5G macro; and (iv) the NH deployment also outperforms Wi-Fi in application-layer HTTP DL performance by 5.05x."
2506.05779,"The paradigm of Intelligent DataPlane (IDP) embeds deep learning (DL) models on the network dataplane to enable intelligent traffic analysis at line-speed. However, the current use of the match-action table (MAT) abstraction on the dataplane is misaligned with DL inference, leading to several key limitations, including accuracy degradation, limited scale, and lack of generality. This paper proposes Pegasus to address these limitations. Pegasus translates DL operations into three dataplane-oriented primitives to achieve generality: Partition, Map, and SumReduce. Specifically, Partition ""divides"" high-dimensional features into multiple low-dimensional vectors, making them more suitable for the dataplane; Map ""conquers"" computations on the low-dimensional vectors in parallel with the technique of fuzzy matching, while SumReduce ""combines"" the computation results. Additionally, Pegasus employs Primitive Fusion to merge computations, improving scalability. Finally, Pegasus adopts full precision weights with fixed-point activations to improve accuracy. Our implementation on a P4 switch demonstrates that Pegasus can effectively support various types of DL models, including Multi-Layer Perceptron (MLP), Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), and AutoEncoder models on the dataplane. Meanwhile, Pegasus outperforms state-of-the-art approaches with an average accuracy improvement of up to 22.8%, along with up to 248x larger model size and 212x larger input scale."
2506.05943,"Nonlinear distortion of a multicarrier signal by a transmitter Power Amplifier (PA) can be a serious problem when designing new highly energy-efficient wireless systems. Although the performance of standard reception algorithms is seriously deteriorated by the nonlinear distortion, the more advanced solutions allow the utilization of additional frequency diversity caused by nonlinear PA. However, while most of the advanced receivers are decision-aided, their gains are observed mostly in a relatively low Bit Error Rate (BER) region, not targeted by adaptive Modulation Coding Schemes utilizing Forward Error Correction (FEC). In this paper, a non-decision-aided Higher-Order Combining (HOC) reception scheme is proposed. While the analytical formulas for finding symbols combining coefficients are not known, machine learning is used for deriving them. The simulation results show an improved BER performance with respect to a standard reception and one of the established decision-aided receivers. However, as HOC has computational complexity that increases rapidly with the number of subcarriers utilized, more studies are needed to apply it in a wideband system."
2506.06074,"To ensure an unprecedented degree of flexibility, next-generation Industry 4.0/5.0 production plants increasingly rely on mobile devices, e.g., autonomous mobile robots and wearables. In these cases, a major requirement is getting rid of cables through the adoption of wireless networks. To this purpose, Wi-Fi is currently deemed one of the most promising solutions. Achieving reliable communications over the air for distributed real-time control applications is, however, not devoid of troubles. In fact, bounded transmission latency must be ensured for most of the exchanged packets. Moreover, for devices powered on batteries, energy consumption also needs to be taken into account. In this paper, a joint simulated analysis of these aspects is carried out to quantitatively evaluate what we can practically expect from Wi-Fi technology."
2506.06469,"The Internet has grown from a humble set of protocols for end-to-end connectivity into a critical global system with no builtin ""immune system"". In the next decade the Internet will likely grow to a trillion nodes and need protection from threats ranging from floods of fake generative data to AI-driven malware. Unfortunately, growing centralisation has lead to the breakdown of mutualism across the network, with surveillance capitalism now the dominant business model. We take lessons from from biological systems towards evolving a more resilient Internet that can integrate adaptation mechanisms into its fabric. We also contribute ideas for how the Internet might incorporate digital immune systems, including how software stacks might mutate to encourage more architectural diversity. We strongly advocate for the Internet to ""re-decentralise"" towards incentivising more mutualistic forms of communication."
2506.06688,"Medium Access Control (MAC) layer protocols are the underlying paradigms which dictate the transmission & reception of data in any network. Particularly for Low-powered Lossy Networks (LLNs), the design and selection of appropiate MAC-layer protocols is crucial inorder to satisfy several networking objectives such as joining time, network lifetime, energy consumption, end-to-end-delay, etc. In this report, we have presented a comparative analysis between Contiki-MAC and Orchestra-enabled TSCH protocol which provides insights towards the network joining & convergence time as well as an estimate of the energy consumption required of build such LLNs. Our results indicates that Contiki-MAC outperforms Orchestra-enabled TSCH by a factor of 13 times in network formation."
2506.06916,"Rogue Base Station (RBS) attacks, particularly those exploiting downgrade vulnerabilities, remain a persistent threat as 5G Standalone (SA) deployments are still limited and User Equipment (UE) manufacturers continue to support legacy network connectivity. This work introduces ARGOS, a comprehensive O-RAN compliant Intrusion Detection System (IDS) deployed within the Near Real-Time RIC, designed to detect RBS downgrade attacks in real time, an area previously unexplored within the O-RAN context. The system enhances the 3GPP KPM Service Model to enable richer, UE-level telemetry and features a custom xApp that applies unsupervised Machine Learning models for anomaly detection. Distinctively, the updated KPM Service Model operates on cross-layer features extracted from Modem Layer 1 (ML1) logs and Measurement Reports collected directly from Commercial Off-The-Shelf (COTS) UEs. To evaluate system performance under realistic conditions, a dedicated testbed is implemented using Open5GS, srsRAN, and FlexRIC, and validated against an extensive real-world measurement dataset. Among the evaluated models, the Variational Autoencoder (VAE) achieves the best balance of detection performance and efficiency, reaching 99.5% Accuracy with only 0.6% False Positives and minimal system overhead."
2506.07715,"The remote identification (Remote ID) broadcast capability allows unmanned aerial vehicles (UAVs) to exchange messages, which is a pivotal technology for inter-UAV communications. Although this capability enhances the operational visibility, low delay in Remote ID-based communications is critical for ensuring the efficiency and timeliness of multi-UAV operations in dynamic environments. To address this challenge, we first establish delay models for Remote ID communications by considering packet reception and collisions across both BLE 4 and Wi-Fi protocols. Building upon these models, we formulate an optimization problem to minimize the long-term communication delay through adaptive protocol selection. Since the delay performance varies with the UAV density, we propose an adaptive BLE/Wi-Fi switching algorithm based on the multi-agent deep Q-network approach. Experimental results demonstrate that in dynamic-density scenarios, our strategy achieves 32.1% and 37.7% lower latency compared to static BLE 4 and Wi-Fi modes respectively."
2506.0788,"Dynamic resource allocation in O-RAN is critical for managing the conflicting QoS requirements of 6G network slices. Conventional reinforcement learning agents often fail in this domain, as their unimodal policy structures cannot model the multi-modal nature of optimal allocation strategies. This paper introduces Diffusion Q-Learning (Diffusion-QL), a novel framework that represents the policy as a conditional diffusion model. Our approach generates resource allocation actions by iteratively reversing a noising process, with each step guided by the gradient of a learned Q-function. This method enables the policy to learn and sample from the complex distribution of near-optimal actions. Simulations demonstrate that the Diffusion-QL approach consistently outperforms state-of-the-art DRL baselines, offering a robust solution for the intricate resource management challenges in next-generation wireless networks."
2506.08132,"Fast training of large machine learning models requires distributed training on AI clusters consisting of thousands of GPUs. The efficiency of distributed training crucially depends on the efficiency of the network interconnecting GPUs in the cluster. These networks are commonly built using RDMA following a Clos-like datacenter topology. To efficiently utilize the network bandwidth, load balancing is employed to distribute traffic across multiple redundant paths. While there exists numerous techniques for load-balancing in traditional datacenters, these are often either optimized for TCP traffic or require specialized network hardware, thus limiting their utility in AI clusters.This paper presents the design and evaluation of Hopper, a new load-balancing technique optimized for RDMA traffic in AI clusters. Operating entirely at the host level, Hopper requires no specialized hardware or modifications to network switches. It continuously monitors the current path for congestion and dynamically switches traffic to a less congested path when congestion is detected. Furthermore, it incorporates a lightweight mechanism to identify alternative paths and carefully controls the timing of path switching to prevent excessive out-of-order packets.We evaluated Hopper using ns-3 simulations and a testbed implementation. Our evaluations show that Hopper reduces the average and 99-percentile tail flow completion time by up to 20% and 14%, respectively, compared to state-of-the-art host-based load balancing techniques."
2506.08386,"The application of small-factor, 5G-enabled Unmanned Aerial Vehicles (UAVs) has recently gained significant interest in various aerial and Industry 4.0 applications. However, ensuring reliable, high-throughput, and low-latency 5G communication in aerial applications remains a critical and underexplored problem. This paper presents the 5th generation (5G) Aero, a compact UAV optimized for 5G connectivity, aimed at fulfilling stringent 3rd Generation Partnership Project (3GPP) requirements. We conduct a set of experiments in an indoor environment, evaluating the UAV's ability to establish high-throughput, low-latency communications in both Line-of-Sight (LoS) and Non-Line-of-Sight (NLoS) conditions. Our findings demonstrate that the 5G Aero meets the required 3GPP standards for Command and Control (C2) packets latency in both LoS and NLoS, and video latency in LoS communications and it maintains acceptable latency levels for video transmission in NLoS conditions. Additionally, we show that the 5G module installed on the UAV introduces a negligible 1% decrease in flight time, showing that 5G technologies can be integrated into commercial off-the-shelf UAVs with minimal impact on battery lifetime. This paper contributes to the literature by demonstrating the practical capabilities of current 5G networks to support advanced UAV operations in telecommunications, offering insights into potential enhancements and optimizations for UAV performance in 5G networks"
2506.08408,"A heterogeneous micro aerial vehicles (MAV) swarm consists of resource-intensive but expensive advanced MAVs (AMAVs) and resource-limited but cost-effective basic MAVs (BMAVs), offering opportunities in diverse fields. Accurate and real-time localization is crucial for MAV swarms, but current practices lack a low-cost, high-precision, and real-time solution, especially for lightweight BMAVs. We find an opportunity to accomplish the task by transforming AMAVs into mobile localization infrastructures for BMAVs. However, translating this insight into a practical system is challenging due to issues in estimating locations with diverse and unknown localization errors of BMAVs, and allocating resources of AMAVs considering interconnected influential factors. This work introduces TransformLoc, a new framework that transforms AMAVs into mobile localization infrastructures, specifically designed for low-cost and resource-constrained BMAVs. We design an error-aware joint location estimation model to perform intermittent joint estimation for BMAVs and introduce a similarity-instructed adaptive grouping-scheduling strategy to allocate resources of AMAVs dynamically. TransformLoc achieves a collaborative, adaptive, and cost-effective localization system suitable for large-scale heterogeneous MAV swarms. We implement and validate TransformLoc on industrial drones. Results show it outperforms all baselines by up to 68\% in localization performance, improving navigation success rates by 60\%. Extensive robustness and ablation experiments further highlight the superiority of its design."
2506.09039,"The next generation of tactical networks (TNs) is poised to further leverage the key enablers of 5G and beyond 5G (B5G) technology, such as radio access network (RAN) slicing and the open RAN (O-RAN) paradigm, to unlock multiple architectural options and opportunities for a wide range of innovative applications. RAN slicing and the O-RAN paradigm are considered game changers in TNs, where the former makes it possible to tailor user services to users requirements, and the latter brings openness and intelligence to the management of the RAN. In TNs, bandwidth scarcity requires a dynamic bandwidth slicing strategy. Although this type of strategy ensures efficient bandwidth utilization, it compromises RAN slicing isolation in terms of quality of service (QoS) performance. To deal with this challenge, we propose a deep reinforcement learning (DRL)-based RAN slicing mechanism that achieves a trade-off between efficient RAN bandwidth sharing and appropriate inter- and intra-slice isolation. The proposed mechanism performs bandwidth allocation in two stages. In the first stage, the bandwidth is allocated to the RAN slices. In the second stage, each slice partitions its bandwidth among its associated users. In both stages, the slicing operation is constrained by several considerations related to improving the QoS of slices and users that in turn foster inter- and intra-slice isolation. The proposed RAN slicing mechanism is based on DRL algorithms to perform the bandwidth sharing operation in each stage. We propose to deploy the mechanism in an O-RAN architecture and describe the O-RAN functional blocks and the main DRL model lifecycle management phases involved. We also develop three different implementations of the proposed mechanism, each based on a different DRL algorithm, and evaluate their performance against multiple baselines across various parameters."
2506.09159,"Stateful migration has emerged as the dominant technology to support microservice mobility at the network edge while ensuring a satisfying experience to mobile end users. This work addresses two pivotal challenges, namely, the implementation and the orchestration of the migration process. We first introduce a novel framework that efficiently implements stateful migration and effectively orchestrates the migration process by fulfilling both network and application KPI targets. Through experimental validation using realistic microservices, we then show that our solution (i) greatly improves migration performance, yielding up to 77% decrease of the migration downtime with respect to the state of the art, and (ii) successfully addresses the strict user QoE requirements of critical scenarios featuring latency-sensitive microservices. Further, we consider two practical use cases, featuring, respectively, a UAV autopilot microservice and a multi-object tracking task, and demonstrate how our framework outperforms current state-of-the-art approaches in configuring the migration process and in meeting KPI targets."
2506.09197,"The concept of spectrum or bandwidth sharing has gained significant global attention as a means to enhance the efficiency of real-time traffic management in wireless networks. Effective bandwidth sharing enables optimal utilization of available resources, reducing congestion and improving QoE for delay-sensitive applications such as real-time video transmission. In this paper, we propose a novel iterative semi-static bandwidth sharing policy that balances the advantages of both static and dynamic sharing approaches. Our approach minimizes the frequency of coordination between network operators while ensuring efficient resource allocation and meeting the stringent QoE demands of real-time traffic. The proposed policy iteratively optimizes both the spectrum sharing between operators and the resource allocation for individual clients. We establish strong theoretical guarantees for the optimality of the proposed policy and prove that it converges to the optimal static sharing policy irrespective of initial conditions or fluctuations in traffic arrival rates. Additionally, we conduct extensive simulations to evaluate the impact of key system parameters - including step size, hyperperiod length, and arrival process dynamics - on the performance of our policy. Our results demonstrate the effectiveness of the proposed approach in achieving near-optimal bandwidth allocation with reduced overhead, making it a practical solution for real-time wireless applications."
2506.09245,"Stringent demands for timely information delivery, driven by the widespread adoption of real-time applications and the Internet of Things, have established the age of information (AoI) as a critical metric for quantifying data freshness. Existing AoI models often assume multi-hop communication networks with fully reliable nodes, which may not accurately capture scenarios involving node transmission failures. This paper presents an analytical framework for two configurations of tandem queue systems, where status updates generated by a single sensor are relayed to a destination monitor through unreliable intermediate nodes. Using the probability generating function, we first derive the sojourn time distribution for an infinite-buffer M/M/1 tandem system with two unreliable nodes. We then extend our analysis to an M/G/1 tandem system with an arbitrary number of unreliable nodes, employing the supplementary variable technique while assuming that only the first node has an infinite buffer. Numerical results demonstrate the impact of key system parameters on the average AoI in unreliable tandem queues with Markovian and non-Markovian service times."
2506.09268,"Integrated terrestrial and non-terrestrial network (TN-NTN) architectures offer a promising solution for expanding coverage and improving capacity for the network. While non-terrestrial networks (NTNs) are primarily exploited for these specific reasons, their role in alleviating terrestrial network (TN) load and enabling energy-efficient operation has received comparatively less attention. In light of growing concerns associated with the densification of terrestrial deployments, this work aims to explore the potential of NTNs in supporting a more sustainable network. In this paper, we propose a novel online optimisation framework for integrated TN-NTN architectures, built on a multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively optimises key system parameters--including bandwidth allocation, user equipment (UE) association, and macro base station (MBS) shutdown--to balance network capacity and energy efficiency in real time. Extensive system-level simulations over a 24-hour period show that our framework significantly reduces the proportion of unsatisfied UEs during peak hours and achieves up to 19% throughput gains and 5% energy savings in low-traffic periods, outperforming standard network settings following 3GPP recommendations."
2506.09647,"Real-time network traffic forecasting is crucial for network management and early resource allocation. Existing network traffic forecasting approaches operate under the assumption that the network traffic data is fully observed. However, in practical scenarios, the collected data are often incomplete due to various human and natural factors. In this paper, we propose a generative model approach for real-time network traffic forecasting with missing data. Firstly, we model the network traffic forecasting task as a tensor completion problem. Secondly, we incorporate a pre-trained generative model to achieve the low-rank structure commonly associated with tensor completion. The generative model effectively captures the intrinsic low-rank structure of network traffic data during pre-training and enables the mapping from a compact latent representation to the tensor space. Thirdly, rather than directly optimizing the high-dimensional tensor, we optimize its latent representation, which simplifies the optimization process and enables real-time forecasting. We also establish a theoretical recovery guarantee that quantifies the error bound of the proposed approach. Experiments on real-world datasets demonstrate that our approach achieves accurate network traffic forecasting within 100 ms, with a mean absolute error (MAE) below 0.002, as validated on the Abilene dataset."
2506.09703,"Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to address communication network split issues and restore connectivity. However, existing graph learning-based resilient algorithms face over-aggregation and non-convergence problems caused by uneven and sparse topology under massive damage scenarios. To alleviate these problems, we propose a novel Multi-Level Damage-Aware Graph Learning (ML-DAGL) algorithm, which generates recovery trajectories by mining information from destroyed UAVs. We first introduce a Multi-Branch Damage Attention (MBDA) module, which forms a sequence of multi-hop Damage Attentive Graphs (mDAG) with different ranges of receptive fields. Each mDAG links only remaining and damaged nodes to ensure a more even degree distribution for mitigating over-aggregation, and utilizes multi-hop dilation to establish more links for sparse topology enhancement. To resort to the mDAG, we propose a Dilated Graph Convolution Network (DGCN), which generates the optimal recovery trajectories with theoretically proven convergence under massive damage cases. Simulation results show that the proposed algorithm can guarantee the connectivity restoration under large swarm and damage scales, while significantly expediting the recovery time by 75.94% and improving the topology uniformity after recovery."
2506.09878,"Virtualizing the Radio-Access Network (RAN) is increasingly viewed as an enabler of affordable 5G expansion and a stepping-stone toward AI-native 6G. Most discussions, however, still approach spectrum policy, cloud engineering and organizational practice as separate topics. This paper offers an integrated perspective spanning four pillars -- science, technology, business strategy and culture. A comparative U.S.\ case study illustrates how mid-band contiguity, complemented by selective mmWave capacity layers, can improve both coverage and churn when orchestrated through software-defined carrier aggregation. We derive analytic capacity and latency bounds for Split 7.2 $\times$ vRAN/O-RAN deployments, quantify the throughput penalty of end-to-end 256-bit encryption, and show how GPU/FPGA off-load plus digital-twin-driven automation keeps the hybrid-automatic-repeat request (HARQ) round-trip within a 0.5 ms budget. When these technical enablers are embedded in a physics-first delivery roadmap, average vRAN cycle time drops an order of magnitude -- even in the presence of cultural head-winds such as dual-ladder'' erosion. Three cybernetic templates -- the Clock-Hierarchy Law, Ashby's Requisite Variety and a delay-cost curve -- are then used to explain why silo-constrained automation can amplify, rather than absorb, integration debt. Looking forward, silicon-paced 6G evolution (9-12 month node shrinks, sub-THz joint communication-and-sensing, chiplet architectures and optical I/O) calls for a dual-resolution planning grid that couples five-year spectrum physics with six-month silicon sprints.'' The paper closes with balanced, action-oriented recommendations for operators, vendors and researchers on sub-THz fronthaul, AI-native security, energy-proportional accelerators and zero-touch assurance."
2506.10111,"The advent of Open Radio Access Networks (O-RAN) has transformed the telecommunications industry by promoting interoperability, vendor diversity, and rapid innovation. However, its disaggregated architecture introduces complex testing challenges, particularly in validating multi-vendor components against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as those provided by Open Testing and Integration Centres (OTICs), rely heavily on manual processes, are fragmented and prone to human error, leading to inconsistency and scalability issues. To address these limitations, we present AI5GTest -- an AI-powered, specification-aware testing framework designed to automate the validation of O-RAN components. AI5GTest leverages a cooperative Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and Debug-LLM. Gen-LLM automatically generates expected procedural flows for test cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references signaling messages against these flows to validate compliance and detect deviations. If anomalies arise, Debug-LLM performs root cause analysis, providing insight to the failure cause. To enhance transparency and trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the Gen-LLM presents top-k relevant official specifications to the tester for approval before proceeding with validation. Evaluated using a range of test cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest demonstrates a significant reduction in overall test execution time compared to traditional manual methods, while maintaining high validation accuracy."
2506.10651,"The rapid advancement of wireless networks has resulted in numerous challenges stemming from their extensive demands for quality of service towards innovative quality of experience metrics (e.g., user-defined metrics in terms of sense of physical experience for haptics applications). In the meantime, large language models (LLMs) emerged as promising solutions for many difficult and complex applications/tasks. These lead to a notion of the integration of LLMs and wireless networks. However, this integration is challenging and needs careful attention in design. Therefore, in this article, we present a notion of rational wireless networks powered by \emph{telecom LLMs}, namely, \emph{LLM-native wireless systems}. We provide fundamentals, vision, and a case study of the distributed implementation of LLM-native wireless systems. In the case study, we propose a solution based on double deep Q-learning (DDQN) that outperforms existing DDQN solutions. Finally, we provide open challenges."
2506.10851,"In this paper, we present a practical deep learning (DL) approach for energy-efficient traffic classification (TC) on resource-limited microcontrollers, which are widely used in IoT-based smart systems and communication networks. Our objective is to balance accuracy, computational efficiency, and real-world deployability. To that end, we develop a lightweight 1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which achieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K parameters, a 20.12K maximum tensor size, and 10.08M floating-point operations (FLOPs). Moreover, it generalizes across various TC tasks, with accuracies ranging from 94% to 99%. To enable deployment, the model is quantized to INT8, suffering only a marginal 1-2% accuracy drop relative to its Float32 counterpart. We evaluate real-world inference performance on two microcontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive Nucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and 115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference, respectively. These results demonstrate the feasibility of on-device encrypted traffic analysis, paving the way for scalable, low-power IoT security solutions."
2506.109,"The increasing demand for reliable, high-capacity communication during large-scale outdoor events poses significant challenges for traditional Terrestrial Networks (TNs), which often struggle to provide consistent coverage in high-density environments. This paper presents a novel 6G radio network planning framework that integrates Non-Terrestrial Networks (NTNs) with Reconfigurable Intelligent Surfaces (RISs) to deliver ubiquitous coverage and enhanced network capacity. Our framework overcomes the limitations of conventional deployable base stations by leveraging NTN architectures, including Low Earth Orbit (LEO) satellites and passive RIS platforms seamlessly integrated with Beyond 5G (B5G) TNs. By incorporating advanced B5G technologies such as Massive Multiple Input Multiple Output (mMIMO) and beamforming, and by optimizing spectrum utilization across the C, S, and Ka bands, we implement a rigorous interference management strategy based on a dynamic SINR model. Comprehensive calculations and simulations validate the proposed framework, demonstrating significant improvements in connectivity, reliability, and cost-efficiency in crowded scenarios. This integration strategy represents a promising solution for meeting the evolving demands of future 6G networks."
2506.10925,"Lunar surface operations impose stringent requirements on wireless communication systems, including autonomy, robustness to disruption, and the ability to adapt to environmental and mission-driven context. While Space-O-RAN provides a distributed orchestration model aligned with 3GPP standards, its decision logic is limited to static policies and lacks semantic integration. We propose a novel extension incorporating a semantic agentic layer enabled by the Model Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols, allowing context-aware decision making across real-time, near-real-time, and non-real-time control layers. Distributed cognitive agents deployed in rovers, landers, and lunar base stations implement wireless-aware coordination strategies, including delay-adaptive reasoning and bandwidth-aware semantic compression, while interacting with multiple MCP servers to reason over telemetry, locomotion planning, and mission constraints."
2506.11319,"This paper presents a hardware-efficient deep neural network (DNN), optimized through hardware-aware neural architecture search (HW-NAS); the DNN supports the classification of session-level encrypted traffic on resource-constrained Internet of Things (IoT) and edge devices. Thanks to HW-NAS, a 1D convolutional neural network (CNN) is tailored on the ISCX VPN-nonVPN dataset to meet strict memory and computational limits while achieving robust performance. The optimized model attains an accuracy of 96.59% with just 88.26K parameters, 10.08M FLOPs, and a maximum tensor size of 20.12K. Compared to state-of-the-art models, it achieves reductions of up to 444-fold, 312-fold, and 15.6-fold in these metrics, respectively, significantly minimizing memory footprint and runtime requirements. The model also demonstrates versatility in classification tasks, achieving accuracies of up to 99.64% in VPN differentiation, VPN-type classification, broader traffic categories, and application identification. In addition, an in-depth approach to header-level preprocessing strategies confirms that the optimized model can provide notable performances across a wide range of configurations, even in scenarios with stricter privacy considerations. Likewise, a reduction in the length of sessions of up to 75% yields significant improvements in efficiency, while maintaining high accuracy with only a negligible drop of 1-2%. However, the importance of careful preprocessing and session length selection in the classification of raw traffic data is still present, as improper settings or aggressive reductions can bring about a 7% reduction in overall accuracy. Those results highlight the method's effectiveness in enforcing cybersecurity for IoT networks, by providing scalable, efficient solutions for the real-time analysis of encrypted traffic within strict hardware limitations."
2506.11556,"The emergence of Agile Earth Observation Satellites (AEOSs) has marked a significant turning point in the field of Earth Observation (EO), offering enhanced flexibility in data acquisition. Concurrently, advancements in onboard satellite computing and communication technologies have greatly enhanced data compression efficiency, reducing network latency and congestion while supporting near real-time information delivery. In this paper, we address the Agile Earth Observation Satellite Scheduling Problem (AEOSSP), which involves determining the optimal sequence of target observations to maximize overall observation profit. Our approach integrates onboard data processing for real-time remote monitoring into the multi-satellite optimization problem. To this end, we define a set of priority indicators and develop a constructive heuristic method, further enhanced with a Local Search (LS) strategy. The results show that the proposed algorithm provides high-quality information by increasing the resolution of the collected frames by up to 10% on average, while reducing the variance in the monitoring frequency of the targets within the instance by up to 83%, ensuring more up-to-date information across the entire set compared to a First-In First-Out (FIFO) method."
2506.1171,"Distributed stream processing systems are widely deployed to process real-time data generated by various devices, such as sensors and software systems. A key challenge in the system is overloading, which leads to an unstable system status and consumes additional system resources. In this paper, we use a graph neural network-based deep reinforcement learning to collaboratively control the data emission rate at which the data is generated in the stream source to proactively avoid overloading scenarios. Instead of using a traditional multi-layer perceptron-styled network to control the rate, the graph neural network is used to process system metrics collected from the stream processing engine. Consequently, the learning agent (i) avoids storing past states where previous actions may affect the current state, (ii) is without waiting a long interval until the current action has been fully effective and reflected in the system's specific metrics, and more importantly, (iii) is able to adapt multiple stream applications in multiple scenarios. We deploy the rate control approach on three applications, and the experimental results demonstrate that the throughput and end-to-end latency are improved by up to 13.5% and 30%, respectively."
2506.11738,"We propose a novel framework for wireless network scheduling with fairness using determinantal (point) processes. Our approach incorporates the repulsive nature of determinantal processes, generalizing traditional Aloha protocols that schedule transmissions independently. We formulate the scheduling problem with an utility function representing fairness. We then recast this formulation as a convex optimization problem over a certain class of determinantal point processes called $L$-ensembles, which are particularly suited for statistical and numerical treatments. These determinantal processes, which have already proven valuable in subset learning, offer an attractive approach to network resource scheduling and allocating. We demonstrate the suitability of determinantal processes for network models based on the signal-to-interference-plus-noise ratio (SINR). Our results highlight the potential of determinantal scheduling coupled with fairness. This work bridges recent advances in machine learning with wireless communications, providing a mathematically elegant and computationally tractable approach to network scheduling."
2506.11744,"Despite the recent advancements in human-machine interfacing, contemporary assistive bionic limbs face critical challenges, including limited computational capabilities, high latency, and unintuitive control mechanisms, leading to suboptimal user experience and abandonment rates. Addressing these challenges requires a shift toward intelligent, interconnected solutions powered by advances in Internet of Things systems, particularly wireless connectivity and edge/cloud computing. This article presents a conceptual approach to transform bionic limbs by harnessing the pervasive connectivity of 5G and the significant computational power of cloud and edge servers, equipping them with capabilities not available hitherto. The system leverages a hierarchical distributed-computing architecture that integrates local, edge, and cloud computing layers. Time-critical tasks are handled by a local processing unit, while compute-intensive tasks are offloaded to edge and cloud servers, leveraging the high data rate, reliable and low latency capabilities of advanced cellular networks. We perform a proof-of-concept validation in a 5G testbed showing that such networks are capable of achieving data rates and fulfilling latency requirements for a natural prosthetic control, allowing for offloading of compute-intensive jobs to the edge/cloud servers. This is the first step towards the realization and real-world validation of cloud-connected bionic limb systems."
2506.11745,"Time-Triggered Communication is a key technology for many safety-critical systems, with applications spanning the areas of aerospace and industrial control. Such communication relies on time-triggered flows, with each flow consisting of periodic packets originating from a source and destined for a destination node. Each packet needs to reach its destination before its deadline. Different flows can have different cycle lengths. To achieve assured transmission of time-triggered flows, existing efforts constrain the packets of a flow to be cyclically transmitted along the same path. Under such Fixed Cyclic Scheduling (FCS), reservation for flows with different cycle lengths can become incompatible over a shared link, limiting the total number of admissible flows. Considering the cycle lengths of different flows, a hyper-cycle has length equal to their least common multiple (LCM). It determines the time duration over which the scheduling compatibility of the different flows can be checked. In this work, we propose a more flexible schedule scheme called the Hypercycle-level Flexible Schedule (HFS) scheme, where a flow's resource reservation can change across cycles within a hypercycle. HFS can significantly increase the number of admitted flows by providing more scheduling options while remaining perfectly compatible with existing Time-Triggered Ethernet system. We show that, theoretically the possible capacity gain provided by HFS over FCS can be unbounded. We formulate the joint pathfinding and scheduling problem under HFS as an ILP problem which we prove to be NP-Hard. To solve HFS efficiently, we further propose a least-load-first heuristic (HFS-LLF), solving HFS as a sequence of shortest path problems. Extensive study shows that HFS admits up to 6 times the number of flows achieved by FCS. Moreover, our proposed HFS-LLF can run 104 times faster than solving HFS using a generic solver."
2506.11749,"Emerging 6G industrial networks envision autonomous in-X subnetworks to support efficient and cost-effective short range, localized connectivity for autonomous control operations. Supporting timely transmission of event-driven, critical control traffic is challenging in such networks is challenging due to limited radio resources, dynamic device activity, and high mobility. In this paper, we propose a distributed, learning-based random access protocol that establishes implicit inter-subnetwork coordination to minimize the collision probability and improves timely delivery. Each subnetwork independently learns and selects access configurations based on a contention signature signal broadcast by a central access point, enabling adaptive, collision-aware access under dynamic traffic and mobility conditions. The proposed approach features lightweight neural models and online training, making it suitable for deployment in constrained industrial subnetworks. Simulation results show that our method significantly improves the probability of timely packet delivery compared to baseline methods, particularly in dense and high-load scenarios. For instance, our proposed method achieves 21% gain in the probability of timely packet delivery compared to a classical Multi-Armed Bandit (MAB) for an industrial setting of 60 subnetworks and 5 radio channels."
2506.11828,"As the telecommunications industry stands at the crossroads between the fifth generation (5G) and sixth generation (6G) of mobile communications, the 3rd generation partnership project (3GPP) Release 20 emerges as a pivotal point of transition. By striking a balance between enhancing 5G-Advanced capabilities and setting the stage for 6G, Release 20 provides the crucial foundation upon which future mobile communication standards and deployments will be built. This article examines these dual objectives, outlining the key enhancements, the motivations behind them, and their implications for the future of mobile communications."
2506.11947,"In response to the ePrivacy Directive and the consent requirements introduced by the GDPR, websites began deploying consent banners to obtain user permission for data collection and processing. However, due to shared third-party services and technical loopholes, non-consensual cross-site tracking can still occur. In fact, contrary to user expectations of seemingly isolated consent, a user's decision on one website may affect tracking behavior on others. In this study, we investigate the technical and behavioral mechanisms behind these discrepancies. Specifically, we disclose a persistent tracking mechanism exploiting web cookies. These cookies, which we refer to as intractable, are initially set on websites with accepted banners, persist in the browser, and are subsequently sent to trackers before the user provides explicit consent on other websites. To meticulously analyze this covert tracking behavior, we conduct an extensive measurement study performing stateful crawls on over 20k domains from the Tranco top list, strategically accepting banners in the first half of domains and measuring intractable cookies in the second half. Our findings reveal that around 50% of websites send at least one intractable cookie, with the majority set to expire after more than 10 days. In addition, enabling the Global Privacy Control (GPC) signal initially reduces the number of intractable cookies by 30% on average, with a further 32% reduction possible on subsequent visits by rejecting the banners. Moreover, websites with Consent Management Platform (CMP) banners, on average, send 6.9 times more intractable cookies compared to those with native banners. Our research further reveals that even if users reject all other banners, they still receive a large number of intractable cookies set by websites with cookie paywalls."
2506.11995,"We consider a Low Earth Orbit (LEO) satellite network with each satellite capable of establishing inter-satellite link (ISL) connections for satellite-to-satellite communication. Since ISLs can be reoriented to change the topology, we optimize the topology to minimize the average shortest path length (ASPL). We characterize the optimal ASPL ISL topology in two families of topologies, 1) vertex-symmetric in which the ISL connections at a satellite node represent a motif that is repeated at all other satellite nodes, and 2) general regular topologies in which no such repeating pattern need exist. We establish ASPL lower bounds for both scenarios and show constructions for which they are achievable assuming each satellite makes 3 or 4 ISL connections. For the symmetric case, we show that the mesh grid is suboptimal in both ASPL and diameter. Additionally, we show there are constructions that maintain intra-orbital ISL connections while still achieving near-optimal ASPL performance. For the general case we show it is possible to construct networks with ASPL close to the general lower bound when the network is sufficiently dense. Simulation results show that for both scenarios, one can find topologies that are very close to the lower bounds as the network size scales."
2506.12003,"The emerging Internet of AI Agents challenges existing web infrastructure designed for human-scale, reactive interactions. Unlike traditional web resources, autonomous AI agents initiate actions, maintain persistent state, spawn sub-agents, and negotiate directly with peers: demanding millisecond-level discovery, instant credential revocation, and cryptographic behavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes whether to upgrade existing infrastructure or implement purpose-built index architectures for autonomous agents. We identify critical failure points: DNS propagation (24-48 hours vs. required milliseconds), certificate revocation unable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate for agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2) Switch options, (3) Hybrid index/registries. Drawing parallels to dialup-to-broadband transitions, we find that agent requirements constitute qualitative, and not incremental, changes. While upgrades offer compatibility and faster deployment, clean-slate solutions provide better performance but require longer for adoption. Our analysis suggests hybrid approaches will emerge, with centralized indexes for critical agents and federated meshes for specialized use cases."
2506.1207,"With the advent of 5G, mobile networks are becoming more dynamic and will therefore present a wider attack surface. To secure these new systems, we propose a multi-domain anomaly detection method that is distinguished by the study of traffic correlation on three dimensions: temporal by analyzing message sequences, semantic by abstracting the parameters these messages contain, and topological by linking them in the form of a graph. Unlike traditional approaches, which are limited to considering these domains independently, our method studies their correlations to obtain a global, coherent and explainable view of anomalies."
2506.12074,"Mobile traffic prediction is an important enabler for optimizing resource allocation and improving energy efficiency in mobile wireless networks. Building on the advanced contextual understanding and generative capabilities of large language models (LLMs), this work introduces a context-aware wireless traffic prediction framework powered by LLMs. To further enhance prediction accuracy, we leverage in-context learning (ICL) and develop a novel two-step demonstration selection strategy, optimizing the performance of LLM-based predictions. The initial step involves selecting ICL demonstrations using the effectiveness rule, followed by a second step that determines whether the chosen demonstrations should be utilized, based on the informativeness rule. We also provide an analytical framework for both informativeness and effectiveness rules. The effectiveness of the proposed framework is demonstrated with a real-world fifth-generation (5G) dataset with different application scenarios. According to the numerical results, the proposed framework shows lower mean squared error and higher R2-Scores compared to the zero-shot prediction method and other demonstration selection methods, such as constant ICL demonstration selection and distance-only-based ICL demonstration selection."
2506.12081,"In this paper, we study a novel latency minimization problem in wireless federated learning (FL) across multi-hop networks. The system comprises multiple routes, each integrating leaf and relay nodes for FL model training. We explore a personalized learning and adaptive aggregation-aware FL (PAFL) framework that effectively addresses data heterogeneity across participating nodes by harmonizing individual and collective learning objectives. We formulate an optimization problem aimed at minimizing system latency through the joint optimization of leaf and relay nodes, as well as relay routing indicator. We also incorporate an additional energy harvesting scheme for the relay nodes to help with their relay tasks. This formulation presents a computationally demanding challenge, and thus we develop a simple yet efficient algorithm based on block coordinate descent and successive convex approximation (SCA) techniques. Simulation results illustrate the efficacy of our proposed joint optimization approach for leaf and relay nodes with relay routing indicator. We observe significant latency savings in the wireless multi-hop PAFL system, with reductions of up to 69.37% compared to schemes optimizing only one node type, traditional greedy algorithm, and scheme without relay routing indicator."
2506.12118,"Virtualized Passive Optical Networks (vPONs) offer a promising solution for modern access networks, bringing enhanced flexibility, reduced capital expenditures (CapEx), and support for multi-tenancy. By decoupling network functions from physical infrastructure, vPONs enable service providers to efficiently share network resources among multiple tenants. In this paper, we propose a novel merging DBA algorithm, called the Dynamic Time and Wavelength Allocation (DTWA) algorithm, for a virtualized DBA (vDBA) architecture in multi-tenant PON environments. The Algorithm, which enables the merging of multiple virtual DBAs into a physical bandwidth map, introduces multi-channel support, allowing each Optical Network Unit (ONU) to dynamically change, taking into consideration different switching times, transmission wavelength. Leveraging the Numba APIs for high-performance optimization, the algorithm achieves real-time performance with minimal additional latency, meeting the stringent requirements of SLA-compliant, latency-critical 6G applications and services. Our analysis highlights an important trade-off in terms of throughput in multi-tenant conditions, between single-channel vs. multi-channel PONs, as a function of ONUs tuning time. We also compare the performance of our algorithm for different traffic distributions. Finally, in order to assess the time computing penalty of dynamic wavelength optimisation in the merging DBA algorithm, we compare it against a baseline Static Wavelength Allocation (SWA) algorithm, where ONUs are designated a fixed wavelength for transmission."
2506.12265,"In Multi-access Edge Computing (MEC) networks, users covered by a mobile network can exploit edge clouds (ECs), computational resources located at the network's edge, to execute virtual network functions (VNFs). ECs are particularly useful when deploying VNFs with strict delay and availability requirements. As users roam in the network and get handed over between cells, deployed VNFs must follow users to retain the benefits of edge computing. Yet, having VNFs ready at the closest EC can be challenging: (i) ECs are not usually powerful enough to store and run any combination of VNFs simultaneously; (ii) if a VNF is not available at the needed EC, a series of time-consuming operations has to be performed before the VNF becomes operational. These limitations can be addressed by proactively starting VNFs instances at (likely) future locations, balancing better latency properties against higher resource usage. Such proactive deployment does need forecasting of user movements, but these will be imperfect, creating yet another tradeoff. We present our approach to this service provisioning problem, SWAVES. When compared on the ratio of users' unsuccessful packets, SWAVES improves such metric by orders of magnitude with respect to other proposed heuristic."
2506.12461,"The dense deployment of 5G heterogeneous networks (HetNets) has improved network capacity. However, it also brings frequent and unnecessary handover challenges to high-speed mobile user equipment (UE), resulting in unstable communication and degraded quality of service. Traditional handovers ignore the type of target next-generation Node B (gNB), resulting in high-speed UEs being able to be handed over to any gNB. This paper proposes a NR cell identity (NCI)-based handover decision-making algorithm (HDMA) to address this issue. The proposed HDMA identifies the type of the target gNB (macro/small/mmWave gNB) using the gNB identity (ID) within the NCI to improve the handover decision-making strategy. The proposed HDMA aims to improve the communication stability of high-speed mobile UE by enabling high-speed UEs to identify the target gNB type during the HDMA using the gNB ID. Simulation results show that the proposed HDMA outperforms other HDMAs in enhanced connection stability."
2506.12462,"Quantum networks (QNs) transmit delicate quantum information across noisy quantum channels. Crucial applications, like quantum key distribution (QKD) and distributed quantum computation (DQC), rely on efficient quantum information transmission. Learning the best path between a pair of end nodes in a QN is key to enhancing such applications. This paper addresses learning the best path in a QN in the online learning setting. We explore two types of feedback: ""link-level"" and ""path-level"". Link-level feedback pertains to QNs with advanced quantum switches that enable link-level benchmarking. Path-level feedback, on the other hand, is associated with basic quantum switches that permit only path-level benchmarking. We introduce two online learning algorithms, BeQuP-Link and BeQuP-Path, to identify the best path using link-level and path-level feedback, respectively. To learn the best path, BeQuP-Link benchmarks the critical links dynamically, while BeQuP-Path relies on a subroutine, transferring path-level observations to estimate link-level parameters in a batch manner. We analyze the quantum resource complexity of these algorithms and demonstrate that both can efficiently and, with high probability, determine the best path. Finally, we perform NetSquid-based simulations and validate that both algorithms accurately and efficiently identify the best path."
2506.13003,"Mobile edge computing (MEC) enhances the performance of 5G networks by enabling low-latency, high-speed services through deploying data units of the base station on edge servers located near mobile users. However, determining the optimal capacity of these servers while dynamically offloading tasks and allocating computing resources to meet uncertain user demands presents significant challenges. This paper focuses on the design and planning of edge servers with the dual objectives of minimizing capacity requirements and reducing service latency for 5G services. To handle the complexity of uncertain user demands, we formulate the problem as a two-stage stochastic model, which can be linearized into a mixed-integer linear programming (MILP) problem. We propose a novel approach called accelerated Benders decomposition (ABD) to solve the problem at a large network scale. Numerical experiments demonstrate that ABD achieves the optimal solution of MILP while significantly reducing computation time."
2506.13153,"An internet network service provider manages its network with multiple objectives, such as high quality of service (QoS) and minimum computing resource usage. To achieve these objectives, a reinforcement learning-based (RL) algorithm has been proposed to train its network management agent. Usually, their algorithms optimize their agents with respect to a single static reward formulation consisting of multiple objectives with fixed importance factors, which we call preferences. However, in practice, the preference could vary according to network status, external concerns and so on. For example, when a server shuts down and it can cause other servers' traffic overloads leading to additional shutdowns, it is plausible to reduce the preference of QoS while increasing the preference of minimum computing resource usages. In this paper, we propose new RL-based network management agents that can select actions based on both states and preferences. With our proposed approach, we expect a single agent to generalize on various states and preferences. Furthermore, we propose a numerical method that can estimate the distribution of preference that is advantageous for unbiased training. Our experiment results show that the RL agents trained based on our proposed approach significantly generalize better with various preferences than the previous RL approaches, which assume static preference during training. Moreover, we demonstrate several analyses that show the advantages of our numerical estimation method."
2506.13287,"Unmanned Aerial Vehicles (UAVs) have emerged as a key enabler for next-generation wireless networks due to their on-demand deployment, high mobility, and ability to provide Line-of-Sight (LoS) connectivity. These features make UAVs particularly well-suited for dynamic and mission-critical applications such as intelligent transportation systems and emergency communications. However, effectively positioning multiple UAVs in real-time to meet non-uniform, time-varying traffic demands remains a significant challenge, especially when aiming to optimize network throughput and resource utilization. In this paper, we propose an Efficient Multi-UAV Traffic-Aware Deployment (EMTAD) Algorithm, a scalable and adaptive framework that dynamically adjusts UAV placements based on real-time user locations and spatial traffic distribution. In contrast to existing methods, EMTAD jointly optimizes UAV positioning and minimizes the number of deployed UAVs, ensuring efficient UE-UAV association while satisfying the traffic demand of users. Simulation results demonstrate that EMTAD significantly improves network performance while reducing deployment overhead by minimizing the number of UAVs required in dynamic and traffic-aware environments."
2506.13626,"Emerging edge computing paradigms enable heterogeneous devices to collaborate on complex computation applications. However, for arbitrary heterogeneous edge networks, delay-optimal forwarding and computation offloading remains an open problem. In this paper, we jointly optimize data/result routing and computation placement in arbitrary networks with heterogeneous node capabilities, and congestion-dependent nonlinear transmission and processing delay. Despite the non-convexity of the formulated problem, based on analyzing the KKT condition, we provide a set of sufficient optimality conditions that solve the problem globally. To provide the insights for such global optimality, we show that the proposed non-convex problem is geodesic-convex with mild assumptions. We also show that the proposed sufficient optimality condition leads to a lower hemicontinuous solution set, providing stability against user-input perturbation. We then extend the framework to incorporate utility-based congestion control and fairness. A fully distributed algorithm is developed to converge to the global optimum. Numerical results demonstrate significant improvements over multiple baselines algorithms."
2506.13822,"The field of backscatter communication has undergone a profound transformation, evolving from a niche technology for radio-frequency identification (RFID) into a sophisticated paradigm poised to enable a truly battery-free Internet of Things (IoT). This evolution is built upon a deepening understanding of the fundamental principles governing these ultra-low-power links. Modern backscatter systems are no longer simple reflectors of continuous waves but are increasingly designed to interact with complex, data-carrying ambient signals from ubiquitous sources like WiFi, ZigBee, and cellular networks. This review systematically charts the journey of ambient backscatter, particularly focusing on its interaction with ZigBee and other commodity wireless protocols over the last decade. We analyze the progression from foundational proof-of-concept systems that established productive backscatter to modern high-throughput, concurrent, and cross-technology communication architectures. Key advancements in fine-grained modulation, robust synchronization, cross-technology physical layer emulation, and multi-tag coordination are detailed. A comparative analysis of state-of-the-art systems highlights the core trade-offs between performance metrics like data rate and range, power consumption, and compatibility with commodity hardware. Finally, we synthesize the primary challenges, including networking scalability, security vulnerabilities, the near-far problem, and practical deployment hurdles, and outline future research directions, such as integration with Reconfigurable Intelligent Surfaces (RIS) and 6G networks, that promise to further expand the capabilities of this transformative technology."
2506.13934,"This report aims to conduct an in-depth comparison of DTN (Delay/Disconnection Tolerant Network) performance and characteristics in the developing country of Barbados versus two major UK cities Nottingham and London. We aim to detect any common patterns or deviations between the two region areas and use the results of our network simulations to draw well-founded conclusions on the reasons for these similarities and differences. In the end we hope to be able to assimilate specific portions of the island to these major cities in regard to DTN characteristics. We also want to investigate the viability of DTN use in the transport sector which has struggled from a range of issues related to efficiency and finance, by recording and analysing the same metrics for a DTN that consists of only buses. This work is intended to serve as a bridge for expanding the breadth of research done on developed countries allowing other researchers to be able to make well informed assumptions about how that research may apply to developing nations. It will consist of results that show graphical trends and analysis of why these trends might exist and how they apply to real world scenarios."
2506.14151,"Traffic classification has a significant impact on maintaining the Quality of Service (QoS) of the network. Since traditional methods heavily rely on feature extraction and large scale labeled data, some recent pre-trained models manage to reduce the dependency by utilizing different pre-training tasks to train generic representations for network packets. However, existing pre-trained models typically adopt pre-training tasks developed for image or text data, which are not tailored to traffic data. As a result, the obtained traffic representations fail to fully reflect the information contained in the traffic, and may even disrupt the protocol information. To address this, we propose TraGe, a novel generic packet representation model for traffic classification. Based on the differences between the header and payload-the two fundamental components of a network packet-we perform differentiated pre-training according to the byte sequence variations (continuous in the header vs. discontinuous in the payload). A dynamic masking strategy is further introduced to prevent overfitting to fixed byte positions. Once the generic packet representation is obtained, TraGe can be finetuned for diverse traffic classification tasks using limited labeled data. Experimental results demonstrate that TraGe significantly outperforms state-of-the-art methods on two traffic classification tasks, with up to a 6.97% performance improvement. Moreover, TraGe exhibits superior robustness under parameter fluctuations and variations in sampling configurations."
2506.14208,"As Internet of Vehicles (IoV) technology continues to advance, edge computing has become an important tool for assisting vehicles in handling complex tasks. However, the process of offloading tasks to edge servers may expose vehicles to malicious external attacks, resulting in information loss or even tampering, thereby creating serious security vulnerabilities. Blockchain technology can maintain a shared ledger among servers. In the Raft consensus mechanism, as long as more than half of the nodes remain operational, the system will not collapse, effectively maintaining the system's robustness and security. To protect vehicle information, we propose a security framework that integrates the Raft consensus mechanism from blockchain technology with edge computing. To address the additional latency introduced by blockchain, we derived a theoretical formula for system delay and proposed a convex optimization solution to minimize the system latency, ensuring that the system meets the requirements for low latency and high reliability. Simulation results demonstrate that the optimized data extraction rate significantly reduces system delay, with relatively stable variations in latency. Moreover, the proposed optimization solution based on this model can provide valuable insights for enhancing security and efficiency in future network environments, such as 5G and next-generation smart city systems."
2506.14221,"With the rapid advancements in coherent Passive Optical Network (PON) technologies featuring 100G and higher data rates, this paper addresses the urgent requirement for sophisticated simulation and MAC layer development within the domain of coherent Time Division Multiplexing (TDM) PON and coherent Time and Frequency Division Multiplexing (TFDM) PON networks. The ever-growing demand for latency-sensitive services and expanding user populations in next-generation 100G and beyond coherent PONs, underscores the crucial need for low-latency bandwidth management and efficient Dynamic Bandwidth Allocation (DBA) mechanisms. In this paper, we present a pioneering analysis of two established DBAs from the perspective of temporal misalignments. Subsequently, a novel DBA algorithm tailored for coherent PONs featuring 100 Gbps data rate and up to 512 end-users is introduced, named the Hybrid-Switch DBA. This innovative approach allows for adaptive switching of the DBA scheme in response to real-time traffic conditions. To the best of our knowledge, this paper represents the first attempt to address the misalignment problem of DBA and proposes a novel DBA solution for both TDM- and TFDM-based coherent PON networks. This research significantly contributes to the development of coherent TDM PON and coherent TFDM PON networks by enhancing the efficiency of bandwidth allocation and addressing the challenges associated with misalignments in DBA mechanisms. As optical access networks continue to evolve to meet the ever-increasing demands of modern communication services, the Hybrid-Switch DBA algorithm presented in this paper offers a promising solution for optimizing network performance and accommodating latency-sensitive applications."
2506.14987,"Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a CNN-based dynamic priority prediction mechanism for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach uses a Convolutional Neural Network and graph coloring to adaptively assign link priorities based on real-time traffic, transmission opportunities, and network conditions. Assuming that first training phase is performed offline, our approach introduced minimal overhead, while enabling more efficient resource allocation, boosting network capacity, SINR, and schedulability. Simulation results show SINR gains of up to 113\%, 94\%, and 49\% over LDP across three network configurations, highlighting its effectiveness for complex URLLC scenarios."
2506.15011,"Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph Convolutional Network (GCN) integrated with a Deep Q-Network (DQN) reinforcement learning framework for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach dynamically learns link priorities based on real-time traffic demand, network topology, remaining transmission opportunities, and interference patterns. The GCN captures spatial dependencies, while the DQN enables adaptive scheduling decisions through reward-guided exploration. Simulation results show that our GCN-DQN model achieves mean SINR improvements of 179.6\%, 197.4\%, and 175.2\% over LDP across three network configurations. Additionally, the GCN-DQN model demonstrates mean SINR improvements of 31.5\%, 53.0\%, and 84.7\% over our previous CNN-based approach across the same configurations. These results underscore the effectiveness of our GCN-DQN model in addressing complex URLLC requirements with minimal overhead and superior network performance."
2506.15947,"Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm to support various low-altitude services through integrated air-ground infrastructure. To satisfy low-latency and high-computation demands, the integration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC) systems plays a vital role, which offloads computing tasks from terminal devices to nearby UAVs, enabling flexible and resilient service provisions for ground users. To promote the development of LAENets, it is significant to achieve low-carbon multi-UAV-assisted MEC networks. However, several challenges hinder this implementation, including the complexity of multi-dimensional UAV modeling and the difficulty of multi-objective coupled optimization. To this end, this paper proposes a novel Retrieval Augmented Generation (RAG)-based Large Language Model (LLM) agent framework for model formulation. Specifically, we develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG, empowering LLM agents to efficiently retrieve structural information from expert databases and generate more accurate optimization problems compared with traditional RAG-based LLM agents. After customizing carbon emission optimization problems for multi-UAV-assisted MEC networks, we propose a Double Regularization Diffusion-enhanced Soft Actor-Critic (R\textsuperscript{2}DSAC) algorithm to solve the formulated multi-objective optimization problem. The R\textsuperscript{2}DSAC algorithm incorporates diffusion entropy regularization and action entropy regularization to improve the performance of the diffusion policy. Furthermore, we dynamically mask unimportant neurons in the actor network to reduce the carbon emissions associated with model training. Simulation results demonstrate the effectiveness and reliability of the proposed HybridRAG-based LLM agent framework and the R\textsuperscript{2}DSAC algorithm."
2506.16409,"LoRa is a promising communication technology for enabling the next-generation indoor Internet of Things applications. Very few studies, however, have analyzed its performance indoors. Besides, these indoor studies investigate mostly the RSSI and SNR of the received packets at the gateway, which, as we show, may not unfold the poor performance of LoRa and its MAC protocol, LoRaWAN, indoors in terms of reliability and energy-efficiency. In this paper, we extensively evaluate the performance of LoRaWAN indoors and then use the key insights to boost its reliability and energy-efficiency by proposing LoRaIN, LoRa Indoor Network, a new link-layer protocol that can be effectively used for indoor deployments. The approach to boosting the reliability and energy efficiency in LoRaIN is underpinned by enabling constructive interference with specific timing requirements analyzed both empirically and mathematically for different pairs of channel bandwidth and spreading factor and relaying precious acknowledgments to the end-devices with the assistance of several booster nodes. The booster nodes do not need any special capability and can be a subset of the LoRa end-devices. To our knowledge, LoRaIN is the first protocol for boosting reliability and energy-efficiency in indoor LoRa networks. We evaluate its performance in an indoor testbed consisting of one LoRaWAN gateway and 20 end-devices. Our extensive evaluation shows that when 15% of the end-devices operate as booster nodes, the reliability at the gateway increases from 62% to 95%, and the end-devices are approximately 2.5x energy-efficient."
2506.16808,"With the emergence of Multi-Access Edge Computing in 5G and beyond, it has become essential for operators to optimize the data path for the end-user while ensuring resources are used according to their policy. In this paper, we review existing solutions to access edge resources, underline their limits, and propose the use of Segment Routing over IPv6 (SRv6) in a 5G/edge architecture."
2506.16914,"Network Calculus (NC) is a versatile methodology based on min-plus algebra to derive worst-case per-flow performance bounds in networked systems with many concurrent flows. In particular, NC can analyze many scheduling disciplines; yet, somewhat surprisingly, an aggregate FIFO server is a notoriously hard case due to its min-plus non-linearity. A resort is to represent the FIFO residual service by a family of functions with a free parameter instead of just a single curve. For simple token-bucket arrival curves, literature provides optimal choices for that free parameter to minimize delay and backlog bounds. In this paper, we tackle the challenge of more general arrival curves than just token buckets. In particular, we derive residual service curves resulting in minimal backlog bounds for general piecewise-linear arrival curves. To that end, we first show that a backlog bound can always be calculated at a breakpoint of either the arrival curve of the flow of interest or its residual service curve. Further, we define a set of curves that characterize the backlog for a fixed breakpoint, depending on the free parameter of the residual service curve. We show that the backlog-minimizing residual service curve family parameter corresponds to the largest intersection of those curves with the arrival curve. In more complex scenarios finding this largest intersection can become inefficient as the search space grows in the number of flows. Therefore, we present an efficient heuristic that finds, in many cases, the optimal parameter or at least a close conservative approximation. This heuristic is evaluated in terms of accuracy and execution time. Finally, we utilize these backlog-minimizing residual service curves to enhance the DiscoDNC tool and observe considerable reductions in the corresponding backlog bounds."
2506.17063,"The exponential growth of IoT devices presents critical challenges in bandwidth-constrained wireless networks, particularly regarding efficient data transmission and privacy preservation. This paper presents a novel federated semantic communication (SC) framework that enables collaborative training of bandwidth-efficient models for image reconstruction across heterogeneous IoT devices. By leveraging SC principles to transmit only semantic features, our approach dramatically reduces communication overhead while preserving reconstruction quality. We address the fundamental challenge of client selection in federated learning environments where devices exhibit significant disparities in dataset sizes and data distributions. Our framework implements three distinct client selection strategies that explore different trade-offs between system performance and fairness in resource allocation. The system employs an end-to-end SC architecture with semantic bottlenecks, coupled with a loss-based aggregation mechanism that naturally adapts to client heterogeneity. Experimental evaluation on image data demonstrates that while Utilitarian selection achieves the highest reconstruction quality, Proportional Fairness maintains competitive performance while significantly reducing participation inequality and improving computational efficiency. These results establish that federated SC can successfully balance reconstruction quality, resource efficiency, and fairness in heterogeneous IoT deployments, paving the way for sustainable and privacy-preserving edge intelligence applications."
2506.17259,"As artificial intelligence capabilities rapidly advance, Telco operators face a growing need to unify fragmented AI efforts across customer experience, network operations, and service orchestration. This paper proposes the design and deployment of a horizontal federated AI operating system tailored for the telecommunications domain. Unlike vertical vendor-driven platforms, this system acts as a common execution and coordination layer, enabling Telcos to deploy AI agents at scale while preserving data locality, regulatory compliance, and architectural heterogeneity. We argue that such an operating system must expose tightly scoped abstractions for telemetry ingestion, agent execution, and model lifecycle management. It should support federated training across sovereign operators, offer integration hooks into existing OSS and BSS systems, and comply with TM Forum and O-RAN standards. Importantly, the platform must be governed through a neutral foundation model to ensure portability, compatibility, and multi-vendor extensibility. This architecture offers a path to break the current silos, unlock ecosystem-level intelligence, and provide a foundation for agent-based automation across the Telco stack. The case for this horizontal layer is not only technical but structural, redefining how intelligence is deployed and composed in a distributed network environment."
2506.17343,"Dhaka, Bangladesh, one of the world's most densely populated cities, faces severe challenges in maintaining reliable, high-speed internet connectivity. This paper presents an innovative framework that addresses poor mobile data connections through the integration of advanced WebRTC technology with adaptive streaming and server-side recording solutions. Focusing on the unique network conditions in Dhaka in 2025, our approach combines dynamic transcoding, real-time error correction, and optimized interface selection to enhance connectivity. We analyze empirical data on connection speeds, mobile tower density, district-level population statistics, and social media usage. Extensive mathematical formulations, including novel models for bitrate estimation, round-trip time optimization, and reliability analysis, are provided alongside detailed diagrams and multiple examples of code in both Python and C++. Experimental results demonstrate significant improvements in throughput, latency reduction, and overall service quality, offering a scalable blueprint for next-generation communication systems in hyper-dense urban environments."
2506.1757,"Virtual reality (VR) has recently proliferated significantly, consisting of headsets or head-mounted displays (HMDs) and hand controllers for an embodied and immersive experience. The VR device is usually embedded with different kinds of IoT sensors, such as cameras, microphones, communication sensors, etc. However, VR security has not been scrutinized from a physical hardware point of view, especially electromagnetic emanations (EM) that are automatically and unintentionally emitted from the VR headset. This paper presents VReaves, a system that can eavesdrop on the electromagnetic emanation side channel of a VR headset for VR app identification and activity recognition. To do so, we first characterize the electromagnetic emanations from the embedded IoT sensors (e.g., cameras and microphones) in the VR headset through a signal processing pipeline and further propose machine learning models to identify the VR app and recognize the VR app activities. Our experimental evaluation with commercial off-the-shelf VR devices demonstrates the efficiency of VR app identification and activity recognition via electromagnetic emanation side channel."
2506.17658,"The last decade has witnessed the proliferation of network function virtualization (NFV) in the telco industry, thanks to its unparalleled flexibility, scalability, and cost-effectiveness. However, as the NFV infrastructure is shared by virtual network functions (VNFs), sporadic resource contentions are inevitable. Such contention makes it extremely challenging to guarantee the performance of the provisioned network services, especially in high-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely on direct traffic analysis (e.g., packet- or flow-level measurements) to detect performance degradation and identify bottlenecks, which is not always applicable due to significant integration overhead and system-level constraints. This paper complements existing solutions with a lightweight, non-intrusive framework for online performance inference that easily adapts to drift (i.e., a change over time of the actual state of our system). Instead of direct data-plane collection, we reuse hardware features in the underlying NFV infrastructure, introducing negligible interference in the data-plane. Our Drift-Resilient and Self-Tuning (DRST) framework can be integrated into existing NFV systems with minimal engineering effort and operate without the need for predefined traffic models or VNF-specific customization. DRST is deployed via a lightweight MLOps pipeline that automates the adaptation under runtime drift. We show how DRST can deliver accurate performance inference or diagnose run-time bottlenecks, as demonstrated through a comprehensive evaluation across diverse NFV scenarios."
2506.17678,"SDR (Software Defined Radio) provides flexible, reproducible, and longer-lasting radio tools for military and civilian wireless communications infrastructure. SDR is a radio communication system whose components are implemented as software. This study aims to establish multi-channel wireless communication with FANET between two SDRs to share location information and examine it in a realistic test environment. We used multi-channel token circulation as a channel access protocol and GNU Radio platform for SDR software development. The structures of the communication layer, including the protocols, communication systems, and network structures suggested in the studies in the literature, are generally tested in the simulation environment. The simulation environment provides researchers with fast and easy development and testing, but disadvantages exist. These cause a product to be isolated from hardware, software, and cost effects encountered while developing and environmental factors affecting the communication channel while testing. Another contribution of the study is to present the developed block diagrams and codes as clear and reproducible. The developed software and block diagrams are available atthis http URL."
2506.17821,"Deep learning has significant potential to make the Internet's Border Gateway Protocol (BGP) secure by detecting anomalous routing activity. However, all but a few of these approaches rely on the implicit assumption that anomalies manifest as noisy, high-complexity outliers from some normal baseline. This work challenges this assumption by investigating if a best-in-class detection model built on this assumption can effectively deal with real-world security events' diverse signatures. We employ an LSTM-based autoencoder, a classical example of a reconstruction-based anomaly detector, as our test vehicle. We then contrast this model with a representative sampling of historical BGP anomalies, including the Slammer worm and the Moscow blackout, and with a simulated 'BGP storm' designed as a positive control. Our experience unveils a blind spot of our model: the model easily identifies the synthetic anomaly of high complexity but invariably fails to identify real-world events that manifest in the form of a ""signal loss"" (e.g., Slammer, Moscow Blackout) or ""low-deviation"" (e.g., WannaCry) signature. We demonstrate that the model mistakenly recognizes the abrupt cut-off of BGP updates during catastrophic failures as a signal of extreme stability, leading to reconstruction errors of virtually zero and total failure to detect. We conclude that the characterization of BGP anomalies as high-reconstruction-error events alone is a weak and dangerous oversimplification. Our research provides the data-driven case for why hybrid, multi-modal detection systems capable of identifying both high-complexity and signal-loss signatures are required to enable end-to-end BGP security."
2506.17877,"Networked mission-critical applications (e.g., avionic control and industrial automation systems) require deterministic packet transmissions to support a range of sensing and control tasks with stringent timing constraints. While specialized network infrastructure (e.g., time-sensitive networking (TSN) switches) provides deterministic data transport across the network, achieving strict end-to-end timing guarantees requires equally capable end devices to support deterministic traffic. These end devices, however, often employ general-purpose computing platforms like standard PCs, which lack native support for deterministic traffic and suffer from unpredictable delays introduced by their software stack and system architecture. Although specialized NICs with hardware scheduling offload can mitigate this problem, the limited compatibility hinders their widespread adoption, particularly for cost-sensitive applications or in legacy devices.To fill this gap, this paper proposes a novel software-based driver model, namely KeepON, to enable the support of deterministic packet transmissions on end devices equipped with standard NICs. The key idea of KeepON is to have the NIC keep on transmitting fixed-size data chunks as placeholders, thereby maintaining a predictable temporal transmission pattern. The real-time packets generated by the mission-critical application(s) will then be precisely inserted into this stream by replacing placeholders at the designated position to ensure their accurate transmission time. We implement and evaluate KeepON by modifying the network driver on a Raspberry Pi using its standard NIC. Our experiments demonstrate that KeepON can achieve x162 times scheduling accuracy comparable to its default driver, and x2.6 times compared to hardware-based solution, thus enabling precise timing control on standard commodity hardware."
2506.17911,"Routing Protocol for Low-Power and Lossy Networks (RPL) is an energy-efficient routing solution for IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN), recommended for resource-constrained devices. While RPL offers significant benefits, its security vulnerabilities pose challenges, particularly due to unauthenticated control messages used to establish and maintain routing information. These messages are susceptible to manipulation, enabling malicious nodes to inject false routing data. A notable security concern is the Routing Table Falsification (RTF) attack, where attackers forge Destination Advertisement Object (DAO) messages to promote fake routes via a parent nodes routing table. Experimental results indicate that RTF attacks significantly reduce packet delivery ratio, increase end-to-end delay, and leverage power consumption. Currently, no effective countermeasures exist in the literature, reinforcing the need for a security solution to prevent network disruption and protect user applications. This paper introduces a Lightweight Security Solution against Routing Table Falsification Attack (LiSec-RTF), leveraging Physical Unclonable Functions (PUFs) to generate unique authentication codes, termed Licenses. LiSec-RTF mitigates RTF attack impact while considering the resource limitations of 6LoWPAN devices in both static and mobile scenarios. Our testbed experiments indicate that LiSec-RTF significantly improves network performance compared to standard RPL under RTF attacks, thereby ensuring reliable and efficient operation."
2506.18159,"This article describes a novel dataset that maps the network layer of the Invisible Internet Project (I2P). The data was collected using SWARM-I2P framework, which deployed I2P routers as a network of mapping agents that gather information on the network's topology and traffic over an extended period. The dataset documents over 50,000 nodes, including subsets of high-performance (FastSet) nodes and high-capacity nodes characterized by metrics such as bandwidth, latency, and uptime. It also contains detailed records of network traffic and the geographic distribution of thousands of nodes. Data was collected using a combination of methods, including querying router consoles, analysing the network database (netDb), and passive network monitoring. All node identifiers were anonymized to maintain user privacy. The data is publicly available in CSV and TXT formats on Zenodo, with mapping scripts provided on GitHub. This resource provides a foundational understanding of the decentralized routing behaviours that underpin I2P's anonymity, making it suitable for reuse in analyses of tunnel node selection, anonymity network resilience, and adversarial modelling."
2506.18381,"We propose a theoretical framework for consistent channel hopping algorithms to address the multichannel rendezvous problem (MRP) in wireless networks with heterogeneous available channel sets. A channel selection function is called consistent if the selected channel remains unchanged when the available channel set shrinks, provided the selected channel is still available. We show that all consistent channel selection functions are equivalent to the function that always selects the smallest-index channel under appropriate channel relabeling. This leads to a natural representation of a consistent channel hopping algorithm as a sequence of permutations. For the two-user MRP, we characterize rendezvous time slots using a fictitious user and derive tight bounds on the maximum time-to-rendezvous (MTTR) and expected time-to-rendezvous (ETTR). Notably, the ETTR is shown to be the inverse of the Jaccard index when permutations are randomly selected. We also prove that consistent channel hopping algorithms maximize the rendezvous probability. To reduce implementation complexity, we propose the modulo algorithm, which uses modular arithmetic with one-cycle permutations and achieves performance comparable to locality-sensitive hashing (LSH)-based algorithms. The framework is extended to multiple users, with novel strategies such as stick-together, spread-out, and a hybrid method that accelerates rendezvous in both synchronous and asynchronous settings. Simulation results confirm the effectiveness and scalability of the proposed algorithms."
2506.18584,"Extended reality (XR) devices, commonly known as wearables, must handle significant computational loads under tight latency constraints. To meet these demands, they rely on a combination of on-device processing and edge offloading. This letter focuses on offloading strategies for wearables by considering their impact across three time scales: instantaneous power consumption, short-term temperature fluctuations, and long-term battery duration. We introduce a comprehensive system model that captures these temporal dynamics, and propose a stochastic and stationary offloading strategy, called TAO (for temperature-aware offloading), designed to minimize the offloading cost while adhering to power, thermal, and energy constraints. Our performance evaluation, leveraging COMSOL models of real-world wearables, confirms that TAO reduces offloading cost by over 35% compared to state-of-the-art approaches, without violating the wearable operational limits."
2506.1866,"Semantic communication (SemCom) is an emerging paradigm that leverages semantic-level understanding to improve communication efficiency, particularly in resource-constrained scenarios. However, existing SemCom systems often overlook diverse computational and communication capabilities and requirements among different users. Motivated by the need to adaptively balance semantic accuracy, latency, and energy consumption, this paper presents a reinforcement learning (RL)-driven framework for semantic compression model (SCM) selection and resource allocation in multi-user SemCom systems. To address the challenges of balancing image reconstruction quality and communication performance, a system-level optimization metric called Rate-Distortion Efficiency (RDE) has been defined. The framework considers multiple SCMs with varying complexity and resource requirements. A proximal policy optimization (PPO)-based RL approach is developed to dynamically select SCMs and allocate bandwidth and power under non-convex constraints. Simulations demonstrate that the proposed method outperforms several baseline strategies. This paper also discusses the generalization ability, computational complexity, scalability, and practical implications of the framework for real-world SemCom systems."
2506.1903,"Large Language Model (LLM) services fundamentally differ from traditional Deep Neural Network (DNN) applications in wireless networks. We identify three critical distinctions: (1) unlike traditional DNNs with unidirectional data flows, LLM's multimodal interactions create bidirectional heavy loads with contrasting bottlenecks, requiring direction-aware resource scheduling; (2) while traditional DNNs exhibit fixed computational patterns, LLM's highly variable inference times interact complexly with network slicing, causing dynamic bottleneck migration; and (3) in contrast to predictable DNN traffic, LLM's token streams demonstrate unprecedented burstiness and state dependencies. These insights motivate WiLLM, the first open-source framework, implemented as a wireless platform, for LLM service research. Built on OpenAirInterface, WiLLM introduces several technical innovations: dynamic slice compatibility, universal UE compatibility through application-layer tunneling, multi-UE multi-slice scheduling, dual-mode resource allocation, and cross-layer APIs. In addition, WiLLM eliminates the need for specialized wireless expertise, enabling researchers and developers to experiment with LLM services over realistic cellular networks. We demonstrate the platform's capabilities through a smart glasses case study and provide a comprehensive dataset of \~1.6 million synchronized measurements. The complete system, dataset, and appendix are available atthis https URL."
2506.19044,"The detonation of an improvised nuclear device (IND) in an urban area would cause catastrophic damage, followed by hazardous radioactive fallout. Timely dissemination of radiation data is crucial for evacuation and casualty reduction. However, conventional communication infrastructure is likely to be severely disrupted. This study designs and builds a pseudorealistic, geospatially and temporally dynamic post-nuclear event (PNE) scenario using the Opportunistic Network Environment (ONE) simulator. It integrates radiation sensing by emergency responders, unmanned aerial vehicles (UAVs), and civilian devices as dynamic nodes within Delay-Tolerant Networks (DTNs). The performance of two DTN routing protocols, Epidemic and PRoPHET, was evaluated across multiple PNE phases. Both protocols achieve high message delivery rates, with PRoPHET exhibiting lower network overhead but higher latency. Findings demonstrate the potential of DTN-based solutions to support emergency response and evacuation safety by ensuring critical radiation data propagation despite severe infrastructure damage."
2506.19304,"Platooning within autonomous vehicles has proven effective in addressing driver shortages and reducing fuel consumption. However, as platooning lengths increase, traditional C-V2X (cellular vehicle-to-everything) architectures are susceptible to end-to-end (E2E) latency increases. This is due to the necessity of relaying information through multiple hops from the leader vehicle to the last vehicle. To address this problem, this paper proposes a hybrid communication architecture based on a simulation that integrates light fidelity (LiFi) and C-V2X. The proposed architecture introduces multiple-leader vehicles equipped with outdoor LiFi communication nodes in platoons to achieve high-speed and low-delay communication between leader vehicles, which reduces E2E delay."
2506.19366,"Wireless mesh networks (WMNs) depend on the spatial distribution of nodes, which directly influences connectivity, routing efficiency, and overall network performance. Conventional models typically assume uniform or random node placement, which inadequately represent the complex, hierarchical spatial patterns observed in practical deployments. In this study, we present a novel algorithm that constructs WMN topologies with tunable fractal dimensions, allowing precise control over spatial self-similarity. By systematically varying the fractal dimension, the algorithm generates network layouts spanning a continuum of spatial complexities, ranging from sparse fragmented clusters to dense, cohesive structures. Through NS-3 simulations, Key performance metrics including throughput, latency, jitter, and packet delivery ratio were evaluated across a range of fractal dimensions. Comparative evaluations against classical random, small-world, scale-free, grid and hierarchical tree networks models reveal that high-dimensional fractal topologies achieve enhanced resilience and throughput under equivalent conditions. These findings demonstrate the potential of fractal geometry as a design paradigm for scalable and efficient WMN architectures."
2506.1976,"Open Radio Access Network (RAN) is a key paradigm to attain unprecedented flexibility of the RAN via disaggregation and Artificial Intelligence (AI)-based applications called xApps. In dense areas with many active RAN nodes, compute resources are engineered to support potentially hundreds of xApps monitoring and controlling the RAN to achieve operator's intents. However, such resources might become underutilized during low-traffic periods, where most cells are sleeping and, given the reduced RAN complexity, only a few xApps are needed for its control. In this paper, we propose CORMO-RAN, a data-driven orchestrator that dynamically activates compute nodes based on xApp load to save energy, and performs lossless migration of xApps from nodes to be turned off to active ones while ensuring xApp availability during migration. CORMO-RAN tackles the trade-off among service availability, scalability, and energy consumption while (i) preserving xApps' internal state to prevent RAN performance degradation during migration; (ii) accounting for xApp diversity in state size and timing constraints; and (iii) implementing several migration strategies and providing guidelines on best strategies to use based on resource availability and requirements. We prototype CORMO-RAN as an rApp, and experimentally evaluate it on an O-RAN private 5G testbed hosted on a Red Hat OpenShift cluster with commercial radio units. Results demonstrate that CORMO-RAN is effective in minimizing energy consumption of the RAN Intelligent Controller (RIC) cluster, yielding up to 64% energy saving when compared to existing approaches."
2506.19947,"Channel hopping (CS) communication systems must adapt to interference changes in the wireless network and to node mobility for maintaining throughput efficiency. Optimal scheduling requires up-to-date network state information (i.e., of channel occupancy) to select non-overlapping channels for links in interference regions. However, state sharing among nodes introduces significant communication overhead, especially as network size or node mobility scale, thereby decreasing throughput efficiency of already capacity-limited networks. In this paper, we eschew state sharing while adapting the CS schedule based on a learning-based channel occupancy prediction. We propose the MiLAAP attention-based prediction framework for machine learning models of spectral, spatial, and temporal dependencies among network nodes. MiLAAP uses a self-attention mechanism that lets each node capture the temporospectral CS pattern in its interference region and accordingly predict the channel occupancy state within that region. Notably, the prediction relies only on locally and passively observed channel activities, and thus introduces no communication overhead. To deal with node mobility, MiLAAP also uses a multi-head self-attention mechanism that lets each node locally capture the spatiotemporal dependencies on other network nodes that can interfere with it and accordingly predict the motion trajectory of those nodes. Detecting nodes that enter or move outside the interference region is used to further improve the prediction accuracy of channel occupancy. We show that for dynamic networks that use local CS sequences to support relatively long-lived flow traffics, the channel state prediction accuracy of MiLAAP is remarkably ~100% across different node mobility patterns and it achieves zero-shot generalizability across different periods of CS sequences."
2506.19974,"Degeneracy is the ability of structurally different elements to perform the same function or yield the same output under certain constraints. In contrast to redundancy, which implies identical backups, degeneracy allows diverse components to step in and perform the same or similar role. Mathematically, it is about mapping multiple distinct elements into the same function. In a degenerate system, failure in one part can be compensated by others not structurally linked. System functions are distributed within the system itself or the entire network. This renders faster and more adaptive recovery. In this work, we define and formulate several novel metrics for resource fungibility to address robustness in networks (static/mobile/dynamic)."
2506.20111,"In this article, we extend a statistical test of graph clusterability, the $\delta$ test, to directed graphs with no self loops. The $\delta$ test, originally designed for undirected graphs, is based on the premise that graphs with a clustered structure display a mean local density that is statistically higher than the graph's global density. We posit that graphs that do not meet this necessary (but not sufficient) condition for clusterability can be considered unsuited to clustering. In such cases, vertex clusters do not offer a meaningful summary of the broader graph. Additionally in this study, we aim to determine the optimal sample size (number of neighborhoods). Our test, designed for the analysis of large networks, is based on sampling subsets of neighborhoods/nodes. It is designed for cases where computing the density of every node's neighborhood is infeasible. Our results show that the $\delta$ test performs very well, even with very small samples of neighborhoods ($1\%$). It accurately detects unclusterable graphs and is also shown to be robust to departures from the underlying assumptions of the $t$ test."
2506.20383,"Scanners are daily visitors of public IPv4 hosts. Scanning IPv6 nodes successfully is still a challenge, which an increasing crowd of actors tries to master. In this paper, we analyze current IPv6 scanning under various network conditions. We observe scanner behavior during eleven months in four network telescopes, one of which is periodically reconfigured by changing BGP announcements. We analyze and classify the observed scanners w.r.t. their temporal behavior, their target, and network selection strategy, as well as their individual tools, fingerprints, and correlations across categories. We find that silent subnets of larger prefixes remain invisible, whereas BGP prefix announcements quickly attract attention by scanners. Based on our findings, we derive operational guidance on how to deploy network telescopes to increase visibility of IPv6 scanners."
2506.2042,"The rapid growth of web content has led to increasingly large webpages, posing significant challenges for Internet affordability, especially in developing countries where data costs remain prohibitively high. We propose semantic caching using Large Language Models (LLMs) to improve web affordability by enabling reuse of semantically similar images within webpages. Analyzing 50 leading news and media websites, encompassing 4,264 images and over 40,000 image pairs, we demonstrate potential for significant data transfer reduction, with some website categories showing up to 37% of images as replaceable. Our proof-of-concept architecture shows users can achieve approximately 10% greater byte savings compared to exact caching. We evaluate both commercial and open-source multi-modal LLMs for assessing semantic replaceability. GPT-4o performs best with a low Normalized Root Mean Square Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA 3.1 model shows comparable performance, highlighting its viability for large-scale applications. This approach offers benefits for both users and website operators, substantially reducing data transmission. We discuss ethical concerns and practical challenges, including semantic preservation, user-driven cache configuration, privacy concerns, and potential resistance from website operators"
2506.20762,"In this paper, we propose a novel drift-adaptive slicing-based resource management scheme for cooperative integrated sensing and communication (ISAC) networks. Particularly, we establish two network slices to provide sensing and communication services, respectively. In the large-timescale planning for the slices, we partition the sensing region of interest (RoI) of each mobile device and reserve network resources accordingly, facilitating low-complexity distance-based sensing target assignment in small timescales. To cope with the non-stationary spatial distributions of mobile devices and sensing targets, which can result in the drift in modeling the distributions and ineffective planning decisions, we construct digital twins (DTs) of the slices. In each DT, a drift-adaptive statistical model and an emulation function are developed for the spatial distributions in the corresponding slice, which facilitates closed-form decision-making and efficient validation of a planning decision, respectively. Numerical results show that the proposed drift-adaptive slicing-based resource management scheme can increase the service satisfaction ratio by up to 18% and reduce resource consumption by up to 13.1% when compared with benchmark schemes."
2506.21406,"Network latency severely impacts the performance of applications running on supercomputers. Adaptive routing algorithms route packets over different available paths to reduce latency and improve network utilization. However, if a switch routes packets belonging to the same network flow on different paths, they might arrive at the destination out-of-order due to differences in the latency of these paths. For some transport protocols like TCP, QUIC, and RoCE, out-of-order (OOO) packets might cause large performance drops or significantly increase CPU utilization. In this work, we propose flowcut switching, a new adaptive routing algorithm that provides high-performance in-order packet delivery. Differently from existing solutions like flowlet switching, which are based on the assumption of bursty traffic and that might still reorder packets, flowcut switching guarantees in-order delivery under any network conditions, and is effective also for non-bursty traffic, as it is often the case for RDMA."
2506.21933,"With the rapid development of the low-altitude economy, air-ground integrated multi-access edge computing (MEC) systems are facing increasing demands for real-time and intelligent task scheduling. In such systems, task offloading and resource allocation encounter multiple challenges, including node heterogeneity, unstable communication links, and dynamic task variations. To address these issues, this paper constructs a three-layer heterogeneous MEC system architecture for low-altitude economic networks, encompassing aerial and ground users as well as edge servers. The system is systematically modeled from the perspectives of communication channels, computational costs, and constraint conditions, and the joint optimization problem of offloading decisions and resource allocation is uniformly abstracted into a graph-structured modeling task. On this basis, we propose a graph attention diffusion-based solution generator (GADSG). This method integrates the contextual awareness of graph attention networks with the solution distribution learning capability of diffusion models, enabling joint modeling and optimization of discrete offloading variables and continuous resource allocation variables within a high-dimensional latent space. We construct multiple simulation datasets with varying scales and topologies. Extensive experiments demonstrate that the proposed GADSG model significantly outperforms existing baseline methods in terms of optimization performance, robustness, and generalization across task structures, showing strong potential for efficient task scheduling in dynamic and complex low-altitude economic network environments."
2506.22148,"Delay Tolerant Networks (DTNs) offer a promising paradigm for maintaining communication in infrastructure limited environments, such as those encountered during natural disasters. This paper investigates the viability of leveraging an existing national transport system - the Swiss rail network - as a data mule backbone for disseminating critical avalanche alerts. Using The Opportunistic Network Environment (ONE) simulator, we model the entire Swiss rail network and conduct a rigorous comparative analysis of two seminal DTN routing protocols: Epidemic and PROPHET. Experiments are performed in two distinct scenarios: alerts originating from dense urban centres and from sparse, remote mountainous regions. Our results demonstrate that the rail network provides robust connectivity for opportunistic communication in both environments thus validating the integration of DTN principles in remote scenarios."
2506.22223,"This paper introduces a novel intention-sharing mechanism for Electrically Power-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing the ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete predicted trajectory points with a compact elliptical geographical area representation derived via quadratic polynomial fitting and Least Squares Method (LSM). This approach encodes trajectory predictions with fixed-size data payloads, independent of the number of forecasted points, enabling higher-frequency transmissions and improved network reliability. Simulation results demonstrate superior inter-packet gap (IPG) performance compared to standard ETSI VAMs, particularly under constrained communication conditions. A physical experiment validates the feasibility of real-time deployment on embedded systems. The method supports scalable, low-latency intention sharing, contributing to cooperative perception and enhanced safety for vulnerable road users in connected and automated mobility ecosystems. Finally, we discuss the viability of LSM and open the door to other methods for prediction."
2506.2226,"Wi-Fi networks have long relied on the Enhanced Distributed Channel Access (EDCA) mechanism, allowing stations to compete for transmission opportunities. However, as networks become denser and emerging applications demand lower latency and higher reliability, the limitations of EDCA such as overhead due to contention and collisions have become more pronounced. To address these challenges, Orthogonal Frequency Division Multiple Access (OFDMA) has been introduced in Wi-Fi, enabling more efficient channel utilization through scheduled resource allocation. Furthermore, Wi-Fi 6 defines Uplink Orthogonal Frequency Division Multiple Random Access (UORA), a hybrid mechanism that combines both scheduled and random access, balancing efficiency and responsiveness in resource allocation. Despite significant research on UORA, most studies rely on custom simulators that are not publicly available, limiting reproducibility and preventing validation of the presented results. The only known open-source UORA implementation in the ns-3 simulator exhibits key limitations, such as usage of the same trigger frame (TF) to schedule resources for buffer status reports and data transmissions, and lack of signaling for UORA configuration. In this paper, we present a fully standard-compliant and open source UORA implementation that is compatible with ns-3 version 3.38, addressing these limitations to improve resource allocation efficiency and adaptability. This implementation enables more accurate and flexible evaluation of UORA, fostering future research on Wi-Fi resource allocation strategies."
2506.22359,"The telecommunications and networking domain stands at the precipice of a transformative era, driven by the necessity to manage increasingly complex, hierarchical, multi administrative domains (i.e., several operators on the same path) and multilingual systems. Recent research has demonstrated that Large Language Models (LLMs), with their exceptional general-purpose text analysis and code generation capabilities, can be effectively applied to certain telecom problems (e.g., auto-configuration of data plan to meet certain application requirements). However, due to their inherent token-by-token processing and limited capacity for maintaining extended context, LLMs struggle to fulfill telecom-specific requirements such as cross-layer dependency cascades (i.e., over OSI), temporal-spatial fault correlation, and real-time distributed coordination. In contrast, Large Concept Models (LCMs), which reason at the abstraction level of semantic concepts rather than individual lexical tokens, offer a fundamentally superior approach for addressing these telecom challenges. By employing hyperbolic latent spaces for hierarchical representation and encapsulating complex multi-layered network interactions within concise concept embeddings, LCMs overcome critical shortcomings of LLMs in terms of memory efficiency, cross-layer correlation, and native multimodal integration. This paper argues that adopting LCMs is not simply an incremental step, but a necessary evolutionary leap toward achieving robust and effective AI-driven telecom management."
2506.22464,"This paper presents a novel localization algorithm for wireless sensor networks (WSNs) called Golden Ratio Localization (GRL), which leverages the mathematical properties of the golden ratio (phi 1.618) to optimize both node placement and communication range. GRL introduces phi-based anchor node deployment and hop-sensitive weighting using phi-exponents to improve localization accuracy while minimizing energy consumption. Through extensive simulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10 anchors, GRL achieved an average localization error of 2.35 meters, outperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of energy efficiency, GRL reduced localization energy consumption to 1.12 microJ per node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid. These results confirm that GRL provides a more balanced and efficient localization approach, making it especially suitable for energy-constrained and large-scale WSN deployments."
2506.2247,"Delay/Disruption Tolerant Networking (DTN) employs the Licklider Transmission Protocol (LTP) with Automatic Repeat reQuest (ARQ) for reliable data delivery in challenging interplanetary networks. While previous studies have integrated packet-level Forward Erasure Correction (FEC) into LTP to reduce retransmission time costs, existing static and delay-feedback-based dynamic coding methods struggle with highly variable and unpredictable deep space channel conditions. This paper proposes a reinforcement learning (RL)-based adaptive FEC algorithm to address these limitations. The algorithm utilizes historical feedback and system state to predict future channel conditions and proactively adjust the code rate. This approach aims to anticipate channel quality degradation, thereby preventing decoding failures and subsequent LTP retransmissions and improving coding efficiency by minimizing redundancy during favorable channel conditions. Performance evaluations conducted in simulated Earth-Moon and Earth-Mars link scenarios demonstrate this algorithm's effectiveness in optimizing data transmission for interplanetary networks. Compared to existing methods, this approach demonstrates significant improvement, with matrix decoding failures reduced by at least 2/3."
2506.22474,"The Internet of Things (IoT) has been increasingly used in our everyday lives as well as in numerous industrial applications. However, due to limitations in computing and power capabilities, IoT devices need to send their respective tasks to cloud service stations that are usually located at far distances. Having to transmit data far distances introduces challenges for services that require low latency such as industrial control in factories and plants as well as artificial intelligence assisted autonomous driving. To solve this issue, mobile edge computing (MEC) is deployed at the networks edge to reduce transmission time. In this regard, this study proposes a new offloading scheme for MEC-assisted ultra dense cellular networks using reinforcement learning (RL) techniques. The proposed scheme enables efficient resource allocation and dynamic offloading decisions based on varying network conditions and user demands. The RL algorithm learns from the networks historical data and adapts the offloading decisions to optimize the networks overall performance. Non-orthogonal multiple access is also adopted to improve resource utilization among the IoT devices. Simulation results demonstrate that the proposed scheme outperforms other stateof the art offloading algorithms in terms of energy efficiency, network throughput, and user satisfaction."
2506.22477,"This paper introduces an innovative design for robotic operating platforms, underpinned by a transformative Internet of Things (IoT) architecture, seamlessly integrating cutting-edge technologies such as large language models (LLMs), generative AI, edge computing, and 5G networks. The proposed platform aims to elevate the intelligence and autonomy of IoT systems and robotics, enabling them to make real-time decisions and adapt dynamically to changing environments. Through a series of compelling case studies across industries including smart manufacturing, healthcare, and service sectors, this paper demonstrates the substantial potential of IoT-enabled robotics to optimize operational workflows, enhance productivity, and deliver innovative, scalable solutions. By emphasizing the roles of LLMs and generative AI, the research highlights how these technologies drive the evolution of intelligent robotics and IoT, shaping the future of industry-specific advancements. The findings not only showcase the transformative power of these technologies but also offer a forward-looking perspective on their broader societal and industrial implications, positioning them as catalysts for next-generation automation and technological convergence."
2506.2248,"As users in small cell networks increasingly rely on computation-intensive services, cloud-based access often results in high latency. Multi-access edge computing (MEC) mitigates this by bringing computational resources closer to end users, with small base stations (SBSs) serving as edge servers to enable low-latency service delivery. However, limited edge capacity makes it challenging to decide which services to deploy locally versus in the cloud, especially under unknown service demand and dynamic network conditions. To tackle this problem, we model service demand as a linear function of service attributes and formulate the service placement task as a linear bandit problem, where SBSs act as agents and services as arms. The goal is to identify the service that, when placed at the edge, offers the greatest reduction in total user delay compared to cloud deployment. We propose a distributed and adaptive multi-agent best-arm identification (BAI) algorithm under a fixed-confidence setting, where SBSs collaborate to accelerate learning. Simulations show that our algorithm identifies the optimal service with the desired confidence and achieves near-optimal speedup, as the number of learning rounds decreases proportionally with the number of SBSs. We also provide theoretical analysis of the algorithm's sample complexity and communication overhead."
2506.22482,"With the advent of Internet of Things, Wireless Home Automation Systems WHAS are gradually gaining popularity. These systems are faced with multiple challenges such as security; controlling a variety of home appliances with a single interface and user friendliness. In this paper we propose a system that uses secure authentication systems of social networking websites such as Twitter, tracks the end-users activities on the social network and then control his or her domestic appliances. At the end, we highlight the applications of the proposed WHAS and compare the advantages of our proposed system over traditional home automation systems."
2506.22484,"Urban cellular networks face complex performance challenges due to high infrastructure density, varied user mobility, and diverse service demands. While several datasets address network behaviour across different environments, there is a lack of datasets that captures user centric Quality of Experience (QoE), and diverse mobility patterns needed for efficient network planning and optimization solutions, which are important for QoE driven optimizations and mobility management. This study presents a curated dataset of 30,925 labelled records, collected using GNetTrack Pro within a 2 km2 dense urban area, spanning three major commercial network operators. The dataset captures key signal quality parameters (e.g., RSRP, RSRQ, SNR), across multiple real world mobility modes including pedestrian routes, canopy walkways, shuttle buses, and Bus Rapid Transit (BRT) routes. It also includes diverse network traffic scenarios including (1) FTP upload and download, (2) video streaming, and (3) HTTP browsing. A total of 132 physical cell sites were identified and validated through OpenCellID and on-site field inspections, illustrating the high cell density characteristic of 5G and emerging heterogeneous network deployment. The dataset is particularly suited for machine learning applications, such as handover optimization, signal quality prediction, and multi operator performance evaluation. Released in a structured CSV format with accompanying preprocessing and visualization scripts, this dataset offers a reproducible, application ready resource for researchers and practitioners working on urban cellular network planning and optimization."
2506.22487,"The integration of the Internet of Everything (IoX) and Artificial General Intelligence (AGI) has given rise to a transformative paradigm aimed at addressing critical bottlenecks across sensing, network, and application layers in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide a systematic and comprehensive review of AGI-enhanced IoX research, focusing on three key components: sensing-layer data management, network-layer protocol optimization, and application-layer decision-making frameworks. Specifically, this survey explores how AGI can mitigate IoX bottlenecks challenges by leveraging adaptive sensor fusion, edge preprocessing, and selective attention mechanisms at the sensing layer, while resolving network-layer issues such as protocol heterogeneity and dynamic spectrum management, neuro-symbolic reasoning, active inference, and causal reasoning, Furthermore, the survey examines AGI-enabled frameworks for managing identity and relationship explosion. Key findings suggest that AGI-driven strategies, such as adaptive sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions to sensing-layer data overload, network-layer protocol heterogeneity, and application-layer identity explosion. The survey underscores the importance of cross-layer integration, quantum-enabled communication, and ethical governance frameworks for future AGI-enabled IoX systems. Finally, the survey identifies unresolved challenges, such as computational requirements, scalability, and real-world validation, calling for further research to fully realize AGI's potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is emerging as a critical research field at the intersection of interconnected systems and advanced AI."
2506.22507,"The evolution towards 6G networks requires the intelligent integration of communication and sensing capabilities to support diverse and complex applications, such as autonomous driving and immersive services. However, existing integrated sensing and communication (ISAC) systems predominantly rely on single-modal sensors as primary participants, which leads to a limited representation of environmental features and significant performance bottlenecks under the emerging requirements of 6G applications. This limitation motivates a paradigm shift from single-modal to multimodal ISAC. In this article, we first analyze the key challenges in realizing multimodal ISAC, including the fusion of heterogeneous multimodal data, the high communication overhead among distributed sensors, and the design of efficient and scalable system architectures. We then introduce several enabling technologies, such as large AI models, semantic communication, and multi-agent systems, that hold promise for addressing these challenges. To operationalize these technologies, we zoom into three architectural paradigms: fusion-based multimodal ISAC (F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal ISAC (R-MAC), each tailored to organize devices and modalities for efficient collaboration in different scenarios. Thereafter, a case study is presented based on the F-MAC scheme, demonstrating that the scheme achieves more comprehensive sensing and improves sensing accuracy by approximately 80% compared to conventional single-modal ISAC systems. Finally, we discuss several open issues to be addressed in the future."
2506.22671,"Cyclic Queuing and Forwarding (CQF) is a Time-Sensitive Networking (TSN) shaping mechanism that provides bounded latency and deterministic Quality of Service (QoS). However, CQF's use of a single cycle restricts its ability to support TSN traffic with diverse timing requirements. Multi-Cyclic Queuing and Forwarding (Multi-CQF) is a new and emerging TSN shaping mechanism that uses multiple cycles on the same egress port, allowing it to accommodate TSN flows with varied timing requirements more effectively than CQF. Despite its potential, current Multi-CQF configuration studies are limited, leading to a lack of comprehensive research, poor understanding of the mechanism, and limited adoption of Multi-CQF in practical applications. Previous work has shown the impact of Time Injection (TI), defined as the start time of Time-Triggered (TT) flows at the source node, on CQF queue resource utilization. However, the impact of TI has not yet been explored in the context of Multi-CQF. This paper introduces a set of constraints and leverages Domain Specific Knowledge (DSK) to reduce the search space for Multi-CQF configuration. Building on this foundation, we develop an open-source Genetic Algorithm (GA) and a hybrid GA-Simulated Annealing (GASA) approach to efficiently configure Multi-CQF networks and introduce TI in Multi-CQF to enhance schedulability. Experimental results show that our proposed algorithms significantly increase the number of scheduled TT flows compared to the baseline Simulated Annealing (SA) model, improving scheduling by an average of 15%. Additionally, GASA achieves a 20% faster convergence rate and lower time complexity, outperforming the SA model in speed, and efficiency."
2506.22745,"Due to the scalability and portability, the low-altitude intelligent networks (LAINs) are essential in various fields such as surveillance and disaster rescue. However, in LAINs, unmanned aerial vehicles (UAVs) are characterized by the distributed topology and high dynamic mobility, and vulnerable to security threats, which may degrade the routing performance for data transmission. Hence, how to ensure the routing stability and security of LAINs is a challenge. In this paper, we focus on the routing process in LAINs with multiple UAV clusters and propose the blockchain-enabled zero-trust architecture to manage the joining and exiting of UAVs. Furthermore, we formulate the routing problem to minimize the end-to-end (E2E) delay, which is an integer linear programming and intractable to solve. Therefore, considering the distribution of LAINs, we reformulate the routing problem into a decentralized partially observable Markov decision process. With the proposed soft hierarchical experience replay buffer, the multi-agent double deep Q-network based adaptive routing algorithm is designed. Finally, simulations are conducted and numerical results show that the total E2E delay of the proposed mechanism decreases by 22.38\% than the benchmark on average."
2506.22793,"In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm and study the possibility of learning the optimal Cell Individual Offset tuning using offline Reinforcement Learning. Such methods make use of collected offline datasets to learn the optimal policy, without further exploration. We adapt and apply a sequence-based method called Decision Transformers as well as a value-based method called Conservative Q-Learning to learn the optimal policy for the same target reward as the vanilla rule-based MRO. The same input features related to failures, ping-pongs, and other handover issues are used. Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on a traffic mix including diverse user service types and a specific tunable cell-pair shows that offline-RL methods outperform rule-based MRO, offering up to 7% improvement. Furthermore, offline-RL can be trained for diverse objective functions using the same available dataset, thus offering operational flexibility compared to rule-based methods."
2506.22875,"Developments in communication and automation have driven the expansion of distributed networks, essential for IoT and CPS development in industrial applications requiring reliable image processing and real-time adaptability. Although broadly adopted, there is a literature gap regarding the performance of MQTT protocol for image sharing and transmission under high-traffic scenarios with intermittent connectivity, restricting its use in critical IoT and CPS applications. In this context, the present work examines the reliability of real-time image transmission in IoT and CPS industrial systems that utilize the MQTT-based publish/subscribe communication model. It focuses on scenarios with network interruptions and high data traffic, evaluating the performance of a distributed system through a series of controlled testbed validation experiments. Experimental validation demonstrated that while the MQTT-based system sustains reliable transmission under normal conditions, its recovery capability depends on the failure point, with complete restoration occurring when disruptions affect the Orchestrator Node and partial recovery when the Producer Node or Broker are affected. The study also confirmed that the system prevents duplicate errors and adapts well to increasing network demands, reinforcing its suitability for industrial applications that require efficient and resilient data handling."
2506.22991,"Just like power, water, and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient. This requires them to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Unlike robustness and reliability, resilience is based on the understanding that disruptions will inevitably happen. Resilience, as elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents and networks that can flexibly expand their states and hypotheses through real-time adaptation and reconfiguration. This situational awareness and active preparedness, adapting world models and counterfactually reasoning about potential system failures and the best responses, is a core aspect of resilience. This article will first disambiguate resilience from reliability and robustness, before delving into key mathematical foundations of resilience grounded in abstraction, compositionality and emergence. Subsequently, we focus our attention on a plethora of techniques and methodologies pertaining to the unique characteristics of resilience, as well as their applications through a comprehensive set of use cases. Ultimately, the goal of this paper is to establish a unified foundation for understanding, modeling, and engineering resilience in wireless communication systems, while laying a roadmap for the next-generation of resilient-native and intelligent wireless systems."
2506.23083,"Fast diagnosis and repair of enterprise network failures is critically important since disruptions cause major business impacts. Prior works focused on diagnosis primitives or procedures limited to a subset of the problem, such as only data plane or only control plane faults. This paper proposes a new paradigm, model-based network diagnosis, that provides a systematic way to derive automated procedures for identifying the root cause of network failures, based on reports of end-to-end user-level symptoms. The diagnosis procedures are systematically derived from a model of packet forwarding and routing, covering hardware, firmware, and software faults in both the data plane and distributed control plane. These automated procedures replace and dramatically accelerate diagnosis by an experienced human operator. Model-based diagnosis is inspired by, leverages, and is complementary to recent work on network verification. We have built NetDx, a proof-of-concept implementation of model-based network diagnosis. We deployed NetDx on a new emulator of networks consisting of P4 switches with distributed routing software. We validated the robustness and coverage of NetDx with an automated fault injection campaign, in which 100% of faults were diagnosed correctly. Furthermore, on a data set of 33 faults from a large cloud provider that are within the domain targeted by NetDx, 30 are efficiently diagnosed in seconds instead of hours."
2506.2319,"Unmanned Aerial Vehicles (UAVs) offer a promising solution for enhancing wireless connectivity and Quality of Service (QoS) in urban environments, acting as aerial Wi-Fi access points or cellular base stations. Their flexibility and rapid deployment capabilities make them suitable for addressing infrastructure gaps and traffic surges. However, optimizing UAV positions to maintain Line of Sight (LoS) links with ground User Equipment (UEs) remains challenging in obstacle-dense urban scenarios. This paper proposes VTOPA, a Vision-Aided Traffic- and Obstacle-Aware Positioning Algorithm that autonomously extracts environmental information -- such as obstacles and UE locations -- via computer vision and optimizes UAV positioning accordingly. The algorithm prioritizes LoS connectivity and dynamically adapts to user traffic demands in real time. Evaluated through simulations in ns-3, VTOPA achieves up to a 50% increase in aggregate throughput and a 50% reduction in delay, without compromising fairness, outperforming benchmark approaches in obstacle-rich environments."
2506.2335,"Underwater wireless communications face significant challenges due to propagation constraints, limiting the effectiveness of traditional radio and optical technologies. Long-range acoustic communications support distances up to a few kilometers, but suffer from low bandwidth, high error ratios, and multipath interference. Semantic communications, which focus on transmitting extracted semantic features rather than raw data, present a promising solution by significantly reducing the volume of data transmitted over the wireless link.This paper evaluates the resilience of SAGE, a semantic-oriented communications framework that combines semantic processing with Generative Artificial Intelligence (GenAI) to compress and transmit image data as textual descriptions over acoustic links. To assess robustness, we use a custom-tailored simulator that introduces character errors observed in underwater acoustic channels. Evaluation results show that SAGE can successfully reconstruct meaningful image content even under varying error conditions, highlighting its potential for robust and efficient underwater wireless communication in harsh environments."
2506.23488,"Wireless communication systems face challenges in meeting the demand for higher data rates and reliable connectivity in complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for advanced wave-domain signal processing, where mobile SIMs can outperform fixed counterparts. In this paper, we propose a novel unmanned aerial vehicle (UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude economy (LAE) networks, where UAVs act as both cache-enabled base stations and mobile SIM carriers to enhance uplink transmissions. To maximize network capacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that integrates user association, UAV-SIM three-dimensional positioning, and multi-layer SIM phase shift design. Due to the non-convexity and NP-hardness of USBJOP, we decompose it into three subproblems, which are the association between UAV-SIMs and users optimization problem (AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase shifts optimization problem (USPSOP). Then, we solve them through an alternating optimization strategy. Specifically, AUUOP and ULOP are transformed into convex forms solvable via the CVX tool, while USPSOP is addressed by a generative artificial intelligence (GAI)-based hybrid optimization algorithm. Simulation results show that the proposed approach achieves approximately 1.5 times higher network capacity compared with suboptimal schemes, effectively mitigates multi-user interference with increasing SIM layers and meta-atoms, and reduces runtime by 10\% while maintaining solution quality, thereby demonstrating its practicality for real-world deployments."
2506.23493,"Low-altitude wireless networks (LAWNs) have garnered significant attention in the forthcoming 6G networks. In LAWNs, satellites with wide coverage and unmanned aerial vehicles (UAVs) with flexible mobility can complement each other to form integrated satellite-UAV networks, providing ubiquitous and high-speed connectivity for low-altitude operations. However, the higher line-of-sight probability in low-altitude airspace increases transmission security concerns. In this work, we present a collaborative beamforming-based physical layer security scheme for LAWNs. We introduce the fundamental aspects of integrated satellite-UAV networks, physical layer security, UAV swarms, and collaborative beamforming for LAWN applications. Following this, we highlight several opportunities for collaborative UAV swarm secure applications enabled by satellite networks, including achieving physical layer security in scenarios involving data dissemination, data relay, eavesdropper collusion, and imperfect eavesdropper information. Next, we detail two case studies: a secure relay system and a two-way aerial secure communication framework specifically designed for LAWN environments. Simulation results demonstrate that these physical layer security schemes are effective and beneficial for secure low-altitude wireless communications. A short practicality analysis shows that the proposed method is applicable to LAWN scenarios. Finally, we discuss current challenges and future research directions for enhancing security in LAWNs."
2506.23628,"Traditional Kubernetes networking struggles to meet the escalating demands of AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes Network Drivers (KNDs), a transformative, modular, and declarative architecture designed to overcome current imperative provisioning and API limitations. KNDs integrate network resource management into Kubernetes' core by utilizing Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements, and upcoming OCI Runtime Specification changes. Our DraNet implementation demonstrates declarative attachment of network interfaces, including Remote Direct Memory Access (RDMA) devices, significantly boosting high-performance AI/ML workloads. This capability enables sophisticated cloud-native applications and lays crucial groundwork for future Telco solutions, fostering a ""galaxy"" of specialized KNDs for enhanced application delivery and reduced operational complexity."
2506.2364,"Recently, researchers have explored ML-based Traffic Engineering (TE), leveraging neural networks to solve TE problems traditionally addressed by optimization. However, existing ML-based TE schemes remain impractical: they either fail to handle topology changes or suffer from poor scalability due to excessive computational and memory overhead. To overcome these limitations, we propose Geminet, a lightweight and scalable ML-based TE framework that can handle changing topologies. Geminet is built upon two key insights: (i) a methodology that decouples neural networks from topology by learning an iterative gradient-descent-based adjustment process, as the update rule of gradient descent is topology-agnostic, relying only on a few gradient-related quantities; (ii) shifting optimization from path-level routing weights to edge-level dual variables, reducing memory consumption by leveraging the fact that edges are far fewer than paths. Evaluations on WAN and data center datasets show that Geminet significantly improves scalability. Its neural network size is only 0.04% to 7% of existing schemes, while handling topology variations as effectively as HARP, a state-of-the-art ML-based TE approach, without performance degradation. When trained on large-scale topologies, Geminet consumes under 10 GiB of memory, more than eight times less than the 80-plus GiB required by HARP, while achieving 5.45 times faster convergence speed, demonstrating its potential for large-scale deployment."
2506.2374,"Mobile networks are embracing disaggregation, reflected by the industry trend towards Open RAN. Private 5G networks are viewed as particularly suitable contenders as early adopters of Open RAN, owing to their setting, high degree of control, and opportunity for innovation they present. Motivated by this, we have recently deployed Campus5G, the first of its kind campus-wide, O-RAN-compliant private 5G testbed across the central campus of the University of Edinburgh. We present in detail our process developing the testbed, from planning, to architecting, to deployment, and measuring the testbed performance. We then discuss the lessons learned from building the testbed, and highlight some research opportunities that emerged from our deployment experience."
2506.23755,"One primary focus of next generation wireless communication networks is the millimeterwave (mmWave) spectrum, typically considered in the 30 GHz to 300 GHz frequency range. Despite their promise of high data rates, mmWaves suffer from severe attenuation while passing through obstacles. Unmanned aerial vehicles (UAVs) have been proposed to offset this limitation on account of their additional degrees of freedom, which can be leveraged to provide line of sight (LoS) transmission paths. While some prior works have proposed analytical frameworks to compute the LoS probability for static ground users and a UAV, the same is lacking for mobile users on the ground. In this paper, we consider the popular Manhattan point line process (MPLP) to model an urban environment, within which a ground user moves with a known velocity for a small time interval along the roads. We derive an expression for the expected duration of LoS between a static UAV in the air and a mobile ground user, and validate the same through simulations. To demonstrate the efficacy of the proposed analysis, we propose a simple user association algorithm that greedily assigns the UAVs to users with the highest expected LoS time, and show that it outperforms the existing benchmark schemes that assign the users to the nearest UAVs with LoS without considering the user mobility."
2506.23964,"Generative ML models are increasingly popular in networking for tasks such as telemetry imputation, prediction, and synthetic trace generation. Despite their capabilities, they suffer from two shortcomings: (i) their output is often visibly violating well-known networking rules, which undermines their trustworthiness; and (ii) they are difficult to control, frequently requiring retraining even for minor changes.To address these limitations and unlock the benefits of generative models for networking, we propose a new paradigm for integrating explicit network knowledge in the form of first-order logic rules into ML models used for networking tasks. Rules capture well-known relationships among used signals, e.g., that increased latency precedes packet loss. While the idea is conceptually straightforward, its realization is challenging: networking knowledge is rarely formalized into rules, and naively injecting them into ML models often hampers ML's effectiveness. This paper introduces NetNomos a multi-stage framework that (1) learns rules directly from data (e.g., measurements); (2) filters them to distinguish semantically meaningful ones; and (3) enforces them through a collaborative generation between an ML model and an SMT solver."
2507.00237,"Network virtualization allows hosting applications with diverse computation and communication requirements on shared edge infrastructure. Given a set of requests for deploying virtualized applications, the edge provider has to deploy a maximum number of them to the underlying physical network, subject to capacity constraints. This challenge is known as the virtual network embedding (VNE) problem: it models applications as virtual networks, where virtual nodes represent functions and virtual links represent communication between the virtual nodes.All variants of VNE are known to be strongly NP-hard. Because of its centrality to network virtualization, VNE has been extensively studied. We focus on the online variant of VNE, in which deployment requests are not known in advance. This reflects the highly skewed and unpredictable demand intrinsic to the edge. Unfortunately, existing solutions to online VNE do not scale well with the number of requests per second and the physical topology size.We propose a novel approach in which our new online algorithm, OLIVE, leverages a nearly optimal embedding for an aggregated expected demand. This embedding is computed offline. It serves as a plan that OLIVE uses as a guide for handling actual individual requests while dynamically compensating for deviations from the plan. We demonstrate that our solution can handle a number of requests per second greater by two orders of magnitude than the best results reported in the literature. Thus, it is particularly suitable for realistic edge environments."
2507.00337,"Delay-based protocols rely on end-to-end delay measurements to detect network congestion. However, in cellular networks, Radio Access Network (RAN) buffers introduce significant delays unrelated to congestion, fundamentally challenging these protocols' assumptions. We identify two major types of RAN buffers - retransmission buffers and uplink scheduling buffers - that can introduce delays comparable to congestion-induced delays, severely degrading protocol performance. We present CellNinjia, a software-based system providing real-time visibility into RAN operations, and Gandalf, which leverages this visibility to systematically handle RAN-induced delays. Unlike existing approaches that treat these delays as random noise, Gandalf identifies specific RAN operations and compensates for their effects. Our evaluation in commercial 4G LTE and 5G networks shows that Gandalf enables substantial performance improvements - up to 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols' core algorithms, demonstrating that delay-based protocols can realize their full potential in cellular networks."
2507.00623,"The increasing complexity of Extended Reality (XR) applications demands substantial processing power and high bandwidth communications, often unavailable on lightweight devices. Remote rendering consists of offloading processing tasks to a remote node with a powerful GPU, delivering the rendered content to the end device. The delivery is usually performed through popular streaming protocols such as Web Real-Time Communications (WebRTC), offering a data channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH), better suitable for scalability. Moreover, new streaming protocols based on QUIC are emerging as potential replacements for WebRTC and DASH and offer benefits like connection migration, stream multiplexing and multipath delivery. This work describes the integration of the two most popular multimedia frameworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote Renderer, and analyzes their performance when offering different protocols for delivering the rendered content to the end device over WIFI or 5G. This solution constitutes a beyond state-of-the-art testbed to conduct cutting-edge research in the XR field."
2507.00672,"Edge computing enables real-time data processing closer to its source, thus improving the latency and performance of edge-enabled AI applications. However, traditional AI models often fall short when dealing with complex, dynamic tasks that require advanced reasoning and multimodal data processing. This survey explores the integration of multi-LLMs (Large Language Models) to address this in edge computing, where multiple specialized LLMs collaborate to enhance task performance and adaptability in resource-constrained environments. We review the transition from conventional edge AI models to single LLM deployment and, ultimately, to multi-LLM systems. The survey discusses enabling technologies such as dynamic orchestration, resource scheduling, and cross-domain knowledge transfer that are key for multi-LLM implementation. A central focus is on trusted multi-LLM systems, ensuring robust decision-making in environments where reliability and privacy are crucial. We also present multimodal multi-LLM architectures, where multiple LLMs specialize in handling different data modalities, such as text, images, and audio, by integrating their outputs for comprehensive analysis. Finally, we highlight future directions, including improving resource efficiency, trustworthy governance multi-LLM systems, while addressing privacy, trust, and robustness concerns. This survey provides a valuable reference for researchers and practitioners aiming to leverage multi-LLM systems in edge computing applications."
2507.00856,"This paper aims to enhance the performance of Vehicular Platooning (VP) systems integrated with Wireless Federated Learning (WFL). In highly dynamic environments, vehicular platoons experience frequent communication changes and resource constraints, which significantly affect information exchange and learning model synchronization. To address these challenges, we first formulate WFL in VP as a joint optimization problem that simultaneously considers Age of Information (AoI) and Federated Learning Model Drift (FLMD) to ensure timely and accurate control. Through theoretical analysis, we examine the impact of FLMD on convergence performance and develop a two-stage Resource-Aware Control framework (RACE). The first stage employs a Lagrangian dual decomposition method for resource configuration, while the second stage implements a multi-agent deep reinforcement learning approach for vehicle selection. The approach integrates Multi-Head Self-Attention and Long Short-Term Memory networks to capture spatiotemporal correlations in communication states. Experimental results demonstrate that, compared to baseline methods, the proposed framework improves AoI optimization by up to 45%, accelerates learning convergence, and adapts more effectively to dynamic VP environments on the AI4MARS dataset."
2507.00896,"A new congestion and delay control algorithm named QUIC Delay Control (QUIC-DC) is proposed for controlling not only congestion but also the queueing delay encountered along the forward communication path. The core idea is to estimate the one-way queueing delay of a connection to trigger an early reaction to congestion. This idea, along with a the TCP Westwood+ congestion control algorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2, NewReno, Westwood+. The results obtained in the emulated and real network environments show that QUIC-DC can significantly reduce packet losses along with end-to-end communication delays, while preserving network utilization, features that are both very useful for real-time applications."
2507.01239,"To mitigate the restrictive centralising and monopolistic tendencies of platformisation, we aim to empower local communities by democratising platforms for self-organised social coordination. Our approach is to develop an open-source, full-stack architecture for platform development that supports ease of distribution and cloning, generativity, and a variety of hosting options. The architecture consists of a meta-platform that is used to instantiate a base platform with supporting libraries for generic functions, and plugins (intended to be supplied by third parties) for customisation of application-specification functionality for self-organised social coordination. Associated developer- and user-oriented toolchains support the instantiation and customisation of a platform in a two-stage process. This is demonstrated through the proof-of-concept implementation of two case studies: a platform for regular sporting association, and a platform for collective group study. We conclude by arguing that self-organisation at the application layer can be achieved by the specific supporting functionality of a full-stack architecture with complimentary developer and user toolchains."
2507.01289,"With the rapid development of aerial infrastructure, unmanned aerial vehicles (UAVs) that function as aerial base stations (ABSs) extend terrestrial network services into the sky, enabling on-demand connectivity and enhancing emergency communication capabilities in cellular networks by leveraging the flexibility and mobility of UAVs. In such a UAV-assisted network, this paper investigates position-based beamforming between ABSs and ground users (GUs). To mitigate inter-cell interference, we propose a novel fluid aerial network that leverages ABS rotation to increase multi-cell capacity and overall network efficiency. Specifically, considering the line-of-sight channel model, the spatial beamforming weights are determined by the orientation angles of the GUs. In this direction, we examine the beamforming gain of a two-dimensional multiple-input multiple-output (MIMO) array at various ground positions, revealing that ABS rotation significantly affects multi-user channel correlation and inter-cell interference. Based on these findings, we propose an alternative low-complexity algorithm to design the optimal rotation angle for ABSs, aiming to reduce inter-cell interference and thus maximize the sum rate of multi-cell systems. In simulations, exhaustive search serves as a benchmark to validate the optimization performance of the proposed sequential ABS rotation scheme. Moreover, simulation results demonstrate that, in interference-limited regions, the proposed ABS rotation paradigm can significantly reduce inter-cell interference in terrestrial networks and improve the multi-cell sum rate by approximately 10\% compared to fixed-direction ABSs without rotation."
2507.01333,"With the booming development of generative artificial intelligence (GAI), semantic communication (SemCom) has emerged as a new paradigm for reliable and efficient communication. This paper considers a multi-user downlink SemCom system, using vehicular networks as the representative scenario for multi-user content dissemination. To address diverse yet overlapping user demands, we propose a multi-user Generative SemCom-enhanced intent-aware semantic-splitting multiple access (SS-MGSC) framework. In the framework, we construct an intent-aware shared knowledge base (SKB) that incorporates prior knowledge of semantic information (SI) and user-specific preferences. Then, we designate the common SI as a one-hot semantic map that is broadcast to all users, while the private SI is delivered as personalized text for each user. On the receiver side, a diffusion model enhanced with ControlNet is adopted to generate high-quality personalized images. To capture both semantic relevance and perceptual similarity, we design a novel semantic efficiency score (SES) metric as the optimization objective. Building on this, we formulate a joint optimization problem for multi-user semantic extraction and beamforming, solved using a reinforcement learning-based algorithm due to its robustness in high-dimensional settings. Simulation results demonstrate the effectiveness of the proposed scheme."
2507.0136,"Backscatter tags provide a low-power solution for sensor applications, yet many real-world scenarios require multiple sensors-often of different types-for complex sensing tasks. However, existing designs support only a single sensor per tag, increasing spatial overhead. State-of-the-art approaches to multiplexing multiple sensor streams on a single tag rely on onboard clocks or multiple modulation chains, which add cost, enlarge form factor, and remain prone to timing drift-disrupting synchronization across sensors.We present mmBack, a low-power, clock-free backscatter tag that enables synchronous multi-sensor data acquisition and multiplexing over a single modulation chain. mmBack synchronizes sensor inputs in parallel using a shared reference signal extracted from ambient RF excitation, eliminating the need for an onboard timing source. To efficiently multiplex sensor data, mmBack designs a voltage-division scheme to multiplex multiple sensor inputs as backscatter frequency shifts through a single oscillator and RF switch. At the receiver, mmBack develops a frequency tracking algorithm and a finite-state machine for accurate demultiplexing. mmBack's ASIC design consumes 25.56uW, while its prototype supports 5 concurrent sensor streams with bandwidths of up to 5kHz and 3 concurrent sensor streams with bandwidth of up to 18kHz. Evaluation shows that mmBack achieves an average SNR surpassing 15dB in signal reconstruction."
2507.01773,"While interest in the application of generative AI (GenAI) in network optimization has surged in recent years, its rapid progress has often overshadowed critical limitations intrinsic to generative models that remain insufficiently examined in existing literature. This survey provides a comprehensive review and critical analysis of GenAI in network optimization. We focus on the two dominant paradigms of GenAI including generative diffusion models (GDMs) and large pre-trained models (LPTMs), and organize our discussion around a categorization we introduce, dividing network optimization problems into two primary formulations: one-shot optimization and Markov decision process (MDP). We first trace key works, including foundational contributions from the AI community, and categorize current efforts in network optimization. We also review frontier applications of GDMs and LPTMs in other networking tasks, providing additional context. Furthermore, we present theoretical generalization bounds for GDMs in both one-shot and MDP settings, offering insights into the fundamental factors affecting model performance. Most importantly, we reflect on the overestimated perception of GenAI's general capabilities and caution against the all-in-one illusion it may convey. We highlight critical limitations, including difficulties in constraint satisfying, limited concept understanding, and the inherent probabilistic nature of outputs. We also propose key future directions, such as bridging the gap between generation and optimization. Although they are increasingly integrated in implementations, they differ fundamentally in both objectives and underlying mechanisms, necessitating a deeper understanding of their theoretical connections. Ultimately, this survey aims to provide a structured overview and a deeper insight into the strengths, limitations, and potential of GenAI in network optimization."
2507.01976,"Synthetic network traffic generation has emerged as a promising alternative for various data-driven applications in the networking domain. It enables the creation of synthetic data that preserves real-world characteristics while addressing key challenges such as data scarcity, privacy concerns, and purity constraints associated with real data. In this survey, we provide a comprehensive review of synthetic network traffic generation approaches, covering essential aspects such as data types, generation models, and evaluation methods. With the rapid advancements in AI and machine learning, we focus particularly on deep learning-based techniques while also providing a detailed discussion of statistical methods and their extensions, including commercially available tools. Furthermore, we highlight open challenges in this domain and discuss potential future directions for further research and development. This survey serves as a foundational resource for researchers and practitioners, offering a structured analysis of existing methods, challenges, and opportunities in synthetic network traffic generation."
2507.01988,"As AI models outpace the capabilities of single processors, interconnects across chips have become a critical enabler for scalable computing. These processors exchange massive amounts of data at cache-line granularity, prompting the adoption of new interconnect protocols like CXL, NVLink, and UALink, designed for high bandwidth and small payloads. However, the increasing transfer rates of these protocols heighten susceptibility to errors. While mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction (FEC) are standard for reliable data transmission, scaling chip interconnects to multi-node configurations introduces new challenges, particularly in managing silently dropped flits in switching devices. This paper introduces Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit drop detection and in-order delivery without adding header overhead. Additionally, we propose Reliability Extended Link (RXL), an extension of CXL that incorporates ISN to support scalable, reliable multi-node interconnects while maintaining compatibility with the existing flit structure. By elevating CRC to a transport-layer mechanism for end-to-end data and sequence integrity, and relying on FEC for link-layer error correction and detection, RXL delivers robust reliability and scalability without compromising bandwidth efficiency."
2507.01994,"Despite advancements, Radio Access Networks (RAN) still account for over 50\% of the total power consumption in 5G networks. Existing RAN split options do not fully harness data potential, presenting an opportunity to reduce operational expenditures. This paper addresses this opportunity through a twofold approach. First, highly accurate network traffic and user predictions are achieved using the proposed Curated Collaborative Learning (CCL) framework, which selectively collaborates with relevant correlated data for traffic forecasting. CCL optimally determines whom, when, and what to collaborate with, significantly outperforming state-of-the-art approaches, including global, federated, personalized federated, and cyclic institutional incremental learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep reinforcement learning and prediction inferences from CCL to reduce the number of active DU servers efficiently. DUPS dynamically redirects traffic from underutilized DU servers to optimize resource use, improving energy efficiency by up to 89% over conventional strategies, translating into substantial monetary benefits for operators. By integrating CCL-driven predictions with DUPS, this paper demonstrates a transformative approach for minimizing energy consumption and operational costs in 5G RANs, significantly enhancing efficiency and cost-effectiveness."
2507.01997,"Recent research has demonstrated the effectiveness of Artificial Intelligence (AI), and more specifically, Large Language Models (LLMs), in supporting network configuration synthesis and automating network diagnosis tasks, among others. In this preliminary work, we restrict our focus to the application of AI agents to network troubleshooting and elaborate on the need for a standardized, reproducible, and open benchmarking platform, where to build and evaluate AI agents with low operational effort."
2507.02013,"The Internet of Vehicles (IoV) transforms the transportation ecosystem promising pervasive connectivity and data-driven approaches. Deep learning and generative Artificial Intelligence (AI) have the potential to significantly enhance the operation of applications within IoV by facilitating efficient decision-making and predictive capabilities, including intelligent navigation, vehicle safety monitoring, accident prevention, and intelligent traffic management. Nevertheless, efficiently transmitting and processing the massive volumes of data generated by the IoV in real-time remains a significant challenge, particularly in dynamic and unpredictable wireless channel conditions. To address these challenges, this paper proposes a semantic communication framework based on channel perception to improve the accuracy and efficiency of data transmission. The semantic communication model extracts and compresses the information to be transmitted. In addition, the wireless channel is estimated by using a generative diffusion model, which is employed to predict the dynamic channel states, thereby improving the quality of IoV service. In dynamic scenarios, however, the channel estimation performance may be degraded when substantially new scenarios take place, which will adversely affect user experience. To mitigate this limitation, we employ a large model to fine-tune the channel generation model to enhance its adaptability for varying scenarios. The performance and reliability of the proposed framework are evaluated on the two public datasets."
2507.02021,"With the rise of Software-Defined Networking (SDN) for managing traffic and ensuring seamless operations across interconnected devices, challenges arise when SDN controllers share infrastructure with deep learning (DL) workloads. Resource contention between DL training and SDN operations, especially in latency-sensitive IoT environments, can degrade SDN's responsiveness and compromise network performance. Federated Learning (FL) helps address some of these concerns by decentralizing DL training to edge devices, thus reducing data transmission costs and enhancing privacy. Yet, the computational demands of DL training can still interfere with SDN's performance, especially under the continuous data streams characteristic of IoT systems. To mitigate this issue, we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks), a resampling technique that optimizes DL training by prioritizing misclassified samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the number of training samples per epoch, thereby conserving computational resources, reducing energy consumption, and accelerating convergence without significantly impacting accuracy. Applied within an FL setup, REDUS enhances the efficiency of model training on resource-limited edge devices while maintaining network performance. In this paper, REDUS is evaluated on the CICIoT2023 dataset for IoT attack detection, showing a training time reduction of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable and practical solution for intelligent networks."
2507.02613,"This paper presents a complete signal-processing chain for multistatic integrated sensing and communications (ISAC) using 5G Positioning Reference Signal (PRS). We consider a distributed architecture in which one gNB transmits a periodic OFDM-PRS waveform while multiple spatially separated receivers exploit the same signal for target detection, parameter estimation and tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a range-Doppler map from which the bistatic delay and radial velocity are extracted for every target. For a single target, bistatic delays are fused through nonlinear least-squares trilateration, yielding a geometric position estimate, and a regularized linear inversion of the radial-speed equations yields a two-dimensional velocity vector, where speed and heading are obtained. The approach is applied to 2D and 3D settings, extended to account for receiver clock synchronization bias, and generalized to multiple targets by resolving target association. The sequence of position-velocity estimates is then fed to standard and extended Kalman filters to obtain smoothed tracks. Our results show high-fidelity moving-target detection, positioning, and tracking using 5G PRS signals for multistatic ISAC."
2507.0268,"The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks (NTNs) poses unique architectural and functional challenges due to heterogeneous propagation conditions, dynamic topologies and limited on-board processing capabilities. This paper examines architectural and functional split strategies that are consistent with O-RAN principles for future integrated TN-NTN systems. A taxonomy of split options is proposed that distributes RAN and core functions between satellites and ground nodes, and trade-offs in terms of performance, latency, autonomy and deployment are analysed. In particular, we evaluate configurations ranging from pure on-board DU deployments to full gNB and UPF integration into satellites, including variations based on intra- and inter-satellite processing. In addition, the placement of Near-RT and Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible split strategies between space and ground to optimise the performance and scalability of the control loop. A comprehensive mapping between architectural splits and RIC placement options is provided, emphasising implementation constraints and interoperability considerations. The paper concludes by identifying key challenges and outlining future directions to enable standardised, modular and efficient TN-NTN convergence in the context of the O-RAN."
2507.03158,"Modern machine learning workloads such as large language model training, fine-tuning jobs are highly distributed and span across hundreds of systems with multiple GPUs. Job completion time for these workloads is the artifact of the application, compute, network and storage performance. In case of failure or degraded performance it is imperative to understand the root cause and possible remediation for the problem for end-to-end assurance. This demo showcases SaaSbased observability and automated troubleshooting for AI/ML workload performance issues using cross-layer telemetry and logs (e.g., Application telemetry, Collective communication logs, GPU Health metrics, Network Flow Data, NIC ROCEv2 telemetry). Different use cases are demonstrated for end-to-end assurance such as Cross-layer Dependency Graph, Cross-layer Service Level Expectations, Automated Root Cause Analysis, GPU-toGPU application path tracing."
2507.03224,"Ensuring the reliability and availability of complex networked services demands effective root cause analysis (RCA) across cloud environments, data centers, and on-premises networks. Traditional RCA methods, which involve manual inspection of data sources such as logs and telemetry data, are often time-consuming and challenging for on-call engineers. While statistical inference methods have been employed to estimate the causality of network events, these approaches alone are similarly challenging and suffer from a lack of interpretability, making it difficult for engineers to understand the predictions made by black-box models. In this paper, we present RCACopilot, an advanced on-call system that combines statistical tests and large language model (LLM) reasoning to automate RCA across various network environments. RCACopilot gathers and synthesizes critical runtime diagnostic information, predicts the root cause of incidents, provides a clear explanatory narrative, and offers targeted action steps for engineers to resolve the issues. By utilizing LLM reasoning techniques and retrieval, RCACopilot delivers accurate and practical support for operators."
2507.03248,"Low-earth-orbit (LEO) satellite constellations (e.g., Starlink) are becoming a necessary component of future Internet. There have been increasing studies on LEO satellite networking. It is a crucial problem how to evaluate these studies in a systematic and reproducible manner. In this paper, we present OpenSN, i.e., an open source library for emulating large-scale satellite network (SN). Different from Mininet-based SN emulators (e.g., LeoEM), OpenSN adopts container-based virtualization, thus allows for running distributed routing software on each node, and can achieve horizontal scalability via flexible multi-machine extension. Compared to other container-based SN emulators (e.g., StarryNet), OpenSN streamlines the interaction with Docker command line interface and significantly reduces unnecessary operations of creating virtual links. These modifications improve emulation efficiency and vertical scalability on a single machine. Furthermore, OpenSN separates user-defined configuration from container network management via a Key-Value Database that records the necessary information for SN emulation. Such a separation architecture enhances the function extensibility. To sum up, OpenSN exhibits advantages in efficiency, scalability, and extensibility, thus is a valuable open source library that empowers research on LEO satellite networking. Experiment results show that OpenSN constructs mega-constellations 5X-10X faster than StarryNet, and updates link state 2X-4X faster than LeoEM. We also verify the scalability of OpenSN by successfully emulating the five-shell Starlink constellation with a total of 4408 satellites."
2507.03317,"The goal of this project is to explore the feasibility of building a scalable & easy-to-deploy real-time LoRa testbed, made from multiple units of Raspberry Pi (RPI), where each RPI manages its own set of LoRa radios. This project is motivated by the lack of concrete large-scale LoRa testbeds that effectively integrate LoRa communications into the real-time world. The paper introduces how the idea of using RPI came about and why it should work in theory. The paper then carries out experiments on a component of the large-scale testbed, to evaluate the feasibility of the said component based on performance metrics such as RSSI, SNR, PLR and the ability to carry out millisecond-accurate transmissions. The performance metrics are also used to explore the impact of using different combinations of spread factors and transmission frequencies, as well as making comparisons between time-division multiple access (TDMA) and carrier-sense multiple access (CSMA) approaches. The results show that with the right parameters configured, the system can achieve stable and low-latency communications, proving some feasibility to operate under real-time situations. Future work includes giving each RPI control over more radios, carrying out true parallel transmissions, and finally integrating multiple RPIs for a more complete large-scale real-time LoRa testbed."
2507.03401,"This paper designs a post-disaster powered communication intelligent network (PDPCIN) to address communication disruptions caused by ground base station (GBS) failures within the post-disaster area. PDPCIN employs unmanned aerial vehicles (UAVs) to provide wireless data collection (WDC) and wireless energy transmission (WET) for affected areas and leverages low earth orbit satellites (LEO SATs) to relay UAV data to the nearest survival GBS. To ensure basic post-disaster communication while co-optimizing age of information (AoI), energy efficiency, and spectrum efficiency, intelligent synchronization-UAV (IS-UAV) architecture, AoI-based four thresholds updating (AFTU) mechanism, and Dynamic multi-LEO access (DMLA) strategy are proposed. However, three key challenges remain: time-varying task-resource imbalances, complex topology caused by multi-device scheduling, and nonlinear coupling in multidimensional metric optimization, making system optimization NP-hard. Therefore, this paper proposes a hierarchical heterogeneous graph neural networks (HHGNN) framework. It models heterogeneous device nodes and their communication relations as a hierarchical heterogeneous graph structure, integrating our defined graph sensing, exchange, and mask layer to handle the network's input, feature propagation, and output. To search appropriate number of single-LEO SATs, we propose single-LEO SAT density optimization (S-LSDO) algorithm. Finally, we compare the proposed scheme with state-of-the-art benchmarks to validate its superior collaborative optimization of AoI, energy efficiency, and spectrum efficiency. Based on this, we derive the expressions for the expected values of AoI and stagnant AoI proportion."
2507.03873,"A Wi-Fi-enabled device, or simply Wi-Fi device, sporadically broadcasts probe request frames (PRFs) to discover nearby access points (APs), whether connected to an AP or not. To protect user privacy, unconnected devices often randomize their MAC addresses in the PRFs, known as MAC address randomization. While prior works have achieved accurate device counting under MAC address randomization, they typically rely on machine learning, resulting in inefficient deployment due to the time-consuming processes of data cleaning, model training, and hyperparameter tuning. To enhance deployment efficiency, we propose RateCount, an accurate, lightweight, and learning-free counting approach based on the rate at which APs receive PRFs within a window. RateCount employs a provably unbiased closed-form expression to estimate the device count time-averaged over the window and an error model to compute the lower bound of the estimation variance. We also demonstrate how to extend RateCount to people counting by incorporating a device-to-person calibration scheme. Through extensive real-world experiments conducted at multiple sites spanning a wide range of counts, we show that RateCount, without any deployment costs for machine learning, achieves comparable counting accuracy with the state-of-the-art learning-based device counting and improves previous people counting schemes by a large margin."
2507.0395,"Devices operating in Internet of Things (IoT) networks may be deployed across vast geographical areas and interconnected via multi-hop communications. Further, they may be unguarded. This makes them vulnerable to attacks and motivates operators to check on devices frequently. To this end, we propose and study an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in IoT networks with a charging station powered by solar. A key challenge is optimizing the trajectory of the UAV to ensure it attests as many devices as possible. A trade-off here is that devices being checked by the UAV are offline, which affects the amount of data delivered to a gateway. Another challenge is that the charging station experiences time-varying energy arrivals, which in turn affect the flight duration and charging schedule of the UAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL) solution to optimize the UAV's charging schedule and the selection of devices to be attested during each flight. The simulation results show that our solution reduces the average age of trust by 88% and throughput loss due to attestation by 30%."
2507.04001,"SmartNICs have been increasingly utilized across various applications to offload specific computational tasks, thereby enhancing overall system performance. However, this offloading process introduces several communication challenges that must be addressed for effective integration. A key challenge lies in establishing efficient communication between the offloaded components and the main application running on the host. In this study, we evaluate different approaches for achieving memory access between the host and SmartNIC. We analyze memory access performance on both the SmartNIC and the host to support in-network applications and guide the selection of an appropriate memory access design."
2507.04081,"As a key component of low-altitude economic networks, aerial base stations (AeBSs) provide flexible and reliable wireless coverage to support 6G ultra-reliable and low-latency communication (URLLC) services. However, limited spectrum resources and severe co-channel interference pose significant challenges to the deployment and resource allocation of AeBSs. To address these limitations, this paper proposes a novel rate-splitting multiple access (RSMA)-enabled transmission design to flexibly manage interference and effectively enhance URLLC services in spectrum-constrained multi-AeBS networks. On this basis, we formulate a joint optimization problem involving AeBS deployment, user association, and resource allocation to maximize the achievable sum rate and coverage of the total system. Given the NP-hard nature of the problem and the highly dynamic environment, we propose a novel alternating optimization framework based on the generative graph diffusion models. Specifically, we model AeBSs and ground users as graph nodes, then we employ a discrete graph generation process solved via denoising diffusion is employed to explore the combinatorial space of deployment and association strategies. Moreover, the algorithm adopts the successive convex approximation (SCA) method to optimize AeBS beamforming and RSMA rate allocation under finite blocklength constraints. Extensive simulations demonstrate that the proposed algorithm outperforms existing methods in terms of convergence speed, sum rate, and coverage, while also exhibiting robust performance under varying network densities and interference levels."
2507.04421,"Many UAV-related applications require group communications between UAVs to reliably and efficiently deliver rich media content as well as to extend line-of-sight coverage between sky and ground. This paper studies fast yet resource-efficient UAV transitions while maintaining high multicasting performance. We develop a set of analytic and algorithmic results to form the efficient transition formation (ETF) algorithm that deals with different UAV transition scenarios in a multicasting environment. The ETF algorithm first evaluates the seamlessness of a straight-line trajectory (SLT), by processing low-complexity computations (e.g., Euclidean distances) or a chain of fast checks with controlled traffic overheads. For an interrupted SLT, ETF establishes a new trajectory consisting of a minimum number of seamless straight lines that join at specially selected locations in terms of controlling mobile UAVs' seamless travel distances. Our simulation studies quantify the multicasting performance gains that ETF allows, outperforming compared studies when seamlessly transiting UAV group members."
2507.04425,"Telerobotic technologies are becoming increasingly essential in fields such as remote surgery, nuclear decommissioning, and space exploration. Reliable datasets and testbeds are essential for evaluating telerobotic system performance prior to real-world deployment. However, there is a notable lack of datasets that capture the impact of network delays, as well as testbeds that realistically model the communication link between the operator and the robot. This paper introduces TeleSim, a network-aware teleoperation dataset and testbed designed to assess the performance of telerobotic applications under diverse network conditions. TeleSim systematically collects performance data from fine manipulation tasks executed under three predefined network quality tiers: High, Medium, and Low. Each tier is characterized through controlled settings of bandwidth, latency, jitter, and packet loss. Using OMNeT++ for precise network simulation, we record a wide range of metrics, including completion time, success rates, video quality indicators (Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM)), and quality of service (QoS) parameters. TeleSim comprises 300 experimental trials, providing a robust benchmark for evaluating teleoperation systems across heterogeneous network scenarios. In the worst network condition, completion time increases by 221.8% and success rate drops by 64%. Our findings reveal that network degradation leads to compounding negative impacts, notably reduced video quality and prolonged task execution, highlighting the need for adaptive, resilient teleoperation protocols. The full dataset and testbed software are publicly available on our GitHub repository:this https URLand YouTube channel:this https URL."
2507.04589,"The exponential growth of multimedia data traffic in 6G networks poses unprecedented challenges for immersive communication, where ultra-high-definition, multi-quality streaming must be delivered on demand while minimizing network operational costs. Traditional routing approaches, such as shortest-path algorithms, fail to optimize flow multiplexing across multiple destinations, while conventional Steiner tree methods cannot accommodate heterogeneous quality-of-service (QoS) requirements-a critical need for 6G's personalized services. In this paper, we address a fundamental but unsolved challenge: the minimum flow problem (MFP) with multi-destination, heterogeneous outflow demands, which is pivotal for efficient multimedia distribution such as adaptive-resolution video streaming. To overcome the limitations of existing methods, we propose a two-stage dynamic programming-enhanced On-demand Steiner Tree (OST) algorithm, the first approach that jointly optimizes flow aggregation and QoS-aware path selection for arbitrary outflow requirements. We rigorously prove the optimality of OST using mathematical induction, demonstrating that it guarantees the minimum-cost multicast flow under differentiated service constraints. Extensive experiments in 6G-like multimedia transmission scenarios show that OST reduces total network flow by over 10% compared to state-of-the-art methods while ensuring on-demand QoS fulfillment. The complete code is available atthis https URL."
2507.04734,"This paper presents our low-latency Polar code encoders and decoders developed for the 2025 International Symposium on Topics in Coding (ISTC 2025) contest, which challenges participants to implement the fastest possible channel code encoders and decoders in terms of average and maximum latency on a CPU target. Our solution is based on Polar codes with an Adaptive Successive Cancellation List (ASCL) decoder. We introduce a novel ASCL unrolled decoder generator. We conduct an extensive exploration of the design space, including code construction, CRC selection, and list size, to identify optimal trade-offs between signal-to-noise ratio and decoding time across various operating points. The considered operating points are frame error rates of 10^{-3} and 10^{-5}, information bit lengths of 64, 128, 256, and 512, and code rates of 1/4, 1/2, and 4/5. We also propose an optimized bit-packed encoder. All implementations of the encoders and decoders, along with the code construction and the unrolled decoders generator, are released as open source in the AFF3CT toolbox."
2507.04968,"In wireless networks, algorithms for user association, i.e., the task of choosing the base station (BS) that every arriving user should join, significantly impact the network performance. A wireless network with multiple BSs, operating on non-overlapping channels, is considered. The channels of the BSs are susceptible to jamming by attackers. During every time slot, a user arrives with a certain probability. There exists a holding cost in each slot for every user associated with a BS. The goal here is to design a user association scheme, which assigns a BS to each user upon arrival with the objective of minimizing the long-run total average holding cost borne within the network. This objective results in low average delays attained by the users. This association problem is an instance of restless multi-armed bandit problems, and is known to be hard to solve. By making use of the framework presented by Whittle, the hard per-stage constraint that every arriving user must connect to exactly one BS in a time slot is relaxed to a long-term time-averaged constraint. Subsequently, we employ the Lagrangian multiplier strategy to reformulate the problem into an unconstrained form and decompose it into separate Markov Decision Processes at the BSs. Further, the problem is proven to be Whittle indexable and a method for calculating the Whittle indices corresponding to different BSs is presented. We design a user association policy under which, upon arrival of a user in a time slot, it is assigned to the BS having the least Whittle index in that slot. Through extensive simulations, we show that our proposed association policy based on the Whittle index outperforms various user association policies proposed in previous work in terms of different metrics such as average cost, average delay, and Jain's fairness index."
2507.05597,"Wi-Fi contact-free sensing systems have attracted widespread attention due to their ubiquity and convenience. The integrated sensing and communication (ISAC) technology utilizes off-the-shelf Wi-Fi communication signals for sensing, which further promotes the deployment of intelligent sensing applications. However, current Wi-Fi sensing systems often require prolonged and unnecessary communication between transceivers, and brief communication interruptions will lead to significant performance degradation. This paper proposes Baton, the first system capable of accurately tracking targets even under severe Wi-Fi feature deficiencies. To be specific, we explore the relevance of the Wi-Fi feature matrix from both horizontal and vertical dimensions. The horizontal dimension reveals feature correlation across different Wi-Fi links, while the vertical dimension reveals feature correlation among different time slots. Based on the above principle, we propose the Simultaneous Tracking And Predicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi features over time and across different links, akin to passing a baton. We implement the system on commercial devices, and the experimental results show that our system outperforms existing solutions with a median tracking error of 0.46m, even when the communication duty cycle is as low as 20.00%. Compared with the state-of-the-art, our system reduces the tracking error by 79.19% in scenarios with severe Wi-Fi feature deficiencies."
2507.05731,"Recently, large vision-language models (LVLMs) unleash powerful analysis capabilities for low Earth orbit (LEO) satellite Earth observation images in the data center. However, fast satellite motion, brief satellite-ground station (GS) contact windows, and large size of the images pose a data download challenge. To enable near real-time Earth observation applications (e.g., disaster and extreme weather monitoring), we should explore how to deploy LVLM in LEO satellite networks, and design SpaceVerse, an efficient satellite-ground synergistic LVLM inference system. To this end, firstly, we deploy compact LVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs to handle computationally intensive tasks. Then, we propose a computing and communication co-design framework comprised of a progressive confidence network and an attention-based multi-scale preprocessing, used to identify on-satellite inferring data, and reduce data redundancy before satellite-GS transmission, separately. We implement and evaluate SpaceVerse on real-world LEO satellite constellations and datasets, achieving a 31.2% average gain in accuracy and a 51.2% reduction in latency compared to state-of-the-art baselines."
2507.05829,"Deploying deep neural networks (DNNs) on resource-constrained mobile devices presents significant challenges, particularly in achieving real-time performance while simultaneously coping with limited computational resources and battery life. While Mobile Edge Computing (MEC) offers collaborative inference with GPU servers as a promising solution, existing approaches primarily rely on layer-wise model partitioning and undergo significant transmission bottlenecks caused by the sequential execution of DNN operations. To address this challenge, we present Intra-DP, a high-performance collaborative inference system optimized for DNN inference on MEC. Intra DP employs a novel parallel computing technique based on local operators (i.e., operators whose minimum unit input is not the entire input tensor, such as the convolution kernel). By decomposing their computations (operations) into several independent sub-operations and overlapping the computation and transmission of different sub-operations through parallel execution, Intra-DP mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient inference. The evaluation demonstrates that Intra-DP reduces per-inference latency by up to 50% and energy consumption by up to 75% compared to state-of-the-art baselines, without sacrificing accuracy."
2507.05876,"Asynchronous Distributed Reinforcement Learning (DRL) can suffer from degraded convergence when model updates become stale, often the result of network congestion and packet loss during large-scale training. This work introduces a network data-plane acceleration architecture that mitigates such staleness by enabling inline processing of DRL model updates as they traverse the accelerator engine. To this end, we design and prototype a novel queueing mechanism that opportunistically combines compatible updates sharing a network element, reducing redundant traffic and preserving update utility. Complementing this we provide a lightweight transmission control mechanism at the worker nodes that is guided by feedback from the in-network accelerator. To assess model utility at line rate, we introduce the Age-of-Model (AoM) metric as a proxy for staleness and verify global fairness and responsiveness properties using a formal verification method. Our evaluations demonstrate that this architecture significantly reduces update staleness and congestion, ultimately improving the convergence rate in asynchronous DRL workloads."
2507.06001,"Self-Sovereign Identity (SSI) is a paradigm for digital identity management that offers unique privacy advantages. A key technology in SSI is Decentralized Identifiers (DIDs) and their associated metadata, DID Documents (DDOs). DDOs contain crucial verification material such as the public keys of the entity identified by the DID (i.e., the DID subject) and are often anchored on a distributed ledger to ensure security and availability. Long-lived DIDs need to support updates (e.g., key rotation). Ideally, only the DID subject should authorize DDO updates. However, in practice, update capabilities may be shared or delegated. While the DID specification acknowledges such scenarios, it does not define how updates should be authorized when multiple entities jointly control a DID (i.e., group control). This article examines the implementation of an on-chain, trustless mechanism enabling DID controllers under group control to program their governance rules. The main research question is the following: Can a technical mechanism be developed to orchestrate on-chain group control of a DDO in a ledger-agnostic and adaptable manner?"
2507.0643,"Existing website fingerprinting and traffic classification solutions do not work well when the evaluation context changes, as their performances often heavily rely on context-specific assumptions. To clarify this problem, we take three prior solutions presented for different but similar traffic classification and website fingerprinting tasks, and apply each solution's model to another solution's dataset. We pinpoint dataset-specific and model-specific properties that lead each of them to overperform in their specific evaluation context.As a realistic evaluation context that takes practical labeling constraints into account, we design an evaluation framework using two recent real-world TLS traffic datasets from large-scale networks. The framework simulates a futuristic scenario in which SNIs are hidden in some networks but not in others, and the classifier's goal is to predict destination services in one network's traffic, having been trained on a labelled dataset collected from a different network. Our framework has the distinction of including real-world distribution shift, while excluding concept drift. We show that, even when abundant labeled data is available, the best solutions' performances under distribution shift are between 30% and 40%, and a simple 1-Nearest Neighbor classifier's performance is not far behind. We depict all performances measured on different models, not just the best ones, for a fair representation of traffic models in practice."
2507.06632,"With rapid urbanization and increasing population density, urban traffic congestion has become a critical issue, and traditional ground transportation methods are no longer sufficient to address it effectively. To tackle this challenge, the concept of Advanced Air Mobility (AAM) has emerged, aiming to utilize low-altitude airspace to establish a three-dimensional transportation system. Among various components of the AAM system, electric vertical take-off and landing (eVTOL) aircraft plays a pivotal role due to their flexibility and efficiency. However, the immaturity of Ultra Reliable Low Latency Communication (URLLC) technologies poses significant challenges to safety-critical AAM operations. Specifically, existing Stacked Intelligent Metasurfaces (SIM)-based eVTOL systems lack rigorous mathematical frameworks to quantify probabilistic delay bounds under dynamic air traffic patterns, a prerequisite for collision avoidance and airspace management. To bridge this gap, we employ network calculus tools to derive the probabilistic upper bound on communication delay in the AAM system for the first time. Furthermore, we formulate a complex non-convex optimization problem that jointly minimizes the probabilistic delay bound and the propagation delay. To solve this problem efficiently, we propose a solution based on the Block Coordinate Descent (BCD) algorithm and Semidefinite Relaxation (SDR) method. In addition, we conduct a comprehensive analysis of how various factors impact regret and transmission rate, and explore the influence of varying load intensity and total delay on the probabilistic delay bound."
2507.06911,"The proliferation of data-intensive Artificial Intelligence (AI) applications at the network edge demands a fundamental shift in RAN design, from merely consuming AI for network optimization, to actively enabling distributed AI workloads. This paradigm shift presents a significant opportunity for network operators to monetize AI at the edge while leveraging existing infrastructure investments. To realize this vision, this article presents a novel converged O-RAN and AI-RAN architecture that unifies orchestration and management of both telecommunications and AI workloads on shared infrastructure. The proposed architecture extends the Open RAN principles of modularity, disaggregation, and cloud-nativeness to support heterogeneous AI deployments. We introduce two key architectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN Service Management and Orchestration (SMO) to enable integrated resource and allocation across RAN and AI workloads; and (ii) AI-RAN sites that provide distributed edge AI platforms with real-time processing capabilities. The proposed system supports flexible deployment options, allowing AI workloads to be orchestrated with specific timing requirements (real-time or batch processing) and geographic targeting. The proposed architecture addresses the orchestration requirements for managing heterogeneous workloads at different time scales while maintaining open, standardized interfaces and multi-vendor interoperability."
2507.07118,"Wireless localization and sensing technologies are essential in modern wireless networks, supporting applications in smart cities, the Internet of Things (IoT), and autonomous systems. High-performance localization and sensing systems are critical for both network efficiency and emerging intelligent applications. Integrating channel state information (CSI) with deep learning has recently emerged as a promising solution. Recent works have leveraged the spatial diversity of multiple input multiple output (MIMO) systems and the frequency granularity of orthogonal frequency division multiplexing (OFDM) waveforms to improve spatial resolution. Nevertheless, the joint modeling of localization and sensing under the high-dimensional CSI characteristics of MIMO-OFDM systems remains insufficiently investigated. This work aims to jointly model and optimize localization and sensing tasks to harness their potential synergy. We first formulate localization and sensing as a mixed-integer bilevel deep learning problem and then propose a novel stochastic proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO) algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale datasets, leveraging mini-batch training at each step for computational and memory efficiency. The algorithm is also supported by theoretical convergence guarantees. Extensive experiments on multiple datasets validate its effectiveness and highlight the performance gains from joint localization and sensing optimization."
2507.07149,"Recent advancements in on-device training for deep neural networks have underscored the critical need for efficient activation compression to overcome the memory constraints of mobile and edge devices. As activations dominate memory usage during training and are essential for gradient computation, compressing them without compromising accuracy remains a key research challenge. While existing methods for dynamic activation quantization promise theoretical memory savings, their practical deployment is impeded by system-level challenges such as computational overhead and memory fragmentation.To address these challenges, we introduce DAF, a Dynamic Activation Framework that enables scalable and efficient on-device training through system-level optimizations. DAF achieves both memory- and time-efficient dynamic quantization training by addressing key system bottlenecks. It develops hybrid reduction operations tailored to the memory hierarchies of mobile and edge SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic quantization, and implements an importance-aware paging memory management scheme to reduce fragmentation and support dynamic memory adjustments.These optimizations collectively enable DAF to achieve substantial memory savings and speedup without compromising model training accuracy. Evaluations on various deep learning models across embedded and mobile platforms demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$ speedup, making DAF a scalable and practical solution for resource-constrained environments."
2507.07437,"The construction of Low Earth Orbit (LEO) satellite constellations has recently attracted tremendous attention from both academia and industry. The 5G and 6G standards have identified LEO satellite networks as a key component of future mobile networks. However, due to the high-speed movement of satellites, ground terminals often experience frequent and high-latency handovers, which significantly deteriorate the performance of latency-sensitive applications. To address this challenge, we propose a parallel handover mechanism for mobile satellite networks that can considerably reduce handover latency. The main idea is to employ plan-based handovers instead of measurement-based handovers to avoid interactions between the access and core networks, thereby eliminating the significant time overhead associated with traditional handover procedures. Specifically, we introduce a novel network function named the Satellite Synchronized Function (SSF), which is designed to be fully compliant with the standard 5G core network. In addition, we propose a machine learning model for signal strength prediction, coupled with an efficient handover scheduling algorithm. We have conducted extensive experiments, and the results demonstrate that our proposed handover scheme can reduce handover latency by 21\times compared to the standard NTN handover scheme and two other existing handover approaches, along with significant improvements in network stability and user-level performance."
2507.07481,"The integration of wireless power transfer (WPT) with Internet of Things (IoT) offers promising solutions for sensing applications, but faces significant challenges when deployed in hard-to-access areas such as high-temperature environments. In such extreme conditions, traditional fixed WPT infrastructure cannot be safely installed, and batteries rapidly degrade due to hardware failures. In this paper, we propose an uncrewed aerial vehicle (UAV)-assisted data collection and WPT framework for batteryless sensor (BLS) networks deployed in these challenging environments. Specifically, we consider a practical scenario where a UAV first transfers energy to BLS nodes via WPT, enabling these nodes to subsequently transmit their collected data to the UAV through orthogonal frequency-division multiple access (OFDMA). Then, we formulate a multi-objective optimization problem that aims to maximize the fair data collection volume while minimizing the UAV energy consumption through joint optimization of transmit power allocation and flight trajectory planning. Due to the non-convex nature and dynamic characteristics of this problem, conventional optimization methods prove inadequate. To address these challenges, we propose an enhanced soft actor-critic algorithm with parameter-free attention, prioritized experience replay, and value-based reward centering (SAC-PPV), thereby improving the exploration efficiency and learning stability of the algorithm in complex WPT scenarios. Simulation results demonstrate that the proposed approach consistently outperforms benchmark algorithms under various network configurations."
2507.07535,"Computing Power Network (CPN) unifies wide-area computing resources through coordinated network control, while cloud-native abstractions enable flexible resource orchestration and on-demand service provisioning atop the elastic infrastructure CPN provides. However, current approaches fall short of fully integrating computing resources via network-enabled coordination as envisioned by CPN. In particular, optimally mapping services to an underlying infrastructure to maximize resource efficiency and service satisfaction remains challenging. To overcome this challenge, we formally define the service mapping problem in CPN, establish its theoretical intractability, and identify key challenges in practical optimization. We propose Adaptive Bilevel Search (ABS), a modular framework featuring (1) graph partitioning-based reformulation to capture variable coupling, (2) a bilevel optimization architecture for efficient global exploration with local optimality guarantees, and (3) fragmentation-aware evaluation for global performance guidance. Implemented using distributed particle swarm optimization, ABS is extensively evaluated across diverse CPN scenarios, consistently outperforming existing approaches. Notably, in complex scenarios, ABS achieves up to 73.2% higher computing resource utilization and a 60.2% higher service acceptance ratio compared to the best-performing baseline."
2507.07677,"This paper experimentally analyzes the negative impact of contention caused by neighboring Wi-Fi networks operating on overlapping channels on Virtual Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full channel overlap within an 80 MHz channel. Our results show that (i) increasing the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies contention and degrades VR streaming performance; (ii) OBSS activity on the secondary-sided 40 MHz portion degrades performance more than activity on the primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel overlap with two 40 MHz OBSS contenders is less detrimental than partial overlap with a single high-load 40 MHz contender, but more disruptive than full overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40 MHz OBSS contenders has a smaller impact on VR streaming under symmetric traffic loads than under asymmetric loads. Moreover, our results demonstrate that our previously proposed Network-aware Step-wise adaptive bitrate algorithm for VR streaming (NeSt-VR) effectively mitigates performance degradation in OBSS environments, enabling VR streaming under heavier OBSS traffic conditions."
2507.07841,"Events such as catastrophes and disasters are, in most cases, unpredictable. Consequently, reusing existing infrastructures to develop alternative communication strategies after disasters is essential to minimise the impact of these events on the population's ability to communicate and promptly receive alerts from authorities. In this context, the emergence of smart cities, characterised by dense and geographically distributed IoT networks, presents significant potential for such reuse. This work proposes HaLert, a resilient architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network, whose resources can be readily reallocated to support a emergency communication system to exchange messages (including text, location, image, audio, and video) between citizens, authorities, and between both parties. To facilitate remote monitoring and configuration of the network, the architecture incorporates the SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled flooding mesh network. A prototype was developed based on this architecture and tested in a real urban scenario comprising both indoor and outdoor environments. The results demonstrated that, despite the significant impact of obstacles, lack of line-of-sight, and terrain slopes on the latency (average latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and 726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow network, it remained stable and resilient, successfully providing all functionalities associated with the HaLert architecture. The tests conducted on the LoRa network revealed a high average message success rate of 94.96%."
2507.08119,"Rail-optimized network fabrics have become the de facto datacenter scale-out fabric for large-scale ML training. However, the use of high-radix electrical switches to provide all-to-all connectivity in rails imposes massive power, cost, and complexity overheads. We propose a rethinking of the rail abstraction by retaining its communication semantics, but realizing it using optical circuit switches. The key challenge is that optical switches support only one-to-one connectivity at a time, limiting the fan-out of traffic in ML workloads using hybrid parallelisms. We introduce parallelism-driven rail reconfiguration as a solution that leverages the sequential ordering between traffic from different parallelisms. We design a control plane, Opus, to enable time-multiplexed emulation of electrical rail switches using optical switches. More broadly, our work discusses a new research agenda: datacenter fabrics that co-evolve with the model parallelism dimensions within each job, as opposed to the prevailing mindset of reconfiguring networks before a job begins."
2507.08134,"The rapid growth of Internet paths in heterogeneity, scale, and dynamics has made existing emulators increasingly insufficient in flexibility, scalability, and usability. To address these limitations, we present Rattan, an extensible and scalable software network path emulator for modern Internet conditions. Rattan's core innovation lies in its cell-based architecture: by splitting emulation functions into modular ""cells"" with well-documented asynchronous interfaces, users are allowed to easily compose different cells by hierarchically linking them and easily construct new cells by using standard cell interfaces. This design enables: (1) scalability, supporting hundreds of concurrent gigabit-level paths on a single machine and cluster-level experiments composed of multiple machines; (2) extensibility, simulating new network conditions by constructing new cells. Rattan empowers developers and researchers to efficiently and confidently evaluate, validate, and diagnose diverse network transport innovations for online services."
2507.08164,"The emergence of large language models (LLMs) and agentic systems is enabling autonomous 6G networks with advanced intelligence, including self-configuration, self-optimization, and self-healing. However, the current implementation of individual intelligence tasks necessitates isolated knowledge retrieval pipelines, resulting in redundant data flows and inconsistent interpretations. Inspired by the service model unification effort in Open-RAN (to support interoperability and vendor diversity), we propose KP-A: a unified Network Knowledge Plane specifically designed for Agentic network intelligence. By decoupling network knowledge acquisition and management from intelligence logic, KP-A streamlines development and reduces maintenance complexity for intelligence engineers. By offering an intuitive and consistent knowledge interface, KP-A also enhances interoperability for the network intelligence agents. We demonstrate KP-A in two representative intelligence tasks: live network knowledge Q&A and edge AI service orchestration. All implementation artifacts have been open-sourced to support reproducibility and future standardization efforts."
2507.08403,"Artificial Intelligence/Machine Learning (AI/ML) has become the most certain and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not natively integrated but rather an add-on feature over existing architecture, 6G shall incorporate AI from the onset to address its complexity and support ubiquitous AI applications. Based on our extensive mobile network operation and standardization experience from 2G to 5G, this paper explores the design and standardization principles of AI-Native radio access networks (RAN) for 6G, with a particular focus on its critical Day 1 architecture, functionalities and capabilities. We investigate the framework of AI-Native RAN and present its three essential capabilities to shed some light on the standardization direction; namely, AI-driven RAN processing/optimization/automation, reliable AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The standardization of AI-Native RAN, in particular the Day 1 features, including an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale field trial with over 5000 5G-A base stations have been built and delivered significant improvements in average air interface latency, root cause identification, and network energy consumption with the proposed architecture and the supporting AI functions. This paper aims to provide a Day 1 framework for 6G AI-Native RAN standardization design, balancing technical innovation with practical deployment."
2507.08429,"The integration of unmanned aerial vehicles (UAVs) with Internet of Things (IoT) networks offers promising solutions for efficient data collection. However, the limited energy capacity of UAVs remains a significant challenge. In this case, laser beam directors (LBDs) have emerged as an effective technology for wireless charging of UAVs during operation, thereby enabling sustained data collection without frequent returns to charging stations (CSs). In this work, we investigate the age of information (AoI) optimization in LBD-powered UAV-assisted IoT networks, where multiple UAVs collect data from distributed IoTs while being recharged by laser beams. We formulate a joint optimization problem that aims to minimize the peak AoI while determining optimal UAV trajectories and laser charging strategies. This problem is particularly challenging due to its non-convex nature, complex temporal dependencies, and the need to balance data collection efficiency with energy consumption constraints. To address these challenges, we propose a novel multi-agent proximal policy optimization with temporal memory and multi-agent coordination (MAPPO-TM) framework. Specifically, MAPPO-TM incorporates temporal memory mechanisms to capture the dynamic nature of UAV operations and facilitates effective coordination among multiple UAVs through decentralized learning while considering global system objectives. Simulation results demonstrate that the proposed MAPPO-TM algorithm outperforms conventional approaches in terms of peak AoI minimization and energy efficiency. Ideally, the proposed algorithm achieves up to 15.1% reduction in peak AoI compared to conventional multi-agent deep reinforcement learning (MADRL) methods."
2507.08507,"Unmanned aerial vehicle (UAV) swarms utilizing collaborative beamforming (CB) in low-altitude wireless networks (LAWN) demonstrate significant potential for enhanced communication range, energy efficiency, and signal directivity through the formation of virtual antenna arrays (VAA). However, environmental disturbances, particularly wind fields, significantly degrade CB performance by introducing positional errors that disrupt beam patterns, thereby compromising transmission reliability. This paper investigates the critical challenge of maintaining CB performance in UAV-based VAAs operating in LAWN under wind field disturbances. We propose a comprehensive framework that models the impact of three distinct wind conditions (constant, shear, and turbulent) on UAV array performance, and formulate a long-term real-time optimization problem to maximize directivity while minimizing maximum sidelobe levels through adaptive excitation current weight adjustments. To address the inherent complexity of this problem, we propose a novel proximal policy optimization algorithm with long short-term memory (LSTM) structure and adaptive learning rate (PPO-LA), which effectively captures temporal patterns in wind field disturbances and enables real-time adaptation without requiring extensive prior training for specific wind conditions. Our simulation results demonstrate that the proposed PPO-LA algorithm successfully recovers degraded CB performance across various wind scenarios, and thus significantly outperforming benchmark algorithms."
2507.08549,"The low Earth orbit (LEO) mega-constellation network (LMCN), which uses thousands of satellites across multi-shell architectures to deliver different services, is facing challenges in inter-shell routing stability due to dynamic network topologies and frequent inter-satellite link (ISL) switching. Existing strategies, such as the Minimum Hop Path set, prioritize minimizing hop counts to reduce latency, but ignore ISL switching costs, which leads to high instability. To overcome this, the Adaptive Path Routing Scheme introduces path similarity thresholds to reduce the ISL switching frequency between shells. However, the greedy approach of Adaptive Path Routing Scheme is often trapped in local optima, sacrificing inter-shell path distance efficiency. To address these limitations, we propose the Dynamic Programming-based Integrated Routing Cost (DP-IRC) algorithm, which is designed explicitly for inter-shell routing optimization. By formulating multi-shell paths as a multistage decision problem, DP-IRC balances hop counts and ISL stability through an Integrated Routing Cost (IRC) metric, combining inter-/intra-shell hops and switching costs. Experiments over 60 time slots with real-world Starlink and OneWeb configurations show that DP-IRC reduces inter-shell ISL switching rates by 39.1% and 22.0% compared to the Minimum Hop Path set strategy and Adaptive Path Routing Scheme, respectively, while still maintaining near-optimal end-to-end distances."
2507.08677,"There are currently many communication options in the Internet of Things, even in particular areas such as constrained and battery-powered devices, such as Low Power Wide Area Networks. Understanding the differences and characteristics of each option is a challenge, even for professionals and researchers in the field. To meet this need, this work analyses the qualitative characteristics of Low Power Wide Area Network protocols and the challenges and opportunities of using constrained devices for sparse networks based on long-life batteries. For this study, a bibliographic survey of the literature was carried out as an analysis of three protocols (LoRaWAN, NB-IoT, and Sigfox), and a detailing of the first one. As a result, there is a discussion about the chosen network protocol and its use in IoT solutions with sparse sensors."
2507.08717,"Previous generations of cellular communication, such as 5G, have been designed with the objective of improving key performance indicators (KPIs) such as throughput, latency, etc. However, to meet the evolving KPI demands as well as the ambitious sustainability targets for the ICT industry, 6G will need to be designed differently. Concretely, 6G will need to consider both the performance and sustainability targets for the various use cases it will serve. Moreover, like previous generations, 6G will have various candidate technological enablers, making the design space of the system even more complex. Furthermore, given the subjective nature of the sustainability indicators, in particular social sustainability, there is a significant gap in literature on how technical enablers and 6G System design can be linked to them. Hence, in this article a novel method for 6G end-to-end (E2E) system design based on Knowledge graphs (KG) has been introduced. It considers as its input: the use case KPIs, use case sustainability requirements expressed as Key Values (KV) and KV Indicators (KVIs), the ability of the technological enablers to satisfy these KPIs and KVIs, the 6G system design principles defined in Hexa-X-II project, the maturity of a technological enabler and the dependencies between the various enablers. As part of the KG method, a novel approach for determining the key values a technological enabler addresses, has also been introduced. The effectiveness of the KG method was demonstrated by its application in designing the 6G E2E system for the cooperating mobile robot use case defined in the Hexa-X-II project, where 82 enablers were selected. Lastly, results from proof-of-concept demonstrations for a subset of the selected enablers have also been provided, which reinforce the efficacy of the KG method for designing a sustainable 6G system."
2507.09094,"In this paper, a novel Three dimensional (3D) positioning framework of fluid antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In the proposed framework, a set of controlled UAVs cooperatively estimate the real-time 3D position of a target UAV. Here, the active UAV transmits a measurement signal to the passive UAVs via the reflection from the target UAV. Each passive UAV estimates the distance of the active-target-passive UAV link and selects an antenna port to share the distance information with the base station (BS) that calculates the real-time position of the target UAV. As the target UAV is moving due to its task operation, the controlled UAVs must optimize their trajectories and select optimal antenna port, aiming to estimate the real-time position of the target UAV. We formulate this problem as an optimization problem to minimize the target UAV positioning error via optimizing the trajectories of all controlled UAVs and antenna port selection of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use the local Q function to determine its trajectory and antenna port while optimizing the target UAV positioning performance without knowing the trajectories and antenna port selections of other controlled UAVs. Different from current MARL methods, the proposed method uses a recurrent neural network (RNN) that incorporates historical state-action pairs of each controlled UAV, and an attention mechanism to analyze the importance of these historical state-action pairs, thus improving the global Q function approximation accuracy and the target UAV positioning accuracy. Simulation results show that the proposed AR-MARL scheme can reduce the average positioning error by up to 17.5% and 58.5% compared to the VD-MARL scheme and the proposed method without FAS."
2507.09124,"The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims to unlock a unified 6G platform capable of seamlessly supporting AI and RAN workloads over shared infrastructure. However, the architectural framework and intelligent resource orchestration strategies necessary to realize this vision remain largely unexplored. In this paper, we propose a Converged AI-and-ORAN Architectural (CAORA) framework based on O-RAN specifications, enabling the dynamic coexistence of real-time RAN and computationally intensive AI workloads. We design custom xApps within the Near-Real-Time RAN Intelligent Controller (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an End-to-End (E2E) orchestrator via the recently introduced Y1 interface. The orchestrator incorporates workload forecasting and anomaly detection modules, augmenting a Soft Actor-Critic (SAC) reinforcement learning agent that proactively manages resource allocation, including Multi-Instance GPU (MIG) partitioning. Using real-world 5G traffic traces from Barcelona, our trace-driven simulations demonstrate that CAORA achieves near 99\% fulfillment of RAN demands, supports dynamic AI workloads, and maximizes infrastructure utilization even under highly dynamic conditions. Our results reveal that predictive orchestration significantly improves system adaptability, resource efficiency, and service continuity, offering a viable blueprint for future AI-and-RAN converged 6G systems."
2507.09153,"Natural disasters often disrupt communication networks and severely hamper emergency response and disaster management. Existing solutions, such as portable communication units and cloud-based network architectures, have improved disaster resilience but fall short if both the Radio Access Network (RAN) and backhaul infrastructure become inoperable. To address these challenges, we propose a demand-driven communication system supported by High Altitude Platform Stations (HAPS) to restore communication in an affected area and enable effective disaster relief. The proposed emergency response network is a promising solution as it provides a rapidly deployable, resilient communications infrastructure. The proposed HAPS-based communication can play a crucial role not only in ensuring connectivity for mobile users but also in restoring backhaul connections when terrestrial networks fail. As a bridge between the disaster management center and the affected areas, it can facilitate the exchange of information in real time, collect data from the affected regions, and relay crucial updates to emergency responders. Enhancing situational awareness, coordination between relief agencies, and ensuring efficient resource allocation can significantly strengthen disaster response capabilities. In this paper, simulations show that HAPS with hybrid optical/THz links boosts backhaul capacity and resilience, even in harsh conditions. HAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first responders and disaster-affected populations. This paper also explores the integration of HAPS into emergency communication frameworks and standards, as it has the potential to improve network resilience and support effective disaster management."
2507.0927,"In this paper, we consider a semantic-aware reconfigurable intelligent surface (RIS)-assisted wireless network, where multiple semantic users (SUs) simultaneously transmit semantic information to an access point (AP) by using the non-orthogonal multiple access (NOMA) method. The SUs can reshape their traffic demands by modifying the semantic extraction factor, while the RIS can reconfigure the channel conditions via the passive beamforming. This provides the AP with greater flexibility to decode the superimposed signals from the SUs. We aim to minimize the system's overall energy consumption, while ensuring that each SU's traffic demand is satisfied. Hence, we formulate a joint optimization problem of the SUs' decoding order and semantic control, as well as the RIS's passive beamforming strategy. This problem is intractable due to the complicated coupling in constraints. To solve this, we decompose the original problem into two subproblems and solve them by using a series of approximate methods. Numerical results show that the joint traffic reshaping and channel reconfiguration scheme significantly improves the energy saving performance of the NOMA transmissions compared to the benchmark methods."
2507.09341,"Vehicular Mobile Edge Computing (VEC) drives the future by enabling low-latency, high-efficiency data processing at the very edge of vehicular networks. This drives innovation in key areas such as autonomous driving, intelligent transportation systems, and real-time analytics. Despite its potential, VEC faces significant challenges, particularly in adhering to strict task offloading deadlines, as vehicles remain within the coverage area of Roadside Units (RSUs) for only brief periods. To tackle this challenge, this paper evaluates the performance boundaries of task processing by initially establishing a theoretical limit using Particle Swarm Optimization (PSO) in a static environment. To address more dynamic and practical scenarios, PSO, Deep Q-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented in an online setting. The objective is to minimize dropped tasks and reduce end-to-end (E2E) latency, covering both communication and computation delays. Experimental results demonstrate that the DQN model considerably surpasses the dynamic PSO approach, achieving a 99.2% reduction in execution time. Furthermore, It leads to a reduction in dropped tasks by 2.5% relative to dynamic PSO and achieves 18.6\% lower E2E latency, highlighting the effectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and efficient task management for VEC systems."
2507.09346,"Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for meeting the low-latency demands of modern IoT and dynamic task scheduling scenarios. MEC reduces the processing burden on resource-constrained devices by enabling task execution at nearby edge servers. However, efficient task scheduling remains a challenge in dynamic, time-sensitive environments. Conventional methods -- such as heuristic algorithms and mixed-integer programming -- suffer from high computational overhead, limiting their real-time applicability. Existing deep learning (DL) approaches offer faster inference but often lack scalability and adaptability to dynamic workloads. To address these issues, we propose a Pointer Network-based architecture for task scheduling in dynamic edge computing scenarios. Our model is trained on a generated synthetic dataset using genetic algorithms to determine the optimal task ordering. Experimental results show that our model achieves lower drop ratios and waiting times than baseline methods, and a soft sequence accuracy of up to 89.2%. Our model consistently achieves inference times under 2 seconds across all evaluated task counts, whereas the integer and binary programming approaches require approximately up to 18 seconds and 90 seconds, respectively. It also shows strong generalization across varying scenarios, and adaptability to real-time changes, offering a scalable and efficient solution for edge-based task management."
2507.09352,"Mobile Edge Computing (MEC) enables low-latency applications by bringing computation closer to the user, but dynamic task arrivals and communication threats like jamming complicate reliable task offloading and resource allocation. In this paper, we formulate a dynamic MEC framework considering the transmission diversity that jointly addresses task scheduling and resource block (RB) assignment in the presence of jamming. First, we define and evaluate key network metrics-including dropped task ratio and bandwidth utilization-while maintaining service continuity by accounting for the existing commitments of the edge server to previously offloaded tasks. Then, we propose a jamming-aware offloading and RB allocation framework that leverages transmission diversity and optimal scheduling across distributed gNBs. The proposed solution is compared to a similar scenario without transmission diversity and two baseline strategies of first-come-first-served (FCFS) and shortest task first (STF). The proposed algorithm effectively mitigates the impact of jamming while enhancing resource utilization and minimizing task drop rates, making it highly suitable for mission-critical MEC applications. At signal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves a $0.26$ task drop rate, outperforming the scenario without transmission diversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52 and 0.63 task drop rates, respectively."
2507.09462,"Accurate modeling and simulation of mobile networks are essential for enabling intelligent and cost-effective network optimization. In this paper, we propose MobiWorld, a generative world model designed to support high-fidelity and flexible environment simulation for mobile network planning and optimization. Unlike traditional predictive models constrained by limited generalization capabilities, MobiWorld exhibits strong universality by integrating heterogeneous data sources, including sensors, mobile devices, and base stations, as well as multimodal data types such as sequences and images. It is capable of generating both network element-level observations (e.g., traffic load, user distribution) and system-level performance indicators (e.g., throughput, energy consumption) to support a wide range of planning and optimization tasks. Built upon advanced diffusion models, MobiWorld offers powerful controllable generation capabilities by modeling the joint distribution between mobile network data and diverse conditional factors including spatio temporal contexts, user behaviors, and optimization policies. This enables accurate simulation of dynamic network states under varying policy configurations, providing optimization agents with precise environmental feedback and facilitating effective decision-making without relying on costly real-network interactions. We demonstrate the effectiveness of MobiWorld in a collaborative energy-saving scenario, where an agent uses observations and rewards generated by MobiWorld to optimize base station sleep and user offloading policies. Experimental results show that MobiWorld exhibits strong controllable generation performance and outperforms traditional methods in energy optimization."
2507.09613,"Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding name, today's Wi-Fi boasts entirely new capabilities that were not even on the roadmap 25 years ago. This article aims to provide a holistic and comprehensive technical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi 1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial article to span these eight generations. Rather than a generation-by-generation exposition, we describe the key mechanisms that have advanced Wi-Fi. We begin by discussing spectrum allocation and coexistence, and detailing the IEEE 802.11 standardization cycle. Second, we provide an overview of the physical layer and describe key elements that have enabled data rates to increase by over 1,000x. Third, we describe how Wi-Fi Medium Access Control has been enhanced from the original Distributed Coordination Function to now include capabilities spanning from frame aggregation to wideband spectrum access. Fourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and introduced multi-user access. Fifth, given the increasing use of mobile, battery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the generations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate spectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput, reliability, and latency. Finally, we describe how Wi-Fi enables nearby Access Points to coordinate in order to improve performance and efficiency. In the Appendix, we further discuss Wi-Fi developments beyond 802.11bn, including integrated mmWave operations, sensing, security and privacy extensions, and the adoption of AI/ML."
2507.09798,"Google's congestion control (GCC) has become a cornerstone for real-time video and audio communication, yet its performance remains fragile in emerging Low Earth Orbit (LEO) networks. In this paper, we study the behavior of videoconferencing systems in LEO constellations. We observe that video quality degrades due to inherent delays and network instability introduced by the high altitude and rapid movement of LEO satellites, with these effects exacerbated by WebRTC's conventional ""one-size-fits-all"" sender-side pacing queue management. To address these challenges, we introduce a data-driven queue management mechanism that tunes the maximum pacing queue capacity based on predicted handover activity, minimizing latency during no-handover periods and prioritizing stability when entering periods of increased handover activity. Our method yields up to 3x improvements in video bitrate and reduces freeze rate by 62% in emulation, while delivering up to a 41% reduction in freeze rate and 40% decrease in mean packet loss on real Starlink constellations compared to WebRTC's default pacing queue policy."
2507.09852,"In unmanned aerial vehicle (UAV) networks, communication protocols and algorithms are essential for cooperation and collaboration between UAVs. Simulation provides a cost-effective solution for prototyping, debugging, and analyzing protocols and algorithms, avoiding the prohibitive expenses of field experiments. In this paper, we present ``UavNetSim-v1'', an open-source Python-based simulation platform designed for rapid development, testing, and evaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1'' provides most of the functionalities developers may need, including routing/medium access control (MAC) protocols, topology control algorithms and mobility/energy models, while maintaining ease of use. Furthermore, the platform supports comprehensive performance evaluation and features an interactive visualization interface for in-depth algorithm analysis. In short, ``UavNetSim-v1'' lends itself to both rapid prototyping and educational purposes, and can serve as a lightweight yet powerful alternative to mature network simulators for UAV communication research."
2507.09942,"This letter investigates the optimal allocation of large language model (LLM) inference workloads across heterogeneous edge data centers (DCs) over time. Each DC features on-site renewable generation and faces dynamic electricity prices and spatiotemporal variability in renewable availability. The central question is: how can inference workloads be optimally distributed to the DCs to minimize energy consumption, carbon emissions, and water usage while enhancing user experience? This letter proposes a novel optimization model for LLM service providers to reduce operational costs and environmental impacts. Numerical results validate the efficacy of the proposed approach."
2507.1021,"Proper coordination is needed to guarantee the performance of wireless networks in dense deployments. Contention-based systems suffer badly in terms of latency when multiple devices compete for the same resources. Coordinated Orthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi 8 to remedy this, as it enables multiple Access Points (APs) to share spectrum more efficiently. However, fine-grained resource allocation, namely within 20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling overhead and complexity in terms of physical layer signaling. A wired backhaul mitigates the need for over-the-air scheduling and synchronization, and it allows for coordination even if APs are not in each others' range. Furthermore, it forms the basis for more advanced multi-AP coordination schemes like coordinated beamforming and joint transmission. In this work we demonstrate the realization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul, enabled by the open-source platforms openwifi and White Rabbit. We show that the performance in terms of carrier frequency offset pre-compensation and time synchronization between two APs exceeds related wireless standard requirements. Furthermore, the quality of the received constellation of the Co-OFDMA frame as reported by a wireless connectivity tester is better than individual frames sent by the APs."
2507.1051,"AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from ""humans watching video"" to ""AI understanding video"". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat."
2507.10903,"Effective management of Service Function Chains (SFCs) and optimal Virtual Network Function (VNF) placement are critical challenges in modern Software-Defined Networking (SDN) and Network Function Virtualization (NFV) environments. Although Deep Reinforcement Learning (DRL) is widely adopted for dynamic network decision-making, its inherent dependency on structured data and fixed action rules often limits adaptability and responsiveness, particularly under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a novel approach combining Lightweight Language Model (LiLM) with Relational Database (RDB) to answer network state queries to guide DRL model for efficient SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5 (FLAN-T5), to interpret network data and support diverse query types related to SFC demands, data center resources, and VNF availability. Results demonstrate that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to 0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time (2h 2min compared to 2h 38min). Moreover, when compared to the large language model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min)."
2507.10928,"Global Accelerator (GA) services play a vital role in ensuring low-latency, high-reliability communication for real-time interactive applications. However, existing GA offerings are tightly bound to specific cloud providers, resulting in high costs, rigid deployment, and limited flexibility, especially for large-scale or budget-sensitive deployments. Arcturus is a cloud-native GA framework that revisits the design of GA systems by leveraging low-cost, heterogeneous cloud resources across multiple providers. Rather than relying on fixed, high-end infrastructure, Arcturus dynamically constructs its acceleration network and balances performance, stability, and resource efficiency. To achieve this, Arcturus introduces a two-plane design: a forwarding plane that builds a proxy network with adaptive control, and a scheduling plane that coordinates load and routing through lightweight, quantitative optimization. Evaluations under millions of RPS show that Arcturus outperforms commercial GA services by up to 1.7X in acceleration performance, reduces cost by 71%, and maintains over 80% resource efficiency--demonstrating efficient use of cloud resources at scale."
2507.11014,"Large language models (LLMs) have demonstrated remarkable capabilities in code generation across various domains. However, their effectiveness in generating simulation scripts for domain-specific environments like ns-3 remains underexplored. Despite the growing interest in automating network simulations, existing tools primarily focus on interactive automation over rigorous evaluation. To facilitate systematic evaluation, we introduce SIMCODE, the first benchmark to evaluate LLMs' ability to generate ns-3 simulation code from natural language. SIMCODE includes 400 tasks across introductory, intermediate, and advanced levels, with solutions and test cases. Using SIMCODE, we evaluate three prominent LLMs, Gemini-2.0, GPT-4.1, and Qwen-3, across six prompt techniques. Furthermore, investigating task-specific fine-tuning's impact reveals that while GPT-4.1 outperforms others, execution accuracy remains modest, with substantial room for improvement. Error analysis identifies missing headers and API mismatches as dominant failures. Nevertheless, SIMCODE provides a foundational step toward evaluating LLMs and research in domain-aware generative systems."
2507.11038,"WiFi received signal strength (RSS) environment evolves over time due to movement of access points (APs), AP power adjustment, installation and removal of APs, etc. We study how to effectively update an existing database of fingerprints, defined as the RSS values of APs at designated locations, using a batch of newly collected unlabelled (possibly crowdsourced) WiFi signals. Prior art either estimates the locations of the new signals without updating the existing fingerprints or filters out the new APs without sufficiently embracing their features. To address that, we propose GUFU, a novel effective graph-based approach to update WiFi fingerprints using unlabelled signals with possibly new APs. Based on the observation that similar signal vectors likely imply physical proximity, GUFU employs a graph neural network (GNN) and a link prediction algorithm to retrain an incremental network given the new signals and APs. After the retraining, it then updates the signal vectors at the designated locations. Through extensive experiments in four large representative sites, GUFU is shown to achieve remarkably higher fingerprint adaptivity as compared with other state-of-the-art approaches, with error reduction of 21.4% and 29.8% in RSS values and location prediction, respectively."
2507.11168,"The increasing need for robustness, reliability, and determinism in wireless networks for industrial and mission-critical applications is the driver for the growth of new innovative methods. The study presented in this work makes use of machine learning techniques to predict channel quality in a Wi-Fi network in terms of the frame delivery ratio. Predictions can be used proactively to adjust communication parameters at runtime and optimize network operations for industrial applications. Methods including convolutional neural networks and long short-term memory were analyzed on datasets acquired from a real Wi-Fi setup across multiple channels. The models were compared in terms of prediction accuracy and computational complexity. Results show that the frame delivery ratio can be reliably predicted, and convolutional neural networks, although slightly less effective than other models, are more efficient in terms of CPU usage and memory consumption. This enhances the model's usability on embedded and industrial systems."
2507.1125,"Time-Sensitive Networking (TSN) is increasingly adopted in industrial systems to meet strict latency, jitter, and reliability requirements. However, evaluating TSN's fault tolerance under realistic failure conditions remains challenging. This paper presents IN2C, a modular OMNeT++/INET-based simulation framework that models two synchronized production cells connected to centralized infrastructure. IN2C integrates core TSN features, including time synchronization, traffic shaping, per-stream filtering, and Frame Replication and Elimination for Redundancy (FRER), alongside XML-driven fault injection for link and node failures. Four fault scenarios are evaluated to compare TSN performance with and without redundancy. Results show that FRER eliminates packet loss and achieves submillisecond recovery, though with 2-3x higher link utilization. These findings offer practical guidance for deploying TSN in bandwidth-constrained industrial environments."
2507.11483,"Wireless networks are vulnerable to jamming attacks due to the shared communication medium, which can severely degrade performance and disrupt services. Despite extensive research, current jamming detection methods often rely on simulated data or proprietary over-the-air datasets with limited cross-layer features, failing to accurately represent the real state of a network and thus limiting their effectiveness in real-world scenarios. To address these challenges, we introduce JamShield, a dynamic jamming detection system trained on our own collected over-the-air and publicly available dataset. It utilizes hybrid feature selection to prioritize relevant features for accurate and efficient detection. Additionally, it includes an auto-classification module that dynamically adjusts the classification algorithm in real-time based on current network conditions. Our experimental results demonstrate significant improvements in detection rate, precision, and recall, along with reduced false alarms and misdetections compared to state-of-the-art detection algorithms, making JamShield a robust and reliable solution for detecting jamming attacks in real-world wireless networks."
2507.11678,"Twelve years have passed since World IPv6 Launch Day, but what is the current state of IPv6 deployment? Prior work has examined IPv6 status as a binary: can a user do any IPv6? As deployment increases, we must consider a more nuanced, non-binary perspective on IPv6: how much and often can a user or a service use IPv6? We consider this question as a client, server, and cloud provider. Considering the client's perspective, we observe user traffic. We see that the fraction of IPv6 traffic a user sends varies greatly, both across users and day-by-day, with a standard deviation of over 15%. We show this variation occurs for two main reasons. First, IPv6 traffic is primarily human-generated, thus showing diurnal patterns. Second, some services lead with full IPv6 adoption, while others lag with partial or no support, so as users do different things their fraction of IPv6 varies. We look at server-side IPv6 adoption in two ways. First, we expand analysis of web services to examine how many are only partially IPv6 enabled due to their reliance on IPv4-only resources. Our findings reveal that only 12.6% of top 100k websites qualify as fully IPv6-ready. Finally, we examine cloud support for IPv6. Although all clouds and CDNs support IPv6, we find that tenant deployment rates vary significantly across providers. We find that ease of enabling IPv6 in the cloud is correlated with tenant IPv6 adoption rates, and recommend best practices for cloud providers to improve IPv6 adoption. Our results suggest IPv6 deployment is growing, but many services lag, presenting a potential for improvement."
2507.11798,"We analyzed spatial complexity, defined as the relationship between the required bitrate and a corresponding picture Quality of Experience (QoE) metric, for realistic, long, real-time, interactive video clips. Apart from variation across different content types, e.g., game genres, we discovered time-variability within a clip from second to second, and explored the ramifications for traffic management. We introduced utility as an elegant way to manage resource sharing preferences. Our analysis of resource sharing methods shows that frequent QoE-aware reallocation has significant performance advantages compared to static rate allocation, even in case the latter is based on rich information about long-term average spatial complexity. We have also shown that utility-based resource allocation has clear advantages over methods targeting equal QoE allocation, it increases the average QoE, while it still controls the worst case QoE."
2507.11935,"As the path toward 6G networks is being charted, the emerging applications have motivated evolutions of network architectures to realize the efficient, reliable, and flexible wireless networks. Among the potential architectures, the non-terrestrial network (NTN) and open radio access network (ORAN) have received increasing interest from both academia and industry. Although the deployment of NTNs ensures coverage, enhances spectral efficiency, and improves the resilience of wireless networks. The high altitude and mobility of NTN present new challenges in the development and operations (DevOps) lifecycle, hindering intelligent and scalable network management due to the lack of native artificial intelligence (AI) capability. With the advantages of ORAN in disaggregation, openness, virtualization, and intelligence, several works propose integrating ORAN principles into the NTN, focusing mainly on ORAN deployment options based on transparent and regenerative systems. However, a holistic view of how to effectively combine ORAN and NTN throughout the DevOps lifecycle is still missing, especially regarding how intelligent ORAN addresses the scalability challenges in NTN. Motivated by this, in this paper, we first provide the background knowledge about ORAN and NTN, outline the state-of-the-art research on ORAN for NTNs, and present the DevOps challenges that motivate the adoption of ORAN solutions. We then propose the ORAN-based NTN framework, discussing its features and architectures in detail. These include the discussion about flexible fronthaul split, RAN intelligent controllers (RICs) enhancement for distributed learning, scalable deployment architecture, and multi-domain service management. Finally, the future research directions, including combinations of the ORAN-based NTN framework and other enabling technologies and schemes, as well as the candidate use cases, are highlighted."
2507.12265,"Ever since Clos topologies were used in datacenter networks (DCNs), a practical centralized scheduling algorithm that supports dynamic scheduling has been absent. The introduction of optical switches in DCNs as a future-proof solution exacerbates this problem due to several properties of optical switches, such as the fact that they are generally bufferless and therefore rely on centralized scheduling, and that they have long switching times and therefore require the number of rearrangements to be minimized.In this paper, we propose a centralized scheduling algorithm that achieves theoretical maximum throughput even in one-rate bidirectional Clos networks, while producing schemes with near-minimal numbers of rearrangements. It is the only algorithm that directly supports bidirectional Clos networks and has a time efficiency high enough to support dynamic scheduling to date. For static minimal rewiring, its running time ranges from a fraction to a few hundredths of other algorithms, and the number of rearrangements has also been steadily improved, allowing for more frequent adjustments and less impact on ongoing communications. In addition, the algorithm is very flexible and can support various functional requirements in real-world environments. We achieve this result through the replacement chain concept and bitset optimization."
2507.12443,"Beyond hallucinations, another problem in program synthesis using LLMs is ambiguity in user intent. We illustrate the ambiguity problem in a networking context for LLM-based incremental configuration synthesis of route-maps and ACLs. These structures frequently overlap in header space, making the relative priority of actions impossible for the LLM to infer without user interaction. Measurements in a large cloud identify complex ACLs with 100's of overlaps, showing ambiguity is a real problem. We propose a prototype system, Clarify, which uses an LLM augmented with a new module called a Disambiguator that helps elicit user intent. On a small synthetic workload, Clarify incrementally synthesizes routing policies after disambiguation and then verifies them. Our treatment of ambiguities is useful more generally when the intent of updates can be correctly synthesized by LLMs, but their integration is ambiguous and can lead to different global behaviors."
2507.12445,"Reducing latency in the Internet of Things (IoT) is a critical concern. While cloud computing facilitates communication, it falls short of meeting real-time requirements reliably. Edge and fog computing have emerged as viable solutions by positioning computing nodes closer to end users, offering lower latency and increased processing power. An edge-fog framework comprises various components, including edge and fog nodes, whose strategic placement is crucial as it directly impacts latency and system cost. This paper presents an effective and tunable node placement strategy based on a genetic algorithm to address the optimization problem of deploying edge and fog nodes. The main objective is to minimize latency and cost through optimal node placement. Simulation results demonstrate that the proposed framework achieves up to 2.77% latency and 31.15% cost reduction."
2507.1291,"The growing demand for low-latency computing in 6G is driving the use of UAV-based low-altitude mobile edge computing (MEC) systems. However, limited spectrum often leads to severe uplink interference among ground terminals (GTs). In this paper, we investigate a rate-splitting multiple access (RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists multiple GTs in concurrently offloading their tasks over a shared uplink. We formulate a joint optimization problem involving the UAV 3D trajectory, RSMA decoding order, task offloading decisions, and resource allocation, aiming to mitigate multi-user interference and maximize energy efficiency. Given the high dimensionality, non-convex nature, and dynamic characteristics of this optimization problem, we propose a generative AI-enhanced deep reinforcement learning (DRL) framework to solve it efficiently. Specifically, we embed a diffusion model into the actor network to generate high-quality action samples, improving exploration in hybrid action spaces and avoiding local optima. In addition, a priority-based RSMA decoding strategy is designed to facilitate efficient successive interference cancellation with low complexity. Simulation results demonstrate that the proposed method for low-altitude MEC systems outperforms baseline methods, and that integrating GDM with RSMA can achieve significantly improved energy efficiency performance."
2507.1314,"Sixth generation (6G) networks demand tight integration of artificial intelligence (AI) into radio access networks (RANs) to meet stringent quality of service (QoS) and resource efficiency requirements. Existing solutions struggle to bridge the gap between high level user intents and the low level, parameterized configurations required for optimal performance. To address this challenge, we propose RIDAS, a multi agent framework composed of representation driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open interface with tunable control parameters (rank and quantization bits, enabling explicit trade) offs between distortion and transmission rate. The IDA employs a two stage planning scheme (bandwidth pre allocation and reallocation) driven by a large language model (LLM) to map user intents and system state into optimal RDA configurations. Experiments demonstrate that RIDAS supports 36.47% more users than WirelessAgent under equivalent QoS constraints. These results validate ability of RIDAS to capture user intent and allocate resources more efficiently in AI RAN environments."
2507.13179,"As 6G networks are developed and defined, offloading of XR applications is emerging as one of the strong new use cases. The reduced 6G latency coupled with edge processing infrastructure will for the first time provide a realistic offloading scenario in cellular networks where several computationally intensive functions, including rendering, can migrate from the user device and into the network. A key advantage of doing so is the lowering of the battery needs in the user devices and the possibility to design new devices with smaller form factors. However, offloading introduces increased delays compared to local execution, primarily due to network transmission latency and queuing delays at edge servers, especially under multi-user concurrency. Despite the computational power of edge platforms, the resulting motion-to-photon (MTP) latency negatively impacts user experience. To mitigate this, motion prediction has been proposed to offset delays. Existing approaches build on either deep learning or Kalman filtering. Deep learning techniques face scalability limitations at the resource-constrained edge, as their computational expense intensifies with increasing user concurrency, while Kalman filtering suffers from poor handling of complex movements and fragility to packet loss inherent in 6G's high-frequency radio interfaces. In this work, we introduce a context-aware error-state Kalman filter (ESKF) prediction framework, which forecasts the user's head motion trajectory to compensate for MTP latency in remote XR. By integrating a motion classifier that categorizes head motions based on their predictability, our algorithm demonstrates reduced prediction error across different motion classes. Our findings demonstrate that the optimized ESKF not only surpasses traditional Kalman filters in positional and orientational accuracy but also exhibits enhanced robustness and resilience to packet loss."
2507.13312,"Virtual dynamic environments (VDEs) such as the Metaverse and digital twins (DTs) require proper representation of the interacting entities to map their characteristics within the simulated or augmented space. Keeping these representations accurate and up-to-date is crucial for seamless interaction and system reliability. In this paper, we propose bidirectional age of incorrect information (BAoII) to address this aspect. BAoII quantifies the time-dependent penalty paid by an entity in a VDE due to incorrect or outdated knowledge about itself and the overall dynamically changing space. This extends the concept of age of incorrect information for a bidirectional information exchange, capturing that a VDE requires mutual awareness of the entity's own representation, measured in the virtual space, and what the other entities share about their representations. Using a continuous-time Markov chain model, we derive a closed-form expression for long-term BAoII and identify a transmission cost threshold for optimal update strategies. We describe a trade-off between communication cost and information freshness and validate our model through numerical simulations, demonstrating the impact of BAoII on evaluating system performance and highlighting its relevance for real-time collaboration in the Metaverse and DTs."
2507.13476,"Last-mile access networks are often the dominant bottlenecks for Internet applications, creating demand for data-generation approaches that are both realistic and reusable. Meeting this goal requires five properties: fidelity (capturing real network behaviors), controllability (systematic variation of network conditions), diversity (coverage of heterogeneous network behaviors), composability (construction of complex scenarios from simpler elements), and replicability (consistent outcomes across runs). Existing approaches satisfy only a subset of these requirements. This paper introduces NETREPLICA, a programmable substrate for last-mile data generation that achieves all five. NETREPLICA decomposes bottlenecks into static attributes (capacity, base latency, buffer size, shaping and active queue management policies) and dynamic attributes derived from passive traces. It introduces Cross-Traffic Profiles (CTPs) that transform passive production traces into reusable, parameterizable building blocks. By trimming, scaling, and recombining CTPs, NETREPLICA generates realistic yet tunable conditions, replaying non-reactive cross traffic alongside reactive application workloads and enabling reproducible construction of heterogeneous scenarios. In a case study on adaptive bitrate streaming, models trained with NETREPLICA-generated traces reduced transmission-time prediction error by up to 47% in challenging slow-path domains (>=400 ms RTT, <=6 Mbps throughput) compared to models trained solely on production traces -- demonstrating the utility of NETREPLICA-generated data. Overall, NETREPLICA represents a first step toward a fully programmable data-generation substrate for networking."
2507.13676,"This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to provide Integrated Sensing and Communication (ISAC) services. The performance of both communication and sensing fundamentally depends on the availability of accurate and up-to-date channel state information (CSI). In modern 5G networks, uplink CSI is derived from two reference signals: the demodulation reference signal (DMRS) and the sounding reference signal (SRS). However, current base station implementations treat these CSI measurements as separate information streams. The key innovation of CARTS is to fuse these two CSI streams, thereby increasing the frequency of CSI updates and extending sensing opportunities to more users. CARTS addresses two key challenges: (i) a novel channel stitching and compensation method that integrates asynchronous CSI estimates from DMRS and SRS, despite their different time and frequency allocations, and (ii) a real-time SRS triggering algorithm that complements the inherently uncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing opportunities for all users. Our trace-driven evaluation shows that CARTS significantly improves scalability, achieving a channel estimation error (NMSE) of 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of users as a periodic SRS-only baseline with similar performance. By opportunistically combining DMRS and SRS, CARTS therefore provides a practical, standard-compliant solution to improve CSI availability for ISAC without requiring additional radio resources."
2507.13717,"Reconfigurable data center networks (DCNs) enhance traditional architectures with optical circuit switches (OCSs), enabling dynamic reconfiguration of inter-pod links, i.e., the logical topology. Optimizing this topology is crucial for adapting to traffic dynamics but is challenging due to its combinatorial nature. The complexity increases further when demands can be distributed across multiple paths, requiring joint optimization of topology and routing. We propose Alternating Topology and Routing Optimization (ATRO), a unified framework that supports both one-hop topology optimization (where traffic is routed via direct paths) and multi-hop joint optimization (where routing is also optimized). Although these settings differ in constraints, both are combinatorially hard and challenge solver-based methods. ATRO addresses both cases efficiently: in the one-hop case, it guarantees the global optimum via an accelerated binary search; in the multi-hop case, it alternates between topology and routing updates, with routing steps optionally accelerated by existing traffic engineering (TE) methods. ATRO supports warm-starting and improves solution quality monotonically across iterations. ATRO remains competitive even when paired with solver-free TE methods, forming a fully solver-free optimization pipeline that still outperforms prior approaches in runtime and maximum link utilization across diverse workloads."
2507.13889,"This paper investigates the integration of active reconfigurable intelligent surfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance non-terrestrial network (NTN) performance in next-generation wireless systems. While prior studies focused on passive RIS architectures, the severe path loss and double fading in long-distance HAPS links make active RIS a more suitable alternative due to its inherent signal amplification capabilities. We formulate a sum-rate maximization problem to jointly optimize power allocation and RIS element assignment for ground user equipments (UEs) supported by a HAPS-based active RIS-assisted communication system. To reduce power consumption and hardware complexity, several sub-connected active RIS architectures are also explored. Simulation results reveal that active RIS configurations significantly outperform passive RIS in terms of quality of service (QoS). Moreover, although fully-connected architectures achieve the highest throughput, sub-connected schemes demonstrate superior energy efficiency under practical power constraints. These findings highlight the potential of active RIS-enabled HAPS systems to meet the growing demands of beyond-cellular coverage and green networking."
2507.13933,"Increasingly, web content is automatically generated by large language models (LLMs) with little human input. We call this ""LLM-dominant"" content. Since LLMs plagiarize and hallucinate, LLM-dominant content can be unreliable and unethical. Yet, websites rarely disclose such content, and human readers struggle to distinguish it. Thus, we must develop reliable detectors for LLM-dominant content. However, state-of-the-art LLM detectors are inaccurate on web content, because web content has low positive rates, complex markup, and diverse genres, instead of clean, prose-like benchmark data SoTA detectors are optimized for.We propose a highly reliable, scalable pipeline that classifies entire websites. Instead of naively classifying text extracted from each page, we classify each site based on an LLM text detector's outputs of multiple prose-like pages to boost accuracies. We train and evaluate our detector by collecting 2 distinct ground truth datasets totaling 120 sites, and obtain 100% accuracies testing across them. In the wild, we detect a sizable portion of sites as LLM-dominant among 10k sites in search engine results and 10k in Common Crawl archives. We find LLM-dominant sites are growing in prevalence and rank highly in search results, raising questions about their impact on end users and the overall Web ecosystem."
2507.14183,"In mid-2025, Iran experienced a novel, stealthy Internet shutdown that preserved global routing presence while isolating domestic users through deep packet inspection, aggressive throttling, and selective protocol blocking. This paper analyzes active network measurements such as DNS poisoning, HTTP injection, TLS interception, and protocol whitelisting, traced to a centralized border gateway. We quantify an approximate 707 percent rise in VPN demand and describe the multi-layered censorship infrastructure, highlighting implications for circumvention and digital rights monitoring."
2507.14186,"The expansion of the low-altitude economy has underscored the significance of Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors. While accurate LANC forecasting hinges on the antenna beam patterns of Base Stations (BSs), these patterns are typically proprietary and not readily accessible. Operational parameters of BSs, which inherently contain beam information, offer an opportunity for data-driven low-altitude coverage prediction. However, collecting extensive low-altitude road test data is cost-prohibitive, often yielding only sparse samples per BS. This scarcity results in two primary challenges: imbalanced feature sampling due to limited variability in high-dimensional operational parameters against the backdrop of substantial changes in low-dimensional sampling locations, and diminished generalizability stemming from insufficient data samples. To overcome these obstacles, we introduce a dual strategy comprising expert knowledge-based feature compression and disentangled representation learning. The former reduces feature space complexity by leveraging communications expertise, while the latter enhances model generalizability through the integration of propagation models and distinct subnetworks that capture and aggregate the semantic representations of latent features. Experimental evaluation confirms the efficacy of our framework, yielding a 7% reduction in error compared to the best baseline algorithm. Real-network validations further attest to its reliability, achieving practical prediction accuracy with MAE errors at the 5dB level."
2507.14188,"In 2023, satellite and mobile networks crossed a historic threshold: standard smartphones, using unmodified 3GPP protocols, connected directly to low Earth orbit (LEO) satellites. This first wave of direct-to-device (D2D) demonstrations validated the physical feasibility of satellite-based mobile access. However, these systems remain fallback-grade--rural-only, bandwidth-limited, and fully dependent on Earth-based mobile cores for identity, session, and policy control. This paper asks a more ambitious question: Can a complete mobile network, including radio access, core functions, traffic routing, and content delivery, operate entirely from orbit? And can it deliver sustained, urban-grade service in the world's densest cities? We present the first end-to-end system architecture for a fully orbital telco, integrating electronically steered phased arrays with 1000-beam capacity, space-based deployment of 5G core functions (UPF, AMF), and inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam capacity, and link budgets under dense urban conditions, accounting for path loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight users can sustain 64-QAM throughput, while street-level access is feasible with relay or assisted beam modes. The paper outlines the remaining constraints, power, thermal dissipation, compute radiation hardening, and regulatory models, and demonstrates that these are engineering bottlenecks, not physical limits. Finally, we propose a staged 15-year roadmap from today's fallback D2D systems to autonomous orbital overlays delivering 50-100 Mbps to handhelds in megacities, with zero reliance on terrestrial infrastructure."
2507.14199,"Semantic communication represents a promising technique towards reducing communication costs, especially when dealing with image segmentation, but it still lacks a balance between computational efficiency and bandwidth requirements while maintaining high image segmentation accuracy, particularly in resource-limited environments and changing channel conditions. On the other hand, the more complex and larger semantic image segmentation models become, the more stressed the devices are when processing data. This paper proposes a novel approach to implementing semantic communication based on splitting the semantic image segmentation process between a resource constrained transmitter and the receiver. This allows saving bandwidth by reducing the transmitted data while maintaining the accuracy of the semantic image segmentation. Additionally, it reduces the computational requirements at the resource constrained transmitter compared to doing all the semantic image segmentation in the transmitter. The proposed approach is evaluated by means of simulation-based experiments in terms of different metrics such as computational resource usage, required bit rate and segmentation accuracy. The results when comparing the proposal with the full semantic image segmentation in the transmitter show that up to 72% of the bit rate was reduced in the transmission process. In addition, the computational load of the transmitter is reduced by more than 19%. This reflects the interest of this technique for its application in communication systems, particularly in the upcoming 6G systems."
2507.14205,"We propose an integrated architecture combining Software-Defined Wireless Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based hybrid cloud streaming to improve wireless network performance in both urban and rural settings. The approach addresses urban congestion and rural digital exclusion through traffic offloading, enhanced fault tolerance, and equitable resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3 \cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok, Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth offloading of 40%, rural coverage gain of 28%, and fairness index rising from 0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and device mandates to promote adoption. This scalable, fault-tolerant design supports equitable digital transformation and suggests directions for future research."
2507.14209,"As a leading research institute in software-intensive systems, fortiss is actively shaping the vision of Sixth Generation Mobile Communication (6G). Our mission is to ensure that 6G technologies go beyond technical advancements and are aligned with societal needs. fortiss plays a key role in 6G initiatives worldwide, including contributions to standardization bodies and collaborative Research and Development programs. We focus on software-defined, AI-enabled, and sustainable communication services that prioritize human values and long-term impact. 6G will redefine digital connectivity through cognitive intelligence, decentralized orchestration, and sustainability-oriented architectures. As expectations rise for ultra-reliable low-latency communication (URLLC) and personalized digital services, 6G must outperform prior generations. It will rely on AI-native networking, Edge-Cloud resource orchestration, and energy-aware data frameworks, ensuring both technical performance and societal relevance. This white paper presents the fortiss vision for a human-centric, sustainable, and AI-integrated 6G network. It outlines key research domains such as semantic communication, green orchestration, and distributed AI, all linked to societal and technological challenges. The white paper is aimed at researchers, industry experts, policymakers, and developers. It articulates the strategic direction and contributions of fortiss to 6G, emphasizing responsible innovation and interdisciplinary collaboration toward a meaningful 2030 vision."
2507.14211,"Predictive Quality of Service (PQoS) makes it possible to anticipate QoS changes, e.g., in wireless networks, and trigger appropriate countermeasures to avoid performance degradation. Hence, PQoS is extremely useful for automotive applications such as teleoperated driving, which poses strict constraints in terms of latency and reliability. A promising tool for PQoS is given by Reinforcement Learning (RL), a methodology that enables the design of decision-making strategies for stochastic optimization. In this manuscript, we present PRATA, a new simulation framework to enable PRedictive QoS based on AI for Teleoperated driving Applications. PRATA consists of a modular pipeline that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access Network (RAN), (ii) a tool for generating automotive data, and (iii) an Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the segmentation level of teleoperated driving data in the event of resource saturation or channel degradation. Hence, we show that the RAN-AI entity efficiently balances the trade-off between QoS and Quality of Experience (QoE) that characterize teleoperated driving applications, almost doubling the system performance compared to baseline approaches. In addition, by varying the learning settings of the RAN-AI entity, we investigate the impact of the state space and the relative cost of acquiring network data that are necessary for the implementation of RL."
2507.1423,"Advanced intelligent automation becomes an important feature to deal with the increased complexity in managing wireless networks. This paper proposes a novel automation approach of intent-based network for Radio Access Networks (RANs) management by leveraging Large Language Models (LLMs). The proposed method enhances intent translation, autonomously interpreting high-level objectives, reasoning over complex network states, and generating precise configurations of the RAN by integrating LLMs within an agentic architecture. We propose a structured prompt engineering technique and demonstrate that the network can automatically improve its energy efficiency by dynamically optimizing critical RAN parameters through a closed-loop mechanism. It showcases the potential to enable robust resource management in RAN by adapting strategies based on real-time feedback via LLM-orchestrated agentic systems."
2507.14234,"Long-term wildlife tracking is crucial for biodiversity monitoring, but energy limitations pose challenges, especially for animal tags, where replacing batteries is impractical and stressful for the animal due to the need to locate, possibly sedate, and handle it. Energy harvesting offers a sustainable alternative, yet most existing systems rely on a single energy source and infrastructure-limited communication technologies. This paper presents an energy-neutral system that combines solar and kinetic energy harvesting to enable the tracking and monitoring of wild animals. Harvesting from multiple sources increases the total available energy. Uniquely, the kinetic harvester also serves as a motion proxy by sampling harvested current, enabling activity monitoring without dedicated sensors. Our approach also ensures compatibility with existing cellular infrastructure, using Narrowband Internet of Things (NB-IoT). We present a simulation framework that models energy harvesting, storage, and consumption at the component level. An energy-aware scheduler coordinates task execution based on real-time energy availability. We evaluate performance under realistically varying conditions, comparing task frequencies and capacitor sizes. Results show that our approach maintains energy-neutral operation while significantly increasing data yield and reliability compared to single-source systems, with the ability to consistently sample GPS location data and kinetic harvesting data every two minutes while transmitting these results over NB-IoT every hour. These findings demonstrate the potential for maintenance-free, environmentally friendly tracking in remote habitats, enabling more effective and scalable wildlife monitoring."
2507.14263,"The Internet is poised to host billions to trillions of autonomous AI agents that negotiate, delegate, and migrate in milliseconds and workloads that will strain DNS-centred identity and discovery. In this paper, we describe the NANDA index architecture, which we envision as a means for discoverability, identifiability and authentication in the internet of AI agents. We present an architecture where a minimal lean index resolves to dynamic, cryptographically verifiable AgentFacts that supports multi-endpoint routing, load balancing, privacy-preserving access, and credentialed capability assertions. Our architecture design delivers five concrete guarantees: (1) A quilt-like index proposal that supports both NANDA-native agents as well as third party agents being discoverable via the index, (2) rapid global resolution for newly spawned AI agents, (3) sub-second revocation and key rotation, (4) schema-validated capability assertions, and (5) privacy-preserving discovery across organisational boundaries via verifiable, least-disclosure queries. We formalize the AgentFacts schema, specify a CRDT-based update protocol, and prototype adaptive resolvers. The result is a lightweight, horizontally scalable foundation that unlocks secure, trust-aware collaboration for the next generation of the Internet of AI agents, without abandoning existing web infrastructure."
2507.14398,"Intent-Based Networking (IBN) often leverages the programmability of Software-Defined Networking (SDN) to simplify network management. However, significant challenges remain in automating the entire pipeline, from user-specified high-level intents to device-specific low-level configurations. Existing solutions often rely on rigid, rule-based translators and fixed APIs, limiting extensibility and adaptability. By contrast, recent advances in large language models (LLMs) offer a promising pathway that leverages natural language understanding and flexible reasoning. However, it is unclear to what extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a first-of-its-kind benchmarking suite comprising four novel datasets: Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS. These datasets are specifically designed for evaluating LLMs performance in intent translation and conflict detection tasks within the industry-grade SDN controllers ODL and ONOS. Our results provide the first comprehensive comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a wide range of performance outcomes. However, while these results demonstrate the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully autonomous IBN pipeline remains unexplored. Thus, our second contribution is NetIntent, a unified and adaptable framework that leverages LLMs to automate the full IBN lifecycle, including translation, activation, and assurance within SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting dynamic re-prompting and contextual feedback to robustly execute user-defined intents with minimal human intervention. Our implementation of NetIntent across both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end IBN realization."
2507.14512,"The rapid proliferation of satellite constellations in Space-Air-Ground Integrated Networks (SAGIN) presents significant challenges for network management. Conventional flat network architectures struggle with synchronization and data transmission across massive distributed nodes. In response, hierarchical domain-based satellite network architectures have emerged as a scalable solution, highlighting the critical importance of controller provisioning strategies. However, existing network management architectures and traditional search-based algorithms fail to generate efficient controller provisioning solutions due to limited computational resources in satellites and strict time constraints. To address these challenges, we propose a three-layer domain-based architecture that enhances both scalability and adaptability. Furthermore, we introduce Dora, a reinforcement learning-based controller provisioning strategy designed to optimize network performance while minimizing computational overhead. Our comprehensive experimental evaluation demonstrates that Dora significantly outperforms state-of-the-art benchmarks, achieving 10% improvement in controller provisioning quality while requiring only 1/30 to 1/90 of the computation time compared to traditional algorithms. These results underscore the potential of reinforcement learning approaches for efficient satellite network management in next-generation SAGIN deployments."
2507.14627,"Wireless-powered underground communication networks (WPUCNs), which allow underground devices (UDs) to harvest energy from wireless signals for battery-free communication, offer a promising solution for sustainable underground monitoring. However, the severe wireless signal attenuation in challenging underground environments and the costly acquisition of channel state information (CSI) make large-scale WPUCNs economically infeasible in practice. To address this challenge, we introduce flexible unmanned aerial vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this system, a UAV is first charged by a terrestrial hybrid access point (HAP), then flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV collects data from the UDs and finally returns to the HAP for data offloading. Based on the proposed UAV-enabled WPUCN system, we first propose its energy consumption model and a hybrid wireless energy transfer (WET) approach (i.e., UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and CSI-free multi-antenna beamforming. Then, we formulate and address a time allocation problem to minimize the energy consumption of UAV, while ensuring that the throughput requirements of all UDs are met and all sensor data is offloaded. Through simulations of a realistic farming scenario, we demonstrate that the proposed hybrid WET approach outperforms other WET approaches, with performance gains influenced by the number of antennas, communication distance, number of UDs, and underground conditions. Additionally, under the optimized time allocation, we found that the proposed hybrid WET approach based on a CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption among all WET mechanisms, thereby enabling sustainable underground monitoring in WPUCNs."
2507.14633,"The development of satellite-augmented low-altitude economy and terrestrial networks (SLAETNs) demands intelligent and autonomous systems that can operate reliably across heterogeneous, dynamic, and mission-critical environments. To address these challenges, this survey focuses on enabling agentic artificial intelligence (AI), that is, artificial agents capable of perceiving, reasoning, and acting, through generative AI (GAI) and large language models (LLMs). We begin by introducing the architecture and characteristics of SLAETNs, and analyzing the challenges that arise in integrating satellite, aerial, and terrestrial components. Then, we present a model-driven foundation by systematically reviewing five major categories of generative models: variational autoencoders (VAEs), generative adversarial networks (GANs), generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs. Moreover, we provide a comparative analysis to highlight their generative mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on this foundation, we examine how these models empower agentic functions across three domains: communication enhancement, security and privacy protection, and intelligent satellite tasks. Finally, we outline key future directions for building scalable, adaptive, and trustworthy generative agents in SLAETNs. This survey aims to provide a unified understanding and actionable reference for advancing agentic AI in next-generation integrated networks."
2507.14842,"Poor security of Internet routing enables adversaries to divert user data through unintended infrastructures (hijack). Of particular concern -- and the focus of this paper -- are cases where attackers reroute domestic traffic through foreign countries, exposing it to surveillance, bypassing legal privacy protections, and posing national security threats. Efforts to detect and mitigate such attacks have focused primarily on the control plane while data-plane signals remain largely overlooked. In particular, change in propagation delay caused by rerouting offers a promising signal: the change is unavoidable and the increased propagation delay is directly observable from the affected networks. In this paper, we explore the practicality of using delay variations for hijack detection, addressing two key questions: (1) What coverage can this provide, given its heavy dependence on the geolocations of the sender, receiver, and adversary? and (2) Can an always-on latency-based detection system be deployed without disrupting normal network operations? We observe that for 86% of victim-attacker country pairs in the world, mid-attack delays exceed pre-attack delays by at least 25% in real deployments, making delay-based hijack detection promising. To demonstrate practicality, we design HiDe, which reliably detects delay surges from long-distance hijacks at line rate. We measure HiDe's accuracy and false-positive rate on real-world data and validate it with ethically conducted hijacks."
2507.14876,"Indoor mobile networks handle the majority of data traffic, with their performance limited by building materials and structures. However, building designs have historically not prioritized wireless performance. Prior to the advent of reconfigurable intelligent surfaces (RIS), the industry passively adapted to wireless propagation challenges within buildings. Inspired by RIS's successes in outdoor networks, we propose embedding RIS into building structures to manipulate and enhance building wireless performance comprehensively. Nonetheless, the ubiquitous mobility of users introduces complex dynamics to the channels of RIS-covered buildings. A deep understanding of indoor human behavior patterns is essential for achieving wireless-friendly building design. This article is the first to systematically examine the tidal evolution phenomena emerging in the channels of RIS-covered buildings driven by complex human behaviors. We demonstrate that a universal channel model is unattainable and focus on analyzing the challenges faced by advanced deep learning-based prediction and control strategies, including high-order Markov dependencies, concept drift, and generalization issues caused by human-induced disturbances. Possible solutions for orchestrating the coexistence of RIS-covered buildings and crowd mobility are also laid out."
2507.14891,"Machine learning (ML) is increasingly used in network data planes for advanced traffic analysis, but existing solutions (such as FlowLens, N3IC, BoS) still struggle to simultaneously achieve low latency, high throughput, and high accuracy. To address these challenges, we present FENIX, a hybrid in-network ML system that performs feature extraction on programmable switch ASICs and deep neural network inference on FPGAs. FENIX introduces a Data Engine that leverages a probabilistic token bucket algorithm to control the sending rate of feature streams, effectively addressing the throughput gap between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model Engine to enable high-accuracy deep neural network inference in the network, overcoming the difficulty of deploying complex models on resource-constrained switch chips. We implement FENIX on a programmable switch platform that integrates a Tofino ASIC and a ZU19EG FPGA directly, and evaluate it on real-world network traffic datasets. Our results show that FENIX achieves microsecond-level inference latency and multi-terabit throughput with low hardware overhead, and delivers over 90% accuracy on mainstream network traffic classification tasks, outperforming the state of the art."
2507.15145,"This paper proposes a communication-efficient, event-triggered inference framework for cooperative edge AI systems comprising multiple user devices and edge servers. Building upon dual-threshold early-exit strategies for rare-event detection, the proposed approach extends classical single-device inference to a distributed, multi-device setting while incorporating proportional fairness constraints across users. A joint optimization framework is formulated to maximize classification utility under communication, energy, and fairness constraints. To solve the resulting problem efficiently, we exploit the monotonicity of the utility function with respect to the confidence thresholds and apply alternating optimization with Benders decomposition. Experimental results show that the proposed framework significantly enhances system-wide performance and fairness in resource allocation compared to single-device baselines."
2507.15254,"The evolution towards future generation of mobile systems and fixed wireless networks is primarily driven by the urgency to support high-bandwidth and low-latency services across various vertical sectors. This endeavor is fueled by smartphones as well as technologies like industrial internet of things, extended reality (XR), and human-to-machine (H2M) collaborations for fostering industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To ensure an ideal immersive experience and avoid cyber-sickness for users in all the aforementioned usage scenarios, it is typically challenging to synchronize XR content from a remote machine to a human collaborator according to their head movements across a large geographic span in real-time over communication networks. Thus, we propose a novel H2M collaboration scheme where the human's head movements are predicted ahead with highly accurate models like bidirectional long short-term memory networks to orient the machine's camera in advance. We validate that XR frame size varies in accordance with the human's head movements and predict the corresponding bandwidth requirements from the machine's camera to propose a human-machine coordinated dynamic bandwidth allocation (HMC-DBA) scheme. Through extensive simulations, we show that end-to-end latency and jitter requirements of XR frames are satisfied with much lower bandwidth consumption over enterprise networks like Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in network resource utilization is achieved by employing our proposed HMC-DBA over state-of-the-art schemes."
2507.15338,"This paper investigates how to achieve both low-power operations of sensor nodes and accurate state estimation using Kalman filter for internet of things (IoT) monitoring employing wireless sensor networks under radio resource constraint. We consider two policies used by the base station to collect observations from the sensor nodes: (i) an oblivious policy, based on statistics of the observations, and (ii) a decentralized policy, based on autonomous decision of each sensor based on its instantaneous observation. This work introduces a wake-up receiver and wake-up signaling to both policies to improve the energy efficiency of the sensor nodes. The decentralized policy designed with random access prioritizes transmissions of instantaneous observations that are highly likely to contribute to the improvement of state estimation. Our numerical results show that the decentralized policy improves the accuracy of the estimation in comparison to the oblivious policy under the constraint on the radio resource and consumed energy when the correlation between the processes observed by the sensor nodes is low. We also clarify the degree of correlation in which the superiority of two policies changes."
2507.15382,"Modern traffic generators are essential tools for evaluating the performance of network environments. P4TG is a P4-based traffic generator implemented for Intel Tofino switches that offers high-speed packet generation with fine-grained measurement capabilities. However, P4TG samples time-based metrics such as the round-trip time (RTT) in the data plane and collects them at the controller. This leads to a reduced accuracy. In this paper, we introduce a histogram-based RTT measurement feature for P4TG. It enables accurate analysis at line rate without sampling. Generally, histogram bins are modeled as ranges, and values are matched to a bin. Efficient packet matching in hardware is typically achieved using ternary content addressable memory (TCAM). However, representing range matching rules in TCAM poses a challenge. Therefore, we implemented a range-to-prefix conversion algorithm that models range matching with multiple ternary entries. This paper describes the data plane implementation and runtime configuration of RTT histograms in P4TG. Further, we discuss the efficiency of the ternary decomposition. Our evaluation demonstrates the applicability of the histogram-based RTT analysis by comparing the measured values with a configured theoretical distribution of RTTs."
2507.15391,"The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a generalized encoding for manifold extensions such as network slicing and in-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries (LSEs) and are added to the MPLS stack. Routers have a physical limit on the number of LSEs they can read, called the readable label depth (RLD). With MNA, routers must be able to process a minimum number of LSEs which requires a relatively large RLD. In this paper, we perform a hardware analysis of an MNA implementation and identify the reason for a large RLD requirement in the MNA protocol design. Based on this, we present a mechanism that reduces the required RLD for MNA nodes by restructuring the MPLS stack during forwarding. We then introduce the novel stack management network action that enables the proposed mechanism as well as its integration in networks with MNA-incapable nodes. The feasibility of the mechanism on programmable hardware is verified by providing a P4-based implementation. Further, the effects on the required RLD, ECMP, and packet overhead are discussed."
2507.15423,"In the evolution towards 6G user-centric networking, the moving network (MN) paradigm can play an important role. In a MN, some small cell base stations (BS) are installed on top of vehicles, and enable a more dynamic, flexible and sustainable, network operation. By ""following"" the users movements and adapting dynamically to their requests, the MN paradigm enables a more efficient utilization of network resources, mitigating the need for dense small cell BS deployments at the cost of an increase in resource utilization due to wireless backhauling. This aspect is at least partly compensated by the shorter distance between users and BS, which allows for lower power and Line-of-Sight communications. While the MN paradigm has been investigated for some time, to date, it is still unclear in which conditions the advantages of MN outweigh the additional resource costs. In this paper, we propose a stochastic geometry framework for the characterization of the potential benefits of the MN paradigm as part of an HetNet in urban settings. Our approach allows the estimation of user-perceived performance, accounting for wireless backhaul connectivity as well as base station resource scheduling. We formulate an optimization problem for determining the resource-optimal network configurations and BS scheduling which minimize the overall amount of deployed BSs in a QoS-aware manner, and the minimum vehicular flow between different urban districts required to support them, and we propose an efficient stochastic heuristic to solve it. Our numerical assessment suggests that the MN paradigm, coupled with appropriate dynamic network management strategies, significantly reduces the amount of deployed network infrastructure while guaranteeing the target QoS perceived by users."
2507.15659,"This paper presents a cost-effective and distributed flow monitoring platform for collecting unsampled IPFIX data exclusively using open-source tools, which is implemented at the University of Tbingen. An overview of all tools is given and their use is explained."
2507.1567,"Edge Computing (EC) is a computational paradigm that involves deploying resources such as CPUs and GPUs near end-users, enabling low-latency applications like augmented reality and real-time gaming. However, deploying and maintaining a vast network of EC nodes is costly, which can explain its limited deployment today. A new paradigm called Vehicular Cloud Computing (VCC) has emerged and inspired interest among researchers and industry. VCC opportunistically utilizes existing and idle vehicular computational resources for external task offloading. This work is the first to systematically address the following question: Can VCC replace EC for low-latency applications? Answering this question is highly relevant for Network Operators (NOs), as VCC could eliminate costs associated with EC given that it requires no infrastructural investment. Despite its potential, no systematic study has yet explored the conditions under which VCC can effectively support low-latency applications without relying on EC. This work aims to fill that gap. Extensive simulations allow for assessing the crucial scenario factors that determine when this EC-to-VCC substitution is feasible. Considered factors are load, vehicles mobility and density, and availability. Potential for substitution is assessed based on multiple criteria, such as latency, task completion success, and cost. Vehicle mobility is simulated in SUMO, and communication in NS3 5G-LENA. The findings show that VCC can effectively replace EC for low-latency applications, except in extreme cases when the EC is still required (latency < 16 ms)."
2507.16438,"Recently we have witnessed the explosion of proposals that, inspired by Language Models like BERT, exploit Representation Learning models to create traffic representations. All of them promise astonishing performance in encrypted traffic classification (up to 98% accuracy). In this paper, with a networking expert mindset, we critically reassess their performance. Through extensive analysis, we demonstrate that the reported successes are heavily influenced by data preparation problems, which allow these models to find easy shortcuts - spurious correlation between features and labels - during fine-tuning that unrealistically boost their performance. When such shortcuts are not present - as in real scenarios - these models perform poorly. We also introduce Pcap-Encoder, an LM-based representation learning model that we specifically design to extract features from protocol headers. Pcap-Encoder appears to be the only model that provides an instrumental representation for traffic classification. Yet, its complexity questions its applicability in practical settings. Our findings reveal flaws in dataset preparation and model training, calling for a better and more conscious test design. We propose a correct evaluation methodology and stress the need for rigorous benchmarking."
2507.16594,"Running deep learning inference directly on ultra-low-power edge/IoT nodes has been limited by the tight memory and compute budgets of microcontrollers. Split learning (SL) addresses this limitation in which it executes part of the inference process on the sensor and off-loads the remainder to a companion device. In the context of constrained devices and the related impact of low-power, over-the-air transport protocols, the performance of split learning remains largely unexplored. TO the best of our knowledge, this paper presents the first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards, designed to benchmark the over-the-air performance of split learning TinyML in edge/IoT environments. We benchmark the performance of a MobileNetV2 image recognition model, which is quantized to 8-bit integers, partitioned, and delivered to the nodes via over-the-air updates. The intermediate activations are exchanged through different wireless communication methods: ESP-NOW, BLE, and traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on identical hardware. Measurements show that splitting the model after block_16_project_BN layer generates a 5.66 kB tensor that traverses the link in 3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s. ESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery life further but increases latency beyond 10s."
2507.17188,"This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and communication, we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model (LLM)-guided heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics policy generated by the LLM, enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time LLM calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds."
2507.17195,"Timely and efficient dissemination of server status is critical in compute-first networking systems, where user tasks arrive dynamically and computing resources are limited and stochastic. In such systems, the access point plays a key role in forwarding tasks to a server based on its latest received server status. However, modeling the task-success probability suffering the factors of stochastic arrivals, limited server capacity, and bidirectional link delays. Therefore, we introduce a unified analytical framework that abstracts the AP forwarding rule as a single probability and models all network and waiting delays via their Laplace transforms. This approach yields a closed form expression for the end to end task success probability, together with upper and lower bounds that capture Erlang loss blocking, information staleness, and random uplink/downlink delays. We validate our results through simulations across a wide range of parameters, showing that theoretical predictions and bounds consistently enclose observed success rates. Our framework requires only two interchangeable inputs (the forwarding probability and the delay transforms), making it readily adaptable to alternative forwarding policies and delay distributions. Experiments demonstrate that our bounds are able to achieve accuracy within 0.01 (upper bound) and 0.016 (lower bound) of the empirical task success probability."
2507.17403,"As space missions increase, there is a growing need to replace point-to-point communication with an efficient and reliable network-centric communication approach. Disruption/Delay Tolerant Networking (DTN) with the Bundle Protocol (BP) has been selected as an interoperable network protocol in the LunaNet Interoperability Specification. It is also considered for future Earth Observation and Mars communication scenarios. In a DTN, the ""bundle"" -- the fundamental data unit of BP -- requires dedicated mechanisms to ensure reliability due to the challenges posed by intermittent connectivity and long delays. The previous version of BP, BPv6, contained a mechanism for reliable transfer between ""custodial nodes"" called ""custody transfer"". However, this approach has been removed from the core protocol specification for BPv7, which requires a corresponding BP reliability extension to be defined separately. This paper introduces a new custody transfer process for BPv7 (expected to be published by CCSDS as an experimental specification in 2025). The core features of this new custody transfer method for BPv7 are: (1) A strategy to efficiently identify sets of bundles by sequence numbering (2) A new Custody Transfer Extension Block and a corresponding administrative record, Compressed Custody Signal, to efficiently report on the acceptance or rejection of custody using sequence numbering (3) A new Compressed Reporting Extension Block requesting reporting on bundle processing steps using a corresponding administrative record with sequence numbering for efficiency. The paper will describe those concepts and their design, specification, and implementation in detail. These mechanisms have been prototyped in the ESA BP implementation and tested in Earth Observation and Lunar communication simulation scenarios. The results will be presented, as will an outlook on future work in the DTN reliable transfer domain."
2507.17835,"In future AI-native wireless networks, the presence of mismatches between the latent spaces of independently designed and trained deep neural network (DNN) encoders may impede mutual understanding due to the emergence of semantic channel noise. This undermines the receiver's ability to interpret transmitted representations, thereby reducing overall system performance. To address this issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based semantic channel equalizer that aligns latent spaces of heterogeneous encoders without requiring system retraining. PFE enables dynamic signal compression and expansion, mitigating semantic noise while preserving performance on downstream tasks. Building on this capability, we introduce a dynamic optimization strategy that coordinates communication, computation, and learning resources to balance energy consumption, end-to-end (E2E) latency, and task performance in multi-agent semantic communication scenarios. Extensive simulations confirm the effectiveness of our approach in maintaining semantic consistency and meeting long-term constraints on latency and accuracy under diverse and time-varying network conditions."
2507.17861,"Artificial Intelligence (AI) plays a key role in developing 6G networks. While current specifications already include Network Data Analytics Function (NWDAF) as a network element responsible for providing information about the core, a more comprehensive approach will be needed to enable automation of network segments that are not yet fully explored in the context of 5G. In this paper, we present Automated Radio Coverage Anomalies Detection and Evaluation (ARCADE), a methodology for identifying and diagnosing anomalies in the cellular access network. Furthermore, we demonstrate how a hybrid architecture of network analytics functions in the evolution toward 6G can enhance the application of AI in a broader network context, using ARCADE as a practical example of this approach."
2507.17865,"The convergence of Large Language Models (LLMs) and Internet of Things (IoT) networks open new opportunities for building intelligent, responsive, and user-friendly systems. This work presents an edge-centric framework that integrates LLMs into IoT architectures to enable natural language-based control, context-aware decision-making, and enhanced automation. The proposed modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are deployed on edge computing devices connected to IoT gateways, enabling local processing of user commands and sensor data for reduced latency, improved privacy, and enhanced inference quality. We validate the framework through a smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart devices. Experimental results highlight the trade-offs between model accuracy and inference time with respect to models size. At last, we also discuss the potential applications that can use LLM-based IoT systems, and a few key challenges associated with such systems."
2507.17905,"LPWANs have become ubiquitous due to their ability to connect sensors over large geographic areas in a single hop. It is, however, very challenging to achieve massive scalability in LPWANs, where numerous sensors can transmit data efficiently and with low latency, which emerging IoT and CPS applications may require. In this paper, we address the above challenges by significantly advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal frequency division multiplexing, D-OFDM, subcarriers to enable parallel reception of data to a BS from multiple asynchronous sensors, each using a different subcarrier. In this paper, we achieve massive scalability in SNOW by enabling the BS to decode concurrent data from numerous asynchronous sensors on the same subcarrier while parallelly decoding from other subcarriers as well. Additionally, we enable numerous asynchronous sensors to receive distinct data from the BS on the same subcarrier while other sensors also receive data parallelly on other subcarriers. To do this, we develop a set of Gold code-based pseudorandom noise or PN sequences that are mutually non-interfering within and across the subcarriers. Each sensor uses its PN sequence from the set for encoding or decoding data on its subcarriers, enabling massive concurrency. Our evaluation results demonstrate that we can achieve approximately 9x more scalability in SNOW while being timely in data collection at the BS and energy efficient at the sensors. This may enable emerging IoT and CPS applications requiring tens of thousands of sensors with longer battery life and making data-driven, time-sensitive decisions."
2507.18328,"In this paper, we consider the fair access problem and the Age of Information (AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in vehicular networks. Specifically, vehicles follow Mode 2 to communicate with Roadside Units (RSUs) to obtain accurate data for drivingthis http URL, vehicles often have different velocity when they are moving in adjacent lanes, leading to difference in RSU dwelltime and communication duration. This results in unfair access to network resources, potentially influencing driving safety. To ensure the freshness of received data, the AoI should be analyzed. Mode 2 introduces a novel preemption mechanism, necessitating simultaneous optimization of fair access and AoI to guarantee timely and relevant data delivery. We propose a joint optimization framework for vehicular network, defining a fairness index and employing Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS) in Mode 2, we address the optimization of fairness and AoI. We apply a large language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate the effectiveness of our scheme in balancing fair access and minimizing AoI."
2507.1848,"IEEE 802.11 networks continuously adapt to meet the stringent requirements of emerging applications like cloud gaming, eXtended Reality (XR), and video streaming services, which require high throughput, low latency, and high reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can potentially contribute to optimizing spectrum resource utilization. This mechanism is expected to enable simultaneous transmissions, thereby boosting spectral efficiency in dense environments and increasing the overall network performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi 8 networks. For that, we propose an implementation of Co-SR aligned with ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi simulator, which allows us to study the performance of the proposed Co-SR mechanisms in relevant scenarios. The results obtained in a Wireless Local Area Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging from 31% to 95% when compared to Distributed Coordination Function (DCF)."
2507.18834,"The web experience using mobile devices is important since a significant portion of the Internet traffic is initiated from mobile devices. In the era of 5G, users expect a high-performance data network to stream media content and for other latency-sensitive applications. In this paper, we characterize mobile experience in terms of latency, throughput, and stability measured from a commercial, globally-distributed CDN. Unlike prior work, CDN data provides a relatively neutral, carrier-agnostic perspective, providing a clear view of multiple and international providers. Our analysis of mobile client traffic shows mobile users sometimes experience markedly low latency, even as low as 6 ms. However, only the top 5% users regularly experience less than 20 ms of minimum latency. While 100 Mb/s throughput is not rare, we show around 60% users observe less than 50 Mb/s throughput. We find the minimum mobile latency is generally stable at a specific location which can be an important characteristic for anomaly detection."
2507.1905,"In this paper, we propose a general digital twin edge computing network comprising multiple vehicles and a server. Each vehicle generates multiple computing tasks within a time slot, leading to queuing challenges when offloading tasks to the server. The study investigates task offloading strategies, queue stability, and resource allocation. Lyapunov optimization is employed to transform long-term constraints into tractable short-term decisions. To solve the resulting problem, an in-context learning approach based on large language model (LLM) is adopted, replacing the conventional multi-agent reinforcement learning (MARL) framework. Experimental results demonstrate that the LLM-based method achieves comparable or even superior performance to MARL."
2507.19096,"Efficient indoor wireless network (IWN) planning is crucial for providing high-quality 5G in-building services. However, traditional meta-heuristic and artificial intelligence-based planning methods face significant challenges due to the intricate interplay between indoor environments (IEs) and IWN demands. In this article, we present an indoor wireless network Planning with large LANguage models (iPLAN) framework, which integrates multi-modal IE representations into large language model (LLM)-powered optimizers to improve IWN planning. First, we instate the role of LLMs as optimizers, outlining embedding techniques for IEs, and introducing two core applications of iPLAN: (i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE for new wireless-friendly buildings. For the former, we embed essential information into LLM optimizers by leveraging indoor descriptions, domain-specific knowledge, and performance-driven perception. For the latter, we conceptualize a multi-agent strategy, where intelligent agents collaboratively address key planning sub-tasks in a step-by-step manner while ensuring optimal trade-offs between the agents. The simulation results demonstrate that iPLAN achieves superior performance in IWN planning tasks and optimizes building wireless performance through the joint design of IEs and IWNs, exemplifying a paradigm shift in IWN planning."
2507.19124,"Wireless support of virtual reality (VR) has challenges when a network has multiple users, particularly for 3D VR gaming, digital AI avatars, and remote team collaboration. This work addresses these challenges through investigation of the low-rank channels that inevitably occur when there are more active users than there are degrees of spatial freedom, effectively often the number of antennas. The presented approach uses optimal nonlinear transceivers, equivalently generalized decision-feedback or successive cancellation for uplink and superposition or dirty-paper precoders for downlink. Additionally, a powerful optimization approach for the users' energy allocation and decoding order appears to provide large improvements over existing methods, effectively nearing theoretical optima. As the latter optimization methods pose real-time challenges, approximations using deep reinforcement learning (DRL) are used to approximate best performance with much lower (5x at least) complexity. Experimental results show significantly larger sum rates and very large power savings to attain the data rates found necessary to support VR. Experimental results show the proposed algorithm outperforms current industry standards like orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA), as well as the highly researched methods in multi-carrier NOMA (MC-NOMA), enhancing sum data rate by 39%, 28%, and 16%, respectively, at a given power level. For the same data rate, it achieves power savings of 75%, 45%, and 40%, making it ideal for VR applications. Additionally, a near-optimal deep reinforcement learning (DRL)-based resource allocation framework for real-time use by being 5x faster and reaching 83% of the global optimum is introduced."
2507.19234,"Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available atthis https URL."
2507.19377,"Multi-access point coordination (MAPC) is a key feature of IEEE 802.11bn, with a potential impact on future Wi-Fi networks. MAPC enables joint scheduling decisions across multiple access points (APs) to improve throughput, latency, and reliability in dense Wi-Fi deployments. However, implementing efficient scheduling policies under diverse traffic and interference conditions in overlapping basic service sets (OBSSs) remains a complex task. This paper presents a method to minimize the network-wide worst-case latency by formulating MAPC scheduling as a sequential decision-making problem and proposing a deep reinforcement learning (DRL) mechanism to minimize worst-case delays in OBSS deployments. Specifically, we train a DRL agent using proximal policy optimization (PPO) within an 802.11bn-compatible Gymnasium environment. This environment provides observations of queue states, delay metrics, and channel conditions, enabling the agent to schedule multiple AP-station pairs to transmit simultaneously by leveraging spatial reuse (SR) groups. Simulations demonstrate that our proposed solution outperforms state-of-the-art heuristic strategies across a wide range of network loads and traffic patterns. The trained machine learning (ML) models consistently achieve lower 99th-percentile delays, showing up to a 30% improvement over the best baseline."
2507.19653,"We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links in central Rome. We use a real measurement set of 1,664 user-equipments (UEs) and six nominal base-station (BS) sites. Using these fixed positions we systematically vary the main simulation parameters, including path depth, diffuse/specular/refraction flags, carrier frequency, as well as antenna's properties like its altitude, radiation pattern, and orientation. Simulator fidelity is scored for each base station via Spearman correlation between measured and simulated powers, and by a fingerprint-based k-nearest-neighbor localization algorithm using RSSI-based fingerprints. Across all experiments, solver hyper-parameters are having immaterial effect on the chosen metrics. On the contrary, antenna locations and orientations prove decisive. By simple greedy optimization we improve the Spearman correlation by 5% to 130% for various base stations, while kNN-based localization error using only simulated data as reference points is decreased by one-third on real-world samples, while staying twice higher than the error with purely real data. Precise geometry and credible antenna models are therefore necessary but not sufficient; faithfully capturing the residual urban noise remains an open challenge for transferable, high-fidelity outdoor RF simulation."
2507.19657,"The development of next-generation networking systems has inherently shifted from throughput-based paradigms towards intelligent, information-aware designs that emphasize the quality, relevance, and utility of transmitted information, rather than sheer data volume. While classical network metrics, such as latency and packet loss, remain significant, they are insufficient to quantify the nuanced information quality requirements of modern intelligent applications, including autonomous vehicles, digital twins, and metaverse environments. In this survey, we present the first comprehensive study of the ``X of Information'' continuum by introducing a systematic four-dimensional taxonomic framework that structures information metrics along temporal, quality/utility, reliability/robustness, and network/communication dimensions. We uncover the increasing interdependencies among these dimensions, whereby temporal freshness triggers quality evaluation, which in turn helps with reliability appraisal, ultimately enabling effective network delivery. Our analysis reveals that artificial intelligence technologies, such as deep reinforcement learning, multi-agent systems, and neural optimization models, enable adaptive, context-aware optimization of competing information quality objectives. In our extensive study of six critical application domains, covering autonomous transportation, industrial IoT, healthcare digital twins, UAV communications, LLM ecosystems, and metaverse settings, we illustrate the revolutionary promise of multi-dimensional information metrics for meeting diverse operational needs. Our survey identifies prominent implementation challenges, including ..."
2507.19925,"Network capacity expansion is a critical challenge for telecom operators, requiring strategic placement of new cell sites to ensure optimal coverage and performance. Traditional approaches, such as manual drive tests and static optimization, often fail to consider key real-world factors including user density, terrain features, and financial constraints. In this paper, we propose a machine learning-based framework that combines deep neural networks for signal coverage prediction with spatial clustering to recommend new tower locations in underserved areas. The system integrates geospatial, demographic, and infrastructural data, and incorporates budget-aware constraints to prioritize deployments. Operating within an iterative planning loop, the framework refines coverage estimates after each proposed installation, enabling adaptive and cost-effective expansion. While full-scale simulation was limited by data availability, the architecture is modular, robust to missing inputs, and generalizable across diverse deployment scenarios. This approach advances radio network planning by offering a scalable, data-driven alternative to manual methods."
2507.19938,"The deployment of mobile LoRa gateways using low-cost single-channel hardware presents a significant challenge in maintaining reliable communication due to the lack of dynamic configuration support. In traditional LoRaWAN networks, Adaptive Data Rate (ADR) mechanisms optimize communication parameters in real time. However, such features are typically supported only by expensive multi-channel gateways. This study proposes a cost-effective and energy-efficient solution by statically selecting the optimal Spreading Factor (SF) using a two-phase algorithm. The method first applies rule-based exclusion to eliminate SFs that violate constraints related to distance, data rate, link margin, and regulatory limits. Remaining candidates are then evaluated using a weighted scoring model incorporating Time-on-Air, energy consumption, data rate, and link robustness. The proposed algorithm was validated through extensive field tests and NS-3 simulations under line-of-sight conditions. Results demonstrate that the selected SF matched the optimal SF in over 92% of cases across 672 simulated scenarios, confirming the algorithm's effectiveness. This approach offers a scalable alternative to dynamic protocols, enabling reliable mobile LoRa deployments in cost-sensitive environments such as agriculture and rural sensing applications."
2507.19963,"This work presents a perspective on addressing the underutilization of computing resources in FPGA SoC devices deployed in 5G radio and edge computing infrastructure. The initial step in this approach involves developing a resource management layer capable of dynamically migrating and scaling functions within these devices in response to contextual events. This layer serves as the foundation for designing a hierarchical, data-driven micro-orchestrator responsible for managing the lifecycle of functions in FPGA SoC devices. In this paper, the proposed resource management layer is utilized to reconfigure a function based on events identified by a computer vision edge application."
2507.2005,"The Multi-user Immersive Reality (MIR) landscape is evolving rapidly, with applications spanning virtual collaboration, entertainment, and training. However, wireless network limitations create a critical bottleneck, struggling to meet the high-bandwidth and ultra-low latency demands essential for next-generation MIR experiences. This paper presents Hera, a modular framework for next-generation immersive applications, comprising a high-level streaming and synchronization layer for AR/VR systems and a low-level delay-based QoE-aware rate control protocol optimized for dynamic wireless environments. The Hera framework integrates application-aware streaming logic with a QoE-centric rate control core, enabling adaptive video quality, multi-user fairness, and low-latency communication across challenging 5G network conditions. We demonstrate that Hera outperforms existing state-of-the-art rate control algorithms by maintaining up to 66% lower latencies with comparable throughput performance, higher visual quality with 50% average bitrate improvements in our analysis, and improved fairness. By bridging the gap between application-level responsiveness and network-level adaptability, Hera lays the foundation for more scalable, robust, and high-fidelity multi-user immersive experiences."
2507.20115,"In response to Distributed Denial of Service (DDoS) attacks, recent research efforts increasingly rely on Machine Learning (ML)-based solutions, whose effectiveness largely depends on the quality of labeled training datasets. To address the scarcity of such datasets, data augmentation with synthetic traces is often employed. However, current synthetic trace generation methods struggle to capture the complex temporal patterns and spatial distributions exhibited in emerging DDoS attacks. This results in insufficient resemblance to real traces and unsatisfied detection accuracy when applied to ML tasks. In this paper, we propose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view, multi-stream network traffic generative model based on diffusion models, featuring two main streams: The field stream utilizes spatial mapping to bridge network data characteristics with pre-trained realms of stable diffusion models, effectively translating complex network interactions into formats that stable diffusion can process, while the spatial stream adopts a dynamic temporal modeling approach, meticulously capturing the intrinsic temporal patterns of network traffic. Extensive experiments demonstrate that data generated by our model exhibits higher statistical similarity to originals compared to current state-of-the-art solutions, and enhance performances on a wide range of downstream tasks."
2507.20116,"Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions using a sliding window mechanism. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both physical edge devices and Docker-based emulations. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\times$, 1.79$\times$, and 1.28$\times$ compared to the Baseline, Dragonfly, and Kraken, respectively, while significantly reducing peak cross-network traffic by 90.72\% under congested and varying network conditions."
2507.20234,"Decentralized autonomous organizations (DAOs) rely on governance mechanism without centralized leadership. This paper presents an empirical study of user behavior in governance for a variety of DAOs, ranging from DeFi to gaming, using the Internet Computer Protocol DAO framework called SNS (Service Nervous System). To analyse user engagement, we measure participation rates and frequency of proposals submission and voter approval rates. We evaluate decision duration times to determine DAO agility. To investigate dynamic aspects, we also measure metric shifts in time. We evaluate over 3,000 proposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected DAO have been existing between 6 and 20 months and cover a wide spectrum of use cases, treasury sizes, and number of participants. We also compare our results for SNS DAOs with DAOs from other blockchain platforms. While approval rates are generally high for all DAOs studied, SNS DAOs show slightly more alignment. We observe that the SNS governance mechanisms and processes in ICP lead to higher activity, lower costs and faster decisions. Most importantly, in contrast to studies which report a decline in participation over time for other frameworks, SNS DAOs exhibit sustained or increasing engagement levels over time."
2507.20367,"Integrated access and backhaul (IAB) is one of the promising techniques for 5G networks and beyond (6G), in which the same node/hardware is used to provide both backhaul and cellular services in a multi-hop architecture. Due to the sensitivity of the backhaul links with high rate/reliability demands, proper network planning is needed to ensure the IAB network performs with the desired performance levels. In this paper, we study the effect of infrastructure planning and optimization on the coverage of IAB networks. We concentrate on the cases where the fiber connectivity to the nodes is constrained due to cost. Thereby, we study the performance gains and energy efficiency in the presence of free-space optical (FSO) communication links. Our results indicate hybrid fiber/FSO deployments offer substantial cost savings compared to fully fibered networks, suggesting a beneficial trade-off for strategic link deployment while improving the service coverage probability. As we show, with proper network planning, the service coverage, energy efficiency, and cost efficiency can be improved."
2507.20438,"Remote driving, or teleoperating Autonomous Vehicles (AVs), is a key application that emerging 5G networks aim to support. In this paper, we conduct a systematic feasibility study of AV teleoperation over commercial 5G networks from both cross-layer and end-to-end (E2E) perspectives. Given the critical importance of timely delivery of sensor data, such as camera and LiDAR data, for AV teleoperation, we focus in particular on the performance of uplink sensor data delivery. We analyze the impacts of Physical Layer (PHY layer) 5G radio network factors, including channel conditions, radio resource allocation, and Handovers (HOs), on E2E latency performance. We also examine the impacts of 5G networks on the performance of upper-layer protocols and E2E application Quality-of-Experience (QoE) adaptation mechanisms used for real-time sensor data delivery, such as Real-Time Streaming Protocol (RTSP) and Web Real Time Communication (WebRTC). Our study reveals the challenges posed by today's 5G networks and the limitations of existing sensor data streaming mechanisms. The insights gained will help inform the co-design of future-generation wireless networks, edge cloud systems, and applications to overcome the low-latency barriers in AV teleoperation."
2507.20467,"Deep Joint Source-Channel Coding (Deep-JSCC) has emerged as a promising semantic communication approach for wireless image transmission by jointly optimizing source and channel coding using deep learning techniques. However, traditional Deep-JSCC architectures employ fixed encoder-decoder structures, limiting their adaptability to varying device capabilities, real-time performance optimization, power constraints and channel conditions. To address these limitations, we propose DD-JSCC: Dynamic Deep Joint Source-Channel Coding for Semantic Communications, a novel encoder-decoder architecture designed for semantic communication systems. Unlike traditional Deep-JSCC models, DD-JSCC is flexible for dynamically adjusting its layer structures in real-time based on transmitter and receiver capabilities, power constraints, compression ratios, and current channel conditions. This adaptability is achieved through a hierarchical layer activation mechanism combined with implicit regularization via sequential randomized training, effectively reducing combinatorial complexity, preventing overfitting, and ensuring consistent feature representations across varying configurations. Simulation results demonstrate that DD-JSCC enhances the performance of image reconstruction in semantic communications, achieving up to 2 dB improvement in Peak Signal-to-Noise Ratio (PSNR) over fixed Deep-JSCC architectures, while reducing training costs by over 40%. The proposed unified framework eliminates the need for multiple specialized models, significantly reducing training complexity and deployment overhead."
2507.20524,"Low altitude uncrewed aerial vehicles (UAVs) are expected to facilitate the development of aerial-ground integrated intelligent transportation systems and unlocking the potential of the emerging low-altitude economy. However, several critical challenges persist, including the dynamic optimization of network resources and UAV trajectories, limited UAV endurance, and imperfect channel state information (CSI). In this paper, we offer new insights into low-altitude economy networking by exploring intelligent UAV-assisted vehicle-to-everything communication strategies aligned with UAV energy efficiency. Particularly, we formulate an optimization problem of joint channel allocation, power control, and flight altitude adjustment in UAV-assisted vehicular networks. Taking CSI feedback delay into account, our objective is to maximize the vehicle-to-UAV communication sum rate while satisfying the UAV's long-term energy constraint. To this end, we first leverage Lyapunov optimization to decompose the original long-term problem into a series of per-slot deterministic subproblems. We then propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm, which innovatively integrates diffusion models to determine optimal channel allocation, power control, and flight altitude adjustment decisions. Through extensive simulations using real-world vehicle mobility traces, we demonstrate the superior performance of the proposed D3PG algorithm compared to existing benchmark solutions."
2507.20806,"There has been a growing interest in Internet user privacy, demonstrated by the popularity of privacy-preserving products such as Telegram and Brave, and the widespread adoption of HTTPS. The Domain Name System (DNS) is a key component of Internet-based communication and its privacy has been neglected for years. Recently, DNS over HTTPS (DoH) has improved the situation by fixing the issue of in-path middleboxes. Further progress has been made with proxy-based solutions such as Oblivious DoH (ODoH), which separate a user's identity from their DNS queries. However, these solutions rely on non-collusion assumptions between DNS resolvers and proxies -- an assumption difficult to guarantee in practice. To address this, we explore integrating single-server Private Information Retrieval (PIR) into DNS to enable encrypted query processing without relying on trust assumptions. However, applying PIR to DNS is challenging due to its hierarchical nature -- particularly, interactions with recursive resolvers can still leak information. Navigating performance and privacy trade-offs, we propose PDNS, a DNS extension leveraging single-server PIR to strengthen privacy guarantees. We have implemented a prototype of PDNS and compared its performance against state-of-the-art solutions via trace-driven experiments. The results show that PDNS achieves acceptable performance (2x faster than DoH over Tor with similar privacy guarantees) and strong privacy guarantees today, mainly at the cost of its scalability, which specialized hardware for PIR can address in the near future."
2507.20871,"Native AI support is a key objective in the evolution of 6G networks, with Federated Learning (FL) emerging as a promising paradigm. FL allows decentralized clients to collaboratively train an AI model without directly sharing their data, preserving privacy. Clients train local models on private data and share model updates, which a central server aggregates to refine the global model and redistribute it for the next iteration. However, client data heterogeneity slows convergence and reduces model accuracy, and frequent client participation imposes communication and computational burdens. To address these challenges, we propose FedABC, an innovative client selection algorithm designed to take a long-term view in managing data heterogeneity and optimizing client participation. Inspired by attention mechanisms, FedABC prioritizes informative clients by evaluating both model similarity and each model's unique contributions to the global model. Moreover, considering the evolving demands of the global model, we formulate an optimization problem to guide FedABC throughout the training process. Following the ""later-is-better"" principle, FedABC adaptively adjusts the client selection threshold, encouraging greater participation in later training stages. Extensive simulations on CIFAR-10 demonstrate that FedABC significantly outperforms existing approaches in model accuracy and client participation efficiency, achieving comparable performance with 32% fewer clients than the classical FL algorithm FedAvg, and 3.5% higher accuracy with 2% fewer clients than the state-of-the-art. This work marks a step toward deploying FL in heterogeneous, resource-constrained environments, thereby supporting native AI capabilities in 6G networks."
2507.20971,"The ability of the network digital twin (NDT) to remain aware of changes in its physical counterpart, known as the physical twin (PTwin), is a fundamental condition to enable timely synchronization, also referred to as twinning. In this way, considering a transport network, a key requirement is to handle unexpected traffic variability and dynamically adapt to maintain optimal performance in the associated virtual model, known as the virtual twin (VTwin). In this context, we propose a self-adaptive implementation of a novel NDT architecture designed to provide accurate delay predictions, even under fluctuating traffic conditions. This architecture addresses an essential challenge, underexplored in the literature: improving the resilience of data-driven NDT platforms against traffic variability and improving synchronization between the VTwin and its physical counterpart. Therefore, the contributions of this article rely on NDT lifecycle by focusing on the operational phase, where telemetry modules are used to monitor incoming traffic, and concept drift detection techniques guide retraining decisions aimed at updating and redeploying the VTwin when necessary. We validate our architecture with a network management use case, across various emulated network topologies, and diverse traffic patterns to demonstrate its effectiveness in preserving acceptable performance and predicting per-flow delay under unexpected traffic variation. The results in all tested topologies, using the normalized mean square error as the evaluation metric, demonstrate that our proposed architecture, after a traffic concept drift, achieves a performance improvement in prediction of at least 56.7% compared to a configuration without NDT synchronization."
2507.21372,"We investigate the performance of various load balancing algorithms for large-scale AI training workloads that are running on dedicated infrastructure. The performance of load balancing depends on both the congestion control and loss recovery algorithms, so our evaluation also sheds light on the appropriate choices for those designs as well."
2507.21385,"3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX) is an important new network energy saving feature for 5G. As a time-domain technique, it periodically aggregates the user data transmissions in a given duration of time when the traffic load is not heavy, so that the remaining time can be kept silent and advanced sleep modes (ASM) can be enabled to shut down more radio components and save more energy for the cell. However, inevitably the packet delay is increased, as during the silent period no transmission is allowed. In this paper we study how to configure cell DTX/DRX to optimally balance energy saving and packet delay, so that for delay-sensitive traffic maximum energy saving can be achieved while the degradation of quality of service (QoS) is minimized. As the optimal configuration can be different for different network and traffic conditions, the problem is complex and we resort to deep reinforcement learning (DRL) framework to train an AI agent to solve it. Through careful design of 1) the learning algorithm, which implements a deep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward function, which utilizes a smooth approximation of a theoretically optimal but discontinuous reward function, we are able to train an AI agent that always tries to select the best possible Cell DTX/DRX configuration under any network and traffic conditions. Simulation results show that compared to the case when cell DTX/DRX is not used, our agent can achieve up to ~45% energy saving depending on the traffic load scenario, while always maintaining no more than ~1% QoS degradation."
2507.21728,"Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers (EDFAs) is essential for optimizing optical network performance, particularly as networks evolve toward multi-vendor solutions. In this work, we propose a generalized few-shot transfer learning architecture based on a Semi-Supervised Self-Normalizing Neural Network (SS-NN) that leverages internal EDFA features - such as VOA input or output power and attenuation, to improve gain spectrum prediction. Our SS-NN model employs a two-phase training strategy comprising unsupervised pre-training with noise-augmented measurements and supervised fine-tuning with a custom weighted MSE loss. Furthermore, we extend the framework with transfer learning (TL) techniques that enable both homogeneous (same-feature space) and heterogeneous (different-feature sets) model adaptation across booster, preamplifier, and ILA EDFAs. To address feature mismatches in heterogeneous TL, we incorporate a covariance matching loss to align second-order feature statistics between source and target domains. Extensive experiments conducted across 26 EDFAs in the COSMOS and Open Ireland testbeds demonstrate that the proposed approach significantly reduces the number of measurements requirements on the system while achieving lower mean absolute errors and improved error distributions compared to benchmark methods."
2507.21739,"Deploying Machine Learning (ML) applications on resource-constrained mobile devices remains challenging due to limited computational resources and poor platform compatibility. While Mobile Edge Computing (MEC) offers offloading-based inference paradigm using GPU servers, existing approaches are divided into non-transparent and transparent methods, with the latter necessitating modifications to the source code. Non-transparent offloading achieves high performance but requires intrusive code modification, limiting compatibility with diverse applications. Transparent offloading, in contrast, offers wide compatibility but introduces significant transmission delays due to per-operator remote procedure calls (RPCs). To overcome this limitation, we propose RRTO, the first high-performance transparent offloading system tailored for MEC inference. RRTO introduces a record/replay mechanism that leverages the static operator sequence in ML models to eliminate repetitive RPCs. To reliably identify this sequence, RRTO integrates a novel Operator Sequence Search algorithm that detects repeated patterns, filters initialization noise, and accelerates matching via a two-level strategy. Evaluation demonstrates that RRTO achieves substantial reductions of up to 98% in both per-inference latency and energy consumption compared to state-of-the-art transparent methods and yields results comparable to non-transparent approaches, all without necessitating any source code modification."
2507.22131,"Service Function Chains (SFCs) are one of the key enablers in providing programmable computer networks, paving the way for network autonomy. However, this also introduces new challenges, such as resource allocation and optimisation related to their operation, requiring new algorithms to address these challenges. Various tools have been used in the literature to evaluate these algorithms. However, these tools suffer from inaccuracy, low fidelity, unscalability, inflexibility, or additional code requirements. This paper introduces an emulator based on Mininet and Docker for SFCs called OpenRASE. The goal of OpenRASE is to enable the exploration of resource allocation algorithms for SFCs in a dynamic setting, allowing real CPU usage and latency to be measured. We describe the design and implementation of OpenRASE and discuss its characteristics. We also experimentally evaluate two different algorithms to address the SFC resource allocation challenge, including an online Genetic Algorithm, using OpenRASE to show its effectiveness and practicality for dynamic network conditions."
2507.22317,"The accurate localization of sensor nodes is a fundamental requirement for the practical application of the Internet of Things (IoT). To enable robust localization across diverse environments, this paper proposes a hybrid meta-heuristic localization algorithm. Specifically, the algorithm integrates the Sine Cosine Algorithm (SCA), which is effective in global search, with Particle Swarm Optimization (PSO), which excels at local search. An adaptive switching module is introduced to dynamically select between the two algorithms. Furthermore, the initialization, fitness evaluation, and parameter settings of the algorithm have been specifically redesigned and optimized to address the characteristics of the node localization problem. Simulation results across varying numbers of sensor nodes demonstrate that, compared to standalone PSO and the unoptimized SCAPSO algorithm, the proposed method significantly reduces the number of required iterations and achieves an average localization error reduction of 84.97%."
2507.22591,"This paper presents a novel multiband passive sensing system that leverages IEEE 802.11bf Wi-Fi signals for environmental sensing, focusing on both sub-7 GHz and millimeter-wave (mmWave) bands. By combining Channel State Information (CSI) from multiple bands, the system enhances accuracy and reliability in detecting human presence, movement, and activities in indoor environments. Utilizing a novel model, called MILAGRO, the system demonstrates robust performance across different scenarios, including monitoring human presence in workspaces and tracking movement in corridors. Experimental results show high accuracy (95-100%), with improved performance by integrating multiband data. The system also addresses key security concerns associated with passive sensing, proposing measures to mitigate potential risks. This work advances the use of Wi-Fi for passive sensing by reducing reliance on active sensing infrastructure and extending the capabilities of low-cost, non-intrusive environmental monitoring."
2507.22687,"Physical spaces are increasingly dense with networked devices, promising seamless coordination and ambient intelligence. Yet today, cloud-first architectures force all communication through wide-area networks regardless of physical proximity. We lack an abstraction for spatial networking: using physical spaces to create boundaries for private, robust, and low-latency communication. We introduce $\textit{Bifrst}$, a programming model that realizes spatial networking using bigraphs to express both containment and connectivity, enabling policies to be scoped by physical boundaries, devices to be named by location, the instantiation of spatial services, and the composition of spaces while maintaining local autonomy. Bifrst enables a new class of spatially-aware applications, where co-located devices communicate directly, physical barriers require explicit gateways, and local control bridges to global coordination."
2507.22711,"The rapid evolution of network infrastructure is bringing new challenges and opportunities for efficient network management, optimization, and security. With very large monitoring databases becoming expensive to explore, the use of AI and Generative AI can help reduce costs of managing these datasets. This paper explores the use of Large Language Models (LLMs) to revolutionize network monitoring management by addressing the limitations of query finding and pattern analysis. We leverage LLMs to enhance anomaly detection, automate root-cause analysis, and automate incident analysis to build a well-monitored network management team using AI. Through a real-world example of developing our own OFCNetLLM, based on the open-source LLM model, we demonstrate practical applications of OFCnetLLM in the OFC conference network. Our model is developed as a multi-agent approach and is still evolving, and we present early results here."
2507.22851,"In this paper, we propose Morph, a LoRa encoder-decoder co-design to enhance communication reliability while improving its computation efficiency in extremely-low signal-to-noise ratio (SNR) situations. The standard LoRa encoder controls 6 Spreading Factors (SFs) to tradeoff SNR tolerance with data rate. SF-12 is the maximum SF providing the lowest SNR tolerance on commercial off-the-shelf (COTS) LoRa nodes. In Morph, we develop an SF-configuration based encoder to mimic the larger SFs beyond SF-12 while it is compatible with COTS LoRa nodes. Specifically, we manipulate four SF configurations of a Morph symbol to encode 2-bit data. Accordingly, we recognize the used SF configuration of the symbol for data decoding. We leverage a Deep Neural Network (DNN) decoder to fully capture multi-dimensional features among diverse SF configurations to maximize the SNR gain. Moreover, we customize the input size, neural network structure, and training method of the DNN decoder to improve its efficiency, reliability, and generalizability. We implement Morph with COTS LoRa nodes and a USRP N210, then evaluate its performance on indoor and campus-scale testbeds. Results show that we can reliably decode data at -28.8~dB SNR, which is 6.4~dB lower than the standard LoRa with SF-12 chirps. In addition, the computation efficiency of our DNN decoder is about 3x higher than state-of-the-art."
2507.23012,"Large-scale distributed training in production data centers place significant demands on network infrastructure. In particular, significant load balancing challenges arise when processing AI/ML workloads, consisting of low-entropy, bursty and long-lived flows. Existing solutions designed for Ethernet, such as Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization. While major industry players (e.g., Ultra Ethernet Consortium) and parts of academia have proposed packet spraying to enhance AI/ML workload performance, we argue that existing packet spraying solutions lead to buffer inflation over time, negatively affecting network performance. Specifically, when ACK coalescing is used, these solutions lead to stale information, degrading network performance. Additionally, in asymmetric network conditions- such as mix of ordered an unordered traffic, or link degradation and failures- existing packet spraying solutions often lead to increased tail latency. In this paper, we present the design and evaluation of PRIME, a pseudo-randomized round-robin approach to packet spraying that considers the network topology to optimize load distribution and performance. PRIME uses congestion as an indicator to re-balance the load. To this extent, PRIME takes into account various congestion signals, accounting for congestion severity, and their decay times to avoid network hotspots. We extensively evaluated PRIME using large-scale production-level simulator. Our results indicate that, compared to existing solutions, PRIME leads to up to 15% improvement for permutation traffic and up to 27% improvement in network degradation scenarios"
2507.23177,"Ultra-dense fifth generation (5G) and beyond networks leverage spectrum sharing and frequency reuse to enhance throughput, but face unpredictable in-band uplink (UL) interference challenges that significantly degrade Signal to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases (gNBs). This is particularly problematic at cell edges, where overlapping regions force User Equipments (UEs) to increase transmit power, and in directional millimeter wave systems, where beamforming sidelobes can create unexpected interference. The resulting signal degradation disrupts protocol operations, including scheduling and resource allocation, by distorting quality indicators like Reference Signal Received Power (RSRP) and Received Signal Strength Indicator (RSSI), and can compromise critical functions such as channel state reporting and Hybrid Automatic Repeat Request (HARQ) acknowledgments. To address this problem, this article introduces InterfO-RAN, a real-time programmable solution that leverages a Convolutional Neural Network (CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical layer, detecting in-band interference with accuracy exceeding 91% in under 650 us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics Processing Unit (GPU), coexisting with the 5G NR physical layer processing of NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial Radio Units (RUs) and smartphones, our solution was trained and tested on more than 7 million NR UL slots collected from real-world environments, demonstrating robust interference detection capabilities essential for maintaining network performance in dense deployments."
2507.23286,"As the demand for low-latency services grows, ensuring the delay performance of random access (RA) networks has become a priority. Existing studies on the queueing delay performance of the Aloha model universally treat packets as atomic transmission units, focusing primarily on delay measured in time slots. However, the impact of packetization on queueing delay has been consistently overlooked, particularly for the mean queueing delay measured in seconds, which serves as a more precise and practically relevant performance metric than its slot-based counterpart. Here, packetization refers to the process of determining the number of bits assembled into a packet. To optimize queueing delay from the perspective of packetization, this paper establishes the mathematical relationship between packetization and mean queueing delay in seconds for both connection-free and connection-based Aloha schemes, and explores the optimal packetization strategy to minimize this delay. We identify the optimal mean queueing delay and its corresponding packet size via numerical methods, and further analyze the influence of various network parameters. We further use simulations to investigate the similar impact of packetization on jitter of queueing delay. We then apply our analysis to re-evaluate the complex trade-off between the connection-free and connection-based schemes through the new perspective of packetization. Furthermore, recognizing that an analysis of the queueing delay performance for RA-SDT in NTN scenarios, especially from a packetization perspective, also remains an unexplored area, we apply the analysis to this scenario as a case study."
2507.23342,"The Internet of Things (IoT) has transformed many industries, and LoRaWAN (Long Range Wide Area Network), built on LoRa (Long Range) technology, has become a crucial solution for enabling scalable, low-cost, and energy-efficient communication in wide-area networks. Simulation tools are essential for optimizing the transmission parameters and, therefore, the energy efficiency and performance of LoRaWAN networks. While existing simulation frameworks accurately replicate real-world scenarios by including multiple layers of communication protocols, they often imply significant computational overhead and simulation times. To address this issue, this paper introduces FAST-LoRa, a novel simulation framework designed to enable fast and efficient evaluation of LoRaWAN networks and selection of transmission parameters. FAST-LoRa streamlines computation by relying on analytical models without complex packet-level simulations and implementing gateway reception using efficient matrix operations. Rather than aiming to replace discrete-event simulators, FAST-LoRa is intended as a lightweight and accurate approximation tool for evaluating transmission parameter strategies in scenarios with stable traffic patterns and uplink-focused communications. In our evaluation, we compare FAST-LoRa with a well-established simulator using multiple network configurations with varying numbers of end devices and gateways. The results show that FAST-LoRa achieves similar accuracy in estimating key network metrics, even in complex scenarios with interference and multi-gateway reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while significantly reducing computational time by up to three orders of magnitude."
2507.23421,"This paper introduces a dual-mode communication framework for wireless devices that integrates query-driven (pull) and event-driven (push) transmissions within a unified time-frame structure. Devices typically respond to information requests in pull mode, but if an anomaly is detected, they preempt the regular response to report the critical condition. Additionally, push-based communication is used to proactively send critical data without waiting for a request. This adaptive approach ensures timely, context-aware, and efficient data delivery across different network conditions. To achieve high energy efficiency, we incorporate a wake-up radio mechanism and we design a tailored medium access control (MAC) protocol that supports data traffic belonging to the different communication classes. A comprehensive system-level analysis is conducted, accounting for the wake-up control operation and evaluating three key performance metrics: the success probability of anomaly reports (push traffic), the success probability of query responses (pull traffic) and the total energy consumption. Numerical results characterize the system's behavior and highlight the inherent trade-off in success probabilities between push- and pull-based traffic as a function of allocated communication resources. Our analysis demonstrates that the proposed approach reduces energy consumption by up to 30% compared to a traditional approach, while maintaining reliable support for both communication paradigms."
2507.23433,"Timely and informative data dissemination in communication networks is essential for enhancing system performance and energy efficiency, as it reduces the transmission of outdated or redundant data. Timeliness metrics, such as Age of Information (AoI), effectively quantify data freshness; however, these metrics fail to account for the intrinsic informativeness of the content itself. To address this limitation, content-based metrics have been proposed that combine both timeliness and informativeness. Nevertheless, existing studies have predominantly focused on evaluating average metric values, leaving the complete distribution-particularly in multi-hop network scenarios-largely unexplored. In this paper, we provide a comprehensive analysis of the stationary distribution of the Version Age of Information (VAoI), a content-based metric, under various scheduling policies, including randomized stationary, uniform, and threshold-based policies, with transmission constraints in single-hop and multi-hop networks. We derive closed-form expressions for the stationary distribution and average VAoI under these scheduling approaches. Furthermore, for threshold-based scheduling, we analytically determine the optimal threshold value that minimizes VAoI and derive the corresponding optimal VAoI in closed form. Numerical evaluations verify our analytical findings, providing valuable insights into leveraging VAoI in the design of efficient communication networks."
2507.23556,"Due to the diverse physical attributes of computing resources and tasks, developing effective mechanisms to facilitate task and resource matching in complex connected systems for value-oriented task completion has become increasingly challenging. To address the challenge, this paper proposes a networked physical computing system that integrates the physical attributes of computing resources and tasks as well as task-specific trust relationships among devices to enable value-driven task completion. Specifically, we propose a state-of-the-art hypergraph-aided trusted task-resource matching (TTR-matching) framework to achieve the envisioned physical computing. First, a task-specific trusted physical resource hypergraph is defined, which integrates task-specific trust, the physical attributes of resources, and task types. This enables accurate modeling of device collaboration dependencies under specific task types. Next, a task hypergraph is generated to associate the task initiator with the physical attributes of the corresponding tasks. Based on these two hypergraphs, a hypergraph matching algorithm is designed to facilitate task-specific trusted collaborator selection and accurate task-resource matching for value-maximizing task completion. Extensive experimental results demonstrate that the proposed TTR-matching framework outperforms comparison algorithms in identifying task-specific trustworthy collaborators and maximizing the average value of task completion."
2508.00007,"With the development of large models and autonomous decision-making AI, agents are rapidly becoming the new entities of the internet, following mobile apps. However, existing internet infrastructure is primarily designed for human interaction, creating data silos, unfriendly interfaces, and high collaboration costs among agents, making it difficult to support the needs for large-scale agent interconnection and collaboration. The internet is undergoing a profound transformation, showing four core trends: agents replacing traditional software, universal agent interconnection, native protocol-based connections, and autonomous agent organization and collaboration. To align with these trends, Agent Network Protocol (ANP) proposes a new generation of communication protocols for the Agentic Web. ANP adheres to AI-native design, maintains compatibility with existing internet protocols, adopts a modular composable architecture, follows minimalist yet extensible principles, and enables rapid deployment based on existing infrastructure. Through a three-layer protocol system--identity and encrypted communication layer, meta-protocol negotiation layer, and application protocol layer--ANP. systematically solves the problems of agent identity authentication, dynamic negotiation, and capability discovery interoperability."
2508.00009,"Fiber-To-The-Room is a potential solution to achieve in-premise extended reality collaborations. This paper explores predictive bandwidth allocation and seamless handover schemes over FTTR, showing high-quality immersive experience for in-premise collaborations can be achieved. \c{opyright} 2025 The Author(s)."
2508.0001,"With the explosive deployment of non-terrestrial networks (NTNs), the computational complexity of network performance analysis is rapidly escalating. As one of the most suitable mathematical tools for analyzing large-scale network topologies, stochastic geometry (SG) enables the representation of network performance metrics as functions of network parameters, thus offering low-complexity performance analysis solutions. However, choosing between planar and spherical models remains challenging. Planar models neglect Earth's curvature, causing deviations in high-altitude NTN analysis, yet are still often used for simplicity. This paper introduces relative error to quantify the gap between planar and spherical models, helping determine when planar modeling is sufficient. To calculate the relative error, we first propose a point process (PP) generation algorithm that simultaneously generates a pair of homogeneous and asymptotically similar planar and spherical PPs. We then introduce several typical similarity metrics, including topology-related and network-level metrics, and further develop a relative error estimation algorithm based on these metrics. In addition, we derive an analytical expression for the optimal planar altitude, which reduces computational complexity and provides theoretical support for planar approximation. Finally, numerical results investigate how deployment altitude and region affect NTN modeling, with case studies on HAP and LEO satellite constellations."
2508.00011,"Sixth-generation (6G) networks are designed to meet the hyper-reliable and low-latency communication (HRLLC) requirements of safety-critical applications such as autonomous driving. Integrating non-terrestrial networks (NTN) into the 6G infrastructure brings redundancy to the network, ensuring continuity of communications even under extreme conditions. In particular, high-altitude platform stations (HAPS) stand out for their wide coverage and low latency advantages, supporting communication reliability and enhancing information freshness, especially in rural areas and regions with infrastructure constraints. In this paper, we present reinforcement learning-based approaches using deep deterministic policy gradient (DDPG) to dynamically optimize the age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks. The proposed method improves information freshness and overall network reliability by enabling independent learning without centralized coordination. The findings reveal the potential of HAPS-supported solutions, combined with DDPG-based learning, for efficient AoI-aware resource allocation in platoon-based autonomous vehicle systems."
2508.0002,"In recent years, the satellite-aerial-ground integrated network (SAGIN) has become essential in meeting the increasing demands for global wireless communications. In SAGIN, high-altitude platforms (HAPs) can serve as communication hubs and act as relays to enhance communication performance. In this paper, we evaluate network performance and analyze the role of HAPs in SAGIN from the relay perspective. Based on this unique perspective, we introduce three metrics to evaluate the performance, named the average access data rate, the average backhaul data rate, and the backhaul rate exceedance probability (BREP). Considering the need for dynamic topology and interference analysis, we choose spherical stochastic geometry (SSG) as a tool and derive analytical expressions for the above metrics to achieve low-complexity performance evaluation. Specifically, we provide a closed-form expression for the end-to-end performance metric BREP. Given that there is no existing literature in the SSG field studying networks from a relay perspective, we specifically investigate the impact of satellite network topology on performance in our numerical results to further highlight the advantages of the SSG framework. Additionally, we analyze the minimum HAP transmission power required to maintain both short-term and long-term data rate demands."
2508.00028,"Spectrum resources are often underutilized across time and space, motivating dynamic spectrum access strategies that allow secondary users to exploit unused frequencies. A key challenge is predicting when and where spectrum will be available (i.e., unused by primary licensed users) in order to enable proactive and interference-free access. This paper proposes a scalable framework for spectrum availability prediction that combines a two-state Markov chain model of primary user activity with high-fidelity propagation models from the ITU-R (specifically Recommendations P.528 and P.2108). The Markov chain captures temporal occupancy patterns, while the propagation models incorporate path loss and clutter effects to determine if primary signals exceed interference thresholds at secondary user locations. By integrating these components, the proposed method can predict spectrum opportunities both in time and space with improved accuracy. We develop the system model and algorithm for the approach, analyze its scalability and computational efficiency, and discuss assumptions, limitations, and potential applications. The framework is flexible and can be adapted to various frequency bands and scenarios. The results and analysis show that the proposed approach can effectively identify available spectrum with low computational cost, making it suitable for real-time spectrum management in cognitive radio networks and other dynamic spectrum sharing systems."
2508.00042,"AI-native 6G networks promise unprecedented automation and performance by embedding machine-learning models throughout the radio access and core segments of the network. However, the non-stationary nature of wireless environments due to infrastructure changes, user mobility, and emerging traffic patterns, induces concept drifts that can quickly degrade these model accuracies. Existing methods in general are very domain specific, or struggle with certain type of concept drift. In this paper, we introduce two unsupervised, model-agnostic, batch concept drift detectors. Both methods compute an expected-utility score to decide when concept drift occurred and if model retraining is warranted, without requiring ground-truth labels after deployment. We validate our framework on two real-world wireless use cases in outdoor fingerprinting for localization and for link-anomaly detection, and demonstrate that both methods are outperforming classical detectors such as ADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an F1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus reducing the false alarm rate by up to 20 percentage points compared to the best classical detectors."
2508.00228,"In anticipation of the High Luminosity-LHC era, there is a critical need to oversee software readiness for upcoming growth in network traffic for production and user data analysis access. This paper looks into software and hardware required improvements in US-CMS Tier-2 sites to be able to sustain and meet the projected 400 Gbps bandwidth demands while tackling the challenge posed by varying latencies between sites. Specifically, our study focuses on identifying the performance of XRootD HTTP third-party copies across multiple 400 Gbps links and exploring different host and transfer configurations. Our approach involves systematic testing with variations in the number of origins per cluster and CPU allocations for each origin. By replicating real network conditions and creating network ""loops"" that traverse multiple switches across the wide area network, we are able to replicate authentic network conditions"
2508.00234,"Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to a significant increase in user demand for LLM services. However, cloud-based LLM services often suffer from high latency, unstable responsiveness, and privacy concerns. Therefore, multiple LLMs are usually deployed at the network edge to boost real-time responsiveness and protect data privacy, particularly for many emerging smart mobile and IoT applications. Given the varying response quality and latency of LLM services, a critical issue is how to route user requests from mobile and IoT devices to an appropriate LLM service (i.e., edge LLM expert) to ensure acceptable quality-of-service (QoS). Existing routing algorithms fail to simultaneously address the heterogeneity of LLM services, the interference among requests, and the dynamic workloads necessary for maintaining long-term stable QoS. To meet these challenges, in this paper we propose a novel deep reinforcement learning (DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM services. Due to the dynamic nature of the global state, we propose a dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN). Additionally, we introduce an action impact estimator and a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that our proposed algorithm significantly improves average QoS and computing resource efficiency compared to existing baselines."
2508.00256,"Low-altitude wireless networks (LAWNs) have the potential to revolutionize communications by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure communications in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure communications in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models (LLMs) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure communication tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications."
2508.00261,"Mobile edge computing (MEC) is a promising technique to improve the computational capacity of smart devices (SDs) in Internet of Things (IoT). However, the performance of MEC is restricted due to its fixed location and limited service scope. Hence, we investigate an unmanned aerial vehicle (UAV)-assisted MEC system, where multiple UAVs are dispatched and each UAV can simultaneously provide computing service for multiple SDs. To improve the performance of system, we formulated a UAV-based trajectory control and resource allocation multi-objective optimization problem (TCRAMOP) to simultaneously maximize the offloading number of UAVs and minimize total offloading delay and total energy consumption of UAVs by optimizing the flight paths of UAVs as well as the computing resource allocated to served SDs. Then, consider that the solution of TCRAMOP requires continuous decision-making and the system is dynamic, we propose an enhanced deep reinforcement learning (DRL) algorithm, namely, distributed proximal policy optimization with imitation learning (DPPOIL). This algorithm incorporates the generative adversarial imitation learning technique to improve the policy performance. Simulation results demonstrate the effectiveness of our proposed DPPOIL and prove that the learned strategy of DPPOIL is better compared with other baseline methods."
2508.00403,"Mamba has emerged as a powerful model for efficiently addressing tasks involving temporal and spatial data. Regarding the escalating heterogeneity and dynamics in wireless networks, Mamba holds the potential to revolutionize wireless communication and networking designs by balancing the trade-off between computational efficiency and effectiveness. This article presents a comprehensive overview of Mamba' applications in wireless systems. Specifically, we first analyze the potentials of Mamba for wireless signal processing tasks from the perspectives of long-range dependency modeling and spatial feature extraction. Then we propose two application frameworks for Mamba in wireless communications, i.e., replacement of traditional algorithms, and enabler of novel paradigms. Guided by the two frameworks, we conduct case studies on intelligent resource allocation and joint source and channel decoding to demonstrate Mamba's improvements in both feature enhancement and computational efficiency. Finally, we highlight critical challenges and outline potential research directions for Mamba in wireless communications and networking."
2508.00583,"Large vision models (LVMs) have emerged as a foundational paradigm in visual intelligence, achieving state-of-the-art performance across diverse visual tasks. Recent advances in LVMs have facilitated their integration into Internet of Things (IoT) scenarios, offering superior generalization and adaptability for vision-assisted network optimization. In this paper, we first investigate the functionalities and core architectures of LVMs, highlighting their capabilities across classification, segmentation, generation, and multimodal visual processing. We then explore a variety of LVM applications in wireless communications, covering representative tasks across the physical layer, network layer, and application layer. Furthermore, given the substantial model size of LVMs and the challenges of model retraining in wireless domains, we propose a progressive fine-tuning framework that incrementally adapts pretrained LVMs for joint optimization of multiple IoT tasks. A case study in low-altitude economy networks (LAENets) demonstrates the effectiveness of the proposed framework over conventional CNNs in joint beamforming and positioning tasks for Internet of drones, underscoring a promising direction for integrating LVMs into intelligent wireless systems."
2508.00616,"Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for realizing wave-domain signal processing, while the fixed SIMs will limit the communication performance of the system compared to the mobile SIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted communication system, where UAVs as base stations (BSs) can cache the data processed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance the communication performance. To this end, we formulate a UAV-SIM-based joint optimization problem (USBJOP) to comprehensively consider the association between UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of UAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and NP-hardness of USBJOP, we decompose it into three sub-optimization problems, which are the association between UAV-SIMs and users optimization problem (AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase shifts optimization problem (USPSOP). Then, these three sub-optimization problems are solved by an alternating optimization (AO) strategy. Specifically, AUUOP and ULOP are transformed to a convex form and then solved by the CVX tool, while we employ a layer-by-layer iterative optimization method for USPSOP. Simulation results verify the effectiveness of the proposed strategy under different simulation setups."
2508.00629,"The transition toward softwarized Radio Access Networks (RANs), driven by the Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through disaggregation and virtualization of base station functions. However, this shift introduces new challenges in managing CPU resources efficiently under strict real-time constraints. In particular, the interplay between latency-sensitive RAN workloads and general-purpose Operating System (OS) schedulers often leads to sub-optimal performance and unnecessary energy consumption. This work proposes a lightweight, programmable distributed application (dApp) deployed at the Distributed Unit (DU) level to dynamically orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging thread-level telemetry like context switches, Instructions Per Cycle (IPC), and cache metrics, to adapt CPU thread affinity, core isolation, and frequency scaling in real time. Unlike existing solutions, it requires no access to proprietary RAN software, hardware-specific features, or kernel modifications. Fully compliant with the O-RAN architecture and agnostic to the underlying RAN stack, the proposed solution introduces negligible overhead while improving energy efficiency and CPU utilization. Experimental results using a commercial-grade srsRAN deployment demonstrate consistent power savings without compromising real-time processing performance, highlighting the potential of low-latency dApps for fine-grained resource control in next-generation networks"
2508.00688,"Heterogeneous marine-aerial swarm networks encounter substantial difficulties due to targeted communication disruptions and structural weaknesses in adversarial environments. This paper proposes a two-step framework to strengthen the network's resilience. Specifically, our framework combines the node prioritization based on criticality with multi-objective topology optimization. First, we design a three-layer architecture to represent structural, communication, and task dependencies of the swarm networks. Then, we introduce the SurBi-Ranking method, which utilizes graph convolutional networks, to dynamically evaluate and rank the criticality of nodes and edges in real time. Next, we apply the NSGA-III algorithm to optimize the network topology, aiming to balance communication efficiency, global connectivity, and mission success rate. Experiments demonstrate that compared to traditional methods like K-Shell, our SurBi-Ranking method identifies critical nodes and edges with greater accuracy, as deliberate attacks on these components cause more significant connectivity degradation. Furthermore, our optimization approach, when prioritizing SurBi-Ranked critical components under attack, reduces the natural connectivity degradation by around 30%, achieves higher mission success rates, and incurs lower communication reconfiguration costs, ensuring sustained connectivity and mission effectiveness across multi-phase operations."
2508.00715,"Small satellites used for Earth observation generate vast amounts of high-dimensional data, but their operation in low Earth orbit creates a significant communication bottleneck due to limited contact times and harsh, varying channel conditions. While deep joint source-channel coding (DJSCC) has emerged as a promising technique, its practical application to the complex satellite environment remains an open question. This paper presents a comprehensive DJSCC framework tailored for satellite communications. We first establish a basic system, DJSCC-SAT, and integrate a realistic, multi-state statistical channel model to guide its training and evaluation. To overcome the impracticality of using separate models for every channel condition, we then introduce an adaptable architecture, ADJSCC-SAT, which leverages attention modules to allow a single neural network to adjust to a wide range of channel states with minimal overhead. Through extensive evaluation on Sentinel-2 multi-spectral data, we demonstrate that our adaptable approach achieves performance comparable to using multiple specialized networks while significantly reducing model storage requirements. Furthermore, the adaptable model shows enhanced robustness to channel estimation errors, outperforming the non-adaptable baseline. The proposed framework is a practical and efficient step toward deploying robust, adaptive DJSCC systems for real-world satellite missions."
2508.00735,"IP fragmentation and TCP segmentation allow for splitting large data packets into smaller ones, e.g., for transmission across network links of limited capacity. These mechanisms permit complete or partial overlaps with different data on the overlapping portions. IPv4, IPv6, and TCP reassembly policies, i.e., the data chunk preferences that depend on the overlap types, differ across protocol implementations. This leads to vulnerabilities, as NIDSes may interpret the packet differently from the monitored host OSes. Some NIDSes, such as Suricata or Snort, can be configured so that their policies are consistent with the monitored OSes. The first contribution of the paper is PYROLYSE, an audit tool that exhaustively tests and describes the reassembly policies of various IP and TCP implementation types. This tool ensures that implementations reassemble overlapping chunk sequences without errors. The second contribution is the analysis of PYROLYSE artifacts. We first show that the reassembly policies are much more diverse than previously thought. Indeed, by testing all the overlap possibilities for n <= 3 test case chunks and different testing scenarios, we observe from 14 to 20 different behaviors out of 23 tested implementations depending on the protocol. Second, we report eight errors impacting one OS, two NIDSes, and two embedded stacks, which can lead to security issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was assigned to a NIDS error. Finally, we show that implemented IP and TCP policies obtained through chunk pair testing are usually inconsistent with the observed triplet reassemblies. Therefore, contrarily to what they currently do, NIDSes or other network traffic analysis tools should not apply n = 2 pair policies when the number of overlapping chunks exceeds two."
2508.00792,"The Data Movement Manager (DMM) is a prototype interface that connects CERN's data management software, Rucio, with the Sofware-Defined Networking (SDN) service SENSE by ESNet. It enables SDN-enabled high-energy physics data flows using the existing worldwide LHC computing grid infrastructure. A key feature of DMM is transfer priority-based bandwidth allocation, optimizing network usage. Additionally, it provides fine-grained monitoring of underperforming flows by leveraging end-to-end data flow monitoring. This is achieved through access to host-level (network interface) throughput metrics and transfer-tool (FTS) data transfer job-level metrics. This paper details the design and implementation of DMM."
2508.01047,"This paper presents a novel TCP congestion control algorithm based on Deep Reinforcement Learning. The proposed approach utilizes Deep Q-Networks to optimize the congestion window (cWnd) by observing key network parameters and taking real-time actions. The algorithm is trained and evaluated within the NS-3 network simulator using the OpenGym interface. The results demonstrate significant improvements over traditional TCP New Reno in terms of latency and throughput, with better adaptability to changing network conditions. This study emphasizes the potential of reinforcement learning techniques for solving complex congestion control problems in modern networks."
2508.0106,"Managing connectivity in integrated satellite-terrestrial vehicular networks is critical for 6G, yet is challenged by dynamic conditions and partial observability. This letter introduces the Multi-Agent Actor-Critic with Satellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent reinforcement learning framework that enables vehicles to autonomously manage connectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure (V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the integration of a multi-head attention mechanism, which allows for robust state estimation even with fluctuating and limited information sharing among vehicles. The framework further leverages self-imitation learning (SIL) and fingerprinting to improve learning efficiency and real-time decisions. Simulation results, based on realistic SUMO traffic models and 3GPP-compliant configurations, demonstrate that MAAC-SAM outperforms state-of-the-art terrestrial and satellite-assisted baselines by up to 14% in transmission utility and maintains high estimation accuracy across varying vehicle densities and sharing levels."
2508.01298,"Content-Centric Networking (CCN) offers a novel architectural paradigm that seeks to address the inherent limitations of the prevailing Internet Protocol (IP)-based networking model. In contrast to the host-centric communication approach of IP networks, CCN prioritizes content by enabling direct addressing and routing based on content identifiers. The potential performance improvements of CCN can be further amplified through optimized management of coded data storage and transmission strategies. Decentralized Coded Caching (DCC) emerges as a promising technique that harnesses the collective caching power of distributed network elements. By strategically pre-positioning frequently accessed content closer to potential consumers during periods of low network utilization, DCC has the potential to mitigate content transfer rates during peak traffic periods. This paper proposes a series of fundamental modifications to the CCN architecture by integrating DCC. The proposed framework incorporates differentiated coding strategies tailored to user access privileges, thereby eliminating the overhead associated with queue-based searching. Additionally, the framework facilitates recoding of uncoded data encountered along the content delivery path. These combined methodologies demonstrably enhance network throughput, elevate cache hit ratios, and consequently, reduce content delivery latency compared to conventional CCN implementations."
2508.01805,"Current Multimodal Large Language Models (MLLMs) rely on centralized architectures and often suffer from poor alignment between the input task and their fixed visual encoding modules, which limits performance on diverse and dynamic visual tasks. With the increasing deployment of resource-efficient models on edge devices in wireless networks, a new opportunity emerges to dynamically use distributed vision experts for improved MLLM inference quality. To enable this, we propose M3LLM, where the Model Context Protocol (MCP) coordinates a mixture of vision experts to achieve distributed MLLMs. Specifically, MCP is an open protocol that structures the input task context into interpretable representations, enabling wireless network-aware coordination between the central model backbone and edge-hosted vision experts. Based on the MCP representation, M3LLM formulates vision expert routing as a joint optimization problem that balances task-expert semantic compatibility and channel performance. To solve the resulting gradient conflicts, we develop a dual-stream Soft Actor-Critic (SAC) algorithm with decoupled reward signals and introduce an Adaptive Stability Enhancement Module (ASEM) based on hierarchical Bayesian modeling to ensure effective routing. Experiments show that M3LLM improves task accuracy, reduces communication cost, and enhances expert routing adaptability under dynamic wireless network conditions."
2508.01898,"Video caching can significantly improve delivery efficiency and enhance quality of video streaming, which constitutes the majority of wireless communication traffic. Due to limited cache size, caching strategies must be designed to adapt to and dynamic user demand in order to maximize system revenue. The system revenue depends on the benefits of delivering the requested videos and costs for (a) transporting the files to the users and (b) cache replacement. Since the cache content at any point in time impacts the replacement costs in the future, demand predictions over multiple cache placement slots become an important prerequisite for efficient cache planning. Motivated by this, we introduce a novel two-stage privacy-preserving solution for revenue optimization in wireless video caching networks. First, we train a Transformer using privacy-preserving federated learning (FL) to predict multi-slot future demands. Given that prediction results are never entirely accurate, especially for longer horizons, we further combine global content popularity with per-user prediction results to estimate the content demand distribution. Then, in the second stage, we leverage these estimation results to find caching strategies that maximize the long-term system revenue. This latter problem takes on the form of a multi-stage knapsack problem, which we then transform to a integer linear program. Our extensive simulation results demonstrate that (i) our FL solution delivers nearly identical performance to that of the ideal centralized solution and outperforms other existing caching methods, and (ii) our novel revenue optimization approach provides deeper system performance insights than traditional cache hit ratio (CHR)-based optimization approaches."
2508.02001,"Encrypted traffic classification is vital for modern network management and security. To reduce reliance on handcrafted features and labeled data, recent methods focus on learning generic representations through pre-training on large-scale unlabeled data. However, current pre-trained models face two limitations originating from the adopted Transformer architecture: (1) Limited model efficiency due to the self-attention mechanism with quadratic complexity; (2) Unstable traffic scalability to longer byte sequences, as the explicit positional encodings fail to generalize to input lengths not seen during pre-training. In this paper, we investigate whether convolutions, with linear complexity and implicit positional encoding, are competitive with Transformers in encrypted traffic classification with pre-training. We first conduct a systematic comparison, and observe that convolutions achieve higher efficiency and scalability, with lower classification performance. To address this trade-off, we propose NetConv, a novel pre-trained convolution model for encrypted traffic classification. NetConv employs stacked traffic convolution layers, which enhance the ability to capture localized byte-sequence patterns through window-wise byte scoring and sequence-wise byte gating. We design a continuous byte masking pre-training task to help NetConv learn protocol-specific patterns. Experimental results on four tasks demonstrate that NetConv improves average classification performance by 6.88% and model throughput by 7.41X over existing pre-trained models."
2508.02031,"With the continuous development of network environments and technologies, ensuring cyber security and governance is increasingly challenging. Network traffic classification(ETC) can analyzes attributes such as application categories and malicious intent, supporting network management services like QoS optimization, intrusion detection, and targeted billing. As the prevalence of traffic encryption increases, deep learning models are relied upon for content-agnostic analysis of packet sequences. However, the emergence of new services and attack variants often leads to incremental tasks for ETC models. To ensure model effectiveness, incremental learning techniques are essential; however, recent studies indicate that neural networks experience declining plasticity as tasks increase. We identified plasticity issues in existing incremental learning methods across diverse traffic samples and proposed the PRIME framework. By observing the effective rank of model parameters and the proportion of inactive neurons, the PRIME architecture can appropriately increase the parameter scale when the model's plasticity deteriorates. Experiments show that in multiple encrypted traffic datasets and different category increment scenarios, the PRIME architecture performs significantly better than other incremental learning algorithms with minimal increase in parameter scale."
2508.02282,"Traffic classification plays a significant role in network service management. The advancement of deep learning has established pretrained models as a robust approach for this task. However, contemporary encrypted traffic classification systems face dual limitations. Firstly, pretrained models typically exhibit large-scale architectures, where their extensive parameterization results in slow inference speeds and high computational latency. Secondly, reliance on labeled data for fine-tuning restricts these models to predefined supervised classes, creating a bottleneck when novel traffic types emerge in the evolving Internet landscape. To address these challenges, we propose NetClus, a novel framework integrating pretrained models with distillation-enhanced clustering acceleration. During fine-tuning, NetClus first introduces a cluster-friendly loss to jointly reshape the latent space for both classification and clustering. With the fine-tuned model, it distills the model into a lightweight Feed-Forward Neural Network model to retain semantics. During inference, NetClus performs heuristic merge with near-linear runtime, and valid the cluster purity with newly proposed metrics ASI to identify emergent traffic types while expediting classification. Benchmarked against existing pretrained methods, NetClus achieves up to 6.2x acceleration while maintaining classification degradation below 1%."
2508.02373,"Future networks, such as 6G, will need to support a vast and diverse range of interconnected devices and applications, each with its own set of requirements. While traditional network management approaches will suffice, an automated solutions are becoming a must. However, network automation frameworks are prone to errors, and often they employ ML-based techniques that require training to learn how the network can be optimized. In this sense, network digital twins are a useful tool that allows for the simulation, testing, and training of AI models without affecting the real-world networks and users. This paper presents an AI-based Network Digital Twin (AI-NDT) that leverages a multi-layered knowledge graph architecture and graph neural networks to predict network metrics that directly affect the quality of experience of users. An evaluation of the four most prominent Graph Neural Networks (GNN) architectures was conducted to assess their effectiveness in developing network digital twins. We trained the digital twin on publicly available measurement data from RIPE Atlas, therefore obtaining results close to what is expected in real-world applications. The results show that among the four architectures evaluated, GraphTransformer presents the best performance. However, other architectures might fit better in scenarios where shorter training time is important, while also delivering acceptable results. The results of this work are indicative of what might become common practice for proactive network management, offering a scalable and accurate solution aligned with the requirements of the next-generation networks."
2508.02571,"Accurate AS-to-organization mapping underpins Internet measurement and security, yet registries are fragmented, PeeringDB is narrow, and routing views reflect connectivity rather than ownership. We take a pragmatic step: ASINT integrates curated web evidence with retrieval-guided LLM techniques and strict, evidence-cited validation to infer two relations (aliases and directed parent-child) and then revalidates them conservatively. To keep the dataset sustainable, we operate a public dashboard and API where operators can inspect per-ASN evidence and submit feedback that seeds refreshes.At scale, ASINT maps 112,172 ASNs into 82,840 organization families and, on overlapping AS sets, yields fewer, larger families with 21-24% more multi-AS groups than prior datasets (i.e., CAIDA AS2Org [11], AS2ORG+ [4], AS-Sibling [10], and Borges [28]). Quality is high in practice: ASINT achieves a precision of 0.9608, a recall of 0.9915 and an accuracy of 0.9752 under manual validation. Public deployment further drew operator-submitted reports for 595 ASNs across 106 organizations, with only 6 errors (99.0% observed clustering accuracy), with feedback coming from network operators across all RIR regions.Better organization context improves downstream analyses: +27.5% intra-organization RPKI misconfiguration detections, -9.4% benign hijack alerts, and -5.9% corrections to cases mislabeled as IP leasing.We release code, datasets, and the operator platform with APIs; given persistent ambiguity in organizational names and the continual evolution of corporate structures, an operator-in-the-loop process is essential; the platform records per ASN feedback with provenance and incorporates it into periodic refreshes and retraining. The methodology is model-agnostic and stands to improve further as base LLMs advance."
2508.0296,"The increasing complexity of wireless environments, characterized by user mobility and dynamic obstructions, poses challenges for the maintenance of Line-of-Sight (LoS) connectivity. Mobile base stations (gNBs) stand as a promising solution by physically relocating to restore or sustain LoS, thereby necessitating the development of intelligent algorithms for autonomous movement control.As part of the CONVERGE research project, which is developing an experimental chamber to integrate computer vision (CV) into mobile networks and enhance Quality of Service (QoS) in dynamic wireless environments, this paper presents two key contributions. First, we introduce the CONVERGE Chamber Simulator (CC-SIM), a 3D simulation environment for developing, training, and validating mobility control algorithms for mobile gNBs. CC-SIM models user and obstacle mobility, visual occlusion, and Radio Frequency (RF) propagation behavior. It supports both offline reinforcement learning and real-time testing through tight integration with a standalone 5G system via the OpenAirInterface (OAI) RF simulator, enabling validation under realistic network conditions.Second, leveraging CC-SIM, we develop a Deep Q-Network (DQN) agent that learns to reposition the gNB proactively in response to dynamic environmental changes. Experiments across three representative use cases show that the trained agent significantly reduces LoS blockage time - by up to 42% - when compared to static deployments. These results highlight the effectiveness of learning-based mobility control in adaptive next-generation wireless networks."
2508.03095,"Autonomous AI agents now operate across cloud, enterprise, and decentralized domains, creating demand for registry infrastructures that enable trustworthy discovery, capability negotiation, and identity assurance. We analyze five prominent approaches: (1) MCP Registry (centralized publication ofthis http URLdescriptors), (2) A2A Agent Cards (decentralized self-describing JSON capability manifests), (3) AGNTCY Agent Directory Service (IPFS Kademlia DHT content routing extended for semantic taxonomy-based content discovery, OCI artifact storage, and Sigstore-backed integrity), (4) Microsoft Entra Agent ID (enterprise SaaS directory with policy and zero-trust integration), and (5) NANDA Index AgentFacts (cryptographically verifiable, privacy-preserving fact model with credentialed assertions). Using four evaluation dimensions: security, authentication, scalability, and maintainability, we surface architectural trade-offs between centralized control, enterprise governance, and distributed resilience. We conclude with design recommendations for an emerging Internet of AI Agents requiring verifiable identity, adaptive discovery flows, and interoperable capability semantics."
2508.03101,"The proliferation of autonomous AI agents represents a paradigmatic shift from traditional web architectures toward collaborative intelligent systems requiring sophisticated mechanisms for discovery, authentication, capability verification, and secure collaboration across heterogeneous protocol environments. This paper presents a comprehensive framework addressing the fundamental infrastructure requirements for secure, trustworthy, and interoperable AI agent ecosystems. We introduce the NANDA (Networked AI Agents in a Decentralized Architecture) framework, providing global agent discovery, cryptographically verifiable capability attestation through AgentFacts, and cross-protocol interoperability across Anthropic's Modal Context Protocol (MCP), Google's Agent-to-Agent (A2A), Microsoft's NLWeb, and standard HTTPS communications. NANDA implements Zero Trust Agentic Access (ZTAA) principles, extending traditional Zero Trust Network Access (ZTNA) to address autonomous agent security challenges including capability spoofing, impersonation attacks, and sensitive data leakage. The framework defines Agent Visibility and Control (AVC) mechanisms enabling enterprise governance while maintaining operational autonomy and regulatory compliance. Our approach transforms isolated AI agents into an interconnected ecosystem of verifiable, trustworthy intelligent services, establishing foundational infrastructure for large-scale autonomous agent deployment across enterprise and consumer environments. This work addresses the critical gap between current AI agent capabilities and infrastructure requirements for secure, scalable, multi-agent collaboration, positioning the foundation for next-generation autonomous intelligent systems."
2508.03113,"AdaptiveResolver is a dynamic microservice architecture designed to address the limitations of static endpoint resolution for AI agent communication in distributed, heterogeneous environments. Unlike traditional DNS or static URLs, AdaptiveResolver enables context-aware, real-time selection of communication endpoints based on factors such as geographic location, system load, agent capabilities, and security threats. Agents advertise their Agent Name and context requirements through Agent Fact cards in an Agent Registry/Index. A requesting Agent discovers a Target Agent using the registry. The Requester Agent can then resolve the Target Agent Name to obtain a tailored communication channel to the agent based on actual environmental context between the agents. The architecture supports negotiation of trust, quality of service, and resource constraints, facilitating flexible, secure, and scalable agent-to-agent interactions that go beyond the classic client-server model. AdaptiveResolver provides a foundation for robust, future-proof agent communication that can evolve with increasing ecosystem complexity."
2508.03146,"This work focuses on the development and assessment of modern wireless Internet of Things (IoT) architectures, with relevance to emerging 5G and beyond applications. To analyze the growing demands for data, and their impact, we built an IEEE 802.11ah (WiFi Halow) office testbed for real-world experimentation. This deployment allows us to uncover the practical performance and scalability limitations of such networks under various challenging scenarios. To the best of our knowledge, this is the first study to consider complex real-world IEEE 802.11ah implementations, aiming specifically to reveal unexpected performance behaviors, such as significant throughput degradation arising in closely deployed wireless links. Our findings show that intense network contention and Adjacent Channel Interference (ACI), drastically impact the performance of the wireless links involved. Beyond evaluating network performance, our experimental analysis also considers the energy consumption of the devices under test, offering a more holistic perspective on the feasibility of IEEE 802.11ah in real-world deployments. The effective disclosure of such unexpected phenomena, can lead to well planned decisions and energy consumption optimization across the IoT to Cloud continuum."
2508.03171,"In this paper, we propose an unmanned aerial vehicle (UAV)-assisted federated learning (FL) framework that jointly optimizes UAV trajectory, user participation, power allocation, and data volume control to minimize overall system energy consumption. We begin by deriving the convergence accuracy of the FL model under multiple local updates, enabling a theoretical understanding of how user participation and data volume affect FL learning performance. The resulting joint optimization problem is non-convex; to address this, we employ alternating optimization (AO) and successive convex approximation (SCA) techniques to convexify the non-convex constraints, leading to the design of an iterative energy consumption optimization (ECO) algorithm. Simulation results confirm that ECO consistently outperform existing baseline schemes."
2508.03287,"Cloud-based offloading helps address energy consumption and performance challenges in executing resource-intensive vehicle algorithms. Utilizing 5G, with its low latency and high bandwidth, enables seamless vehicle-to-cloud integration. Currently, only non-standalone 5G is publicly available, and real-world applications remain underexplored compared to theoretical studies. This paper evaluates 5G non-standalone networks for cloud execution of vehicle functions, focusing on latency, Round Trip Time, and packet delivery. Tests used two AI-based algorithms -- emotion recognition and object recognition -- along an 8.8 km route in Baden-Wrttemberg, Germany, encompassing urban, rural, and forested areas. Two platforms were analyzed: a cloudlet in Frankfurt and a cloud in Mannheim, employing various deployment strategies like conventional applications and containerized and container-orchestrated setups. Key findings highlight an average signal quality of 84 %, with no connectivity interruptions despite minor drops in built-up areas. Packet analysis revealed a Packet Error Rate below 0.1 % for both algorithms. Transfer times varied significantly depending on the geographical location and the backend servers' network connections, while processing times were mainly influenced by the computation hardware in use. Additionally, cloud offloading seems only be a suitable option, when a round trip time of more than 150 ms is possible."
2508.03321,"While TLS has become the de-facto standard for end-to-end security, its use to secure critical communication in evolving industrial IoT scenarios is severely limited by prevalent resource constraints of devices and networks. Most notably, the TLS handshake to establish secure connections incurs significant bandwidth and processing overhead that often cannot be handled in constrained environments. To alleviate this situation, we present BiTHaC which realizes bidirectional TLS handshake caching by exploiting that significant parts of repeated TLS handshakes, especially certificates, are static. Thus, redundant information neither needs to be transmitted nor corresponding computations performed, saving valuable bandwidth and processing resources. By implementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth consumption of TLS handshakes by up to 61.1% and the computational overhead by up to 8.5%, while incurring only well-manageable memory overhead and preserving the strict security guarantees of TLS."
2508.03674,"We develop Morphlux, a server-scale programmable photonic fabric to interconnect accelerators within servers. We show that augmenting state-of-the-art torus-based ML data-centers with Morphlux can improve the bandwidth of tenant compute allocations by up to 66%, reduce compute fragmentation by up to 70%, and minimize the blast radius of chip failures. We develop a novel end-to-end hardware prototype of Morphlux to demonstrate these performance benefits which translate to 1.72X improvement in training throughput of ML models. By rapidly programming the server-scale fabric in our hardware testbed, Morphlux can replace a failed accelerator chip with a healthy one in 1.2 seconds."
2508.03862,"Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial Vehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility, such as air taxis. A key challenge in these high-mobility aerial corridors is ensuring reliable connectivity, where frequent handovers can degrade network performance. To resolve this, we present a Context-Aware Smart Handover (CASH) protocol that uses a forward-looking scoring mechanism based on UAV trajectory to make proactive handover decisions. We evaluate the performance of the proposed CASH against existing handover protocols in a custom-built simulator. Results show that CASH reduces handover frequency by up to 78% while maintaining low outage probability. We then investigate the impact of base station density and safety margin on handover performance, where their optimal setups are empirically obtained to ensure reliable UAM communication."
2508.03891,"Accurately classifying the application types of network traffic using deep learning models has recently gained popularity. However, we find that these classifiers do not perform well on real-world traffic data due to the presence of non-application-specific generic background traffic originating from advertisements, analytics, shared APIs, and trackers. Unfortunately, state-of-the-art application classifiers overlook such traffic in curated datasets and only classify relevant application traffic. To address this issue, when we label and train using an additional class for background traffic, it leads to additional confusion between application and background traffic, as the latter is heterogeneous and encompasses all traffic that is not relevant to the application sessions. To avoid falsely classifying background traffic as one of the relevant application types, a reliable confidence measure is warranted, such that we can refrain from classifying uncertain samples. Therefore, we design a Gaussian Mixture Model-based classification framework that improves the indication of the deep learning classifier's confidence to allow more reliable classification."
2508.04004,"Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is challenging because the performance emerges from the tight coupling of propagation, beam management, scheduling, and higher-layer interactions. System-level simulation is therefore indispensable, yet the vast majority of studies rely on the statistical 3GPP channel models. These are well suited to capture average behavior across many statistical realizations, but cannot reproduce site-specific phenomena such as corner diffraction, street-canyon blockage, or deterministic line-of-sight conditions and angle-of-departure/arrival relationships that drive directional links. This paper extends 5G-LENA, an NR module for the system-level Network Simulator 3 (ns-3), with a trace-based channel model that processes the Multipath Components (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer (RT)) or measurement campaigns. Our module constructs frequency-domain channel matrices and feeds them to the existing Physical (PHY)/Medium Access Control (MAC) stack without any further modifications. The result is a geometry-based channel model that remains fully compatible with the standard 3GPP implementation in 5G-LENA, while delivering site-specific geometric fidelity. This new module provides a key building block toward Digital Twin (DT) capabilities by offering realistic site-specific channel modeling, unlocking studies that require site awareness, including beam management, blockage mitigation, and environment-aware sensing. We demonstrate its capabilities for precise beam-steering validation and end-to-end metric analysis. In both cases, the trace-driven engine exposes performance inflections that the statistical model does not exhibit, confirming its value for high-fidelity system-level cellular networks research and as a step toward DT applications."
2508.04015,"The proliferation of large-scale artificial intelligence and data-intensive applications has spurred the development of Computing Power Networks (CPNs), which promise to deliver ubiquitous and on-demand computational resources. However, the immense energy consumption of these networks poses a significant sustainability challenge. Simultaneously, power grids are grappling with the instability introduced by the high penetration of intermittent renewable energy sources (RES). This paper addresses these dual challenges through a novel Two-Stage Co-Optimization (TSCO) framework that synergistically manages power system dispatch and CPN task scheduling to achieve low-carbon operations. The framework decomposes the complex, large-scale problem into a day-ahead stochastic unit commitment (SUC) stage and a real-time operational stage. The former is solved using Benders decomposition for computational tractability, while in the latter, economic dispatch of generation assets is coupled with an adaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL) agent. This agent makes intelligent, carbon-aware decisions by responding to dynamic grid conditions, including real-time electricity prices and marginal carbon intensity. Through extensive simulations on an IEEE 30-bus system integrated with a CPN, the TSCO framework is shown to significantly outperform baseline approaches. Results demonstrate that the proposed framework reduces total carbon emissions and operational costs, while simultaneously decreasing RES curtailment by more than 60% and maintaining stringent Quality of Service (QoS) for computational tasks."
2508.0415,"This article introduces a comprehensive metaverse framework, which is designed for the simulation, emulation, and interaction with wireless systems. The proposed framework integrates core metaverse technologies such as extended reality (XR), digital twins (DTs), artificial intelligence (AI), internet of things (IoT), blockchain, and advanced 6G networking solutions to create a dynamic, immersive platform for both system development and management. By leveraging XR, users can visualize and engage with complex systems, while DTs enable real-time monitoring and optimization. AI generates the three-dimensional (3D) content, enhances decision-making and system performance, whereas IoT devices provide real-time sensor data for boosting the simulation accuracy. Additionally, blockchain ensures secure, decentralized interactions, and 5G/6G networks offer the necessary infrastructure for seamless, low-latency communication. This framework serves as a robust tool for exploring, developing, and optimizing wireless systems, aiming to provide valuable insights into the future of networked environments."
2508.04317,"Simulation tools are commonly used in the development and testing of new protocols or new networks. However, as satellite networks start to grow to encompass thousands of nodes, and as companies and space agencies begin to realize the interplanetary internet, existing satellite and network simulation tools have become impractical for use in this context.We therefore present the Deep Space Network Simulator (DSNS): a new network simulator with a focus on large-scale satellite networks. We demonstrate its improved capabilities compared to existing offerings, showcase its flexibility and extensibility through an implementation of existing protocols and the DTN simulation reference scenarios recommended by CCSDS, and evaluate its scalability, showing that it exceeds existing tools while providing better fidelity.DSNS provides concrete usefulness to both standards bodies and satellite operators, enabling fast iteration on protocol development and testing of parameters under highly realistic conditions. By removing roadblocks to research and innovation, we can accelerate the development of upcoming satellite networks and ensure that their communication is both fast and secure."
2508.04415,"The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary healthcare paradigm, shows promise for epidemic control. This paper explores the potential of using molecular communication (MC) to address the challenges in constructing IoBNT for epidemic prevention, specifically focusing on modeling viral transmission, detecting the virus/infected individuals, and identifying virus mutations. First, the MC channels in macroscale and microscale scenarios are discussed to match viral transmission in both scales separately. Besides, the detection methods for these two scales are also studied, along with the localization mechanism designed for the virus/infected individuals. Moreover, an identification strategy is proposed to determine potential virus mutations, which is validated through simulation using the ORF3a protein as a benchmark. Finally, open research issues are discussed. In summary, this paper aims to analyze viral transmission through MC and combat viral spread using signal processing techniques within MC."
2508.04526,"Traditional security architectures are becoming more vulnerable to distributed attacks due to significant dependence on trust. This will further escalate when implementing agentic AI within the systems, as more components must be secured over a similar distributed space. These scenarios can be observed in consumer technologies, such as the dense Internet of things (IoT). Here, zero-trust architecture (ZTA) can be seen as a potential solution, which relies on a key principle of not giving users explicit trust, instead always verifying their privileges whenever a request is made. However, the overall security in ZTA is managed through its policies, and unverified policies can lead to unauthorized access. Thus, this paper explores challenges and solutions for ZTA policy design in the context of distributed networks, which is referred to as zero-trust distributed networks (ZTDN). This is followed by a case-study on formal verification of policies using UPPAAL. Subsequently, the importance of accountability and responsibility in the system's security is discussed."
2508.04556,"Telecommunications and computer vision have evolved independently. With the emergence of high-frequency wireless links operating mostly in line-of-sight, visual data can help predict the channel dynamics by detecting obstacles and help overcoming them through beamforming or handover techniques.This paper proposes a novel architecture for delivering real-time radio and video sensing information to O-RAN xApps through a multi-agent approach, and introduces a new video function capable of generating blockage information for xApps, enabling Integrated Sensing and Communications. Experimental results show that the delay of sensing information remains under 1\,ms and that an xApp can successfully use radio and video sensing information to control the 5G/6G RAN in real-time."
2508.0513,"This paper presents a joint framework that integrates reconfigurable intelligent surfaces (RISs) with Terahertz (THz) communications and non-orthogonal multiple access (NOMA) to enhance smart industrial communications. The proposed system leverages the advantages of RIS and THz bands to improve spectral efficiency, coverage, and reliability key requirements for industrial automation and real-time communications in future 6G networks and beyond. Within this framework, two power allocation strategies are investigated: the first optimally distributes power between near and far industrial nodes, and the second prioritizes network demands to enhance system performance further. A performance evaluation is conducted to compare the sum rate and outage probability against a fixed power allocation scheme. Our scheme achieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results validate the theoretical analysis, demonstrating the effectiveness and robustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial communications."
2508.05249,"This paper presents the concept, architectural design, and performance evaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to User Equipment (UE) in areas with limited fixed 5G infrastructures or subject to adverse radio conditions. We consider two main approaches to MC design: an overlay model, where the MC obtains backhaul connectivity from a 5G overlay network, and an Integrated Access and Backhaul (IAB)-based model, discussing their protocol stacks and architectural implications. In order to validate the MC's performance, we employ an emulation-based testbed using the OpenAirInterface (OAI) implementation, considering different MC positions. The results validate the MC concept and demonstrate that MC positioning significantly influences network performance. This paper has the potential to aid network operators and service providers in selecting and deploying MC architectures for temporary coverage extension and capacity reinforcement in different environments, including seaports, industrial scenarios, and public safety."
2508.06432,"In this work, we aim to address the challenge of slice provisioning in edge-based mobile networks. We propose a solution that learns a service function chain placement policy for Network Slice Requests, to maximize the request acceptance rate, while minimizing the average node resource utilization. To do this, we consider a Hierarchical Multi-Armed Bandit problem and propose a two-level hierarchical bandit solution which aims to learn a scalable placement policy that optimizes the stated objectives in an online manner. Simulations on two real network topologies show that our proposed approach achieves 5% average node resource utilization while admitting over 25% more slice requests in certain scenarios, compared to baseline methods."
2508.06468,"Network Slicing has emerged as a powerful technique to enable cost-effective, multi-tenant communications and services over a shared physical mobile network infrastructure. One major challenge of service provisioning in slice-enabled networks is the uncertainty in the demand for the limited network resources that must be shared among existing slices and potentially new Network Slice Requests. In this paper, we consider admission control of Network Slice Requests in an online setting, with the goal of maximizing the long-term revenue received from admitted requests. We model the Slice Admission Control problem as an Online Multidimensional Knapsack Problem and present two reservation-based policies and their algorithms, which have a competitive performance for Online Multidimensional Knapsack Problems. Through Monte Carlo simulations, we evaluate the performance of our online admission control method in terms of average revenue gained by the Infrastructure Provider, system resource utilization, and the ratio of accepted slice requests. We compare our approach with those of the online First Come First Serve greedy policy. The simulation's results prove that our proposed online policies increase revenues for Infrastructure Providers by up to 12.9 % while reducing the average resource consumption by up to 1.7% In particular, when the tenants' economic inequality increases, an Infrastructure Provider who adopts our proposed online admission policies gains higher revenues compared to an Infrastructure Provider who adopts First Come First Serve."
2508.06615,"The Iris File Extension (IFE) is a low overhead performance-oriented whole slide image (WSI) file format designed to improve the image rendering experience for pathologists and simplify image management for system administrators. However, static hypertext transfer protocol (HTTP) file servers cannot natively stream subregions of high-resolution image files, such as the IFE. The majority of contemporary WSI viewer systems are designed as browser-based web applications and leverage OpenSeaDragon as the tile-based rendering framework. These systems convert WSI files to Deep Zoom Images (DZI) for compatibility with simple static HTTP file servers. To address this limitation, we have developed the Iris RESTful Server, a low-overhead HTTP server with a RESTful API that is natively compatible with the DICOMweb WADO-RS API. Written in C++ with Boost Beast HTTP and Asio networking libraries atop the public IFE libraries, the server offers both security and high performance. Testing shows that a single Raspberry Pi equivalent system can handle a peak of 5,061 req/s (average 3,883 req/s) with a median latency of 21 ms on a private (i.e. hospital) network. We also developed and merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful API, into the next OpenSeaDragon release, enabling simple and immediate drop-in replacement of DZI images within WSI viewer stacks. Designed as a secure cross-origin resource sharing microservice, this architecture includes detailed deployment instructions for new or existing WSI workflows, and the publicthis http URLsubdomain is provided as a development tool to accelerate WSI web viewer development. All relevant Iris software is available under the open-source MIT software license."
2508.06616,"With the emergence of 6G, mobile networks are becoming increasingly heterogeneous and dynamic, necessitating advanced automation for efficient management. Intent-Driven Networks (IDNs) address this by translating high-level intents into optimization policies. Large Language Models (LLMs) can enhance this process by understanding complex human instructions to enable adaptive, intelligent automation. Given the rapid advancements in Generative AI (GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated Radio Access Network (RAN) environments is both timely and critical. This article provides such a survey, along with a case study on a hierarchical learning-enabled IDN architecture that integrates GenAI across three key stages: intent processing, intent validation, and intent execution. Unlike most existing approaches that apply GenAI in the form of LLMs for intent processing only, we propose a hierarchical framework that introduces GenAI across all three stages of IDN. To demonstrate the effectiveness of the proposed IDN management architecture, we present a case study based on the latest GenAI architecture named Mamba. The case study shows how the proposed GenAI-driven architecture enhances network performance through intelligent automation, surpassing the performance of the conventional IDN architectures."
2508.06975,"Terahertz (THz) communication offers a promising solution for high-throughput wireless systems. However, the severe path loss of THz signals raises concerns about its effectiveness compared to radio frequency (RF) communication. In this article, we establish the first stochastic geometry (SG)-based analytical framework for routing in THz systems. We develop a stepwise optimization approach to maximize throughput, including power allocation, relay selection, and number of hops design. Analytical expressions for throughput and coverage probability are derived under the SG framework, enabling low complexity and scalable performance evaluation. Numerical results show that the proposed stepwise-optimal routing strategies not only outperform existing SG-based methods but also approach the ideal upper bound. Moreover, we compare the throughput and coverage performance of THz and RF routing and demonstrate the applications of the proposed analytical framework and routing strategies in system parameter design and unmanned aerial vehicle networks."
2508.07001,"With wireless devices increasingly forming a unified smart network for seamless, user-friendly operations, random access (RA) medium access control (MAC) design is considered a key solution for handling unpredictable data traffic from multiple terminals. However, it remains challenging to design an effective RA-based MAC protocol to minimize collisions and ensure transmission fairness across the devices. While existing multi-agent reinforcement learning (MARL) approaches with centralized training and decentralized execution (CTDE) have been proposed to optimize RA performance, their reliance on centralized training and the significant overhead required for information collection can make real-world applications unrealistic. In this work, we adopt a fully decentralized MARL architecture, where policy learning does not rely on centralized tasks but leverages consensus-based information exchanges across devices. We design our MARL algorithm over an actor-critic (AC) network and propose exchanging only local rewards to minimize communication overhead. Furthermore, we provide a theoretical proof of global convergence for our approach. Numerical experiments show that our proposed MARL algorithm can significantly improve RA network performance compared to other baselines."
2508.07194,"Internet censorship continues to impact billions of people worldwide, and measurement of it remains an important focus of research. However, most Internet censorship measurements have focused solely on the IPv4 Internet infrastructure. Yet, more clients and servers are available over IPv6: According to Google, over a third of their users now have native IPv6 access. Given the slow-but-steady rate of IPv6 adoption, it is important to understand its impact on censorship. In this paper, we measure and analyze how censorship differs over IPv6 compared to the well-studied IPv4 censorship systems in use today. We perform a comprehensive global study of censorship across an array of commonly censored protocols, including HTTP, DNS, and TLS, on both IPv4 and IPv6, and compare the results. We find that there are several differences in how countries censor IPv6 traffic, both in terms of IPv6 resources, and in where and what blocklists or technologies are deployed on IPv6 networks. Many of these differences are not all-or-nothing: we find that most censors have some capacity to block in IPv6, but are less comprehensive or less reliable compared to their IPv4 censorship systems. Our results suggest that IPv6 offers new areas for censorship circumvention researchers to explore, providing potentially new ways to evade censors. As more users gain access to IPv6 addresses and networks, there will be a need for tools that take advantage of IPv6 techniques and infrastructure to bypass censorship."
2508.07197,"Internet censorship impacts large segments of the Internet, but so far, prior work has focused almost exclusively on performing measurements using IPv4. As the Internet grows, and more users connect, IPv6 is increasingly supported and available to users and servers alike. But despite this steady growth, it remains unclear if the information control systems that implement censorship (firewalls, deep packet inspection, DNS injection, etc) are as effective with IPv6 traffic as they are with IPv4. In this paper, we perform the first global measurement of DNS censorship on the IPv6 Internet. Leveraging a recent technique that allows us to discover IPv6-capable open resolvers (along with their corresponding IPv4 address), we send over 20 million A and AAAA DNS requests to DNS resolvers worldwide, and measure the rate at which they block, at the resolver, network, and country level as well examine the characteristics of blocked domains. We observe that while nearly all censors support blocking IPv6, their policies are inconsistent with and frequently less effective than their IPv4 censorship infrastructure. Our results suggest that supporting IPv6 censorship is not all-or-nothing: many censors support it, but poorly. As a result, these censors may have to expend additional resources to bring IPv6 censorship up to parity with IPv4. In the meantime, this affords censorship circumvention researchers a new opportunity to exploit these differences to evade detection and blocking."
2508.07394,"The design of communication systems has traditionally focused on the reliable and timely delivery of data. However, the scalability challenges faced by the evolution to a 6G-driven society demand new communication paradigms that carefully curate the content being transmitted. This paper envisions a joint semantic and task-oriented communication paradigm where Connected and Autonomous Vehicles (CAVs) transmit only the information necessary to convey the desired meaning that is relevant to the intended receivers based on the communication context. The V2X domain offers a unique environment for the development of the envisioned semantic and task-oriented communications paradigm, as CAVs are native semantic devices, and the V2X domain is rich in contextual information. This contextual information can be leveraged to estimate the relevance that information may have for the intended receivers. We illustrate and quantitatively evaluate the potential benefits of semantic and task-oriented V2X communications. Numerical results show that by focusing on the transmission of the most relevant information for the intended receivers, semantic and task-oriented V2X communications can achieve a two-fold improvement in communication efficiency, which will significantly benefit the scalability of V2X networks."
2508.07506,"We introduce new tools and vantage points to develop and integrate proactive techniques to attract IPv6 scan traffic, thus enabling its analysis. By deploying the largest-ever IPv6 proactive telescope in a production ISP network, we collected over 600M packets of unsolicited traffic from 1.9k Autonomous Systems in 10 months. We characterized the sources of unsolicited traffic, evaluated the effectiveness of five major features across the network stack, and inferred scanners' sources of target addresses and their strategies."
2508.07578,"This paper investigates the fair-effective communication and robustness in imperfect and energy-constrained underwater acoustic sensor networks (IC-UASNs). Specifically, we investigate the impact of unexpected node malfunctions on the network performance under the time-varying acoustic channels. Each node is expected to satisfy Quality of Service (QoS) requirements. However, achieving individual QoS requirements may interfere with other concurrent communications. Underwater nodes rely excessively on the rationality of other underwater nodes when guided by fully cooperative approaches, making it difficult to seek a trade-off between individual QoS and global fair-effective communications under imperfect conditions. Therefore, this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that achieves fair-effective communication and robustness in IC-UASNs. The approach is distributed multi-agent reinforcement learning (MARL)-based, and the objectives are twofold. On the one hand, each intelligent node individually decides the transmission power to simultaneously optimize individual and global performance. On the other hand, advanced training algorithms are developed to provide imperfect environments for training robust models that can adapt to the time-varying acoustic channels and handle unexpected node failures in the network. Numerical results are presented to validate our proposed approach."
2508.07604,"Integrated Access and Backhaul (IAB) is critical for dense 5G and beyond deployments, especially in mmWave bands where fiber backhaul is infeasible. We propose a novel Deep Reinforcement Learning (DRL) framework for joint link scheduling and resource slicing in dynamic, interference-prone IAB networks. Our method integrates a greedy Double Deep Q-Network (DDQN) scheduler to activate access and backhaul links based on traffic and topology, with a multi-agent DDQN allocator for bandwidth and antenna assignment across network slices. This decentralized approach respects strict antenna constraints and supports concurrent scheduling across heterogeneous links. Evaluations across 96 dynamic topologies show 99.84 percent scheduling accuracy and 20.90 percent throughput improvement over baselines. The framework's efficient operation and adaptability make it suitable for dynamic and resource-constrained deployments, where fast link scheduling and autonomous backhaul coordination are vital."
2508.07679,"Underwater wireless sensor networks (UWSNs) stand as promising technologies facilitating diverse underwater applications. However, the major design issues of the considered system are the severely limited energy supply and unexpected node malfunctions. This paper aims to provide fair, efficient, and reliable (FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs). Therefore, we formulate a FER-communication optimization problem (FERCOP) and propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through joint link scheduling and power allocation, which automatically learns scheduling algorithms without human intervention. However, conventional RL methods are unable to address the challenges posed by underwater environments and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs and propose an advanced training mechanism to deal with complex acoustic channels, limited energy supplies, and unexpected node malfunctions. Simulation results demonstrate the superiority of the proposed ICRL-JSA scheme with an advanced training mechanism compared to various benchmark algorithms."
2508.07778,"Next-generation open radio access networks (O-RAN) continuously stream tens of key performance indicators (KPIs) together with raw in-phase/quadrature (IQ) samples, yielding ultra-high-dimensional, non-stationary time series that overwhelm conventional transformer architectures. We introduce a reservoir-augmented masked autoencoding transformer (RA-MAT). This time series foundation model employs echo state network (ESN) computing with masked autoencoding to satisfy the stringent latency, energy efficiency, and scalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESN rapidly projects each temporal patch into a rich dynamical embedding without backpropagation through time, converting the quadratic self-attention bottleneck into a lightweight linear operation. These embeddings drive a patch-wise masked autoencoder that reconstructs 30% randomly masked patches, compelling the encoder to capture both local dynamics and long-range structure from unlabeled data. After self-supervised pre-training, RA-MAT is fine-tuned with a shallow task head while keeping the reservoir and most transformer layers frozen, enabling low-footprint adaptation to diverse downstream tasks such as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MAT achieved sub-0.06 mean squared error (MSE) on several continuous and discrete KPIs. This work positions RA-MAT as a practical pathway toward real-time, foundation-level analytics in future 6G networks."
2508.07882,"A new class of Wireless Sensor Network has emerged whereby multiple nodes transmit data simultaneously, exploiting constructive interference to enable data collection frameworks with low energy usage and latency. This paper presents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a scalable, resilient framework for Wireless Sensor Networks that leverages constructive interference and operates effectively under stringent resource constraints. Using constructive interference requires all nodes to transmit the same packet at the same time, thus, only one source node can send data per time slot. STAIR uses coarse-grained topology information to flood a selected subset of the network, relaying sensor readings from individual nodes during their allocated time slots. A submodular optimisation algorithm with proven quality bounds determines near-optimal sensor activation locations and times, aiming to minimise the sum of mean squared prediction errors from a multiple multivariate linear regression model, which is used to estimate values at unselected locations and times. This framework has been extensively validated on a real-world testbed deployment."
2508.07978,"Generative Diffusion Models (GDMs) have emerged as key components of Generative Artificial Intelligence (GenAI), offering unparalleled expressiveness and controllability for complex data generation tasks. However, their deployment in real-time and mobile environments remains challenging due to the iterative and resource-intensive nature of the inference process. Addressing these challenges, this paper introduces a unified optimization framework that jointly tackles service placement and multiple access control for GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement Learning-based algorithm that dynamically partitions denoising blocks across heterogeneous edge nodes, while accounting for latent transmission costs and enabling adaptive reduction of inference steps. Our approach integrates a greedy multiple access scheme with a Double and Dueling Deep Q-Learning (D3QL)-based service placement, allowing for scalable, adaptable, and resource-efficient operation under stringent quality of service requirements. Simulations demonstrate the superior performance of the proposed framework in terms of scalability and latency resilience compared to conventional monolithic and fixed chain-length placement strategies. This work advances the state of the art in edge-enabled GenAI by offering an adaptable solution for GDM services orchestration, paving the way for future extensions toward semantic networking and co-inference across distributed environments."
2508.08225,"6G standardization is to start imminently, with commercial deployments expected before 2030. Its technical components and performance requirements are the focus of this article. Our emphasis is on the 6G radio access, especially MIMO, AI, waveforms, coding, signal constellations and integration with non-terrestrial networks. Whilst standardization has not yet formally started, the scope of the 6G study items has been defined. Our predictions in this paper are speculative as there are no results of the study yet, but our views are guided by implementation and deployment aspects. We expect that the views here will guide researchers and industry practitioners."
2508.0838,"The fundamental information-theoretic limits of covert, or low probability of detection/intercept (LPD/LPI), communication have been extensively studied for over a decade, resulting in the square root law (SRL): only $L\sqrt{n}$ covert bits can be reliably transmitted over time-bandwidth product $n$, for constant $L>0$. Transmitting more either results in detection or decoding errors. The SRL imposes significant constraints on hardware realization of mathematically-guaranteed covert communication. Indeed, they preclude using standard link maintenance operations that are taken for granted in non-covert communication. Thus, experimental validation of covert communication is underexplored: to date, only two experimental studies of SRL-based covert communication are available, both focusing on optical channels. Here, we report a demonstration of provably-secure covert radio-frequency (RF) communication using software-defined radios (SDRs). This validates theoretical predictions, opens practical avenues for implementing covert communication systems, and raises further research questions."
2508.08535,"Wireless Body Area Networks (WBANs) enable continuous monitoring of physiological signals for applications ranging from chronic disease management to emergency response. Recent advances in 6G communications, post-quantum cryptography, and energy harvesting have the potential to enhance WBAN performance. However, integrating these technologies into a unified, adaptive system remains a challenge. This paper surveys some of the most well-known Wireless Body Area Network (WBAN) architectures, routing strategies, and security mechanisms, identifying key gaps in adaptability, energy efficiency, and quantum-resistant security. We propose a novel Large Language Model-driven adaptive WBAN framework in which a Large Language Model acts as a cognitive control plane, coordinating routing, physical layer selection, micro-energy harvesting, and post-quantum security in real time. Our review highlights the limitations of current heuristic-based designs and outlines a research agenda for resource-constrained, 6G-ready medical systems. This approach aims to enable ultra-reliable, secure, and self-optimizing WBANs for next-generation mobile health applications."
2508.08555,"Underwater Wireless Sensor Networks (UWSNs) represent a promising technology that enables diverse underwater applications through acoustic communication. However, it encounters significant challenges including harsh communication environments, limited energy supply, and restricted signal transmission. This paper aims to provide efficient and reliable communication in underwater networks with limited energy and communication resources by optimizing the scheduling of communication links and adjusting transmission parameters (e.g., transmit power and transmission rate). The efficient and reliable communication multi-objective optimization problem (ERCMOP) is formulated as a decentralized partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware Resource Management (TARM) strategy based on deep multi-agent reinforcement learning (MARL) is presented to address this problem. Specifically, a traffic load-aware mechanism that leverages the overhear information from neighboring nodes is designed to mitigate the disparity between partial observations and global states. Moreover, by incorporating a solution space optimization algorithm, the number of candidate solutions for the deep MARL-based decision-making model can be effectively reduced, thereby optimizing the computational complexity. Simulation results demonstrate the adaptability of TARM in various scenarios with different transmission demands and collision probabilities, while also validating the effectiveness of the proposed approach in supporting efficient and reliable communication in underwater networks with limited resources."
2508.08627,"Mobile augmented reality (MAR) is envisioned as a key immersive application in 6G, enabling virtual content rendering aligned with the physical environment through device pose estimation. In this paper, we propose a novel agent-driven communication service provisioning approach for edge-assisted MAR, aiming to reduce communication overhead between MAR devices and the edge server while ensuring the quality of experience (QoE). First, to address the inaccessibility of MAR application-specific information to the network controller, we establish a digital agent powered by large language models (LLMs) on behalf of the MAR service provider, bridging the data and function gap between the MAR service and network domains. Second, to cope with the user-dependent and dynamic nature of data traffic patterns for individual devices, we develop a user-level QoE modeling method that captures the relationship between communication resource demands and perceived user QoE, enabling personalized, agent-driven communication resource management. Trace-driven simulation results demonstrate that the proposed approach outperforms conventional LLM-based QoE-aware service provisioning methods in both user-level QoE modeling accuracy and communication resource efficiency."
2508.08906,"The recently released Ultra Ethernet (UE) 1.0 specification defines a transformative High-Performance Ethernet standard for future Artificial Intelligence (AI) and High-Performance Computing (HPC) systems. This paper, written by the specification's authors, provides a high-level overview of UE's design, offering crucial motivations and scientific context to understand its innovations. While UE introduces advancements across the entire Ethernet stack, its standout contribution is the novel Ultra Ethernet Transport (UET), a potentially fully hardware-accelerated protocol engineered for reliable, fast, and efficient communication in extreme-scale systems. Unlike InfiniBand, the last major standardization effort in high-performance networking over two decades ago, UE leverages the expansive Ethernet ecosystem and the 1,000x gains in computational efficiency per moved bit to deliver a new era of high-performance networking."
2508.09085,"Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness."
2508.09147,"As 6G networks evolve into increasingly AI-driven, user-centric ecosystems, traditional reactive handover mechanisms demonstrate limitations, especially in mobile edge computing and autonomous agent-based service scenarios. This manuscript introduces WAAN, a cross-layer framework that enables intent-aware and proactive handovers by embedding lightweight TinyML agents as autonomous, negotiation-capable entities across heterogeneous edge nodes that contribute to intent propagation and network adaptation. To ensure continuity across mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points that serve as coordination anchors for context transfer and state preservation. The framework's operational capabilities are demonstrated through a multimodal environmental control case study, highlighting its effectiveness in maintaining user experience under mobility. Finally, the article discusses key challenges and future opportunities associated with the deployment and evolution of WAAN."
2508.09149,"Next-generation automotive applications require vehicular edge computing (VEC), but current management systems are essentially fixed and reactive. They are suboptimal in extremely dynamic vehicular environments because they are constrained to static optimization objectives and base their decisions on the current network states. This paper presents a novel Semantic-Aware Proactive LLM Orchestration (SP-LLM) framework to address these issues. Our method transforms the traditional Digital Twin (DT) into a Predictive Digital Twin (pDT) that predicts important network parameters such as task arrivals, vehicle mobility, and channel quality. A Large Language Model (LLM) that serves as a cognitive orchestrator is at the heart of our framework. It makes proactive, forward-looking decisions about task offloading and resource allocation by utilizing the pDT's forecasts. The LLM's ability to decipher high-level semantic commands given in natural language is crucial because it enables it to dynamically modify its optimization policy to match evolving strategic objectives, like giving emergency services priority or optimizing energy efficiency. We show through extensive simulations that SP-LLM performs significantly better in terms of scalability, robustness in volatile conditions, and adaptability than state-of-the-art reactive and MARL-based approaches. More intelligent, autonomous, and goal-driven vehicular networks will be possible due to our framework's outstanding capacity to convert human intent into optimal network behavior."
2508.0915,"This paper presents the design and implementation of a Proof of Concept (PoC) that demonstrates how 5G Advanced Network Functions can be integrated with the Common API Framework (CAPIF) to support enhanced connectivity for automotive applications. The PoC shows the continuous monitoring of the mobile network performance and the on-demand and dynamic adaptation of Quality of Service (QoS) for selected 5G User Equipment (UE) video streaming traffic flows using standard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover, traffic flows are redirected to the edge to improve latency and optimize network resource utilization."
2508.09151,"Abrupt resolution changes in virtual reality (VR) streaming can significantly impair the quality-of-experience (QoE) of users, particularly during transitions from high to low resolutions. Existing QoE models and transmission schemes inadequately address the perceptual impact of these shifts. To bridge this gap, this article proposes, for the first time, an innovative physiological signal-driven QoE modeling and optimization framework that fully leverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin activity signals. This framework precisely captures the temporal dynamics of physiological responses and resolution changes in VR streaming, enabling accurate quantification of resolution upgrades' benefits and downgrades' impacts. Integrated the proposed QoE framework into the radio access network (RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission strategies have been implemented to allocate radio resources dynamically, which mitigates short-term channel fluctuations and adjusts frame resolution in response to channel variations caused by user mobility. By prioritizing long-term resolution while minimizing abrupt transitions, the proposed solution achieves an 88.7\% improvement in resolution and an 81.0\% reduction in handover over the baseline. Experimental results demonstrate the effectiveness of this physiological signal-driven strategy, underscoring the promise of edge AI in immersive media services."
2508.09152,"With the advent of 5G networks and technologies, ensuring the integrity and performance of packet core traffic is paramount. During network analysis, test files such as Packet Capture (PCAP) files and log files will contain errors if present in the system that must be resolved for better overall network performance, such as connectivity strength and handover quality. Current methods require numerous person-hours to sort out testing results and find the faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine designed to classify successful and faulty frames in PCAP files, specifically within the 5G packet core. The FA engine analyses network traffic using natural language processing techniques to identify anomalies and inefficiencies, significantly reducing the effort time required and increasing efficiency. The FA Engine also suggests steps to fix the issue using Generative AI via a Large Language Model (LLM) trained on several 5G packet core documents. The engine explains the details of the error from the domain perspective using documents such as the 3GPP standards and user documents regarding the internal conditions of the tests. Test results on the ML models show high classification accuracy on the test dataset when trained with 80-20 splits for the successful and failed PCAP files. Future scopes include extending the AI engine to incorporate 4G network traffic and other forms of network data, such as log text files and multimodal systems."
2508.09159,"Next-generation mobile networks must reconcile the often-conflicting goals of multiple service owners. However, today's network slice controllers remain rigid, policy-bound, and unaware of the business context. We introduce Agoran Service and Resource Broker (SRB), an agentic marketplace that brings stakeholders directly into the operational loop. Inspired by the ancient Greek agora, Agoran distributes authority across three autonomous AI branches: a Legislative branch that answers compliance queries using retrieval-augmented Large Language Models (LLMs); an Executive branch that maintains real-time situational awareness through a watcher-updated vector database; and a Judicial branch that evaluates each agent message with a rule-based Trust Score, while arbitrating LLMs detect malicious behavior and apply real-time incentives to restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective optimizer, reaching a consensus intent in a single round, which is then deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and evaluated with realistic traces of vehicle mobility, Agoran achieved significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73% reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3% saving in PRB usage compared to a static baseline. An 1B-parameter Llama model, fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80% of GPT-4.1's decision quality, while operating within 6 GiB of memory and converging in only 1.3 seconds. These results establish Agoran as a concrete, standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks. A live demo is presentedthis https URL\&ab_channel=BubbleRAN."
2508.09166,"As the Internet of Things (IoT) continues to evolve, indoor location has become a critical element for enabling smart homes, behavioral monitoring, and elderly care. Existing WiFi-based human tracking solutions typically require specialized equipment or multiple Wi-Fi links, a limitation in most indoor settings where only a single pair of Wi-Fi devices is usually available. However, despite efforts to implement human tracking using one Wi-Fi link, significant challenges remain, such as difficulties in acquiring initial positions and blind spots in DFS estimation of tangent direction. To address these challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure Insoles Fusion System for Single Target Tracking. WPTrack collects Channel State Information (CSI) from a single Wi-Fi link and pressure data from 90 insole sensors. The phase difference and Doppler velocity are computed from the CSI, while the pressure sensor data is used to calculate walking velocity. Then, we propose the CSI-pressure fusion model, integrating CSI and pressure data to accurately determine initial positions and facilitate precise human tracking. The simulation results show that the initial position localization accuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results obtained from experimental data collected in a real-world environment closely align with the actual trajectory."
2508.09171,"Current AI agents create significant barriers for users by requiring extensive processing to understand web pages, making AI-assisted web interaction slow and expensive. This paper introduces webMCP (Web Machine Context & Procedure), a client-side standard that embeds structured interaction metadata directly into web pages, enabling more efficient human-AI collaboration on existing websites. webMCP transforms how AI agents understand web interfaces by providing explicit mappings between page elements and user actions. Instead of processing entire HTML documents, agents can access pre-structured interaction data, dramatically reducing computational overhead while maintaining task accuracy. A comprehensive evaluation across 1,890 real API calls spanning online shopping, authentication, and content management scenarios demonstrates webMCP reduces processing requirements by 67.6% while maintaining 97.9% task success rates compared to 98.8% for traditional approaches. Users experience significantly lower costs (34-63% reduction) and faster response times across diverse web interactions. Statistical analysis confirms these improvements are highly significant across multiple AI models. An independent WordPress deployment study validates practical applicability, showing consistent improvements across real-world content management workflows. webMCP requires no server-side modifications, making it deployable across millions of existing websites without technical barriers. These results establish webMCP as a viable solution for making AI web assistance more accessible and sustainable, addressing the critical gap between user interaction needs and AI computational requirements in production environments."
2508.09173,"Most Large Language Models (LLMs) are currently deployed in the cloud, with users relying on internet connectivity for access. However, this paradigm faces challenges such as network latency, privacy concerns, and bandwidth limits. Thus, deploying LLMs on edge devices has become an important research focus. In edge inference, request latency is critical as high latency can impair real-time tasks. At the same time, edge devices usually have limited battery capacity, making energy consumption another major concern. Balancing energy consumption and inference latency is essential. To address this, we propose an LLM inference energy management framework that optimizes GPU frequency and batch size to balance latency and energy consumption. By effectively managing the exploration-exploitation dilemma in configuration search, the framework finds the optimal settings. The framework was implemented on the NVIDIA Jetson AGX Orin platform, and a series of experimental validations were conducted. Results demonstrate that, compared to the default configuration, our framework reduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance between energy consumption and latency."
2508.09184,"Cellular traffic forecasting is essential for network planning, resource allocation, or load-balancing traffic across cells. However, accurate forecasting is difficult due to intricate spatial and temporal patterns that exist due to the mobility of users. Existing AI-based traffic forecasting models often trade-off accuracy and computational efficiency. We present Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial encoder with a Mamba-based temporal module and attention mechanism. HiSTM employs selective state space methods to capture spatial and temporal patterns in network traffic. In our evaluation, we use a real-world dataset to compare HiSTM against several baselines, showing a 29.4% MAE improvement over the STN baseline while using 94% fewer parameters. We show that the HiSTM generalizes well across different datasets and improves in accuracy over longer time-horizons."
2508.09197,"Future 6G radio access networks (RANs) will be artificial intelligence (AI)-native: observed, reasoned about, and re-configured by autonomous agents cooperating across the cloud-edge continuum. We introduce MX-AI, the first end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of Large-Language-Model (LLM)-powered agents inside the Service Management and Orchestration (SMO) layer, and (iii) exposes both observability and control functions for 6G RAN resources through natural-language intents. On 50 realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0 and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end latency when backed by GPT-4.1. Thus, it matches human-expert performance, validating its practicality in real settings. We publicly release the agent graph, prompts, and evaluation harness to accelerate open research on AI-native RANs. A live demo is presented here:this https URL"
2508.09208,"The proliferation of large language models (LLMs) has driven the adoption of Mixture-of-Experts (MoE) architectures as a promising solution to scale model capacity while controlling computational costs. However, deploying MoE models in resource-constrained mobile edge computing environments presents significant challenges due to their large memory footprint and dynamic expert activation patterns. To address these challenges, we propose a novel dynamic resource-aware collaborative optimization framework that jointly optimizes expert aggregation granularity and offloading strategies based on real-time device resource states, network conditions, and input characteristics in mobile edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze existing expert aggregation techniques, including expert parameter merging,knowledge distillation,and parameter sharing decomposition, identifying their limitations in dynamic mobilethis http URLthen investigate expert offloading strategies encompassing expert prediction and prefetching, expert caching and scheduling, and multi-tier storage architectures, revealing the interdependencies between routing decisions and offloadingthis http URLCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility and varying network conditions, enabling efficient MoE deployment across heterogeneous edge devices. Extensive experiments on real mobile edge testbeds demonstrate that CoMoE achieves approximately 70% reduction in memory usage compared to baseline methods, 10.5% lower inference latency than existing expert offloading techniques, while maintaining model performance stability. For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on resource-constrained mobile edge devices that previously could only support much smaller models."
2508.09229,"Efficient deployment of a pre-trained LLM to a cluster with multiple servers is a critical step for providing fast responses to users' queries. The recent success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy them efficiently, considering their underlying structure. During the inference in MoE LLMs, only a small part of the experts is selected to process a given token. Moreover, in practice, the experts' load is highly imbalanced. For efficient deployment, one has to distribute the model across a large number of servers using a model placement algorithm. Thus, to improve cluster utilization, the model placement algorithm has to take into account the network topology. This work focuses on the efficient topology-aware placement of the pre-trained MoE LLMs in the inference stage. We propose an integer linear program (ILP) that determines the optimal placement of experts, minimizing the expected number of transmissions. Due to the internal structure, this optimization problem can be solved with a standard ILP solver. We demonstrate that ILP-based placement strategy yields lower network traffic than competitors for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models."
2508.0924,"The use of Service-Based Architecture in modern telecommunications has exponentially increased Network Functions (NFs) and Application Programming Interfaces (APIs), creating substantial operational complexities in service discovery and management. We introduce \textit{NEFMind}, a framework leveraging parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to address these challenges. It integrates three core components: synthetic dataset generation from Network Exposure Function (NEF) API specifications, model optimization through Quantized-Low-Rank Adaptation, and performance evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G Service-Based Architecture APIs, our approach achieves 85% reduction in communication overhead compared to manual discovery methods. Experimental validation using the open-source Phi-2 model demonstrates exceptional API call identification performance at 98-100% accuracy. The fine-tuned Phi-2 model delivers performance comparable to significantly larger models like GPT-4 while maintaining computational efficiency for telecommunications infrastructure deployment. These findings validate domain-specific, parameter-efficient LLM strategies for managing complex API ecosystems in next-generation telecommunications networks."
2508.09369,"Wireless networks face severe vulnerabilities from jamming attacks, which can significantly disrupt communication. Existing detection approaches are often unimodal, rely on centralized processing, and demand substantial computational resources, hindering scalability, efficiency, and deployment feasibility. To address these challenges, we introduce a multimodal Federated Learning (FL) framework for on-device jamming detection and classification that integrates spectrograms with cross-layer network Key Performance Indicators (KPIs) through a lightweight dual-encoder architecture equipped with a fusion module and a multimodal projection head. This design enables privacy-preserving training and inference by ensuring that only model parameters are exchanged, while raw data remains on the device. The framework is implemented and evaluated on a wireless experimental testbed using, to the best of our knowledge, the first over-the-air multimodal dataset with synchronized benign and three distinct jamming scenarios. Results show that our approach surpasses state-of-the-art unimodal baselines by up to 15% in detection accuracy, achieves convergence with 60% fewer communication rounds, and maintains low resource usage. Its benefits are most evident under heterogeneous data distributions across devices, where it exhibits strong robustness and reliability."
2508.09573,"This paper addresses the challenges of evaluating network performance in the presence of fluctuating traffic patterns, with a particular focus on the impact of peak data rates on network resources. We introduce a set of metrics to quantify network load and measure the impact of individual flows on the overall network state. By analyzing link and flow data through percentile values and sample distributions, and introducing the Utilization Score metric, the research provides insights into resource utilization under varying network conditions. Furthermore, we employ a modified Shapley value-based approach to measure the influence of individual flows on the network, offering a better understanding of their contribution to network performance. The paper reviews and compares 11 metrics across various network scenarios, evaluating their practical relevance for research and development. Our evaluation demonstrates that these metrics effectively capture changes in network state induced by specific flows, with three of them offering a broad range of valuable insights while remaining relatively easy to maintain. Moreover, the methodology described in this paper serves as a framework for future research, with the potential to expand and refine the set of metrics used to evaluate flow impact on network performance."
2508.09582,"In this paper, we consider the use of visible light communication (VLC) to provide connectivity to indoor fog computing resources and propose an energy-efficient passive optical network (PON)-based backhaul architecture to support the VLC system. We develop a mixed-integer linear programming (MILP) model to optimize the allocation of computing resources over the proposed architecture, aiming to minimize processing and networking power consumption. We evaluate the performance of the proposed architecture under varying workload demands and user distributions. Comparative analysis against a backhaul architecture that is based on the state-of-the-art spine-and-leaf (S&L) network design demonstrates total power savings of up to 82%. Further comparison with centralized cloud processing shows improvements in energy efficiency of up to 93%. Additionally, we examine the improvements in energy efficiency obtained by splitting tasks among multiple processing nodes and propose enhancements to the architecture including dynamic bandwidth allocation, increased wavelength bandwidth and improved connectivity within rooms to alleviate networking bottlenecks. Furthermore, we introduce an inter-building architecture that leverages resources from neighboring buildings to support high-demand scenarios."
2508.0962,"Minimizing energy consumption of low-power wireless nodes is a persistent challenge from the constrained Internet of Things (IoT). In this paper, we start from the observation that constrained IoT devices have largely different hardware (im-)balances than full-scale machines. We find that the performance gap between MCU and network throughput on constrained devices enables minimal energy delay product (EDP) for IoT networking at largely reduced clock frequencies. We analyze the potentials by integrating dynamic voltage and frequency scaling (DVFS) into the RIOT IoT operating system and show that the DVFS reconfiguration overhead stays below the energy saved for a single, downscaled MAC operation. Backed by these findings, we systematically investigate how DVFS further improves energy-efficiency for common networking tasks -- in addition to duty-cycling. We measure IoT communication scenarios between real-world systems and analyze two MAC operating modes -- CSMA/CA and time slotting -- in combination with different CoAP transactions, payload sizes, as well as DTLS transport encryption. Our experiments reveal energy savings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP communication. These results shall encourage research and system design work to integrate DVFS in future IoT devices for performing tasks at their optimal frequencies and thereby significantly extending battery lifetimes."
2508.0966,"Internet of Things (IoT) application providers rely on Mobile Network Operators (MNOs) and roaming infrastructures to deliver their services globally. In this complex ecosystem, where the end-to-end communication path traverses multiple entities, it has become increasingly challenging to guarantee communication availability and reliability. Further, most platform operators use a reactive approach to communication issues, responding to user complaints only after incidents have become severe, compromising service quality. This paper presents our experience in the design and deployment of ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity service of a large global roaming platform. ANCHOR assists engineers by filtering vast amounts of data to identify potential problematic clients (i.e., those with connectivity issues affecting several of their IoT devices), enabling proactive issue resolution before the service is critically impacted. We first describe the IoT service, infrastructure, and network visibility of the IoT connectivity provider we operate. Second, we describe the main challenges and operational requirements for designing an unsupervised anomaly detection solution on this platform. Following these guidelines, we propose different statistical rules, and machine- and deep-learning models for IoT verticals anomaly detection based on passive signaling traffic. We describe the steps we followed working with the operational teams on the design and evaluation of our solution on the operational platform, and report an evaluation on operational IoT customers."
2508.09735,"Quantum Key Distribution (QKD) networks harness the principles of quantum physics in order to securely transmit cryptographic key material, providing physical guarantees. These networks require traditional management and operational components, such as routing information through the network elements. However, due to the limitations on capacity and the particularities of information handling in these networks, traditional shortest paths algorithms for routing perform poorly on both route planning and online routing, which is counterintuitive. Moreover, due to the scarce resources in such networks, often the expressed demand cannot be met by any assignment of routes. To address both the route planning problem and the need for fair automated suggestions in infeasible cases, we propose to model this problem as a Quadratic Programming (QP) problem. For the online routing problem, we showcase that the shortest (available) paths routing strategy performs poorly in the online setting. Furthermore, we prove that the widest shortest path routing strategy has a competitive ratio greater or equal than $\frac{1}{2}$, efficiently addressing both routing modes in QKD networks."
2508.09756,"This article is a position paper which introduces the paradigm of ``Massive Wireless Human Sensing'', i.e. an infrastructure for wireless human sensing based on a plethora of heterogeneous wireless communication signals. More specifically, we aim to exploit signal diversity in the time, frequency, and space domains using opportunistically both device-free and device-based wireless sensing approaches, with the objective of enhancing human sensing capabilities in terms of accuracy and service availability over different environments. The enabling element of this concept is the massive wireless human sensing edge device, that is, an embedded system acting as a multi-technology and multi-approach RF receiver with feature extraction functionality, located within the monitoring area or at its borders. In this framework, architecture solutions and challenges are discussed to lead the future development of this new paradigm."
2508.09769,"Current standardization efforts are advancing the integration of 5G and Time-Sensitive Networking (TSN) to facilitate the deployment of safety-critical industrial applications that require real-time communication. However, there remains a fundamental disconnect between the probabilistic 5G delay characteristics and the often idealistic delay models used to synthesize 5G-TSN network configurations. For time-driven schedules in particular, any delay outlier unforeseen during schedule synthesis can jeopardize the robustness of their real-time guarantees. To address this challenge, we present the (m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time guarantees during unstable network conditions that do not match the expected delay characteristics. It augments the primary time-driven schedule with a dynamic priority-driven scheme to elevate the priority of m out of k consecutive frames if they are delayed. Our evaluations demonstrate that weakly hard real-time guarantees are essential to uphold the quality of control within a networked control system. At the same time, only a small overhead is imposed when the primary schedule can provide stronger quality of service guarantees. Our (m,k)-firm Elevation Policy thereby yields a robust but light-weight fallback mechanism to serve applications with meaningful guarantees during unstable network conditions."
2508.09839,"Starlink delivers Internet services to users across terrestrial, maritime, and aviation domains. The prior works have studied its performance at fixed sites and in-motion vehicles, while an in-depth analysis of in-flight performance remains absent. With major airlines now offering Starlink Internet onboard, there is a growing need to evaluate and improve its performance for aviation users. This paper addresses this shortcoming by conducting in-flight measurements over the Baltic Sea and the Pacific Ocean. Our measurement results show that a single user device experiences median throughputs of 64 Mbps and 24 Mbps for the downlink and uplink, respectively. The median uplink throughput is approximately 33 Mbps when the aircraft maintains an altitude above 17,000 feet. However, a significant reduction in uplink performance is observed during the aircraft descent phase, with the median throughput dropping to around 20 Mbps at lower altitudes. Round-trip time (RTT) is highly dependent on the location of the ground station being pinged and the use of inter-satellite links (ISLs). We dive deeper into 5.5 hours of ping measurements collected over the Pacific Ocean and investigate factors influencing RTT, hypothesizing that ISLs routing, data queuing at satellites, and feeder link congestion contribute to deviations from theoretical values. For comparative analysis, we evaluate the Starlink ground terminal and in-flight connectivity performance from the perspectives of a residential user and an airline passenger, respectively."
2508.10247,"This work presents the design and implementation of a real-time network coding system integrated into the IP layer of a 5G testbed, offering an alternative to conventional retransmission-based reliability mechanisms such as ARQ and HARQ. Using a netfilter-based packet interception framework, we inject forward erasure correction using Random Linear Network Coding (RLNC) into live traffic between a gNB and UE over a 3GPP RF link. We evaluate a block coding scheme, analyzing its impact on throughput, jitter, and resource usage. Results show that with appropriate code rate selection, RLNC can fully recover from packet losses using fewer transmissions than ARQ/HARQ and maintain a high throughput, particularly under moderate-to-high packet loss rates. These findings demonstrate that network coding can effectively replace retransmission-based reliability in future wireless systems, with the potential for more efficient resource utilization."
2508.10283,"Large-scale timers are ubiquitous in network processing, including flow table entry expiration control in software defined network (SDN) switches, MAC address aging in Ethernet bridges, and retransmission timeout management in TCP/IP protocols. Conventional implementations suffer from critical limitations: low timing accuracy due to large-scale timer traversal and high computational overhead for new timer insertion. This paper presents a hybrid-architecture hardware priority queue based on systolic arrays and shift registers for efficient timer queue management. The design uniquely supports five operations: enqueue, dequeue, delete, update, andthis http URLthe best of our knowledge, it is the first hardware priority queue enabling in-queue priority updates. By leveraging centralized Boolean logic encoding within systolic blocks, the design efficiently generates set/shift control signals while the novel push-first operation ensures FIFO ordering for same-priority timers without additional metadata. Experimental results demonstrate that the design operates at over 400 MHz on FPGAs, achieving a 2.2-2.8x reduction in resource consumption compared to state-of-the-art implementations."
2508.10338,"Earth observation (EO) satellites in Low Earth Orbit (LEO) are collecting vast amounts of data, which are invaluable for applications such as monitoring forest fires. However, data downloading from EO satellites faces significant challenges due to the limited number of ground stations and the brief communication windows with them. Conversely, emerging LEO constellations like Starlink have enabled continuous connectivity and revolutionized access for ordinary users globally, who can connect via a simple satellite dish. In this paper, we study the feasibility of supporting EO satellites with Starlink satellite infrastructure and introduce a novel data delivery system, designated as ""Starlink Space User"" (SSU), for relaying data from observation satellites. SSU treats EO satellites as space users of Starlink, facilitating efficient data transfer to Earth. At the core of SSU is a novel class of algorithms designed for link and PoP selection, as well as system scheduling optimization, that operate effectively atop Starlink's proprietary infrastructure. We assess the performance of SSU using trace-driven simulations alongside real-world Starlink performance measurements. Our results demonstrate that the proposed Starlink-aided design can significantly reduce the median backlog (data not delivered) per satellite."
2508.10413,"Robot Operating System 2 (ROS 2) is now the de facto standard for robotic communication, pairing UDP transport with the Data Distribution Service (DDS) publish-subscribe middleware. DDS achieves reliability through periodic heartbeats that solicit acknowledgments for missing samples and trigger selective retransmissions. In lossy wireless networks, the tight coupling among heartbeat period, IP fragmentation, and retransmission interval obscures end to end latency behavior and leaves practitioners with little guidance on how to tune these parameters. To address these challenges, we propose a probabilistic latency analysis (PLA) that analytically models the reliable transmission process of ROS 2 DDS communication using a discrete state approach. By systematically analyzing both middleware level and transport level events, PLA computes the steady state probability distribution of unacknowledged messages and the retransmission latency. We validate our PLA across 270 scenarios, exploring variations in packet delivery ratios, message sizes, and both publishing and retransmission intervals, demonstrating a close alignment between analytical predictions and experimental results. Our findings establish a theoretical basis to systematically optimize reliability, latency, and performance in wireless industrial robotics."
2508.10574,"Federated learning (FL) over long-range (LoRa) low-power wide area networks faces unique challenges due to limited bandwidth, interference, and strict duty-cycle constraints. We develop a Python-based simulator that integrates and extends the Flower and LoRaSim frameworks to evaluate centralized FL over LoRa networks. The simulator employs a detailed link-level model for FL update transfer over LoRa channels, capturing LoRa's receiver sensitivity, interference characteristics, block-fading effects, and constraints on the maximum transmission unit. It supports update sparsification, quantization, compression, forward frame-erasure correction (FEC), and duty cycling. Numerical results illustrate the impact of transmission parameters (spreading factor, FEC rate) and interference on FL performance. Demonstrating the critical role of FEC in enabling FL over LoRa networks, we perform an in-depth evaluation of the impact of FEC on FL convergence and device airtime, providing insights for communication protocol design for FL over LoRa networks."
2508.10588,"Over-the-air firmware updates are crucial for mitigating security threats and maintaining up-to-date device functionality in Long Range Wide Area Networks (LoRaWANs). LoRaWAN end devices are usually energy-constrained, and LoRaWAN transmissions are subject to duty-cycle restrictions. Consequently, controlling the energy expenditure and update-delivery latency of FUOTA are key challenges. We propose a flexible scheme that achieves a tunable trade-off between the energy consumption and delivery delay. The scheme employs the LoRa spreading factors sequentially to transmit update-carrying frames, sending a fixed number of frames with a given spreading factor before moving to the next. By adjusting the smallest spreading factor to be used and the number of transmissions per spreading factor, a suitable energy-delay trade-off can be achieved. Thus, time-sensitive updates, such as security patches, may be sent with a low-delay-high-energy setting, whereas a more energy-efficient but higher-delay setting may be used for non-critical updates."
2508.11342,"Distributed tracing has become an essential technique for debugging and troubleshooting modern microservice-based applications, enabling software engineers to detect performance bottlenecks, identify failures, and gain insights into system behavior. However, implementing distributed tracing in large-scale applications remains challenging due to the need for extensive instrumentation. To reduce this burden, zero-code instrumentation solutions, such as those based on eBPF, have emerged, allowing span data to be collected without modifying application code. Despite this promise, span correlation, the process of establishing causal relationships between spans, remains a critical challenge in zero-code approaches. Existing solutions often rely on thread affinity, compromise system security by requiring the kernel integrity mode to be disabled, or incur significant computational overhead due to complex inference algorithms. This paper presents CrossTrace, a practical and efficient distributed tracing solution designed to support the debugging of microservice applications without requiring source code modifications. CrossTrace employs a greedy algorithm to infer intra-service span relationships from delay patterns, eliminating reliance on thread identifiers. For inter-service correlation, CrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling secure and efficient correlation compromising system security policies. Evaluation results show that CrossTrace can correlate thousands of spans within seconds with over 90% accuracy, making it suitable for production deployment and valuable for microservice observability and diagnosis."
2508.11366,"Wireless transmission of large payloads, such as high-resolution images and LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source robotics middleware. The default Data Distribution Service (DDS) communication stack in ROS 2 exhibits significant performance degradation over lossy wireless links. Despite the widespread use of ROS 2, the underlying causes of these wireless communication challenges remain unexplored. In this paper, we present the first in-depth network-layer analysis of ROS 2's DDS stack under wireless conditions with large payloads. We identify the following three key issues: excessive IP fragmentation, inefficient retransmission timing, and congestive buffer bursts. To address these issues, we propose a lightweight and fully compatible DDS optimization framework that tunes communication parameters based on link and payload characteristics. Our solution can be seamlessly applied through the standard ROS 2 application interface via simple XML-based QoS configuration, requiring no protocol modifications, no additional components, and virtually no integration efforts. Extensive experiments across various wireless scenarios demonstrate that our framework successfully delivers large payloads in conditions where existing DDS modes fail, while maintaining low end-to-end latency."
2508.11475,"In distributed Software-Defined Networking (SDN), distributed SDN controllers require synchronization to maintain a global network state. Despite the availability of synchronization policies for distributed SDN architectures, most policies do not consider joint optimization of network and user performance. In this work, we propose a reinforcement learning-based algorithm called D2Q Synchronizer, to minimize long-term network costs by strategically offloading time-sensitive tasks to cost-effective edge servers while satisfying the latency requirements for all tasks. Evaluation results demonstrate the superiority of our synchronizer compared to heuristic and other learning policies in literature, by reducing network costs by at least 45% and 10%, respectively, while ensuring the QoS requirements for all user tasks across dynamic and multi-domain SDN networks."
2508.11574,"The next generation networks offers significant potential to advance Intelligent Transportation Systems (ITS), particularly through the integration of Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs through efficient computing resource management remains an open challenge. This paper introduces a distributed computing archi tecture that integrates DTs and Mobile Edge Computing (MEC) within a software-defined vehicular networking framework to enable intelligent, low-latency transportation services. A network aware scalable collaborative task provisioning algorithm is de veloped to train an autonomous agent, which is evaluated using a realistic connected autonomous vehicle (CAV) traffic simulation. The proposed framework significantly enhances the robustness and scalability of DT operations by reducing synchronization errors to as low as 5% while achieving up to 99.5% utilization of edge computing resources."
2508.11842,"Error estimating coding (EEC) is a standard technique for estimating the number of bit errors during packet transmission over wireless networks. In this paper, we propose OddEEC, a novel EEC scheme. OddEEC is a nontrivial adaptation of a data sketching technique named Odd Sketch to EEC, addressing new challenges therein by its bit sampling technique and maximum likelihood estimator. Our experiments show that OddEEC overall achieves comparable estimation accuracy as competing schemes such as gEEC and mEEC, with much smaller decoding complexity."
2508.11971,"Wireless power transfer (WPT) is increasingly used to sustain Internet-of-Things (IoT) systems by wirelessly charging embedded devices. Mobile chargers further enhance scalability in wireless-powered IoT (WP-IoT) networks, but pose new challenges due to dynamic channel conditions and limited energy budgets. Most existing works overlook such dynamics or ignore real-time constraints on charging schedules. This paper presents a bandit-based charging framework for WP-IoT systems using mobile chargers with practical beamforming capabilities and real-time charging constraints. We explicitly consider time-varying channel state information (CSI) and impose a strict charging deadline in each round, which reflects the hard real-time constraint from the charger's limited battery capacity. We formulate a temporal-spatial charging policy that jointly determines the charging locations, durations, and beamforming configurations. Area discretization enables polynomial-time enumeration with constant approximation bounds. We then propose two online bandit algorithms for both stationary and non-stationary unknown channel state scenarios with bounded regrets. Our extensive experimental results validate that the proposed algorithms can rapidly approach the theoretical upper bound while effectively tracking the dynamic channel states for adaptive adjustment."
2508.12112,"The traditional black-box and monolithic approach to Radio Access Networks (RANs) has heavily limited flexibility and innovation. The Open RAN paradigm, and the architecture proposed by the O-RAN ALLIANCE, aim to address these limitations via openness, virtualization and network intelligence. In this work, first we propose a novel, programmable scheduler design for Open RAN Distributed Units (DUs) that can guarantee minimum throughput levels to User Equipments (UEs) via configurable weights. Then, we propose an O-RAN xApp that reconfigures the scheduler's weights dynamically based on the joint Complementary Cumulative Distribution Function (CCDF) of reported throughput values. We demonstrate the effectiveness of our approach by considering the problem of asset tracking in 5G-powered Industrial Internet of Things (IIoT) where uplink video transmissions from a set of cameras are used to detect and track assets via computer vision algorithms. We implement our programmable scheduler on the OpenAirInterface (OAI) 5G protocol stack, and test the effectiveness of our xApp control by deploying it on the O-RAN Software Community (OSC) near-RT RAN Intelligent Controller (RIC) and controlling a 5G RAN instantiated on the Colosseum Open RAN digital twin. Our experimental results demonstrate that our approach enhances the success percentage of meeting throughput requirements by 33% compared to a reference scheduler. Moreover, in the asset tracking use case, we show that the xApp improves the detection accuracy, i.e., the F1 score, by up to 37.04%."
2508.12661,"With the development of space-air-ground-aqua integrated networks (SAGAIN), high-speed and reliable network services are accessible at any time and any location. However, the long propagation delay and limited network capacity of underwater communication networks (UCN) negatively impact the service quality of SAGAIN. To address this issue, this paper presents U-HPNF, a hierarchical framework designed to achieve a high-performance network with self-management, self-configuration, and self-optimization capabilities. U-HPNF leverages the sensing and decision-making capabilities of deep reinforcement learning (DRL) to manage limited resources in UCNs, including communication bandwidth, computational resources, and energy supplies. Additionally, we incorporate federated learning (FL) to iteratively optimize the decision-making model, thereby reducing communication overhead and protecting the privacy of node observation information. By deploying digital twins (DT) at both the intelligent sink layer and aggregation layer, U-HPNF can mimic numerous network scenarios and adapt to varying network QoS requirements. Through a three-tier network design with two-levels DT, U-HPNF provides an AI-native high-performance underwater network. Numerical results demonstrate that the proposed U-HPNF framework can effectively optimize network performance across various situations and adapt to changing QoS requirements."
2508.12707,"Energy in Wireless Sensor Networks (WSNs) is critical to network lifetime and data delivery. However, the primary impediment to the durability and dependability of these sensor nodes is their short battery life. Currently, power-saving algorithms such as clustering and routing algorithms have improved energy efficiency in standard protocols. This paper proposes a clustering-based routing approach for creating an adaptive, energy-efficient mechanism. Our system employs a multi-step clustering strategy to select dynamic cluster heads (CH) with optimal energy distribution. We use Game Theory (GT) and Reinforcement Learning (RL) to optimize resource utilization. Modeling the network as a multi-agent RL problem using GT principles allows for self-clustering while optimizing sensor lifetime and energy balance. The proposed AI-powered CH-Finding algorithm improves network efficiency by preventing premature energy depletion in specific nodes while also ensuring uniform energy usage across the network. Our solution enables controlled power consumption, resulting in a deterministic network lifetime. This predictability lowers maintenance costs by reducing the need for node replacement. Furthermore, our proposed method prevents sensor nodes from disconnecting from the network by designating the sensor with the highest charge as an intermediary and using single-hop routing. This approach improves the energy efficiency and stability of Wireless Sensor Network (WSN) deployments."
2508.1271,"The emergence of nomadic mobile communication networks for sixth-generation (6G) introduces a paradigm shift in how network infrastructure is conceptualized, deployed, and operated. Unlike traditional fixed systems, Nomadic Networks (NNs) consist of mobile and self-organizing nodes that provide radio infrastructure capabilities in motion. This paper explores the architectural implications of such systems, with a particular focus on the design and evolution of network interfaces. We analyze the requirements for inter-node communication, service discovery, and control delegation in dynamic environments. Furthermore, we examine the regulatory and licensing challenges that arise when infrastructure elements traverse jurisdictional boundaries. Based on current 6G visions and relevant research, we identify limitations in existing architectures and propose a set of interface principles tailored to nomadicity. By synthesizing findings from mobile, non-terrestrial, and organic network domains, this work contributes to the architectural foundation for future nomadic 6G communication systems and outlines directions for interface standardization in decentralized, mobile infrastructures."
2508.12723,"This paper studies a multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) networked integrated sensing and communication (ISAC) system, in which multiple base stations (BSs) perform beam tracking to communicate with a mobile device. In particular, we focus on the beam tracking over a number of tracking time slots (TTSs) and suppose that these BSs operate at non-overlapping frequency bands to avoid the severe inter-cell interference. Under this setup, we propose a new cooperative sensing-assisted predictive beam tracking design. In each TTS, the BSs use echo signals to cooperatively track the mobile device as a sensing target, and continuously adjust the beam directions to follow the device for enhancing the performance for both communication and sensing. First, we propose a cooperative sensing design to track the device, in which the BSs first employ the two-dimensional discrete Fourier transform (2D-DFT) technique to perform local target estimation, and then use the extended Kalman filter (EKF) method to fuse their individual measurement results for predicting the target parameters. Next, based on the predicted results, we obtain the achievable rate for communication and the predicted conditional Cramr-Rao lower bound (PC-CRLB) for target parameters estimation in the next TTS, as a function of the beamforming vectors. Accordingly, we formulate the predictive beamforming design problem, with the objective of maximizing the achievable communication rate in the following TTS, while satisfying the PC-CRLB requirement for sensing. To address the resulting non-convex problem, we first propose a semi-definite relaxation (SDR)-based algorithm to obtain the optimal solution, and then develop an alternative penalty-based algorithm to get a high-quality low-complexity solution."
2508.12767,"Software-defined networking (SDN) technology aims to create a highly flexible network by decoupling control plane and the data plane and programming them independently. There has been a lot of research on improving and optimizing the control plane, and data plane programming is a relatively new concept, so study on it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar, well-known scientists on computer networking discussed challenges and problems in the field of data plane programming that need to be addressed over the next 10 years. Based on this seminar issues and papers review, we suggested some possible solutions which are for optimizing data plane to improve packet processing performance and link utilization. The suggestions include (i) enriching data plane language with asynchronous external function, (ii) compression based on payload size, (iii) in-network caching for fast packet processing, and (iv) offloading external functions to an additional thread, virtual machine (VM) or server, etc. In addition, we implemented some of these in the P4 data plane language to illustrate the practicality."
2508.12785,"The Service Data Adaptation Protocol (SDAP) plays a central role in 5G New Radio (NR), acting as a bridge between the core and radio networks, by enabling QoS Flow multiplexing over shared Data Radio Bearers (DRBs). However, most 5G simulation frameworks, including the popular OMNet++-based Simu5G, lack SDAP support, limiting their ability to model realistic QoS behavior. This paper presents a modular, standardscompliant SDAP extension for Simu5G. The implementation includes core elements such as QoS Flow Identifer (QFI) flow tagging, SDAP header insertion/removal, and configurable logical DRB mapping. The proposed design supports multi-QFI simulation scenarios and enables researchers to model differentiated QoS flows and flowaware scheduling policies. Validation results confirm correct SDAP behavior and pave the way for advanced 5G simulations involving per-flow isolation, latency-sensitive traffic, and industrial QoS profiles."
2508.12852,"Tomography inference attacks aim to reconstruct network topology by analyzing end-to-end probe delays. Existing defenses mitigate these attacks by manipulating probe delays to mislead inference, but rely on two strong assumptions: (i) probe packets can be perfectly detected and altered, and (ii) attackers use known, fixed inference algorithms. These assumptions often break in practice, leading to degraded defense performance under detection errors or adaptive adversaries. We present RoTO, a robust topology obfuscation scheme that eliminates both assumptions by modeling uncertainty in attacker-observed delays through a distributional formulation. RoTO casts the defense objective as a min-max optimization problem that maximizes expected topological distortion across this uncertainty set, without relying on perfect probe control or specific attacker models. To approximate attacker behavior, RoTO leverages graph neural networks for inference simulation and adversarial training. We also derive an upper bound on attacker success probability, and demonstrate that our approach enhances topology obfuscation performance through the optimization of this upper bound. Experimental results show that RoTO outperforms existing defense methods, achieving average improvements of 34% in structural similarity and 42.6% in link distance while maintaining strong robustness and concealment capabilities."
2508.12857,"Community GPU platforms are emerging as a cost-effective and democratized alternative to centralized GPU clusters for AI workloads, aggregating idle consumer GPUs from globally distributed and heterogeneous environments. However, their extreme hardware/software diversity, volatile availability, and variable network conditions render traditional schedulers ineffective, leading to suboptimal task completion. In this work, we present REACH (Reinforcement Learning for Efficient Allocation in Community and Heterogeneous Networks), a Transformer-based reinforcement learning framework that redefines task scheduling as a sequence scoring problem to balance performance, reliability, cost, and network efficiency. By modeling both global GPU states and task requirements, REACH learns to adaptively co-locate computation with data, prioritize critical jobs, and mitigate the impact of unreliable resources. Extensive simulation results show that REACH improves task completion rates by up to 17%, more than doubles the success rate for high-priority tasks, and reduces bandwidth penalties by over 80% compared to state-of-the-art baselines. Stress tests further demonstrate its robustness to GPU churn and network congestion, while scalability experiments confirm its effectiveness in large-scale, high-contention scenarios."
2508.13469,"The fifth-generation (5G) of cellular communications is expected to be deployed in the next years to support a wide range of services with different demands of peak data rates, latency and quality of experience (QoE). To support higher data rates and latency requirements third-generation partnership project (3GPP) has introduced numerology and bandwidth parts (BWPs), via new radio (NR) for service-tailored resource allocation. Legacy 4G networks have generated extensive data, which combined with crowd-sourced LTE infrastructure insights, enables identification of high-traffic 5G deployment area (5GDA) for planning new services. Given the mission-critical nature of 5G services, QoE is a big challenge for MNOs to guarantee peak data rates for a defined percentage of time. This work studies the fundamentals of 5G network planning methods that reconciles coverage-capacity trade-offs through balanced radio network dimensioning (RND), leveraging pragmatic NR modeling, and data-driven strategies to minimize deployment costs and reduce cost-per-bit."
2508.13474,"Automatic Modulation Recognition (AMR) detectsmodulation schemes of received signals for further processingof signals without any priori information, which is criticallyimportant for civil spectrum regulation, information countermea sures, and communication security. Due to the powerful featureextraction and classification capabilities of Deep Learning (DL),DL-based AMR algorithms have achieved excellent performancegains compared with traditional modulation detection algorithms.However, all existing DL-based AMR algorithms, to the best ofour knowledge, are designed for specific channels and systems,because data dimension of the used training dataset is fixed. Tothis end, we takes the first step to propose a Subgraph EmbeddingLearning (SEL) structure to address the classical AMR problem,and the proposed algorithm is called SEL-AMR. Our algorithmtreats the communication system as a subgraph and uses therelationship between samples to smooth the effects brought bynoise and different channels to extract robust features. Thus,the proposed SEL-AMR algorithm can adapt to any dynamicchannels and systems. We use 5 public real datasets and a smallamount of simulation data to evaluate our SEL-AMR algorithm.Experimental results reveal that SEL-AMR can well adapt todifferent channels and systems, and always outperforms the state of-the-art algorithms by improving up to 20% macro-averagerecognition precision and 30% recognition accuracy."
2508.13512,"The high mobility of satellites in Low Earth Orbit (LEO) mega-constellations induces a highly dynamic network topology, leading to many problems like frequent service disruptions. To mitigate this, Packet-based Load Balancing (PBLB) is employed. However, this paradigm shift introduces two critical challenges for network measurement stemming from the requirement for port-level granularity: memory inflation and severe hash collisions. To tackle these challenges, we propose CountingStars, a low-overhead network-wide measurement architecture. In the ground controller, CountingStars builds a digital twins system to accurately predict the future network topology. This allows ground controller to generate and distribute collision-free hash seeds to satellites in advance. On the satellite, we introduce a port aggregation data structure that decouples the unique flow identifier from its multi-port counter and updates it through efficient bit operations, solving the memory inflation caused by PBLB. Simulation results show that the memory usage of CountingStars is reduced by 70\% on average, and the relative error of measurement is reduced by 90\% on average. Implementation on FPGA shows its prospect to deploy in real system."
2508.13581,"The service-based architecture of 5G network allows network operators to place virtualized network functions on commodity hardware, unlike the traditional vendor-specific hardware-based functionalities. However, it expands the security vulnerabilities and threats to the 5G network. While there exist several theoretical studies on network function placement and service routing, a few focused on the security aspects of the 5G network systems.This paper focuses on safeguarding the 5G core network systems from DoS and DDoS attacks by placing intrusion detection and prevention systems (IDS-IPS) as virtualized network functions following the 5G standalone architecture. To ensure the virtualized placement of IDS-IPS, first, we provide thorough virtual machine (VM)-based and containerized implementation details and evaluate the network performance with two scenarios, IDS and IPS, in the presence of TCP and UDP applications. Second, we apply the VM-based implementation of IDS-IPS on a softwarized 5G core network and study the network performances. The experiment results on network throughput, latency, and packet drop reveal that the softwarized IDS-IPS can meet the QoS requirements of 5G applications, while safeguarding the network from DoS and DDoS attacks."
2508.13652,"As the automotive industry transitions toward centralized Linux-based architectures, ensuring the predictable execution of mixed-criticality applications becomes essential. However, concurrent use of the Linux network stack introduces interference, resulting in unpredictable latency and jitter. To address this challenge, we present a layered software architecture that enforces timing isolation for Ethernet-based data exchange between mixed-criticality applications on Linux-based automotive control units. Our approach integrates traffic prioritization strategies at the middleware layer, the network stack layer, and the hardware layer to achieve isolation across the full software stack. At the middleware layer, we implement a fixed-priority, non-preemptive scheduler to manage publishers of varying criticality. At the network layer, we leverage the express data path (XDP) to route high-priority data directly from the network interface driver into critical application memory, bypassing the standard Linux network stack. At the hardware layer, we dedicate a network interface card (NIC) queue exclusively to real-time traffic. We demonstrate how our architecture performs in a Data Distribution Service (DDS)-based system. Our evaluation shows that the approach leads to consistent and predictable latencies for real-time traffic, even under heavy interference from best-effort applications."
2508.13736,"ISAC is emerging as a foundational capability in 6G, enabling mobile networks to not only offer communication services but also to sense and perceive their environment at scale. This paper explores architectural considerations to enable sensing in 6G, extending on recent developments by (pre-)standardisation bodies such as 3GPP and ETSI. Selected ISAC use cases are presented from the European MultiX project including associated potential functional system requirements. The paper proposes a 6G system architecture that integrates newly proposed NFs for the purpose of sensing and demonstrates how they are being used in offering sensing as a service. Protocol stack adaptations for both control and a newly proposed sensing plane are discussed."
2508.14222,"Streaming videos from resource-constrained front-end devices over networks to resource-rich cloud servers has long been a common practice for surveillance and analytics. Most existing live video analytics (LVA) systems, however, have been built over terrestrial networks, limiting their applications during natural disasters and in remote areas that desperately call for real-time visual data delivery and scene analysis. With the recent advent of space networking, in particular, Low Earth Orbit (LEO) satellite constellations such as Starlink, high-speed truly global Internet access is becoming available and affordable. This paper examines the challenges and potentials of LVA over modern LEO satellite networking (LSN). Using Starlink as the testbed, we have carried out extensive in-the-wild measurements to gain insights into its achievable performance for LVA. The results reveal that the uplink bottleneck in today's LSN, together with the volatile network conditions, can significantly affect the service quality of LVA and necessitate prompt adaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming framework for LVA. At its core, StarStream is empowered by a Transformer-based network performance predictor tailored for LSN and a content-aware configuration optimizer. We discuss a series of key design and implementation issues of StarStream and demonstrate its effectiveness and superiority through trace-driven experiments with real-world network and video processing data."
2508.14237,"With the reduced hardware costs of omnidirectional cameras and the proliferation of various extended reality applications, more and more $360^\circ$ videos are being captured. To fully unleash their potential, advanced video analytics is expected to extract actionable insights and situational knowledge without blind spots from the videos. In this paper, we present OmniSense, a novel edge-assisted framework for online immersive video analytics. OmniSense achieves both low latency and high accuracy, combating the significant computation and network resource challenges of analyzing $360^\circ$ videos. Motivated by our measurement insights into $360^\circ$ videos, OmniSense introduces a lightweight spherical region of interest (SRoI) prediction algorithm to prune redundant information in $360^\circ$ frames. Incorporating the video content and network dynamics, it then smartly scales vision models to analyze the predicted SRoIs with optimized resource utilization. We implement a prototype of OmniSense with commodity devices and evaluate it on diverse real-world collected $360^\circ$ videos. Extensive evaluation results show that compared to resource-agnostic baselines, it improves the accuracy by $19.8\%$ -- $114.6\%$ with similar end-to-end latencies. Meanwhile, it hits $2.0\times$ -- $2.4\times$ speedups while keeping the accuracy on par with the highest accuracy of baselines."
2508.14239,"Distributed Hash Tables (DHTs) are pivotal in numerous high-impact key-value applications built on distributed networked systems, offering a decentralized architecture that avoids single points of failure and improves data availability. Despite their widespread utility, DHTs face substantial challenges in handling range queries, which are crucial for applications such as LLM serving, distributed storage, databases, content delivery networks, and blockchains. To address this limitation, we present LEAD, a novel system incorporating learned models within DHT structures to significantly optimize range query performance. LEAD utilizes a recursive machine learning model to map and retrieve data across a distributed system while preserving the inherent order of data. LEAD includes the designs to minimize range query latency and message cost while maintaining high scalability and resilience to network churn. Our comprehensive evaluations, conducted in both testbed implementation and simulations, demonstrate that LEAD achieves tremendous advantages in system efficiency compared to existing range query methods in large-scale distributed systems, reducing query latency and message cost by 80% to 90%+. Furthermore, LEAD exhibits remarkable scalability and robustness against system churn, providing a robust, scalable solution for efficient data retrieval in distributed key-value systems."
2508.14281,"Routing configurations of a network should constantly adapt to traffic variations to achieve good network performance. Adaptive routing faces two main challenges: 1) how to accurately measure/estimate time-varying traffic matrices? 2) how to control the network and application performance degradation caused by frequent route changes? In this paper, we develop a novel data-enabled predictive traffic engineering (DeeP-TE) algorithm that minimizes the network congestion by gracefully adapting routing configurations over time. Our control algorithm can generate routing updates directly from the historical routing data and the corresponding link rate data, without direct traffic matrix measurement or estimation. Numerical experiments on real network topologies with real traffic matrices demonstrate that the proposed DeeP-TE routing adaptation algorithm can achieve close-to-optimal control effectiveness with significantly lower routing variations than the baseline methods."
2508.14305,"Ensuring uninterrupted data flow in modern networks requires robust fault-tolerant mechanisms, especially in environments where reliability and responsiveness are critical. This paper presents the design and simulation of a fault-tolerant network switching system using Python-based algorithms. A simulated enterprise-level Local Area Network (LAN) was modeled using NetworkX to represent switch-router interconnectivity with redundant links. Fault scenarios, including link failure and congestion, were injected using Scapy, while automatic failover and rerouting were implemented via custom Python logic. The system demonstrates resilience by dynamically detecting path failures, redistributing network traffic through redundant links, and minimizing downtime. Performance evaluations reveal significant improvements in packet delivery continuity, faster recovery times, and reduced packet loss compared to non-fault-tolerant baselines. The implementation provides a scalable and lightweight approach to integrating fault-tolerance features into mid-scale networks, with potential application in enterprise information technology infrastructures and academic simulations."
2508.14335,"In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite constellations has introduced unprecedented opportunities for global connectivity. However, routing efficiency and inter-shell communication remain key challenges in multi-shell architectures. This paper investigates the structural properties and network dynamics of a representative six-shell mega-constellation composed of 10,956 satellites and 198 gateway stations (GSs). Leveraging tools from complex network analysis, we identify several critical findings: (1) the constellation exhibits strong small-world characteristics, enabling efficient routing despite large network diameters; (2) GS relays play a pivotal role in enhancing inter-shell connectivity by bridging otherwise disconnected components; (3) feeder links significantly reduce average path length, making long-haul communication more feasible; (4) betweenness analysis reveals load imbalances among GSs, indicating the need for traffic-aware management strategies; (5) the architecture offers excellent spatial coverage and resilience, maintaining connectivity and low routing costs even under GS failures. These insights not only explain the design rationale behind current mega-constellations like SpaceX Starlink, but also provide valuable guidance for the evolution of future satellite network infrastructures."
2508.14435,"In this paper, we study the virtual network function (VNF) placement problem in mobile edge computing (MEC)-enabled 5G networks to meet the stringent reliability and latency requirements of uRLLC applications. We pose it as a constrained optimization problem, which is NP-hard, to maximize the total reward obtained by a network service provider by serving uRLLC service requests. We propose an approximated randomized rounding approach to solve the NP-hard optimization problem in polynomial time. We prove that the proposed randomized approach achieves performance guarantees while violating the resource constraints boundedly. Furthermore, we present a greedy-heuristic approach to tackle the violations of resource constraints.Simulation results show that the proposed randomized rounding and greedy approaches achieve a total reward which is within 5% and 10% of the optimal solution, respectively. Furthermore, we compare the proposed greedy approach with the existing schemes that do not consider the availability requirements. We observe that the existing schemes perform poorly in terms of total reward, as negligence to the availability requirements negatively impacts the number of successfully served requests. These findings highlight the trade-off between availability and resource efficiency in latency-sensitive uRLLC applications. We also implement a software prototype of a 5G network using open-source software platforms with redundant placement of VNFs. The results on packet delivery ratio and latency obtained from the prototype implementation are also improved in the redundant VNFs with different failure probabilities."
2508.14445,"In this paper, we address the necessity of data related to mobile traffic of the legacy infrastructure to extract useful information and perform network dimensioning for 5G. These data can help us achieve a more efficient network planning design, especially in terms of topology and cost. To that end, a real open database of top three Spanish mobile network operators (MNOs) is used to estimate the traffic and to identify the area of highest user density for the deployment of new services. We propose the data acquisition procedure described to clean the database, to extract meaningful traffic information and to visualize traffic density patterns for new gNB deployments. We present the state of the art in Network Data. We describe the considered network database in detail. The Network Data Acquisition entity along with the proposed procedure is explained. The corresponding results are discussed, following the conclusions."
2508.14471,"This paper presents ANS-V2X, an Adaptive Network Selection framework tailored for latency-aware V2X systems operating under varying vehicle densities and heterogeneous network conditions. Modern vehicular environments demand low-latency and high-throughput communication, yet real-time network selection is hindered by diverse application requirements and the coexistence of multiple Radio Access Technologies (RATs) such as 4G, 5G, and ad hoc links. ANS-V2X employs a heuristic-driven approach to assign vehicles to networks by considering application sensitivity, latency, computational load, and directionality constraints. The framework is benchmarked against a Mixed-Integer Linear Programming (MILP) formulation for optimal solutions and a Q-learning-based method representing reinforcement learning. Simulation results demonstrate that ANS-V2X achieves near-optimal performance, typically within 5 to 10% of the utility achieved by MILP-V2X, while reducing execution time by more than 85%. Although MILP-V2X offers globally optimal results, its computation time often exceeds 100 milliseconds, making it unsuitable for real-time applications. The Q-learning-based method is more adaptable but requires extensive training and converges slowly in dynamic scenarios. In contrast, ANS-V2X completes decisions in under 15 milliseconds and consistently delivers lower latency than both alternatives. This confirms its suitability for real-time, edge-level deployment in latency-critical V2X systems"
2508.14601,"This paper presents a novel multi-tier UAV-assisted edge computing system designed for low-altitude networks. The system comprises vehicle users, lightweight Low-Tier UAVs (L-UAVs), and High-Tier UAV (H-UAV). L-UAVs function as small-scale edge servers positioned closer to vehicle users, while the H-UAV, equipped with more powerful server and larger-capacity battery, serves as mobile backup server to address the limitations in endurance and computing resources of L-UAVs. The primary objective is to minimize task execution delays while ensuring long-term energy stability for L-UAVs. To address this challenge, the problem is first decoupled into a series of deterministic problems for each time slot using Lyapunov optimization. The priorities of task delay and energy consumption for L-UAVs are adaptively adjusted based on real-time energy status. The optimization tasks include assignment of tasks, allocation of computing resources, and trajectory planning for both L-UAVs and H-UAV. Simulation results demonstrate that the proposed approach achieves a reduction of at least 26% in transmission energy for L-UAVs and exhibits superior energy stability compared to existing benchmarks."
2508.14676,"Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of the target area, network size, and sensor coverage to determine initial deployment. This often results in significant overlap to ensure continued network operation despite sensor energy depletion. With the emergence of Mobile Wireless Sensor Networks (MWSNs), issues such as sensor failure and static coverage limitations can be more effectively addressed through mobility. This paper proposes a novel deployment strategy in which mobile sensors autonomously position themselves to maximize area coverage, eliminating the need for predefined policies. A live camera system, combined with deep reinforcement learning (DRL), monitors the network by detecting sensor LED indicators and evaluating real-time coverage. Rewards based on coverage efficiency and sensor movement are computed at each learning step and shared across the network through a Multi-Agent Reinforcement Learning (MARL) framework, enabling decentralized, cooperative sensor control. Key contributions include a vision-based, low-cost coverage evaluation method; a scalable MARL-DRL framework for autonomous deployment; and a self-reconfigurable system that adjusts sensor positioning in response to energy depletion. Compared to traditional distance-based localization, the proposed method achieves a 26.5% improvement in coverage, a 32% reduction in energy consumption, and a 22% decrease in redundancy, extending network lifetime by 45%. This approach significantly enhances adaptability, energy efficiency, and robustness in MWSNs, offering a practical deployment solution within the IoT framework."
2508.14679,"Efficient energy management is essential in Wireless Sensor Networks (WSNs) to extend network lifetime and ensure reliable data transmission. This paper presents a novel method using reinforcement learning-based cluster-head selection and a hybrid multi-hop routing algorithm, which leverages Q-learning within a multi-agent system to dynamically adapt transmission paths based on the energy distribution across sensor nodes. Each sensor node is modeled as an autonomous agent that observes local state parameters, such as residual energy, distance to sink, hop count, and hotspot proximity, and selects routing actions that maximize long-term energy efficiency. After computing the optimal paths, each sensor aggregates sensed data and forwards it through intermediate nodes to a selected transmitter node, chosen based on the highest remaining State of Charge (SoC), thereby avoiding premature node depletion. To promote efficient learning, a carefully designed reward function incentivizes balanced load distribution, hotspot avoidance, and energy-aware forwarding while maintaining signal quality. The learning process occurs either in a decentralized manner or via a cloud-based controller that offloads computation in large-scale deployments. Moreover, the RL-driven routing decisions are fused with classical graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum Spanning Tree (MST), to optimize energy consumption and load balancing. Simulations confirm that the proposed approach significantly improves node survival rate, reduces SoC variance, and enhances network resilience, making it a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor deployments and IoT applications."
2508.15058,"Wireless underground sensor networks (WUSNs), which enable real-time sensing and monitoring of underground resources by underground devices (UDs), hold great promise for delivering substantial social and economic benefits across various verticals. However, due to the harsh subterranean environment, scarce network resources, and restricted communication coverage, WUSNs face significant challenges in supporting sustainable massive machine-type communications (mMTC), particularly in remote, disaster-stricken, and hard-to-reach areas. To complement this, we conceptualize in this study a novel space-air-ground-underground integrated network (SAGUIN) architecture that seamlessly incorporates satellite systems, aerial platforms, terrestrial networks, and underground communications. On this basis, we integrate LoRaWAN and wireless energy transfer (WET) technologies into SAGUIN to enable sustainable subterranean mMTC. We begin by reviewing the relevant technical background and presenting the architecture and implementation challenges of SAGUIN. Then, we employ simulations to model a remote underground pipeline monitoring scenario to evaluate the feasibility and performance of SAGUIN based on LoRaWAN and WET technologies, focusing on the effects of parameters such as underground conditions, time allocation, LoRaWAN spread factor (SF) configurations, reporting periods, and harvested energy levels. Our results evidence that the proposed SAGUIN system, when combined with the derived time allocation strategy and an appropriate SF, can effectively extend the operational lifetime of UDs, thereby facilitating sustainable subterranean mMTC. Finally, we pinpoint key challenges and future research directions for SAGUIN."
2508.15087,"The rapid adoption of QUIC as a transport protocol has transformed content delivery by reducing latency, enhancing congestion control (CC), and enabling more efficient multiplexing. With the advent of 5G networks, which support ultra-low latency and high bandwidth, streaming high-resolution video at 4K and beyond has become increasingly viable. However, optimizing Quality of Experience (QoE) in mobile networks remains challenging due to the complex interactions among Adaptive Bit Rate (ABR) schemes at the application layer, CC algorithms at the transport layer, and Radio Link Control (RLC) queuing at the link layer in the 5G network. While prior studies have largely examined these components in isolation, this work presents a comprehensive analysis of the impact of modern active queue management (AQM) strategies, such as RED and L4S, on video streaming over diverse QUIC implementations--focusing particularly on their interaction with the RLC buffer in 5G environments and the interplay between CC algorithms and ABR schemes. Our findings demonstrate that the effectiveness of AQM strategies in improving video streaming QoE is intrinsically linked to their dynamic interaction with QUIC implementations, CC algorithms and ABR schemes-highlighting that isolated optimizations are insufficient. This intricate interdependence necessitates holistic, cross-layer adaptive mechanisms capable of real-time coordination between network, transport and application layers, which are crucial for fully leveraging the capabilities of 5G networks to deliver robust, adaptive, and high-quality video streaming."
2508.15268,"Future communication networks are expected to achieve deep integration of communication, sensing, and computation, forming a tightly coupled and autonomously operating infrastructure system. However, current reliance on centralized control, static design, and human intervention continues to constrain the multidimensional evolution of network functions and applications, limiting adaptability and resilience in large-scale, layered, and complex environments. To address these challenges, this paper proposes a nature-inspired architectural framework that leverages digital twin technology to organize connected devices at the edge into functional digital populations, while enabling the emergence of an evolvable digital ecosystem through multi-population integration at the cloud. We believe that this framework, which combines engineering methodologies with sociotechnical insights, lays the theoretical foundation for building next-generation communication networks with dynamic coordination, distributed decision-making, continuous adaptation, and evolutionary capabilities."
2508.15307,"The network structure design plays a vital role in the mega-constellation network (MSN) to coordinate massive network nodes to ensure the effectiveness and reliability of operations and services for future space wireless communications networks.One of the critical issues in MCN is how to design an optimal network control structure by configuring the most stable inter-satellite link (ISL) to achieve high available MCN within a limited average transmission delays.To address this problem, this paper introduces a novel MCN structure design paradigm: Structure = Motif + Lattice (SML), which decouples MCN design into local motifs design and global lattices design. Specifically, we formulate the High-Availability and Low-Latency Mega-Constellation Design (HALLMD) problem, aimed at maximizing ISL availability while minimizing the transmission latency. To solve HALLMD, we propose SMLOP, a heuristic algorithm that efficiently finds optimal network structures in polynomial time. Experimental validation on four public state-of-the-art constellations demonstrates significant improvements, including enhanced capacity by $5\sim 18\%$, increased throughput by $1\sim 12\%$, reduced path stretch by $12\sim 23\%$, and Round-Trip Time (RTT) by $8\sim 77\%$."
2508.15595,"Traditional standardized network interfaces face significant limitations, including vendor-specific incompatibilities, rigid design assumptions, and lack of adaptability for new functionalities. We propose a multi-agent framework leveraging large language models (LLMs) to generate control interfaces on demand between network functions (NFs). This includes a matching agent, which aligns required control functionalities with NF capabilities, and a code-generation agent, which generates the necessary API server for interface realization. We validate our approach using simulated multi-vendor gNB and WLAN AP environments. The performance evaluations highlight the trade-offs between cost and latency across LLMs for interface generation tasks. Our work sets the foundation for AI-native dynamic control interface generation, paving the way for enhanced interoperability and adaptability in future mobile networks."
2508.15795,"Mobile edge computing (MEC)-assisted internet of vehicle (IoV) is emerging as a promising paradigm to provide computing services for vehicles. However, meeting the computing-sensitive and computation-intensive demands of vehicles poses several challenges, including the discrepancy between the limited resource provision and stringent computing requirement, the difficulty in capturing and integrating the intricate features of the MEC-assisted IoV system into the problem formulation, and the need for real-time processing and efficient resource management in the dynamic environment. In this work, we explore the AI-enabled task offloading and resource allocation for MEC-assisted consumer IoV systems. Specifically, we first present a multi-MEC-assisted consumer IoV architecture that leverages the computational resources of MEC servers to provide offloading services close to vehicles. Subsequently, we formulate a system cost minimization optimization problem (SCMOP) by integrating the service delay and energy consumption. To efficiently solve this problem, we design a joint task offloading and computing resource allocation approach (JTOCRA) by applying the multi-agent deep deterministic policy gradient (MADDPG) algorithm. Finally, simulation results demonstrate that the proposed JTOCRA can achieve superior system performances and exhibits better scalability compared to other alternative approaches."
2508.15816,"Airborne Base Stations (ABSs) allow for flexible geographical allocation of network resources with dynamically changing load as well as rapid deployment of alternate connectivity solutions during natural disasters. Since the radio infrastructure is carried by unmanned aerial vehicles (UAVs) with limited flight time, it is important to establish the best location for the ABS without exhaustive field trials. This paper proposes a digital twin (DT)-guided approach to achieve this through the following key contributions: (i) Implementation of an interactive software bridge between two open-source DTs such that the same scene is evaluated with high fidelity across NVIDIA's Sionna and Aerial Omniverse Digital Twin (AODT), highlighting the unique features of each of these platforms for this allocation problem, (ii) Design of a back-propagation-based algorithm in Sionna for rapidly converging on the physical location of the UAVs, orientation of the antennas and transmit power to ensure efficient coverage across the swarm of the UAVs, and (iii) numerical evaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies the environmental conditions in which there is agreement or divergence of performance results between these twins. Finally, (iv) we propose a resilience mechanism to provide consistent coverage to mission-critical devices and demonstrate a use case for bi-directional flow of information between the two DTs."
2508.15819,"The current evolution of artificial intelligence introduces a paradigm shift toward agentic AI built upon multi-agent systems (MAS). Agent communications serve as a key to effective agent interactions in MAS and thus have a significant impact on the performance of agentic AI applications. The recent research on agent communications has made exciting rapid progress that leads to a variety of protocol designs, among which the Agent2Agent (A2A) protocol is considered the most representative one. Simultaneously, the rise of edge intelligence is expected to enable agentic AI at the network edge. However, the current agent communication protocols are designed without sufficient consideration of the special challenges of edge computing, and their effectiveness in the edge environment is largely unexamined. In this paper, we attempt to assess the abilities of agent communication technologies to face the challenges of edge computing using the A2A protocol as a representative case. We first discuss the core functionalities of agent communications, present a landscape of agent communication protocols, and identify the main challenges introduced by edge computing. Then, we conduct a case study on the A2A protocol to examine the key technologies leveraged in the protocol for their effectiveness in meeting the requirements of agent communications in edge computing. Based on the insights obtained from this assessment, we identify open issues in the current agent communication technologies and discuss directions for future research to address these issues."
2508.15833,"The rise of 5G communication has transformed the telecom industry for critical applications. With the widespread deployment of 5G base stations comes a significant concern about energy consumption. Key industrial players have recently shown strong interest in incorporating energy storage systems to store excess energy during off-peak hours, reducing costs and participating in demand response. The fast development of batteries opens up new possibilities, such as the transportation area. An effective method is needed to maximize base station battery utilization and reduce operating costs. In this trend towards next-generation smart and integrated energy-communication-transportation (ECT) infrastructure, base stations are believed to play a key role as service hubs. By exploring the overlap between base station distribution and electric vehicle charging infrastructure, we demonstrate the feasibility of efficiently charging EVs using base station batteries and renewable power plants at the Hub. Our model considers various factors, including base station traffic conditions, weather, and EV charging behavior. This paper introduces an incentive mechanism for setting charging prices and employs a deep reinforcement learning-based method for battery scheduling. Experimental results demonstrate the effectiveness of our proposed ECT-Hub in optimizing surplus energy utilization and reducing operating costs, particularly through revenue-generating EV charging."
2508.15838,"The increasing saturation of terrestrial resources has driven the exploration of low-altitude applications such as air taxis. Low altitude wireless networks (LAWNs) serve as the foundation for these applications, and integrated sensing and communication (ISAC) constitutes one of the core technologies within LAWNs. However, the openness nature of low-altitude airspace makes LAWNs vulnerable to malicious channel access attacks, which degrade the ISAC performance. Therefore, this paper develops a game-based framework to mitigate the influence of the attacks on LAWNs. Concretely, we first derive expressions of communication data's signal-to-interference-plus-noise ratio and the age of information of sensing data under attack conditions, which serve as quality of service metrics. Then, we formulate the ISAC performance optimization problem as a Stackelberg game, where the attacker acts as the leader, and the legitimate drone and the ground ISAC base station act as second and first followers, respectively. On this basis, we design a backward induction algorithm that achieves the Stackelberg equilibrium while maximizing the utilities of all participants, thereby mitigating the attack-induced degradation of ISAC performance in LAWNs. We further prove the existence and uniqueness of the equilibrium. Simulation results show that the proposed algorithm outperforms existing baselines and a static Nash equilibrium benchmark, ensuring that LAWNs can provide reliable service for low-altitude applications."
2508.15843,"Open Radio Access Network (O-RAN) is a key architectural paradigm for 5G and beyond cellular networks, enabling the adoption of intelligent and efficient resource management solutions. Meanwhile, diffusion models have demonstrated remarkable capabilities in image and video generation, making them attractive for network optimization tasks. In this paper, we propose xDiff, a diffusion-based reinforcement learning(RL) framework for inter-cell interference management (ICIM) in O-RAN. We first formulate ICIM as a resource allocation optimization problem aimed at maximizing a user-defined reward function and then develop an online learning solution by integrating a diffusion model into an RL framework for near-real-time policy generation. Particularly, we introduce a novel metric, preference values, as the policy representation to enable efficient policy-guided resource allocation within O-RAN distributed units (DUs). We implement xDiff on a 5G testbed consisting of three cells and a set of smartphones in two small-cell scenarios. Experimental results demonstrate that xDiff outperforms state-of-the-art ICIM approaches, highlighting the potential of diffusion models for online optimization of O-RAN. Source code is available on GitHub [1]."
2508.16035,"This paper introduces a novel approach to time series classification using a Markov Transition Field (MTF)-aided Transformer model, specifically designed for Software-Defined Networks (SDNs). The proposed model integrates the temporal dependency modeling strengths of MTFs with the sophisticated pattern recognition capabilities of Transformer architectures. We evaluate the model's performance using the InSDN dataset, demonstrating that our model outperforms baseline classification models, particularly in data-constrained environments commonly encountered in SDN applications. We also highlight the relationship between the MTF and Transformer components, which leads to better performance, even with limited data. Furthermore, our approach achieves competitive training and inference times, making it an efficient solution for real-world SDN applications. These findings establish the potential of MTF-aided Transformers to address the challenges of time series classification in SDNs, offering a promising path for reliable and scalable analysis in scenarios with sparse data."
2508.16074,"Congestion control is a fundamental component of Internet infrastructure, and researchers have dedicated considerable effort to developing improved congestion control algorithms. However, despite extensive study, existing algorithms continue to exhibit suboptimal performance across diverse network environments. In this paper, we introduce a novel approach that automatically optimizes congestion control algorithms using large language models (LLMs). Our framework consists of a structured algorithm generation process, an emulation-based evaluation pipeline covering a broad range of network conditions, and a statistically guided method to substantially reduce evaluation time. Empirical results from four distinct LLMs validate the effectiveness of our approach. We successfully identify algorithms that achieve up to 27% performance improvements over the original BBR algorithm in a production QUIC implementation. Our work demonstrates the potential of LLMs to accelerate the design of high-performance network algorithms and paves the way for broader applications in networking systems."
2508.16119,"We present ANSC, a probabilistic capacity health scoring framework for hyperscale datacenter fabrics. While existing alerting systems detect individual device or link failures, they do not capture the aggregate risk of cascading capacity shortfalls. ANSC provides a color-coded scoring system that indicates the urgency of issues \emph{not solely by current impact, but by the probability of imminent capacity violations}. Our system accounts for both current residual capacity and the probability of additional failures, normalized at datacenter and regional level. We demonstrate that ANSC enables operators to prioritize remediation across more than 400 datacenters and 60 regions, reducing noise and aligning SRE focus on the most critical risks."
2508.16184,"In this letter, we investigate the problem of joint content caching and routing in satellite-terrestrial edge computing networks (STECNs) to improve caching service for geographically distributed users. To handle the challenges arising from dynamic low Earth orbit (LEO) satellite topologies and heterogeneous content demands, we propose a learning-based framework that integrates graph neural networks (GNNs) with deep reinforcement learning (DRL). The satellite network is represented as a dynamic graph, where GNNs are embedded within the DRL agent to capture spatial and topological dependencies and support routing-aware decision-making. The caching strategy is optimized by formulating the problem as a Markov decision process (MDP) and applying soft actor-critic (SAC) algorithm. Simulation results demonstrate that our approach significantly improves the delivery success rate and reduces communication traffic cost."
2508.16268,"This Paper proposes a self-healing, automated network of Raspberry Pi devices designed for deployment in scenarios where traditional networking is unavailable. Leveraging the low-power, long-range capabilities of the LoRa (Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the research addresses challenges such as limited bandwidth, data collisions, and node failures. Given that LoRa's packet-based system is incompatible with conventional IaC tools like Ansible and Terraform, which rely on TCP/IP networking, the research adapts IaC principles within a containerised architecture deployed across a Raspberry Pi cluster. Evaluation experiments indicate that fragmenting data packets and retransmitting any missed fragments can mitigate LoRa's inherent throughput and packet size limitations, although issues such as collisions and line-of-sight interference persist. An automated failover mechanism was integrated into the architecture, enabling unresponsive services to be redeployed to alternative nodes within one second, demonstrating the system's resilience in maintaining operational continuity despite node or service failures. The paper also identifies practical challenges, including the necessity for time-slotting transmissions to prevent data packet overlap and collisions. Future research should explore the integration of mesh networking to enhance range, develop more advanced scheduling algorithms, and adopt cutting-edge low-power wide-area network (LPWAN) techniques."
2508.16816,"The rapid advancement of communication technologies has established cellular networks as the backbone for diverse applications, each with distinct quality of service requirements. Meeting these varying demands within a unified infrastructure presents a critical challenge that can be addressed through advanced techniques such as multi-connectivity. Multiconnectivity enables User equipments to connect to multiple BSs simultaneously, facilitating QoS differentiation and provisioning. This paper proposes a QoS-aware multi-connectivity framework leveraging machine learning to enhance network performance. The approach employs deep neural networks to estimate the achievable QoS metrics of BSs, including data rate, reliability, and latency. These predictions inform the selection of serving clusters and data rate allocation, ensuring that the User Equipment connects to the optimal BSs to meet its QoS needs. Performance evaluations demonstrate that the proposed algorithm significantly enhances Quality of Service (QoS) for applications where traditional and state-of-the-art methods are inadequate. Specifically, the algorithm achieves a QoS success rate of 98%. Furthermore, it improves spectrum efficiency by 30% compared to existing multi-connectivity solutions."
2508.1735,"Single-wavelength 400G coherent optical communications have become a critical solution to meet the explosive traffic demands. However, the single-carrier modulation using low-order modulation formats requires a broader wavelength division multiplexing grid and expands the occupied optical bandwidth. In this paper, we propose the faster-than-Nyquist non-orthogonal frequency division multiplexing (FTN-NOFDM) to improve the spectral efficiency for long-haul coherent optical communications. The subcarrier number is set to eight to enable low-complexity FTN-NOFDM signal generation using a pruned inverse fast Fourier transform and inter-carrier interference (ICI) cancellation. To deal with the conventional timing recovery (TR) failure, a frequency tone-based TR is proposed for FTN-NOFDM. A time-domain multiple-input multiple-output equalizer is designed to update the tap coefficients based on outputs of conventional iterative detection (ID). To further mitigate ICI, a low-density parity check-assisted ID is integrated into the conventional ID module. FTN-NOFDM, probabilistic constellation shaping (PCS)-OFDM, and quadrature phase shift keying-OFDM are experimentally compared in a 400G coherent optical communication system over 11 cascaded 125-GHz wavelength-selective switches (WSSs) and 2000 km transmission. Results show that the FTN-NOFDM exhibits comparable WSS filtering tolerance to PCS-OFDM and superior nonlinearity tolerance, while PCS-OFDM achieves the best bit error ratio performance."
2508.17651,"The Onion Router (Tor) relies on path selection algorithms to balance performance and anonymity by determining how traffic flows through its relay network. As Tor scales and usage patterns evolve, default strategies such as bandwidth-weighted random selection and persistent guard nodes face increasing performance limitations. This study presents a comparative evaluation of five path selection strategies: Random, Guard, Congestion-Aware, and two Geographic approaches (Diversity Driven and Latency-Optimized) using a high-fidelity simulation model inspired by TorPS (Tor Path Simulator). Experiments were conducted across five network scales, simulating 37,500 circuits under realistic relay conditions. Results show that Geographic (Latency-Optimized) consistently achieved the lowest latency (40.0 ms) and highest efficiency, while Congestion-Aware strategies delivered the best throughput, outperforming the baseline by up to 42%. Guard nodes maintained stable routing but exhibited latency increases under larger networks. No single method proved optimal across all scenarios, but each revealed clear strengths for specific use cases. These findings demonstrate that targeted path selection can significantly improve Tor's performance without compromising anonymity, providing guidance for optimizing circuit construction in future development and deployments."
2508.17763,"LEO Satellite Networks (LSNs) are revolutionizing global connectivity, but their reliance on tens of thousands of satellites raises pressing concerns over sustainability and survivability. In this work, we argue that the inefficiencies in LSN designs stem from ignoring the strong spatiotemporal structure of Internet traffic demand (which impacts sustainability) and the physical realities of the near-Earth space environment (which affects survivability). We propose a novel design approach based on sun-synchronous (SS) orbits called SS-plane, which aligns satellite coverage with the Earth's diurnal cycle. We demonstrate that SS-plane constellations can reduce the number of satellites required by up to an order of magnitude and cut radiation exposure by ~23% compared to traditional Walker-delta constellations. These findings suggest a paradigm shift in LSN research from large, disposable megaconstellations to more sustainable, targeted LEO constellations."
2508.17911,"Low-altitude airspace is becoming a new frontier for smart city services and commerce. Networks of drones, electric Vertical Takeoff and Landing (eVTOL) vehicles, and other aircraft, termed Low-Altitude Economic Networks (LAENets), promise to transform urban logistics, aerial sensing, and communication. A key challenge is how to efficiently share and trust the computing utility, termed computility, of these aerial devices. We propose treating the computing power on aircraft as tokenized Real-World Assets (RWAs) that can be traded and orchestrated via blockchain. By representing distributed edge computing resources as blockchain tokens, disparate devices can form Low-Altitude Computility Networks (LACNets), collaborative computing clusters in the sky. We first compare blockchain technologies, non-fungible tokens (NFTs), and RWA frameworks to clarify how physical hardware and its computational output can be tokenized as assets. Then, we present an architecture using blockchain to integrate aircraft fleets into a secure, interoperable computing network. Furthermore, a case study models an urban logistics LACNet of delivery drones and air-taxis. Simulation results indicate improvements in task latency, trust assurance, and resource efficiency when leveraging RWA-based coordination. Finally, we discuss future research directions, including AI-driven orchestration, edge AI offloading and collaborative computing, and cross-jurisdictional policy for tokenized assets."
2508.17941,"The rapid expansion of cellular networks and rising demand for high-quality services require efficient and autonomous network management solutions. Zero Touch Network (ZTN) management has emerged as a key approach to automating network operations, minimizing manual intervention, and improving service reliability. Digital Twin (DT) creates a virtual representation of the physical network in realtime, allowing continuous monitoring, predictive analytics, and intelligent decision-making by simulating what-if scenarios. This paper integrates DT with ZTN proactive bandwidth management in end-to-end (E2E) next-generation networks. The integrated architecture applies Few-Shot Learning (FSL) to a memoryaugmented Bidirectional Long Short Term Memory (BiLSTM) model to predict a new network state to augment the known and trained states. Using Q-learning, it determines the optimal action (e.g. traffic shaping) under varying network conditions such that user Quality of Service (QoS) requirements are met. Three scenarios have been considered: 1) normal ZTN operation with closed-loop control, 2) a what-if scenario of DT, and 3) network state unknown to DT. The simulation results show that the network can adapt to underlying changing conditions. In addition, DT-assisted ZTN achieves better performance than the other techniques."
2508.1799,"ACL configuration is essential for managing network flow reachability, yet its complexity grows significantly with topologies and pre-existing rules. To carry out ACL configuration, the operator needs to (1) understand the new configuration policies or intents and translate them into concrete ACL rules, (2) check and resolve any conflicts between the new and existing rules, and (3) deploy them across the network. Existing systems rely heavily on manual efforts for these tasks, especially for the first two, which are tedious, error-prone, and impractical to scale.We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge of the target network, Xumi automatically and accurately translates the natural language intents into complete ACL rules to reduce operators' manual efforts. Xumi then detects all potential conflicts between new and existing rules and generates resolved intents for deployment with operators' guidance, and finally identifies the best deployment plan that minimizes the rule additions while satisfying all intents. Evaluation shows that Xumi accelerates the entire configuration pipeline by over 10x compared to current practices, addresses O(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud network."
2508.18516,"Although the emergence of 6G IoT networks has accelerated the deployment of enhanced smart city services, the resource limitations of IoT devices remain as a significant problem. Given this limitation, meeting the low-latency service requirement of 6G networks becomes even more challenging. However, existing 6G IoT management strategies lack real-time operation and mostly rely on discrete actions, which are insufficient to optimise energy consumption. To address these, in this study, we propose a Digital Twin (DT)-guided energy management framework to jointly handle the low latency and energy efficiency challenges in 6G IoT networks. In this framework, we provide the twin models through a distributed overlay network and handle the dynamic updates between the data layer and the upper layers of the DT over the Real-Time Publish Subscribe (RTPS) protocol. We also design a Reinforcement Learning (RL) engine with a novel formulated reward function to provide optimal data update times for each of the IoT devices. The RL engine receives a diverse set of environment states from the What-if engine and runs Deep Deterministic Policy Gradient (DDPG) to output continuous actions to the IoT devices. Based on our simulation results, we observe that the proposed framework achieves a 37% improvement in 95th percentile latency and a 30% reduction in energy consumption compared to the existing literature."
2508.18702,"Unmanned aerial vehicles (UAVs) can serve as aerial base stations (BSs) to extend the ubiquitous connectivity for ground users (GUs) in the sixth-generation (6G) era. However, it is challenging to cooperatively deploy multiple UAV swarms in large-scale remote areas. Hence, in this paper, we propose a hierarchical UAV swarms structure for 6G aerial access networks, where the head UAVs serve as aerial BSs, and tail UAVs (T-UAVs) are responsible for relay. In detail, we jointly optimize the dynamic deployment and trajectory of UAV swarms, which is formulated as a multi-objective optimization problem (MOP) to concurrently minimize the energy consumption of UAV swarms and GUs, as well as the delay of GUs. However, the proposed MOP is a mixed integer nonlinear programming and NP-hard to solve. Therefore, we develop a K-means and Voronoi diagram based area division method, and construct Fermat points to establish connections between GUs and T-UAVs. Then, an improved non-dominated sorting whale optimization algorithm is proposed to seek Pareto optimal solutions for the transformed MOP. Finally, extensive simulations are conducted to verify the performance of proposed algorithms by comparing with baseline mechanisms, resulting in a 50% complexity reduction."
2508.18725,"The rapid expansion of sixth-generation (6G) wireless networks and the Internet of Things (IoT) has catalyzed the evolution from centralized cloud intelligence towards decentralized edge general intelligence. However, traditional edge intelligence methods, characterized by static models and limited cognitive autonomy, fail to address the dynamic, heterogeneous, and resource-constrained scenarios inherent to emerging edge networks. Agentic artificial intelligence (Agentic AI) emerges as a transformative solution, enabling edge systems to autonomously perceive multimodal environments, reason contextually, and adapt proactively through continuous perception-reasoning-action loops. In this context, the agentification of edge intelligence serves as a key paradigm shift, where distributed entities evolve into autonomous agents capable of collaboration and continual adaptation. This paper presents a comprehensive survey dedicated to Agentic AI and agentification frameworks tailored explicitly for edge general intelligence. First, we systematically introduce foundational concepts and clarify distinctions from traditional edge intelligence paradigms. Second, we analyze important enabling technologies, including compact model compression, energy-aware computing strategies, robust connectivity frameworks, and advanced knowledge representation and reasoning mechanisms. Third, we provide representative case studies demonstrating Agentic AI's capabilities in low-altitude economy networks, intent-driven networking, vehicular networks, and human-centric service provisioning, supported by numerical evaluations. Furthermore, we identify current research challenges, review emerging open-source platforms, and highlight promising future research directions to guide robust, scalable, and trustworthy Agentic AI deployments for next-generation edge environments."
2508.18803,"The proliferation of Internet of things (IoT) devices in smart cities, transportation, healthcare, and industrial applications, coupled with the explosive growth of AI-driven services, has increased demands for efficient distributed computing architectures and networks, driving cloud-edge-terminal collaborative intelligence (CETCI) as a fundamental paradigm within the artificial intelligence of things (AIoT) community. With advancements in deep learning, large language models (LLMs), and edge computing, CETCI has made significant progress with emerging AIoT applications, moving beyond isolated layer optimization to deployable collaborative intelligence systems for AIoT (CISAIOT), a practical research focus in AI, distributed computing, and communications. This survey describes foundational architectures, enabling technologies, and scenarios of CETCI paradigms, offering a tutorial-style review for CISAIOT beginners. We systematically analyze architectural components spanning cloud, edge, and terminal layers, examining core technologies including network virtualization, container orchestration, and software-defined networking, while presenting categorizations of collaboration paradigms that cover task offloading, resource allocation, and optimization across heterogeneous infrastructures. Furthermore, we explain intelligent collaboration learning frameworks by reviewing advances in federated learning, distributed deep learning, edge-cloud model evolution, and reinforcement learning-based methods. Finally, we discuss challenges (e.g., scalability, heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum computing, digital twin), highlighting how integration of distributed computing and communication can address open issues and guide development of robust, efficient, and secure collaborative AIoT systems."
2508.18855,"Time-Sensitive Networking (TSN) is a set of standards that enables the industry to provide real-time guarantees for time-critical communications with Ethernet hardware. TSN supports various queuing and scheduling mechanisms and allows the integration of multiple traffic types in a single network. Network Calculus (NC) can be used to calculate upper bounds for latencies and buffer sizes within these networks, for example, for safety or real-time traffic. We explain the relevance of NC for TSN-based computer communications and potential areas of application. Different NC analysis approaches have been published to examine different parts of TSN and this paper provides a survey of these publications and presents their main results, dependencies, and differences. We present a consistent presentation of the most important results and suggest an improvement to model the output of sending end-devices. To ease access to the current research status, we introduce a common notation to show how all results depend on each other and also identify common assumptions. Thus, we offer a comprehensive overview of NC for industrial networks and identify possible areas for future work."
2508.18863,"With the advent of edge computing, data generated by end devices can be pre-processed before transmission, possibly saving transmission time and energy. On the other hand, data processing itself incurs latency and energy consumption, depending on the complexity of the computing operations and the speed of the processor. The energy-latency-reliability profile resulting from the concatenation of pre-processing operations (specifically, data compression) and data transmission is particularly relevant in wireless communication services, whose requirements may change dramatically with the application domain. In this paper, we study this multi-dimensional optimization problem, introducing a simple model to investigate the tradeoff among end-to-end latency, reliability, and energy consumption when considering compression and communication operations in a constrained wireless device. We then study the Pareto fronts of the energy-latency trade-off, considering data compression ratio and device processing speed as key design variables. Our results show that the energy costs grows exponentially with the reduction of the end-to-end latency, so that considerable energy saving can be obtained by slightly relaxing the latency requirements of applications. These findings challenge conventional rigid communication latency targets, advocating instead for application-specific end-to-end latency budgets that account for computational and transmission overhead."
2508.18883,"To support reliable and low-latency communication, Time-Sensitive Networking introduced protocols and interfaces for resource allocation in Ethernet. However, the implementation of these allocation algorithms has not yet been covered by the standards. Our work focuses on deadline-guaranteeing resource allocation for networks with static and dynamic traffic. To achieve this, we combine offline network optimization heuristics with online admission control and, thus, allow for new flow registrations while the network is running. We demonstrate our solution on Credit-Based Shaper networks by using the delay analysis framework Network Calculus. We compare our approach with an intuitive and a brute-force algorithm, where we can achieve significant improvements, both, in terms of quality and runtime. Thereby, our results show that we can guarantee maximum end-to-end delays and also increase the flexibility of the network while requiring only minimal user input."
2508.18902,"This paper presents the application of Dynamic Spectrum Management (DSM) for future 6G industrial networks, establishing an efficient controller for the Networks-in-Network (NiN) concept. The proposed architecture integrates nomadic as well as static sub-networks (SNs with diverse Quality of Service (QoS) requirements within the coverage area of an overlayer network, managed by a centralized spectrum manager (SM). Control plane connectivity between the SNs and the DSM is ensured by the self-organizing KIRA routing protocol. The demonstrated system enables scalable, zero-touch connectivity and supports nomadic SNs through seamless discovery and reconfiguration. SNs are implemented for modular Industrial Internet of Things (IIoT) scenarios, as well as for mission-critical control loops and for logistics or nomadic behavior. The DSM framework dynamically adapts spectrum allocation to meet real-time demands while ensuring reliable operation. The demonstration highlights the potential of DSM and NiNs to support flexible, dense, and heterogeneous wireless deployments in reconfigurable manufacturing environments."
2508.19067,"Low-Earth Orbit (LEO) satellite networks consist of thousands of satellites orbiting the Earth, enabling low-latency and high-throughput communications across the globe. Such networks present unprecedented challenges due to their dynamic nature, which state-of-the-art data transport protocols do not address. These challenges include: (1) non-congestive latency variation and loss, caused by continuous satellite movement and fluctuating link quality due to weather effects; (2) transient hotspots leading to buffer build-up, latency inflation, and potential packet loss; and (3) frequent handovers, which may result in temporary connectivity loss and re-routing through paths with unknown congestion and delay characteristics. In this paper, we introduce LeoTCP, a novel data transport protocol designed specifically to address these challenges. LeoTCP leverages in-network telemetry (INT) to gather congestion information on a per-hop basis. Using this information, LeoTCP (1) minimises both buffer occupancy and latency for end users, (2) maximises application throughput and network utilisation, and (3) swiftly reacts to network hotspots. We compare LeoTCP to state-of-the-art data transport protocols using a LEO satellite simulation model and targeted micro-benchmarks, both based on OMNeT++/INET. The simulation model captures RTT dynamics in a simulated LEO satellite constellation, while the micro-benchmarks isolate key LEO-specific characteristics, including non-congestive latency variation and loss, path changes, and congestion hotspots. Our results demonstrate that LeoTCP significantly increases goodput compared to existing state-of-the-art approaches, while simultaneously minimising latency."
2508.1913,"This paper introduces a novel analytical framework for evaluating energy-efficient, QoS-aware network-sharing strategies in cellular networks. Leveraging stochastic geometry, our framework enables the systematic assessment of network performance across a range of sharing paradigms, including both conventional single-operator scenarios and advanced hybrid strategies that enable full integration and cooperation among multiple mobile network operators. Our framework incorporates diverse user densities, rate requirements, and energy consumption models to ensure comprehensive analysis. Applying our results to real-world datasets from French mobile network operators, we demonstrate that hybrid network sharing can yield substantial energy savings, up to $35\%$, while maintaining quality of service. Furthermore, our results allow us to characterizing how the benefits of network sharing vary as a function of the geographical and functional characteristics of the deployment area. These findings highlight the potential of collaborative sharing strategies to enhance operational efficiency and sustainability in next-generation cellular networks."
2508.19141,"The Goal-oriented Communication (GoC) paradigm breaks the separation between communication and the content of the data, tailoring communication decisions to the specific needs of the receiver and targeting application performance. While recent studies show impressive encoding performance in point-to-point scenarios, the multi-node distributed scenario is still almost unexplored. Moreover, the few studies to investigate this consider a centralized collision-free approach, where a central scheduler decides the transmission order of the nodes. In this work, we address the Goal-oriented Multiple Access (GoMA) problem, in which multiple intelligent agents must coordinate to share a wireless channel and avoid mutual interference. We propose a theoretical framework for the analysis and optimization of distributed GoMA, serving as a first step towards its complete characterization. We prove that the problem is non-convex and may admit multiple Nash Equilibrium (NE) solutions. We provide a characterization of each node's best response to others' strategies and propose an optimization approach that provably reaches one such NE, outperforming centralized approaches by up to 100% while also reducing energy consumption. We also design a distributed learning algorithm that operates with limited feedback and no prior knowledge."
2509.0013,"Testbed sharing, a practice in which different researchers concurrently develop independent use cases on top of the same testbed, is ubiquitous in wireless experimental research. Its key drawback is experimental inconvenience: one must delay experiments or tolerate compute and RF interference that harms experimental fidelity. In this paper, we propose \textbf{VOTA}, an open-source, software-only testbed scaling method that leverages real-time virtualization and frequency tuning to maximize parallel experiments while controlling interference. In a demonstration of two interference-sensitive 6G use cases -- \textit{MIMO iDFT/DFT Offloading} and \textit{O-RAN DoS Attack} -- running side-by-side on a 32-core host, we showcase VOTA capabilities: \textbf{dedicated-like} results while allowing \textbf{2.67$\times$} more sharing opportunities."
2509.00286,"Satellite Communication (SatCom) networks represent a fundamental pillar in modern global connectivity, facilitating reliable service and extensive coverage across a plethora of applications. The expanding demand for high-bandwidth services and the proliferation of mega satellite constellations highlight the limitations of traditional exclusive satellite spectrum allocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite (CogSat) networks through Dynamic Spectrum Management (DSM), which enables the dynamic adaptability of radio equipment to environmental conditions for optimal performance, presents a promising solution for the emerging spectrum scarcity. In this survey, we explore the adaptation of intelligent DSM methodologies to SatCom, leveraging satellite network integrations. We discuss contributions and hurdles in regulations and standardizations in realizing intelligent DSM in SatCom, and deep dive into DSM techniques, which enable CogSat networks. Furthermore, we extensively evaluate and categorize state-of-the-art Artificial Intelligence (AI)/Machine Learning (ML) methods leveraged for DSM while exploring operational resilience and robustness of such integrations. In addition, performance evaluation metrics critical for adaptive resource management and system optimization in CogSat networks are thoroughly investigated. This survey also identifies open challenges and outlines future research directions in regulatory frameworks, network architectures, and intelligent spectrum management, paving the way for sustainable and scalable SatCom networks for enhanced global connectivity."
2509.00397,"Machine learning (ML) is increasingly being deployed in programmable data planes (switches and SmartNICs) to enable real-time traffic analysis, security monitoring, and in-network decision-making. Decision trees (DTs) are particularly well-suited for these tasks due to their interpretability and compatibility with data-plane architectures, i.e., match-action tables (MATs). However, existing in-network DT implementations are constrained by the need to compute all input features upfront, forcing models to rely on a small, fixed set of features per flow. This significantly limits model accuracy and scalability under stringent hardware resource constraints.We present SPLIDT, a system that rethinks DT deployment in the data plane by enabling partitioned inference over sliding windows of packets. SPLIDT introduces two key innovations: (1) it assigns distinct, variable feature sets to individual sub-trees of a DT, grouped into partitions, and (2) it leverages an in-band control channel (via recirculation) to reuse data-plane resources (both stateful registers and match keys) across partitions at line rate. These insights allow SPLIDT to scale the number of stateful features a model can use without exceeding hardware limits. To support this architecture, SPLIDT incorporates a custom training and design-space exploration (DSE) framework that jointly optimizes feature allocation, tree partitioning, and DT model depth. Evaluation across multiple real-world datasets shows that SPLIDT achieves higher accuracy while supporting up to 5x more stateful features than prior approaches (e.g., NetBeacon and Leo). It maintains the same low time-to-detection (TTD) as these systems, while scaling to millions of flows with minimal recirculation overhead (<0.05%)."
2509.00567,Interference is the major problem now days in telecommunication sector. One type of interference which is very common now days is FM Cell sites interference between CDMA Cell sites. Which are the types of interference and various observations during this interference is discussed below in this paper.
2509.00603,"Cross-silo Federated Learning (FL) enables multiple institutions to collaboratively train machine learning models while preserving data privacy. In such settings, clients repeatedly exchange model weights with a central server, making the overall training time highly sensitive to network performance. However, conventional routing methods often fail to prevent congestion, leading to increased communication latency and prolonged training. Software-Defined Networking (SDN), which provides centralized and programmable control over network resources, offers a promising way to address this limitation. To this end, we propose SmartFLow, an SDN-based framework designed to enhance communication efficiency in cross-silo FL. SmartFLow dynamically adjusts routing paths in response to changing network conditions, thereby reducing congestion and improving synchronization efficiency. Experimental results show that SmartFLow decreases parameter synchronization time by up to 47% compared to shortest-path routing and 41% compared to capacity-aware routing. Furthermore, it achieves these gains with minimal computational overhead and scales effectively to networks of up to 50 clients, demonstrating its practicality for real-world FL deployments."
2509.00621,"Federated Learning (FL) presents a robust paradigm for privacy-preserving, decentralized machine learning. However, a significant gap persists between the theoretical design of FL algorithms and their practical performance, largely because existing evaluation tools often fail to model realistic operational conditions. Many testbeds oversimplify the critical dynamics among algorithmic efficiency, client-level heterogeneity, and continuously evolving network infrastructure. To address this challenge, we introduce the Federated Learning Emulation and Evaluation Testbed (FLEET). This comprehensive platform provides a scalable and configurable environment by integrating a versatile, framework-agnostic learning component with a high-fidelity network emulator. FLEET supports diverse machine learning frameworks, customizable real-world network topologies, and dynamic background traffic generation. The testbed collects holistic metrics that correlate algorithmic outcomes with detailed network statistics. By unifying the entire experiment configuration, FLEET enables researchers to systematically investigate how network constraints, such as limited bandwidth, high latency, and packet loss, affect the convergence and efficiency of FL algorithms. This work provides the research community with a robust tool to bridge the gap between algorithmic theory and real-world network conditions, promoting the holistic and reproducible evaluation of federated learning systems."
2509.00701,"Traffic classification, a technique for assigning network flows to predefined categories, has been widely deployed in enterprise and carrier networks. With the massive adoption of mobile devices, encryption is increasingly used in mobile applications to address privacy concerns. Consequently, traditional methods such as Deep Packet Inspection (DPI) fail to distinguish encrypted traffic. To tackle this challenge, Artificial Intelligence (AI), in particular Machine Learning (ML), has emerged as a promising solution for encrypted traffic classification. A crucial prerequisite for any ML-based approach is traffic data cleaning, which removes flows that are not useful for training (e.g., irrelevant protocols, background activity, control-plane messages, and long-lived sessions). Existing cleaning solutions depend on manual inspection of every captured packet, making the process both costly and time-consuming. In this poster, we present an unsupervised framework that automatically cleans encrypted mobile traffic. Evaluation on real-world datasets shows that our framework incurs only a 2%~2.5% reduction in classification accuracy compared with manual cleaning. These results demonstrate that our method offers an efficient and effective preprocessing step for ML-based encrypted traffic classification."
2509.00708,"Link failures occur frequently in Internet Service Provider (ISP) networks and pose significant challenges for Traffic Engineering (TE). Existing TE schemes either reroute traffic over vulnerable static paths, leading to performance degradation, or precompute backup routes for a broad range of failure scenarios, which introduces high overhead and limits scalability. Hence, an effective failure recovery mechanism is required to offer sufficient path diversity under constrained overhead, thereby ensuring robust and performant network operation. This paper presents ReWeave, a scalable and efficient link-level TE scheme that enables localized rerouting by equipping each link with a compact set of adjacent-only backup paths. Upon detecting a failure, only the routers at both ends of the failed link reroute traffic dynamically using SRv6-based detours, without controller intervention or full-path recomputation. Evaluation results on large-scale backbone networks demonstrate that ReWeave outperforms existing TE schemes in link failure scenarios. Compared to HARP, the state-of-the-art failure recovery scheme based on centralized control and dynamic traffic reallocation, our approach reduces the average maximum link utilization by 10.5%~20.1%, and lowers the worst-case utilization by 29.5%~40.9%. When compared with Flexile, a protection-based scheme that precomputes routes for multi-failure scenarios, ReWeave achieves a similarly low packet loss rate in 90% of failure cases, while maintaining a response speed comparable to the fastest router-based local rerouting schemes."
2509.00868,"Cellular-connected UAV systems have enabled a wide range of low-altitude aerial services. However, these systems still face many challenges, such as frequent handovers and the inefficiency of traditional transport protocols. To better study these issues, we develop a modular and scalable simulation platform specifically designed for UAVs communication leveraging the research ecology in wireless communication of MATLAB. The platform supports flexible 5G NR node deployment, customizable UAVs mobility models, and multi-network-interface extensions. It also supports multiple transport protocols including TCP, UDP, QUIC, etc., allowing to investigate how different transport protocols affect UAVs communication performance. In addition, the platform includes a handover management module, enabling the evaluation of both traditional and learning-based handover strategies. Our platform can serve as a testbed for the development and evaluation of advanced transmission strategies in cellular-connected UAV systems."
2509.00885,"The multichannel rendezvous problem (MRP) is a critical challenge for neighbor discovery in IoT applications, requiring two users to find each other by hopping among available channels over time. This paper addresses the MRP in scenarios where a global channel enumeration system is unavailable. To tackle this challenge, we propose a suite of low-complexity multichannel rendezvous algorithms based on locality-sensitive hashing (LSH), tailored for environments where channel labels are unique L-bit identifiers rather than globally coordinated indices. Inspired by consistent hashing techniques in distributed systems, we develop the LC-LSH and LC-LSH4 algorithms for synchronous and asynchronous settings, respectively. These algorithms significantly reduce implementation complexity while maintaining expected time-to-rendezvous (ETTR) performance comparable to state-of-the-art methods that require global channel enumeration. To ensure bounded maximum time-to-rendezvous (MTTR) in the asynchronous setting, we further introduce the ASYM-LC-LSH4 and QR-LC-LSH4 algorithms by embedding multiset-enhanced modular clock and quasi-random techniques into our framework. Extensive simulations demonstrate that the proposed algorithms achieve performance comparable to state-of-the-art LSH algorithms in both synchronous and asynchronous settings, even without a global channel enumeration system."
2509.00967,"The BUBBLE-BLUE (BB) project aims to create private Bluetooth bubbles on top of smartphones and to create a kind of terrestrial STARLINK network based on users smartphones.. In each private bubble, participants will be able to communicate autonomously, without recourse to private operator networks, neither data nor cellular, relying solely on the Bluetooth technology of smartphones. The routing strategy is based on dynamic Connected Dominant Sets (CDS). We present the specific features of a BB network as well as some simulation results on their routing performance."
2509.01008,"This work explores the integration of Quantum Machine Learning (QML) and Quantum-Inspired (QI) techniques for optimizing end-to-end (E2E) network services in telecommunication systems, particularly focusing on 5G networks and beyond. The application of QML and QI algorithms is investigated, comparing their performance with classical Machine Learning (ML) approaches. The present study employs a hybrid framework combining quantum and classical computing leveraging the strengths of QML and QI, without the penalty of quantum hardware availability. This is particularized for the optimization of the Quality of Experience (QoE) over cellular networks. The framework comprises an estimator for obtaining the expected QoE based on user metrics, service settings, and cell configuration, and an optimizer that uses the estimation to choose the best cell and service configuration. Although the approach is applicable to any QoE-based network management, its implementation is particularized for the optimization of network configurations for Cloud Gaming services. Then, it is evaluated via performance metrics such as accuracy and model loading and inference times for the estimator, and time to solution and solution score for the optimizer. The results indicate that QML models achieve similar or superior accuracy to classical ML models for estimation, while decreasing inference and loading times. Furthermore, potential for better performance is observed for higher-dimensional data, highlighting promising results for higher complexity problems. Thus, the results demonstrate the promising potential of QML in advancing network optimization, although challenges related to data availability and integration complexities between quantum and classical ML are identified as future research lines."
2509.01201,"Wi-Fi 7 introduces Multi-link operation (MLO) to enhance throughput and latency performance compared to legacy Wi-Fi standards. MLO enables simultaneous transmission and reception through multiple links, departing from conventional single-link operations (SLO). To fully exploit MLO's potential, it is essential to investigate Wi-Fi 7's coexistence performance with legacy Wi-Fi devices. Existing approaches, however, have overlooked some crucial aspects of MLO, necessitating the development of a standards-compliant analytical framework to model the actual channel access mechanism of MLO. Therefore, this paper tries to fill the gap by proposing a set of novel Markov chains (MC) to accurately model the MLO operation aligned with multi-link backoff behaviors specified by the standard. Specifically, we design two separate MCs for AP and non-AP multi-link devices (MLD) respectively, based on which transmit and collision probabilities are derived under the saturated traffic condition. Then, we also derive closed-form expressions for the throughput of various device types in the coexistence scenario between Wi-Fi 7 and legacy Wi-Fi, including AP MLD, non- AP MLD, and legacy devices. To validate the accuracy of our proposed models, we developed an ns-3 based simulator by implementing both STR(simultaneous transmission and reception) and NSTR(non-STR) based MLO operations. Our ns-3 based extensive simulations have demonstrated that the proposed analytic model provides accurate estimates on the per device throughput performance, while also revealing the dynamics of inter-WLAN coexistence scenarios."
2509.01276,"During the development of the Sixth Generation (6G) networks, the integration of Artificial Intelligence (AI) into network systems has become a focal point, leading to the concept of AI-native networks. High quality data is essential for developing such networks. Although some studies have explored data collection and analysis in 6G networks, significant challenges remain, particularly in real-time data acquisition and processing. This paper proposes a comprehensive data collection method that operates in parallel with bitstream processing for wireless communication networks. By deploying data probes, the system captures real-time network and system status data in software-defined wireless communication networks. Furthermore, a data support system is implemented to integrate heterogeneous data and provide automatic support for AI model training and decision making. Finally, a 6G communication testbed using OpenAirInterface5G and Open5GS is built on Kubernetes, as well as the system's functionality is demonstrated via a network traffic prediction case study."
2509.01427,"With the rapid development of low-altitude wireless networking, autonomous aerial vehicles (AAVs) have emerged as critical enablers for timely and reliable data delivery, particularly in remote or underserved areas. In this context, the age of information (AoI) has emerged as a critical performance metric for evaluating the freshness and timeliness of transmitted information in Internet of things (IoT) networks. However, conventional AAV-assisted data transmission is fundamentally limited by finite communication coverage ranges, which requires periodic return flights for data relay operations. This propulsion-repositioning cycle inevitably introduces latency spikes that raise the AoI while degrading service reliability. To address these challenges, this paper proposes a AAV-assisted forwarding system based on distributed beamforming to enhance the AoI in IoT. Specifically, AAVs collaborate via distributed beamforming to collect and relay data between the sensor nodes and remote base station. Then, we formulate an optimization problem to minimize the AoI and AAV energy consumption, by jointly optimizing the AAV trajectories and communication schedules. Due to the non-convex nature of the problem and its pronounced temporal variability, we introduce a deep reinforcement learning solution that incorporates temporal sequence input, layer normalization gated recurrent unit, and a squeeze-and-excitation block to capture long-term dependencies, thereby improving decision-making stability and accuracy, and reducing computational complexity. Simulation results demonstrate that the proposed SAC-TLS algorithm outperforms baseline algorithms in terms of convergence, time average AoI, and energy consumption of AAVs."
2509.01694,"We propose a framework for resource provisioning with QoS guarantees in shared infrastructure networks. Our novel framework provides tunable probabilistic service guarantees for throughput and delay. Key to our approach is a Modified Dirft-plus-Penalty (MDP) policy that ensures long-term stability while capturing short-term probabilistic service guarantees using linearized upper-confidence bounds. We characterize the feasible region of service guarantees and show that our MDP procedure achieves mean rate stability and an optimality gap that vanishes with the frame size over which service guarantees are provided. Finally, empirical simulations validate our theory and demonstrate the favorable performance of our algorithm in handling QoS in multi-infrastructure networks."
2509.01891,"The transformation of 5G networks into software-defined, agile, intelligent and programmable architectures necessitates a paradigm shift in deployment strategies. To deliver superior performance and surpass traditional systems, public and private 5G networks must adopt software-centric cloud native frameworks that enable flexibility through tailored configurations and optimized deployment approaches. In Singapore, the Infocomm Media Development Authority (IMDA) and the National Research Foundation Singapore (NRF) launched the Future Communications Research and Development Programme (FCP) to advance the nation's communications and connectivity landscape. At the core of this initiative is the Future Communications Translation Lab (FCT) at the Singapore Institute of Technology (SIT), which focuses on advancing 5G technologies to higher readiness levels, facilitating their adoption across various industries. A key component is the deployment of FCT O-RAN, a state-of-the-art multi-vendor private 5G platform. The setup includes a 5G core network powered by Microsoft Affirmed and ENEA, O-RAN Centralized and Distributed Units from Radisys. Indoor Remote Units are deployed with Foxconn, while outdoor RUs are deployed with Benetel. To optimize the deployment of remote units, a digital twin was created using Wireless InSite, and performance evaluations were conducted for both the digital twin and the private 5G deployment. Smartphones equipped with QualiPoc were used to measure network performance. The testbed demonstrated effective performance with optimized bandwidth allocations for both indoor and outdoor environments. In the indoor setup, utilizing 50 MHz of bandwidth, a downlink throughput of 713 Mbps and an uplink throughput of 66 Mbps were achieved. Meanwhile, the outdoor setup, utilizing 40 MHz of bandwidth, achieved a downlink throughput of 371 Mbps and an uplink throughput of 55 Mbps."
2509.01906,"Mobile devices increasingly rely on deep neural networks (DNNs) for complex inference tasks, but running entire models locally drains the device battery quickly. Offloading computation entirely to cloud or edge servers reduces processing load at devices but poses privacy risks and can incur high network bandwidth consumption and long delays. Split computing (SC) mitigates these challenges by partitioning DNNs between user equipment (UE) and edge servers. However, 5G wireless channels are time-varying and a fixed splitting scheme can lead to sub-optimal solutions. This paper addresses the limitations of fixed model partitioning in privacy-focused image processing and explores trade-offs in key performance metrics, including end-to-end (E2E) latency, energy consumption, and privacy, by developing an adaptive ML partitioning scheme based on real-time AI-powered throughput estimation. Evaluation in multiple scenarios demonstrates significant performance gains of our scheme."
2509.01926,"We investigate a real-time remote inference system where multiple correlated sources transmit observations over a communication channel to a receiver. The receiver utilizes these observations to infer multiple time-varying targets. Due to limited communication resources, the delivered observations may not be fresh. To quantify data freshness, we employ the Age of Information (AoI) metric. To minimize the inference error, we aim to design a signal-agnostic scheduling policy that leverages AoI without requiring knowledge of the actual target values or the source observations. This scheduling problem is a restless multi-armed bandit (RMAB) problem with a non-separable penalty function. Unlike traditional RMABs, the correlation among sources introduces a unique challenge: the penalty function of each source depends on the AoI of other correlated sources, preventing decomposition of the problem into multiple independent Markov Decision Processes (MDPs), a key step in applying traditional RMAB solutions. To address this, we propose a novel approach by approximating the penalty function of each source and establish an analytical bound on the approximation error. We then develop scheduling policies for two scenarios: (i) full knowledge of the penalty functions and (ii) no knowledge of the penalty functions. For the case of known penalty functions, we present an upper bound on the optimality gap of our policy in the asymptotic regime. For the case of unknown penalty functions and signal distributions, we develop an online learning approach that utilizes bandit feedback to learn an online Maximum Gain First (MGF) policy. Simulation results demonstrate the effectiveness of our proposed policies in minimizing inference error and achieving scalability in the number of sources."
2509.01957,"Foundation models (FMs) have shown remarkable capabilities in generalized intelligence, multimodal understanding, and adaptive learning across a wide range of domains. However, their deployment in harsh or austere environments -- characterized by intermittent connectivity, limited computation, noisy data, and dynamically changing network topologies -- remains an open challenge. Existing distributed learning methods such as federated learning (FL) struggle to adapt in such settings due to their reliance on stable infrastructure, synchronized updates, and resource-intensive training. In this work, we explore the potential of Federated Foundation Models (FFMs) as a promising paradigm to address these limitations. By integrating the scalability and generalization power of FMs with novel decentralized, communication-aware FL frameworks, we aim to enable robust, energy-efficient, and adaptive intelligence in extreme and adversarial conditions. We present a detailed breakdown of system-level constraints in harsh environments, and discuss the open research challenges in communication design, model robustness, and energy-efficient personalization for these unique settings."
2509.02124,"The escalating demands of immersive communications, alongside advances in network softwarization and AI-driven cognition and generative reasoning, create a pivotal opportunity to rethink and reshape the future Internet. In this context, we introduce in this paper, FlexNGIA 2.0, an Agentic AI-driven Internet architecture that leverages LLM-based AI agents to autonomously orchestrate, configure, and evolve the network. These agents can, at runtime, perceive, reason, coordinate among themselves to dynamically design, implement, deploy, and adapt communication protocols, Service Function Chains (SFCs), network functions, resource allocation strategies, congestion control, and traffic engineering schemes, thereby ensuring optimal performance, reliability, and efficiency under evolving conditions.The paper first outlines the overall architecture of FlexNGIA 2.0 and its constituent LLM-Based AI agents. For each agent, we detail its design, implementation, inputs and outputs, prompt structures, interactions with tools and other agents, followed by preliminary proof-of-concept experiments demonstrating its operation and potential. The results clearly highlight the ability of these LLM-based AI agents to automate the design, the implementation, the deployment, and the performance evaluation of transport protocols, service function chains, network functions, congestion control schemes, and resource allocation strategies.FlexNGIA 2.0 paves the way for a new class of Agentic AI-Driven networks, where fully cognitive, self-evolving AI agents can autonomously design, implement, adapt and optimize the network's protocols, algorithms, and behaviors to efficiently operate across complex, dynamic, and heterogeneous environments. To bring this vision to reality, we also identify key research challenges toward achieving fully autonomous, adaptive, and agentic AI-driven networks."
2509.02149,"Large-scale low-Earth-orbit (LEO) constellations demand routing that simultaneously minimizes energy, guarantees delivery under congestion, and meets latency requirements for time-critical flows. We present a segment routing over IPv6 (SRv6) flexible algorithm (Flex-Algo) framework that consists of three logical slices: an energy-efficient slice (Algo 130), a high-reliability slice (Algo 129), and a latency-sensitive slice (Algo 128). The framework provides a unified mixed-integer linear program (MILP) that combines satellite CPU power, packet delivery rate (PDR), and end-to-end latency into a single objective, allowing a lightweight software-defined network (SDN) controller to steer traffic from the source node. Emulation of Telesat's Lightspeed constellation shows that, compared with different routing schemes, the proposed design reduces the average CPU usage by 73%, maintains a PDR above 91% during traffic bursts, and decreases urgent flow delay by 18 ms between Ottawa and Vancouver. The results confirm Flex-Algo's value as a slice-based traffic engineering (TE) tool for resource-constrained satellite networks."
2509.02317,"The rapid development of AI agents leads to a surge in communication demands. Alongside this rise, a variety of frameworks and protocols emerge. While these efforts demonstrate the vitality of the field, they also highlight increasing fragmentation, with redundant innovation and siloed designs hindering cross-domain interoperability. These challenges underscore the need for a systematic perspective to guide the development of scalable, secure, and sustainable AI agent ecosystems. To address this need, this paper provides the first systematic analysis of AI agent communication from the standpoint of Internet architecture-the most successful global-scale distributed system in history. Specifically, we distill decades of Internet evolution into five key elements that are directly relevant to agent communication: scalability, security, real-time performance, high performance, and manageability. We then use these elements to examine both the opportunities and the bottlenecks in developing robust multi-agent ecosystems. Overall, this paper bridges Internet architecture and AI agent communication for the first time, providing a new lens for guiding the sustainable growth of AI agent communication ecosystems."
2509.02366,"Battery management systems (BMSs) are critical to ensuring safety, efficiency, and longevity across electronics, transportation, and energy storage. However, with the rapid growth of lithium-ion batteries, conventional reactive BMS approaches face limitations in health prediction and advanced maintenance management, resulting in increased safety risks and economic costs. To address these challenges, we propose a five-tier digital twin framework for intelligent battery management. The framework spans geometric visualization, predictive modeling, prescriptive optimization, and autonomous operation, enabling full lifecycle optimization. In validation, an electrochemical model calibrated via Bayesian optimization achieved strong alignment with measured voltage and temperature, with Mean Absolute Percentage Errors (MAPE) below 1.57\% and 0.39\%. A Physics-Informed Neural Network (PINN) then combined data and simulations to predict State of Health (SOH), attaining MAPE under 3\% with quantified uncertainty. This framework elevates BMSs into intelligent systems capable of proactive management and autonomous optimization, advancing safety and reliability in critical applications."
2509.02373,"In this work, a set reconciliation setting is considered in which two parties have similar sets that they would like to reconcile. In particular, we focus on a divide-and-conquer strategy known as partitioned set reconciliation (PSR), in which the sets to be reconciled are successively partitioned until they contain a number of differences below some predetermined value. Borrowing techniques from tree-algorithms for random-access protocols, we present and analyze a novel set reconciliation scheme that we term enhanced partitioned set reconciliation (EPSR). This approach improves the efficiency in terms of overhead, i.e., it yields a lower communication cost, while keeping the same time and communication round complexity as PSR. Additionally, we simulate the performance of the proposed algorithm in an event-driven simulator. Our findings indicate that this novel protocol nearly halves the communication cost of PSR while maintaining the same time complexity."
2509.0242,"This paper presents the first ever fully open-source implementation of Load Balancing (LB) in an experimental Fifth Generation (5G) New Radio (NR) Standalone (SA) network using Open Radio Access Network (O-RAN) architecture. The deployment leverages the O-RAN Software Community (SC) Near Real-Time RAN Intelligent Controller (Near-RT RIC), srsRAN stack, Open5GS core, and Software-Defined Radios (SDRs), with Commercial Off-The-Shelf (COTS) User Equipments (UEs). The implementation extends the srsRAN stack to support E2 Service Model for RAN Control (E2SM-RC) Style 3 Action 1 to facilitate Handovers (HOs) and adds Medium Access Control (MAC) downlink (DL) buffer volume reporting to srsRAN's E2 Service Model for Key Performance Measurement (E2SM-KPM). The deployment demonstrates Near-RT RIC closed-loop control where our Mobility Load Balancing (MLB) xApp makes HO decisions based on network load metrics for LB between two Open Distributed Units (O-DUs) operating at different frequencies in the same band."
2509.02551,"The integration of digital twinning technologies is driving next-generation networks toward new capabilities, allowing operators to thoroughly understand network conditions, efficiently analyze valuable radio data, and innovate applications through user-friendly, immersive interfaces. Building on this foundation, network digital twins (NDTs) accurately depict the operational processes and attributes of network infrastructures, facilitating predictive management through real-time analysis and measurement. However, constructing precise NDTs poses challenges, such as integrating diverse data sources, mapping necessary attributes from physical networks, and maintaining scalability for various downstream tasks. Unlike previous works that focused on the creation and mapping of NDTs from scratch, we explore intra- and inter-operations among NDTs within a Unified Twin Transformation (UTT) framework, which uncovers a new computing paradigm for efficient transfer, merging, and splitting of NDTs to create task-oriented twins. By leveraging joint multi-modal and distributed mapping mechanisms, UTT optimizes resource utilization and reduces the cost of creating NDTs, while ensuring twin model consistency. A theoretical analysis of the distributed mapping problem is conducted to establish convergence bounds for this multi-modal gated aggregation process. Evaluations on real-world twin-assisted applications, such as trajectory reconstruction, human localization, and sensory data generation, demonstrate the feasibility and effectiveness of interoperability among NDTs for corresponding task development."
2509.02806,"Mobile application performance relies heavily on the congestion control design of the underlying transport, which is typically bottlenecked by cellular link and has to cope with rapid cellular link bandwidth fluctuations. We observe that radio KPI measurements from the mobile device chipset can be exploited for precise and timely measurement of available bandwidth on the cellular link. Building on this insight, we propose Biscay, a practical and radio KPI-driven congestion control system design for mobile networks. Biscay leverages OpenDiag, the in-kernel real-time radio KPI extraction tool we introduce in this paper, along with our KPI-based accurate bandwidth determination layer towards dynamically adjusting the congestion window to optimally use the available bandwidth while keeping delay to the minimum. Our solution is practical and deployable, as shown through our implementation of Biscay and OpenDiag on unrooted Android 5G phones. We extensively evaluate Biscay against different state-of-the-art congestion control designs including BBR and CUBIC with emulations driven by real measurement traces as well as real-world experiments spanning diverse 4G and 5G scenarios, and show that it provides significant average and tail delay improvements (typically over 90% reduction) while yielding better or similar throughput. These gains are enabled by 100% improvement in the granularity of on-device radio KPI measurements with OpenDiag compared to existing alternatives like MobileInsight."
2509.02811,"The integration of Internet of Things (IoT) and Non-Terrestrial Networks (NTNs) has emerged as a key paradigm to provide connectivity for sensors and actuators via satellite gateways in remote areas where terrestrial infrastructure is limited or unavailable. Among other Low-Power Wide-Area Network (LPWAN) technologies for IoT, Long Range (LoRa) holds great potential given its long range, energy efficiency, and flexibility. In this paper, we explore the feasibility and performance of LoRa to support large-scale IoT connectivity through Low Earth Orbit (LEO) satellite gateways. To do so, we developed a new ns3-LoRa-NTN simulation module, which integrates and extends the ns3-LoRa and ns3-NTN modules, to enable full-stack end-to-end simulation of satellite communication in LoRa networks. Our results, given in terms of average data rate and Packet Reception Ratio (PRR), confirm that LoRa can effectively support direct communication from the ground to LEO satellites, but network optimization is required to mitigate collision probability when end nodes use the same Spreading Factors (SFs) over long distances."
2509.02824,"The 6 GHz spectrum, recently opened for unlicensed use under Wi-Fi 6E and Wi-Fi 7, overlaps with frequencies used by mission-critical incumbent systems such as public safety communications and utility infrastructure. To prevent interference, the FCC mandates the use of Automated Frequency Coordination (AFC) systems, which assign safe frequency and power levels based on Wi-Fi Access Point (AP)-reported locations. In this work, we demonstrate that GPS-based location reporting, which Wi-Fi APs use, can be spoofed using inexpensive, off-the-shelf radio equipment. This enables attackers to manipulate AP behavior, gain unauthorized spectrum access, cause harmful interference, or disable APs entirely by spoofing them into foreign locations. We validate these attacks in a controlled lab setting against a commercial AP and evaluate a commercial AFC system under spoofed scenarios. Our findings highlight critical gaps in the security assumptions of AFC and motivate the need for stronger location integrity protections."
2509.03,"Open Radio Access Network (Open RAN) is reshaping mobile network architecture by promoting openness, disaggregation, and cross-vendor interoperability. However, this architectural flexibility introduces new security challenges, especially in deployments where multiple mobile network operators (MNOs) jointly operate shared components. Existing Zero Trust Architectures (ZTA) in O-RAN, as defined by governmental and industry standards, implicitly assume that authenticated components will comply with operational policies. However, this assumption creates a critical blind spot: misconfigured or compromised components can silently violate policies, misuse resources, or corrupt downstream processes (e.g., ML-based RIC xApps).To address this critical gap, we propose a monitoring framework for low-trust O-RAN environments that proactively verifies configuration state and control behavior against tenant-defined policies. Our system provides scalable, verifiable oversight to enhance transparency and trust in O-RAN operations. We implement and evaluate the framework using standardized O-RAN configurations, with total processing latency of approximately 200 ms, demonstrating its efficiency and practicality for timely policy enforcement and compliance auditing in multi-MNO deployments."
2509.03049,"In the upcoming 6G era, the communication networks are expected to face unprecedented challenges in terms of complexity and dynamics. Digital Twin (DT) technology, with its various digital capabilities, holds great potential to facilitate the transformation of the communication network from passive responding to proactive adaptation. Thus, in this paper, we propose a multi-layer DT system that coordinates local DT, edge DT, and cloud DT for future network architecture and functions. In our vision, the proposed DT system will not only achieve real-time data-driven decision-making and digital agent functions previously handled by centralized DT, but will do so in a more distributed, mobile, layer-by-layer manner. Moreover, it will supply essential data, pre-trained models, and open interfaces for future metaverse applications, enabling creators and users to efficiently develop and experience metaverse services."
2509.0329,"The ever-increasing reliance of critical services on network infrastructure coupled with the increased operational complexity of beyond-5G/6G networks necessitate the need for proactive and automated network fault management. The provision for open interfaces among different radio access network\,(RAN) elements and the integration of AI/ML into network architecture enabled by the Open RAN\,(O-RAN) specifications bring new possibilities for active network health monitoring and anomaly detection. In this paper we leverage these advantages and develop an anomaly detection framework that proactively detect the possible throughput drops for a UE and minimize the post-handover failures. We propose two actionable anomaly detection algorithms tailored for real-world deployment. The first algorithm identifies user equipment (UE) at risk of severe throughput degradation by analyzing key performance indicators (KPIs) such as resource block utilization and signal quality metrics, enabling proactive handover initiation. The second algorithm evaluates neighbor cell radio coverage quality, filtering out cells with anomalous signal strength or interference levels. This reduces candidate targets for handover by 41.27\% on average. Together, these methods mitigate post-handover failures and throughput drops while operating much faster than the near-real-time latency constraints. This paves the way for self-healing 6G networks."
2509.03381,"Robot Operating System 2 (ROS 2) relies on the Data Distribution Service (DDS), which offers more than 20 Quality of Service (QoS) policies governing availability, reliability, and resource usage. Yet ROS 2 users lack clear guidance on safe policy combinations and validation processes prior to deployment, which often leads to trial-and-error tuning and unexpected runtime failures. To address these challenges, we analyze DDS Publisher-Subscriber communication over a life cycle divided into Discovery, Data Exchange, and Disassociation, and provide a user oriented tutorial explaining how 16 QoS policies operate in each phase. Building on this analysis, we derive a QoS dependency chain that formalizes inter-policy relationships and classifies 41 dependency violation rules, capturing constraints that commonly cause communication failures in practice. Finally, we introduce QoS Guard, a ROS 2 package that statically validates DDS XML profiles offline, flags conflicts, and enables safe, predeployment tuning without establishing a live ROS 2 session. Together, these contributions give ROS 2 users both conceptual insight and a concrete tool that enables early detection of misconfigurations, improving the reliability and resource efficiency of ROS 2 based robotic systems."
2509.03386,"As the increasing development of low-altitude aircrafts, the rational design of low-altitude networks directly impacts the aerial safety and resource utilization. To address the challenges of environmental complexity and aircraft diversity in the traffic management, we propose a hierarchical low-altitude wireless network (HLWN) framework. Empowered by the threedimensional spatial discretization and integrated wireless monitoring mechanisms in HLWN, we design low-altitude air corridors to guarantee safe operation and optimization. Besides, we develop the multi-dimensional flight risk assessment through conflict detection and probabilistic collision analysis, facilitating dynamic collision avoidance for heterogeneous aircrafts. Finally, the open issues and future directions are investigated to provide insights into HLAN development."
2509.03667,"Quantum networks rely on high fidelity entangled pairs distributed to nodes, but maintaining their fidelity is challenged by environmental decoherence during storage. Entanglement purification is used to restore fidelity, but the idle periods imposed by the associated classical communication delays counteract this goal by exposing the states to further decoherence. In this work, we analyze the practical viability of entanglement purification protocols (BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet protocol (IP) communications networks. We present a comprehensive performance evaluation of these protocols in various network conditions for a range of quantum memory technologies. We employ a microscopic Lindblad treatment of the underlying quantum dynamics, and use current-generation metropolitan IP network latency statistics and parameters drawn from quantum memory testbeds. In doing so we identify the regions in which entanglement purification succeeds and fails, delineated by break-even iso-fidelity contours in the phase space. We then determine the total number of entangled pairs required to complete a multi-round purification protocol, and the steady-state throughput of entangled pairs with purified fidelities that exceed application-specific thresholds. This provides latency budgets, memory quality targets, and resource-overhead estimates for deploying purification on current and near-future networks."
2509.03762,"We consider the problem of joint routing and scheduling in queueing networks, where the edge transmission costs are unknown. At each time-slot, the network controller receives noisy observations of transmission costs only for those edges it selects for transmission. The network controller's objective is to make routing and scheduling decisions so that the total expected cost is minimized. This problem exhibits an exploration-exploitation trade-off, however, previous bandit-style solutions cannot be directly applied to this problem due to the queueing dynamics. In order to ensure network stability, the network controller needs to optimize throughput and cost simultaneously. We show that the best achievable cost is lower bounded by the solution to a static optimization problem, and develop a network control policy using techniques from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as compared to the best policy that has complete knowledge of arrivals and costs. Finally, we evaluate the proposed policy using simulations and show that its regret is indeed sub-linear."
2509.03818,"In this work, we develop a measurement platform to capture mobile network performance metrics including coverage and quality of service in regions where conventional coverage testing approaches are frequently time-intensive, labor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing methods are used to collect cellular network performance metrics. However, these approaches are inadequate in rural areas due to low-density population, and difficult terrain. The platform described here is a UAV-based and is designed to investigate the mobile network performance through aerial operations and gather Radio Access Network (RAN) signal alongside end-to-end network performance metrics. Our platform gathers metrics through the integration of an onboard computation unit and commercial off-the-shelf cellular modem. The gathered data are subsequently analyzed and displayed using geospatial mapping utilities and statistical techniques to deliver key observations on cellular network performance. Experimental results showed that the received signal power improves at higher altitudes due to enhanced line-of-sight (LoS) conditions as expected. However, the signal quality degrades as a result of increased interference from neighboring cells. The analysis reveals that for most of the geographic area covered in the initial experiments the system maintained acceptable signal quality, with adequate throughput performance for both uplink and downlink communications, while maintaining satisfactory round-trip time characteristics. Notably, the experiment showed that a strong radio signal metric for a given cell does not necessarily translate to consistent spatial coverage across the tested region."
2509.03901,"Indoor positioning is an enabling technology for home, office, and industrial network users because it provides numerous information and communication technology (ICT) and Internet of things (IoT) functionalities such as indoor navigation, smart meter localization, asset tracking, support for emergency services, and detection of hazardous situations. The IEEE 802.11mc fine timing measurement (FTM) protocol (commercially known as Wi-Fi Location) has great potential to enable indoor positioning in future generation devices, primarily because of the high availability of Wi-Fi networks, FTM's high accuracy and device support. Furthermore, new FTM enhancements are available in the released (802.11az) and recently completed (802.11bk) amendments. Despite the multitude of literature reviews on indoor positioning, a survey dedicated to FTM and its recent enhancements has so far been lacking. We fill this gap by classifying and reviewing over 180 research papers related to the practical accuracy achieved with FTM, methods for improving its accuracy (also with machine learning), combining FTM with other indoor positioning systems, FTM-based applications, and security issues. Based on the conducted survey, we summarize the most important research achievements and formulate open areas for further research."
2509.03935,"This work considers a parallel task execution strategy in vehicular edge computing (VEC) networks, where edge servers are deployed along the roadside to process offloaded computational tasks of vehicular users. To minimize the overall waiting delay among vehicular users, a novel task offloading solution is implemented based on the network cooperation balancing resource under-utilization and load congestion. Dual evaluation through theoretical and numerical ways shows that the developed solution achieves a globally optimal delay reduction performance compared to existing methods, which is also approved by the feasibility test over a real-map virtual environment. The in-depth analysis reveals that predicting the instantaneous processing power of edge servers facilitates the identification of overloaded servers, which is critical for determining network delay. By considering discrete variables of the queue, the proposed technique's precise estimation can effectively address these combinatorial challenges to achieve optimal performance."
2509.04219,"In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused widespread damage to infrastructure, impacting over 400 cities and 2.3 million people. This study presents the construction of comprehensive telecommunications datasets during this climatic event, encompassing Internet measurements, fiber cut reports, and Internet Exchange routing data. By correlating network disruptions with hydrological and operational factors, the dataset offers insights into the resilience of fiber networks, data centers, and Internet traffic during critical events. For each scenario, we investigate failures related to the Information and Communication Technology infrastructure and highlight the challenges faced when its resilience is critically tested. Preliminary findings reveal trends in connectivity restoration, infrastructure vulnerabilities, and user behavior changes. These datasets and pre-analysis aim to support future research on disaster recovery strategies and the development of robust telecommunications systems."
2509.04625,"The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave (mmWave) spectrum, imposes stringent demands on the flexibility, scalability, and efficiency of baseband processing. While virtualized Radio Access Networks (vRANs) enable dynamic spectrum sharing across cells, compute resource allocation for baseband processing, especially in multi-cell deployments with heterogeneous workloads, remains underexplored. In this paper, we present NEXUS, the first system to realize real-time, virtualized multi-cell mmWave baseband processing on a single server with heterogeneous compute resources. NEXUS integrates software-based digital signal processing pipelines with hardware-accelerated LDPC decoding, and introduces a novel framework for sharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions (VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based model that predicts the most energy-efficient resource allocation for the given cell configuration with microsecond-level inference latency and high accuracy. For multi-cell scenarios, NEXUS introduces a power-aware scheduler that incorporates a lightweight contention model to adjust resource allocation strategies under concurrent execution. Through extensive evaluation across various Frequency Range 2 (FR2) cell configurations, we show that NEXUS supports up to 16 concurrent cells under full load, achieving 5.37Gbps aggregate throughput, while reducing the multi-cell scheduling search space by orders of magnitude. These results demonstrate that virtualized, resource-aware baseband processing is both practical and efficient for next-generation vRAN systems."
2509.04695,"Path-aware networks promise enhanced performance and resilience through multipath transport, but a lack of empirical data on their real-world dynamics hinders the design of effective protocols. This paper presents a longitudinal measurement study of the SCION architecture on the global SCIONLab testbed, characterizing the path stability, diversity, and performance crucial for protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic environment, with significant control-plane churn and short path lifetimes in parts of the testbed. We identify and characterize path discrepancy, a phenomenon where routing policies create asymmetric path availability between endpoints. Furthermore, we observe a performance trade-off where concurrent multipath transmissions can improve aggregate throughput but may degrade the latency and reliability of individual paths. These findings demonstrate that protocols such as MPQUIC should explicitly account for high churn and path asymmetry, challenging common assumptions in multipath protocol design."
2509.04792,"IPv4 NAT has limited the spread of IoT botnets considerably by default-denying bots' incoming connection requests to in-home devices unless the owner has explicitly allowed them. As the Internet transitions to majority IPv6, however, residential connections no longer require the use of NAT. This paper therefore asks: has the transition from IPv4 to IPv6 ultimately made residential networks more vulnerable to attack, thereby empowering the next generation of IPv6-based IoT botnets? To answer this question, we introduce a large-scale IPv6 scanning methodology that, unlike those that rely on AI, can be run on low-resource devices common in IoT botnets. We use this methodology to perform the largest-scale measurement of IPv6 residential networks to date, and compare which devices are publicly accessible to comparable IPv4 networks. We were able to receive responses from 14.0M distinct IPv6 addresses inside of residential networks (i.e., not the external-facing gateway), in 2,436 ASes across 118 countries. These responses come from protocols commonly exploited by IoT botnets (including telnet and FTP), as well as protocols typically associated with end-user devices (including iPhone-Sync and IPP). Comparing to IPv4, we show that we are able to reach more printers, iPhones, and smart lights over IPv6 than full IPv4-wide scans could. Collectively, our results show that NAT has indeed acted as the de facto firewall of the Internet, and the v4-to-v6 transition of residential networks is opening up new devices to attack."
2509.05447,"In wireless networks characterized by dense connectivity, the significant signaling overhead generated by distributed link scheduling algorithms can exacerbate issues like congestion, energy consumption, and radio footprint expansion. To mitigate these challenges, we propose a distributed link sparsification scheme employing graph neural networks (GNNs) to reduce scheduling overhead for delay-tolerant traffic while maintaining network capacity. A GNN module is trained to adjust contention thresholds for individual links based on traffic statistics and network topology, enabling links to withdraw from scheduling contention when they are unlikely to succeed. Our approach is facilitated by a novel offline constrained {unsupervised} learning algorithm capable of balancing two competing objectives: minimizing scheduling overhead while ensuring that total utility meets the required level. In simulated wireless multi-hop networks with up to 500 links, our link sparsification technique effectively alleviates network congestion and reduces radio footprints across four distinct distributed link scheduling protocols."
2509.05467,"As networks evolve towards 6G, Mobile Network Operators (MNOs) must accommodate diverse requirements and at the same time manage rising energy consumption. Integrated Access and Backhaul (IAB) networks facilitate dense cellular deployments with reduced infrastructure complexity. However, the multi-hop wireless backhauling in IAB networks necessitates proper routing and resource allocation decisions to meet the performance requirements. At the same time, cell densification makes energy optimization crucial. This paper addresses the joint optimization of routing and resource allocation in IAB networks through two distinct objectives: energy minimization and throughput maximization. We develop a novel capacity model that links power levels to achievable data rates. We propose two practical large-scale approaches to solve the optimization problems and leverage the closed-loop control framework introduced by the Open Radio Access Network (O-RAN) architecture to integrate the solutions. The approaches are evaluated on diverse scenarios built upon open data of two months of traffic collected by network operators in the city of Milan, Italy. Results show that the proposed approaches effectively reduces number of activated nodes to save energy and achieves approximately 100 Mbps of minimum data rate per User Equipment (UE) during peak hours of the day using spectrum within the Frequency Range (FR) 3, or upper midband. The results validate the practical applicability of our framework for next-generation IAB network deployment and optimization."
2509.05759,"This paper presents Tiga, a new design for geo-replicated and scalable transactional databases such as Google Spanner. Tiga aims to commit transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of scenarios, while maintaining high throughput with minimal computational overhead. Tiga consolidates concurrency control and consensus, completing both strictly serializable execution and consistent replication in a single round. It uses synchronized clocks to proactively order transactions by assigning each a future timestamp at submission. In most cases, transactions arrive at servers before their future timestamps and are serialized according to the designated timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed and proactive ordering fails, in which case Tiga falls back to a slow path, committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can commit more transactions at 1-WRTT latency, and incurs much less throughput overhead. Evaluation results show that Tiga outperforms all baselines, achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower latency. Tiga is open-sourced atthis https URL."
2509.05889,"Real-time task processing is a critical challenge in vehicular networks, where achieving low latency and minimizing dropped task ratio depend on efficient task execution. Our primary objective is to maximize the number of completed tasks while minimizing overall latency, with a particular focus on reducing number of dropped tasks. To this end, we investigate both static and dynamic versions of an optimization algorithm. The static version assumes full task availability, while the dynamic version manages tasks as they arrive. We also distinguish between online and offline cases: the online version incorporates execution time into the offloading decision process, whereas the offline version excludes it, serving as a theoretical benchmark for optimal performance. We evaluate our proposed Online Dynamic Cost-Driven Algorithm (On-Dyn-CDA) against these baselines. Notably, the static Particle Swarm Optimization (PSO) baseline assumes all tasks are transferred to the RSU and processed by the MEC, and its offline version disregards execution time, making it infeasible for real-time applications despite its optimal performance in theory. Our novel On-Dyn-CDA completes execution in just 0.05 seconds under the most complex scenario, compared to 1330.05 seconds required by Dynamic PSO. It also outperforms Dynamic PSO by 3.42% in task loss and achieves a 29.22% reduction in average latency in complex scenarios. Furthermore, it requires neither a dataset nor a training phase, and its low computational complexity ensures efficiency and scalability in dynamic environments."
2509.05936,"Network log data analysis plays a critical role in detecting security threats and operational anomalies. Traditional log analysis methods for anomaly detection and root cause analysis rely heavily on expert knowledge or fully supervised learning models, both of which require extensive labeled data and significant human effort. To address these challenges, we propose ALPHA, the first Active Learning Pipeline for Human-free log Analysis. ALPHA integrates semantic embedding, clustering-based representative sampling, and large language model (LLM)-assisted few-shot annotation to automate the anomaly detection process. The LLM annotated labels are propagated across clusters, enabling large-scale training of an anomaly detector with minimal supervision. To enhance the annotation accuracy, we propose a two-step few-shot refinement strategy that adaptively selects informative prompts based on the LLM's observed error patterns. Extensive experiments on real-world log datasets demonstrate that ALPHA achieves detection accuracy comparable to fully supervised methods while mitigating human efforts in the loop. ALPHA also supports interpretable analysis through LLM-driven root cause explanations in the post-detection stage. These capabilities make ALPHA a scalable and cost-efficient solution for truly automated log-based anomaly detection."
2509.05938,"Path-aware networking architectures like SCION provide end-hosts with explicit control over inter-domain routing, while multipath transport protocols like MPTCP and MPQUIC enable the concurrent use of multiple paths. This combination promises significant gains in performance and policy enforcement, but it also creates a stark trade-off between individual performance optimization and overall network stability. This paper quantifies this trade-off through a rigorous axiomatic analysis. We evaluate a spectrum of algorithms, from greedy (Min-RTT) and cooperative (Round-Robin) to hybrid approaches (Epsilon-Greedy), against axioms of Efficiency, Loss Avoidance, Stability, and Fairness in a simulated path-aware environment.Our simulations reveal that purely greedy strategies, while efficient under low contention, induce catastrophic packet loss, increasing by over >18,000% as the number of competing agents grow, due to herd effects that cause severe network instability. Conversely, cooperative strategies ensure fairness and stability but at the cost of underutilizing high-capacity paths. Crucially, we demonstrate that hybrid strategies resolve this conflict. The Epsilon-Greedy algorithm, for instance, achieves the highest efficiency of all tested strategies in high-contention scenarios while mitigating the instability inherent to the greedy approach. Our axiomatic analysis suggests that tunable, hybrid algorithms are essential for designing robust and high-performance path selection mechanisms for next-generation networks."
2509.05946,"The rapid advancement toward sixth-generation (6G) wireless networks has significantly intensified the complexity and scale of optimization problems, including resource allocation and trajectory design, often formulated as combinatorial problems in large discrete decision spaces. However, traditional optimization methods, such as heuristics and deep reinforcement learning (DRL), struggle to meet the demanding requirements of real-time adaptability, scalability, and dynamic handling of user intents in increasingly heterogeneous and resource-constrained network environments. Large language models (LLMs) present a transformative paradigm by enabling natural language-driven problem formulation, context-aware reasoning, and adaptive solution refinement through advanced semantic understanding and structured reasoning capabilities. This paper provides a systematic and comprehensive survey of LLM-enabled optimization frameworks tailored for wireless networks. We first introduce foundational design concepts and distinguish LLM-enabled methods from conventional optimization paradigms. Subsequently, we critically analyze key enabling methodologies, including natural language modeling, solver collaboration, and solution verification processes. Moreover, we explore representative case studies to demonstrate LLMs' transformative potential in practical scenarios such as optimization formulation, low-altitude economy networking, and intent networking. Finally, we discuss current research challenges, examine prominent open-source frameworks and datasets, and identify promising future directions to facilitate robust, scalable, and trustworthy LLM-enabled optimization solutions for next-generation wireless networks."
2509.06049,"With mobile networks expected to support services with stringent requirements that ensure high-quality user experience, the ability to apply Feed-Forward Neural Network (FFNN) models to User Equipment (UE) use cases has become critical. Given that UEs have limited resources, running FFNNs directly on UEs is an intrinsically challenging problem. This letter proposes an optimization framework for split computing applications where an FFNN model is partitioned into multiple sections, and executed by UEs, edge- and core-located nodes to reduce the required UE computational footprint while containing the inference time. An efficient heuristic strategy for solving the optimization problem is also provided. The proposed framework is shown to be robust in heterogeneous settings, eliminating the need for retraining and reducing the UE's memory (CPU) footprint by over 33.6% (60%)."
2509.06245,"We present a modular experimental testbed and lightweight visualization tool for evaluating TCP congestion control performance in wireless networks. We compare Google's latest Bottleneck Bandwidth and Round-trip time version 3 (BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management (AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a commercial MikroTik router. Our real-time dashboard visualizes metrics such as throughput, latency, and fairness across competing flows. Results show that BBRv3 significantly improves fairness and convergence under AQM, especially with FQ-CoDel. Our visualization tool and modular testbed provide a practical foundation for evaluating next-generation TCP variants in real-world AQM-enabled home wireless networks."
2509.06451,"Networked Control System (NCS) is a paradigm where sensors, controllers, and actuators communicate over a shared network. One promising application of NCS is the control of Automated Guided Vehicles (AGVs) in the industrial environment, for example to transport goods efficiently and to autonomously follow predefined paths or routes. In this context, communication and control are tightly correlated, a paradigm referred to as Joint Communication and Control (JCC), since network issues such as delays or errors can lead to significant deviations of the AGVs from the planned trajectory. In this paper, we present a simulation framework based on Gazebo and Robot Operating System 2 (ROS 2) to simulate and visualize, respectively, the complex interaction between the control of AGVs and the underlying communication network. This framework explicitly incorporates communication metrics, such as delay and packet loss, and control metrics, especially the Mean Squared Error (MSE) between the optimal/desired and actual path of the AGV in response to driving commands. Our results shed light into the correlation between the network performance, particularly Packet Reception Ratio (PRR), and accuracy of control."
2509.06454,"Time synchronization is essential for industrial IoT and Industry 4.0/5.0 applications, but achieving high synchronization accuracy in Time-Sensitive Networking (TSN)-5G networks is challenging due to jitter and asymmetric delays. 3GPP TS 23.501 defines three 5G synchronization modes: time-aware system, boundary clock (BC), and transparent clock (TC), where TC offers a promising solution. However, to the best of our knowledge, there is no empirical evaluation of TC in a TSN-5G network. This paper empirically evaluates an 5G end-to-end TC in a TSN-5G network, implemented on commercial TSN switches with a single clock. For TC development, we compute the residence time in 5G and recover the clock domain at the slave node. We deploy a TSN-5G testbed with commercial equipment for synchronization evaluation by modifying the Precision Timing Protocol (PTP) message transmission rates. Experimental results show a peak-to-peak synchronization of 500 ns, meeting the industrial requirement of < 1 us, with minimal synchronization offsets for specific PTP message transmission rates."
2509.06515,"The Internet, the world's largest and most pervasive network, lacks a transparent, granular view of its traffic patterns, volumes, and growth trends, hindering the networking community's understanding of its dynamics. This paper leverages publicly available Internet Exchange Point traffic statistics to address this gap, presenting a comprehensive two-year study (2023-2024) from 472 IXPs worldwide, capturing approximately 300 Tbps of peak daily aggregate traffic by late 2024. Our analysis reveals a 49.2% global traffic increase (24.5% annualized), uncovers regionally distinct diurnal patterns and event-driven anomalies, and demonstrates stable utilization rates, reflecting predictable infrastructure scaling. By analyzing biases and confirming high self-similarity, we establish IXP traffic as a robust proxy for overall Internet growth and usage behavior. With transparent, replicable data--covering 87% of the worldwide IXP port capacity--and plans to release our dataset, this study offers a verifiable foundation for long-term Internet traffic monitoring. In particular, our findings shed light on the interplay between network design and function, providing an accessible framework for researchers and operators to explore the Internet's evolving ecosystem."
2509.06639,"Vehicle detection in tunnels is crucial for traffic monitoring and accident response, yet remains underexplored. In this paper, we develop mmTunnel, a millimeter-wave radar system that achieves far-range vehicle detection in tunnels. The main challenge here is coping with ghost points caused by multi-path reflections, which lead to severe localization errors and false alarms. Instead of merely removing ghost points, we propose correcting them to true vehicle positions by recovering their signal reflection paths, thus reserving more data points and improving detection performance, even in occlusion scenarios. However, recovering complex 3D reflection paths from limited 2D radar points is highly challenging. To address this problem, we develop a multi-path ray tracing algorithm that leverages the ground plane constraint and identifies the most probable reflection path based on signal path loss and spatial distance. We also introduce a curve-to-plane segmentation method to simplify tunnel surface modeling such that we can significantly reduce the computational delay and achieve real-time processing.We have evaluated mmTunnel with comprehensive experiments. In two test tunnels, we conducted controlled experiments in various scenarios with cars and trucks. Our system achieves an average F1 score of 93.7% for vehicle detection while maintaining real-time processing. Even in the challenging occlusion scenarios, the F1 score remains above 91%. Moreover, we collected extensive data from a public tunnel with heavy traffic at times and show our method could achieve an F1 score of 91.5% in real-world traffic conditions."
2509.067,"The advent of Generative Artificial Intelligence (GenAI), Large Language Models (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile networks, especially as the telecom industry transitions from 5G's cloud-centric to AI-native 6G architectures. This transition unlocks unprecedented capabilities in real-time automation, semantic networking, and autonomous service orchestration. However, it introduces critical risks related to data sovereignty, security, explainability, and regulatory compliance especially when AI models are trained, deployed, or governed externally. This paper introduces the concept of `Sovereign AI' as a strategic imperative for 6G, proposing architectural, operational, and governance frameworks that enable national or operator-level control over AI development, deployment, and life-cycle management. Focusing on O-RAN architecture, we explore how sovereign AI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure policy-aligned control, secure model updates, and federated learning across trusted infrastructure. We analyse global strategies, technical enablers, and challenges across safety, talent, and model governance. Our findings underscore that Sovereign AI is not just a regulatory necessity but a foundational pillar for secure, resilient, and ethically-aligned 6G networks."
2509.06763,"The integration of Reconfigurable Intelligent Surfaces (RIS) and Integrated Sensing and Communication (ISAC) in vehicular networks enables dynamic spatial resource management and real-time adaptation to environmental changes. However, the coexistence of distinct vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) connectivity requirements, together with highly dynamic and heterogeneous network topologies, presents significant challenges for unified reliability modeling and resource optimization. To address these issues, we propose VariSAC, a graph neural network (GNN)-augmented deep reinforcement learning framework for assured, time-continuous connectivity in RIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically, we introduce the Continuous Connectivity Ratio (CCR), a unified metric that characterizes the sustained temporal reliability of V2I connections and the probabilistic delivery guarantees of V2V links, thus unifying their continuous reliability semantics. Next, we employ a GNN with residual adapters to encode complex, high-dimensional system states, capturing spatial dependencies among vehicles, base stations (BS), and RIS nodes. These representations are then processed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channel allocation, power control, and RIS configurations to maximize CCR-driven long-term rewards. Extensive experiments on real-world urban datasets demonstrate that VariSAC consistently outperforms existing baselines in terms of continuous V2I ISAC connectivity and V2V delivery reliability, enabling persistent connectivity in highly dynamic vehicular environments."
2509.06766,"Mega-satellite constellations have the potential to leverage inter-satellite links to deliver low-latency end-to-end communication services globally, thereby extending connectivity to underserved regions. However, harsh space environments make satellites vulnerable to failures, leading to node removals that disrupt inter-satellite networking. With the high risk of satellite node failures, understanding their impact on end-to-end services is essential. This study investigates the importance of individual nodes on inter-satellite networking and the resilience of mega satellite constellations against node failures. We represent the mega-satellite constellation as discrete temporal graphs and model node failure events accordingly. To quantify node importance for targeted services over time, we propose a service-aware temporal betweenness metric. Leveraging this metric, we develop an analytical framework to identify critical nodes and assess the impact of node failures. The framework takes node failure events as input and efficiently evaluates their impacts across current and subsequent time windows. Simulations on the Starlink constellation setting reveal that satellite networks inherently exhibit resilience to node failures, as their dynamic topology partially restore connectivity and mitigate the long-term impact. Furthermore, we find that the integration of rerouting mechanisms is crucial for unleashing the full resilience potential to ensure rapid recovery of inter-satellite networking."
2509.06898,"The coexistence between incumbent radar signals and commercial 5G signals necessitates a versatile and ubiquitous radar sensing for efficient and adaptive spectrum sharing. In this context, leveraging the densely deployed 5G base stations (BS) for radar sensing is particularly promising, offering both wide coverage and immediate feedback to 5G scheduling. However, the targeting radar signals are superimposed with concurrent 5G uplink transmissions received by the BS, and practical deployment also demands a lightweight, portable radar sensing model. This paper presents BatStation, a lightweight, in-situ radar sensing framework seamlessly integrated into 5G BSs. BatStation leverages uplink resource grids to extract radar signals through three key components: (i) radar signal separation to cancel concurrent 5G transmissions and reveal the radar signals, (ii) resource grid reshaping to align time-frequency resolution with radar pulse characteristics, and (iii) zero-shot template correlation based on a portable model trained purely on synthetic data that supports detection, classification, and localization of radar pulses without fine-tuning using experimental data. We implement BatStation on a software-defined radio (SDR) testbed and evaluate its performance with real 5G traffic in the CBRS band. Results show robust performance across diverse radar types, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH), classification accuracy up to 97.00%, and median localization errors of 2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably, BatStation achieves this performance with a runtime latency of only 0.11/0.94 ms on GPU/CPU, meeting the real-time requirement of 5G networks."
2509.07154,"Path-aware networks promise enhanced performance and resilience through multipath transport, but a lack of empirical data on their real-world dynamics hinders the design of effective protocols. This paper presents a longitudinal measurement study of the SCION architecture on the global SCIONLab testbed, characterizing the path stability, diversity, and performance crucial for protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic environment, with significant control-plane churn and short path lifetimes in parts of the testbed. We identify and characterize path discrepancy, a phenomenon where routing policies create asymmetric path availability between endpoints. Furthermore, we observe a performance trade-off where concurrent multipath transmissions can improve aggregate throughput but may degrade the latency and reliability of individual paths. These findings demonstrate that protocols such as MPQUIC should explicitly account for high churn and path asymmetry, challenging common assumptions in multipath protocol design."
2509.07242,"The fifth generation (5G) of wireless networks must simultaneously support heterogeneous service categories, including Ultra-Reliable Low-Latency Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive Machine-Type Communications (mMTC), each with distinct Quality of Service (QoS) requirements. Meeting these demands under limited spectrum resources requires adaptive and standards-compliant radio resource management. We present DORA (Dynamic O-RAN Resource Allocation), a deep reinforcement learning (DRL) framework for dynamic slice-level Physical Resource Block (PRB) allocation in Open RAN. DORA employs a PPO-based RL agent to allocate PRBs across URLLC, eMBB, and mMTC slices based on observed traffic demands and channel conditions. Intra-slice PRB scheduling is handled deterministically via round-robin among active UEs, simplifying control complexity and improving training stability. Unlike prior work, DORA supports online training and adapts continuously to evolving traffic patterns and cross-slice contention. Implemented in the standards-compliant OpenAirInterface (OAI) RAN stack and designed for deployment as an O-RAN xApp, DORA integrates seamlessly with RAN Intelligent Controllers (RICs). Extensive evaluation under congested regimes shows that DORA outperforms three non-learning baselines and a \texttt{DQN} agent, achieving lower URLLC latency, higher eMBB throughput with fewer SLA violations, and broader mMTC coverage without starving high-priority slices. To our knowledge, this is the first fully online DRL framework for adaptive, slice-aware PRB allocation in O-RAN."
2509.0741,"To support emerging mobile use cases (e.g., AR/VR, autonomous driving, and massive IoT), next-generation mobile cores for 5G and 6G are being re-architected as service-based architectures (SBAs) running on both private and public clouds. However, current performance optimization strategies for scaling these cores still revert to traditional NFV-based techniques, such as consolidating functions into rigid, monolithic deployments on dedicated servers. This raises a critical question: Is there an inherent tradeoff between flexibility and scalability in an SBA-based mobile core, where improving performance (and resiliency) inevitably comes at the cost of one or the other?To explore this question, we introduce resilient SBA microservices design patterns and state-management strategies, and propose TEGRA -- a high-performance, flexible, and scalable SBA-based mobile core. By leveraging the mobile core's unique position in the end-to-end internet ecosystem (i.e., at the last-mile edge), TEGRA optimizes performance without compromising adaptability. Our evaluation demonstrates that TEGRA achieves significantly lower latencies, processing requests 20x, 11x, and 1.75x faster than traditional SBA core implementations -- free5GC, Open5GS, and Aether, respectively -- all while matching the performance of state-of-the-art cores (e.g., CoreKube) while retaining flexibility. Furthermore, it reduces the complexity of deploying new features, requiring orders of magnitude fewer lines of code (LoCs) compared to existing cores."
2509.07431,"Remote Direct Memory Access (RDMA) improves host networking performance by eliminating software and server CPU involvement. However, RDMA has a limited set of operations, is difficult to program, and often requires multiple round trips to perform simple application operations. Programmable SmartNICs provide a different means to offload work from host CPUs to a NIC. This leaves applications with the complex choice of embedding logic as RPC handlers at servers, using RDMA's limited interface to access server structures via client-side logic, or running some logic on SmartNICs. The best choice varies between workloads and over time. To solve this dilemma, we present NAAM, network-accelerated active messages. NAAM applications specify small, portable eBPF functions associated with messages. Each message specifies what data it accesses using an RDMA-like interface. NAAM runs at various places in the network, including at clients, on server-attached SmartNICs, and server host CPU cores. Due to eBPF's portability, the code associated with a message can be run at any location. Hence, the NAAM runtime can dynamically steer any message to execute its associated logic wherever it makes the most sense. To demonstrate NAAM's flexibility, we built several applications, including the MICA hash table and lookups from a Cell-style B-tree. With an NVIDIA BlueField-2 SmartNIC and integrating its NIC-embedded switch, NAAM can run any of these operations on client, server, and NIC cores, shifting load in tens of milliseconds on server compute congestion. NAAM dynamically offloads up to 1.8 million MICA ops/s for YCSB-B and 750,000 Cell lookups/s from server CPUs. Finally, whereas iPipe, the state-of-the-art SmartNIC offload framework, only scales to 8 application offloads on BlueField-2, NAAM scales to hundreds of application offloads with minimal impact on tail latency due to eBPF's low overhead."
2509.07492,"This work develops an LLM-based optimization framework ensuring strict constraint satisfaction in network optimization. While LLMs possess contextual reasoning capabilities, existing approaches often fail to enforce constraints, causing infeasible solutions. Unlike conventional methods that address average constraints, the proposed framework integrates a natural language-based input encoding strategy to restrict the solution space and guarantee feasibility. For multi-access edge computing networks, task allocation is optimized while minimizing worst-case latency. Numerical evaluations demonstrate LLMs as a promising tool for constraint-aware network optimization, offering insights into their inference capabilities."
2509.07548,"The regenerative satellite access network (SAN) architecture deploys next-generation NodeB (gNBs) on satellites to enable enhanced network management capabilities. It supports two types of regenerative payload, on-board gNB and on-board gNB-Distributed Unit (gNB-DU). Measurement results based on our prototype implementation show that the on-board gNB offers lower latency, while the on-board gNB-DU is more cost-effective, and there is often a trade-off between Quality-of-Service (QoS) and operational expenditure (OPEX) when choosing between the two payload types. However, current SAN configurations are static and inflexible -- either deploying the full on-board gNB or only the on-board gNB-DU. This rigidity can lead to resource waste or poor user experiences. In this paper, we propose Flexible SAN (FlexSAN), an adaptive satellite access network architecture that dynamically configures the optimal regenerative payload based on real-time user demands. FlexSAN selects the lowest OPEX payload configuration when all user demands are satisfied, and otherwise maximizes the number of admitted users while ensuring QoS for connected users. To address the computational complexity of dynamic payload selection, we design an adaptive greedy heuristic algorithm. Extensive experiments validate FlexSAN's effectiveness, showing a 36.1% average improvement in user admission rates and a 15% OPEX reduction over static SANs."
2509.07773,"The complexity of large-scale 6G-and-beyond networks demands innovative approaches for multi-objective optimization over vast search spaces, a task often intractable. Quantum computing (QC) emerges as a promising technology for efficient large-scale optimization. We present our vision of leveraging QC to tackle key classes of problems in future mobile networks. By analyzing and identifying common features, particularly their graph-centric representation, we propose a unified strategy involving QC algorithms. Specifically, we outline a methodology for optimization using quantum annealing as well as quantum reinforcement learning. Additionally, we discuss the main challenges that QC algorithms and hardware must overcome to effectively optimize future networks."
2509.07907,"Per-packet load-balancing approaches are increasingly deployed in datacenter networks. However, their combination with existing congestion control algorithms (CCAs) may lead to poor performance, and even state-of-the-art CCAs can collapse due to duplicate ACKs. A typical approach to handle this collapse is to make CCAs resilient to duplicate ACKs.In this paper, we first model the throughput collapse of a wide array of CCAs when some of the paths are congested. We show that addressing duplicate ACKs is insufficient. Instead, we explain that since CCAs are typically designed for single-path routing, their estimation function focuses on the latest feedback and mishandles feedback that reflects multiple paths. We propose to use a median feedback that is more robust to the varying signals that come with multiple paths. We introduce MSwift, which applies this principle to make Google's Swift robust to multi-path routing while keeping its incast tolerance and single-path performance. Finally, we demonstrate that MSwift improves the 99th-percentile FCT by up to 25\%, both with random packet spraying and adaptive routing."
2509.08097,"Manifolds are complex topological spaces that can be used to represent datasets of real-world measurements. Visualizing such manifolds can help with illustrating their topological characteristics (e.g., curvature) and providing insights into important properties of the underlying data (e.g., anomalies in the measurements). In this paper, we describe a new methodology and system for generating and visualizing manifolds that are inferred from actual Internet latency measurements between different cities and are projected over a 2D Euclidean space (e.g., a geographic map). Our method leverages a series of graphs that capture critical information contained in the data, including well-defined locations (for vertices) and Ricci curvature information (for edges). Our visualization approach then generates a curved surface (manifold) in which (a) geographical locations of vertices are maintained and (b) the Ricci curvature values of the graph edges determine the curvature properties of the manifold. The resulting manifold highlights areas of critical connectivity and defines an instance of ""Internet delay space"" where latency measurements manifest as geodesics. We describe details of our method and its implementation in a tool, which we call Matisse, for generating, visualizing and manipulating manifolds projected onto a base map. We illustrate Matisse with two case studies: a simple example to demonstrate key concepts, and visualizations of the US public Internet to show Matisse's utility."
2509.08124,"Proliferation of new classes of airspace participants, including uncrewed and advanced aerial mobility vehicles, necessitates the development and deployment of novel airspace management solutions, such as the Unmanned Traffic Management (UTM) system and the Provider of Services to UAM (PSU) Network. The efficacy of such systems has been demonstrated on multiple occasions via real-world deployments in limited test environments, however exploration of system behavior under stressing conditions requires the development of appropriate modeling and simulation (M&S) environments. Autonomy Networks for Advanced Mobility at Lincoln Laboratory (ANAMLL) is a virtual Systems Integration Laboratory (SIL) designed to host federated autonomy networks, such as a UTM or PSU Network, and to enable test and validation at scales not available in real-world deployments. As an example of ANAMLL's utility, we explore the performance of a representative UTM network during a stressing demand scenario. In a close examination of the demand scenario, ANAMLL demonstrates a UTM system demand point at which in-flight replanning can no longer be accomplished within an allowable time window. In a second analysis of the same scenario, ANAMLL demonstrates the impact of network connectivity performance on end-user airspace access."
2509.08274,"Low-speed internet can negatively affect incident response in a number of ways, including decreased teamwork, delayed detection, inefficient action, and elevated risk. Delayed data acquisition and processing may result from inadequate internet connectivity, hindering security teams' ability to obtain the necessary information for timely and effective responses. Each of these factors may augment the organization's susceptibility to security incidents and their subsequent ramifications. This article establishes a virtual network function service delivery network (VNFSDN) through the integration of virtual network function (VNF) and software-defined networking (SDN) technologies. The VNFSDN approach enhances network security effectiveness and efficiency while reducing the danger of breaches. This method assists security services in rapidly assessing vast quantities of data generated by 6G networks. VNFSDN adapts dynamically to changing safety requirements and connection conditions through the use of SDN and VNF. This flexibility enables enterprises to mitigate or halt the impact of cyberattacks by swiftly identifying and addressing security threats. The VNFSDN enhances network resilience, allowing operators to proactively mitigate possible security attacks and minimize downtime. The incorporation of machine learning and artificial intelligence into VNFSDN can significantly improve network security and threat detection capabilities. The VNFSDN integrates VNF and SDN technologies to deliver security services that analyze vast quantities of 6G data in real time. As security requirements and network conditions evolve, it adapts dynamically to enhance network resilience and facilitate proactive threat detection."
2509.084,"We introduce ubiquitous intelligence as a paradigm where Large Language Models (LLMs) evolve within wireless network-driven ecosystems. Unlike static model deployments, this approach enables scalable and continuous intelligence ascension through coordination between networks and LLMs. Wireless networks support system-orchestrated lifelong learning, while LLMs drive the next-generation network development that is more adaptive and responsive. This co-evolution highlights a shift toward self-improving systems, sustaining capability growth across diverse and resource-constrained environments."
2509.08455,"The rapid growth of space-based services has established LEO satellite networks as a promising option for global broadband connectivity. Next-generation LEO networks leverage inter-satellite links (ISLs) to provide faster and more reliable communications compared to traditional bent-pipe architectures, even in remote regions. However, the high mobility of satellites, dynamic traffic patterns, and potential link failures pose significant challenges for efficient and resilient routing. To address these challenges, we model the LEO satellite network as a time-varying graph comprising a constellation of satellites and ground stations. Our objective is to minimize a weighted sum of average delay and packet drop rate. Each satellite independently decides how to distribute its incoming traffic to neighboring nodes in real time. Given the infeasibility of finding optimal solutions at scale, due to the exponential growth of routing options and uncertainties in link capacities, we propose SKYLINK, a novel fully distributed learning strategy for link management in LEO satellite networks. SKYLINK enables each satellite to adapt to the time-varying network conditions, ensuring real-time responsiveness, scalability to millions of users, and resilience to network failures, while maintaining low communication overhead and computational complexity. To support the evaluation of SKYLINK at global scale, we develop a new simulator for large-scale LEO satellite networks. For 25.4 million users, SKYLINK reduces the weighted sum of average delay and drop rate by 29% compared to the bent-pipe approach, and by 92% compared to Dijkstra. It lowers drop rates by 95% relative to k-shortest paths, 99% relative to Dijkstra, and 74% compared to the bent-pipe baseline, while achieving up to 46% higher throughput. At the same time, SKYLINK maintains constant computational complexity with respect to constellation size."
2509.08488,"Precise and energy-efficient localization is a critical requirement in many Internet of Things (IoT) applications, particularly in large-scale deployments such as asset tagging, agriculture, and smart cities, where long battery life and cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver presents a promising solution for IoT localization. It combines low cost, low power, and precise ranging capability over distances of up to 1 km. However, the ranging process requires two devices to be simultaneously active, one initiating the ranging request and the other responding to it, which can lead to significant energy expenditure if not properly managed. Despite the transceiver's excellent performance, no existing system-level framework effectively manages sleep-wake coordination and role assignment needed for energy-efficient operation. This paper presents a coordination framework that significantly reduces power consumption while maintaining the inherent precise ranging capability of the chip. The framework schedules short, synchronized wake-up windows between the initiator and the responder, allowing devices to remain in deep sleep for most of their duty cycle. This scheduling strategy minimizes reliance on precise continuous timing and mitigates drift in low-cost oscillators. To validate the framework, we designed and developed custom nodes that are compliant with the framework's protocol. Experimental results show that the proposed approach allows a node to stay in ultra-low power mode and wake periodically to check for instructions. The node can remain in standby mode for up to nine months on a single coin cell battery and can perform ranging operations on demand in near real-time, all while maintaining a localization accuracy within five meters."
2509.08595,"This paper investigates the resilience of mobile communication networks during the extreme flooding that affected Rio Grande do Sul, Brazil, in May 2024. Based on regulatory data and technical insights from operators, the study identifies the leading causes of mobile network disruptions, primarily related to flooding and prolonged power outages. The results reveal the significant vulnerability of modern networks (4G/5G) during the event and the essential role played by legacy technologies (2G/3G) in sustaining basic connectivity under adverse conditions. The findings underscore the necessity of disaster-aware infrastructure planning, taking into account the ongoing significance of legacy systems, diversified power supply strategies, and resilient network designs to enhance service continuity during future crises."
2509.09081,"Users around the world face escalating network interference such as censorship, throttling, and interception, largely driven by the commoditization and growing availability of Deep Packet Inspection (DPI) devices. Once reserved for a few well-resourced nation-state actors, the ability to interfere with traffic at scale is now within reach of nearly any network operator. Despite this proliferation, our understanding of DPIs and their deployments on the Internet remains limited -- being network intermediary leaves DPI unresponsive to conventional host-based scanning tools, and DPI vendors actively obscuring their products further complicates measurement efforts.In this work, we present a remote measurement framework, dMAP (DPI Mapper), that derives behavioral fingerprints for DPIs to differentiate and cluster these otherwise indistinguishable middleboxes at scale, as a first step toward active reconnaissance of DPIs on the Internet. Our key insight is that parsing and interpreting traffic as network intermediaries inherently involves ambiguities -- from under-specified protocol behaviors to differing RFC interpretations -- forcing DPI vendors into independent implementation choices that create measurable variance among DPIs. Based on differential fuzzing, dMAP systematically discovers, selects, and deploys specialized probes that translate DPI internal parsing behaviors into externally observable fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its practical feasibility, showing that even a modest set of 20-40 discriminative probes reliably differentiates a wide range of DPI implementations, including major nation-state censorship infrastructures and commercial DPI products. We discuss how our fingerprinting methodology generalizes beyond censorship to other forms of targeted interference."
2509.09193,"Artificial Intelligence (AI) techniques play a pivotal role in optimizing wireless communication networks. However, traditional deep learning approaches often act as closed boxes, lacking the structured reasoning abilities needed to tackle complex, multi-step decision problems. This survey provides a comprehensive review and outlook of reasoning-enabled AI in wireless communication networks, with a focus on Large Language Models (LLMs) and other advanced reasoning paradigms. In particular, LLM-based agents can combine reasoning with long-term planning, memory, tool utilization, and autonomous cross-layer control to dynamically optimize network operations with minimal human intervention. We begin by outlining the evolution of intelligent wireless networking and the limitations of conventional AI methods. We then introduce emerging AI reasoning techniques. Furthermore, we establish a classification system applicable to wireless network tasks. We also present a layer-by-layer examination for AI reasoning, covering the physical, data link, network, transport, and application layers. For each part, we identify key challenges and illustrate how AI reasoning methods can improve AI-based wireless communication performance. Finally, we discuss key research directions for AI reasoning toward future wireless communication networks. By combining insights from both communications and AI, this survey aims to chart a path for integrating reasoning techniques into the next-generation wireless networks."
2509.09343,"Open Radio Access Network (O-RAN) architecture provides an intrinsic capability to exploit key performance monitoring (KPM) within Radio Intelligence Controller (RIC) to derive network optimisation through xApps. These xApps can leverage KPM knowledge to dynamically switch on/off the associated RUs where such a function is supported over the E2 interface. Several existing studies employ artificial intelligence (AI)/Machine Learning (ML) based approaches to realise such dynamic sleeping for increased energy efficiency (EE). Nevertheless, most of these approaches rely upon offloading user equipment (UE) to carve out a sleeping opportunity. Such an approach inherently creates load imbalance across the network. Such load imbalance may impact the throughput performance of offloaded UEs as they might be allocated a lower number of physical resource blocks (PRBs). Maintaining the same PRB allocation while addressing the EE at the network level is a challenging task. To that end, in this article, we present a comprehensive ML-based framework for joint optimisation of load balancing and EE for ORAN deployments. We formulate the problem as a multi-class classification system that predictively evaluates potential RU configurations before optimising the EE, mapping network conditions to three load balance categories (Well Balanced, Moderately Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate, Aggressive) accommodates different operational priorities between energy savings and performance assurance. Experimental evaluation using 4.26 million real network measurements from simulations demonstrates that our Random Forest model achieves 98.3% F1-macro performance, representing 195% improvement over traditional baseline strategies."
2509.09453,"With the advent of quantum computing, the increasing threats to security poses a great challenge to communication networks. Recent innovations in this field resulted in promising technologies such as Quantum Key Distribution (QKD), which enables the generation of unconditionally secure keys, establishing secure communications between remote nodes. Additionally, QKD networks enable the interconnection of multinode architectures, extending the point-to-point nature of QKD. However, due to the limitations of the current state of technology, the scalability of QKD networks remains a challenge toward feasible implementations. When it comes to long-distance implementations, trusted relay nodes partially solve the distance issue through the forwarding of the distributed keys, allowing applications that do not have a direct QKD link to securely share key material. Even though the relay procedure itself has been extensively studied, the establishment of the relaying node path still lacks a solution. This paper proposes an innovative network architecture that solves the challenges of Key Management System (KMS) identification, relay path discovery, and scalability of QKD networks by integrating Software-Defined Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in each node and creating a new entity called the Quantum Security Controller (QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs within the node and abstracting the user from discovering the correct KMS. Additionally, based on the high-level view of the network topology and status, the QuSeC serves the path discovery requests from vKMSs, computing the end-to-end (E2E) relay path and applying security policies. The paper also provides a security analysis of the proposal, identifying the security levels of the architecture and analyzing the core networking security properties."
2509.09537,"The rapid evolution of mobile security protocols and limited availability of current datasets constrains research in app traffic analysis. This paper presents PARROT, a reproducible and portable traffic capture system for systematic app traffic collection using Android Virtual Devices. The system provides automated environment setup, configurable Android versions, traffic recording management, and labeled captures extraction with human-in-the-loop app interaction. PARROT integrates mitmproxy for optional traffic decryption with automated SSL/TLS key extraction, supporting flexible capture modes with or without traffic interception. We collected a dataset of 80 apps selected from the MAppGraph dataset list, providing traffic captures with corresponding SSL keys for decryption analysis. Our comparative analysis between the MAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern evolution across 50 common apps. Key findings include migration from TLSv1.2 to TLSv1.3 protocol, with TLSv1.3 comprising 90.0\% of TCP encrypted traffic in 2025 compared to 6.7\% in 2021. QUIC protocol adoption increased substantially, with all 50 common apps generating QUIC traffic under normal network conditions compared to 30 apps in 2021. DNS communications evolved from predominantly unencrypted Do53 protocol (91.0\% in 2021) to encrypted DoT protocol (81.1\% in 2025). The open-source PARROT system enables reproducible app traffic capture for research community adoption and provides insights into app security protocol evolution."
2509.09898,"DBOS (DataBase Operating System) is a novel capability that integrates web services, operating system functions, and database features to significantly reduce web-deployment effort while increasing resilience. Integration of high performance network sensing enables DBOS web services to collaboratively create a shared awareness of their network environments to enhance their collective resilience and security. Network sensing is added to DBOS using GraphBLAS hypersparse traffic matrices via two approaches: (1) Python-GraphBLAS and (2) OneSparse PostgreSQL. These capabilities are demonstrated using the workflow and analytics from the IEEE/MIT/Amazon Anonymized Network Sensing Graph Challenge. The system was parallelized using pPython and benchmarked using 64 compute nodes on the MIT SuperCloud. The web request rate sustained by a single DBOS instance was ${>}10^5$, well above the required maximum, indicating that network sensing can be added to DBOS with negligible overhead. For collaborative awareness, many DBOS instances were connected to a single DBOS aggregator. The Python-GraphBLAS and OneSparse PostgreSQL implementations scaled linearly up to 64 and 32 nodes respectively. These results suggest that DBOS collaborative network awareness can be achieved with a negligible increase in computing resources."
2509.09997,"Federated Learning (FL) is a promising approach for privacy-preserving network traffic analysis, but its practical deployment is challenged by the non-IID nature of real-world data. While prior work has addressed statistical heterogeneity, the impact of temporal traffic volatility-the natural daily ebb and flow of network activity-on model stability remains largely unexplored. This volatility can lead to inconsistent data availability at clients, destabilizing the entire training process. In this paper, we systematically address the problem of temporal volatility in federated QUIC classification. We first demonstrate the instability of standard FL in this dynamic setting. We then propose and evaluate a client-side data buffer as a practical mechanism to ensure stable and consistent local training, decoupling it from real-time traffic fluctuations. Using the real-world CESNET-QUIC22 dataset partitioned into 14 autonomous clients, we then demonstrate that this approach enables robust convergence. Our results show that a stable federated system achieves a 95.2% F1 score, a mere 2.3 percentage points below a non-private centralized model. This work establishes a blueprint for building operationally stable FL systems for network management, proving that the challenges of dynamic network environments can be overcome with targeted architectural choices."
2509.10001,"Service Function Chaining (SFC) is a networking technique that ensures traffic traverses a predefined sequence of service functions, realizing arbitrary network services through dynamic and efficient communication paths. Inspired by this concept, we propose an SFC-based architecture for Multi-hop Split Inference (MSI), where split sub-models are interpreted as service functions and their composition forms a service chain representing the global model. By leveraging SFC, the proposed architecture dynamically establishes communication paths for split sub-models, ensuring efficient and adaptive execution. Furthermore, we extend this architecture to Multi-hop Split Learning (MSL) by applying SFC to the bidirectional communication required for training tasks. To realize the proposed architecture, we design Neural Service Functions (NSFs) to execute split sub-models as transparent TCP proxies and integrate them with Segment Routing over IPv6 (SRv6) and the extended Berkeley Packet Filter (eBPF)-based SFC proxy. This integration ensures efficient ML processing over dynamic routing while maintaining compatibility with existing applications. Evaluation results demonstrate that (1) the proposed architecture is feasible for both MSI and MSL; (2) it is particularly suitable for real-time inference in MSI scenarios with small mini-batch sizes; (3) it supports dynamic path reconfiguration, enabling adaptive responses to changing network conditions while minimizing the impact of control mechanisms on inference and learning processes."
2509.10097,"The growing demand for high-speed, ultra-reliable, and low-latency communications in 5G and beyond networks has significantly driven up power consumption, particularly within the Radio Access Network (RAN). This surge in energy demand poses critical operational and sustainability challenges for mobile network operators, necessitating innovative solutions that enhance energy efficiency without compromising Quality of Service (QoS). Open Radio Access Network (O-RAN), spearheaded by the O-RAN Alliance, offers disaggregated, programmable, and intelligent architectures, promoting flexibility, interoperability, and cost-effectiveness. However, this disaggregated approach adds complexity, particularly in managing power consumption across diverse network components such as Open Radio Units (RUs). In this paper, we propose a hybrid xApp leveraging heuristic methods and unsupervised machine learning, integrated with digital twin technology through the TeraVM AI RAN Scenario Generator (AI-RSG). This approach dynamically manages RU sleep modes to effectively reduce energy consumption. Our experimental evaluation in a realistic, large-scale emulated Open RAN scenario demonstrates that the hybrid xApp achieves approximately 13% energy savings, highlighting its practicality and significant potential for real-world deployments without compromising user QoS."
2509.10173,"Resilient routing in large-scale Low Earth Orbit (LEO) satellite networks remains a key challenge due to frequent and unpredictable link and node failures, potentially in response to cybersecurity breaches. While prior work has explored rerouting strategies with various levels of network awareness, their relative tradeoffs under dynamic failure conditions remain underexplored. In this work, we extend the Deep Space Network Simulator (DSNS) to systematically compare three rerouting paradigms, each differing in the scope of failure knowledge available to each node. We compare local neighbor-based, segment-based and global-knowledge-based rerouting as well as a naive source routing solution that is unaware of failures. Our main goal is to evaluate how the breadth of failure awareness impacts routing performance and resilience under failures, both random and targeted. We measure delivery ratio, latency, rerouting overhead, and loop occurrence. Our findings show the potential of segment-based rerouting to achieve a favorable tradeoff between local responsiveness and global coordination, offering resilience benefits with minimal overhead--insights that can inform future fault-tolerant satellite network design."
2509.10214,"Monero, the leading privacy-focused cryptocurrency, relies on a peer-to-peer (P2P) network to propagate transactions and blocks. Growing evidence suggests that non-standard nodes exist in the network, posing as honest nodes but are perhaps intended for monitoring the network and spying on other nodes. However, our understanding of the detection and analysis of anomalous peer behavior remains limited. This paper presents a first comprehensive study of anomalous behavior in Monero's P2P network. To this end, we collected and analyzed over 240 hours of network traffic captured from five distinct vantage points worldwide. We further present a formal framework which allows us to analytically define and classify anomalous patterns in P2P cryptocurrency networks. Our detection methodology, implemented as an offline analysis, provides a foundation for real-time monitoring systems. Our analysis reveals the presence of non-standard peers in the network where approximately 14.74% (13.19%) of (reachable) peers in the network exhibit non-standard behavior. These peers exhibit distinct behavioral patterns that might suggest multiple concurrent attacks, pointing to substantial shortcomings in Monero's privacy guarantees and network decentralization. To support reproducibility and enable network operators to protect themselves, we release our examination pipeline to identify and block suspicious peers based on newly captured network traffic."
2509.10216,"Requests for Comments (RFCs) are extensive specification documents for network protocols, but their prose-based format and their considerable length often impede precise operational understanding. We present RFSeek, an interactive tool that automatically extracts visual summaries of protocol logic from RFCs. RFSeek leverages large language models (LLMs) to generate provenance-linked, explorable diagrams, surfacing both official state machines and additional logic found only in the RFC text. Compared to existing RFC visualizations, RFSeek's visual summaries are more transparent and easier to audit against their textual source. We showcase the tool's potential through a series of use cases, including guided knowledge extraction and semantic diffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.In practice, RFSeek not only reconstructs the RFC diagrams included in some specifications, but, more interestingly, also uncovers important logic such as nodes or edges described in the text but missing from those diagrams. RFSeek further derives new visualization diagrams for complex RFCs, with QUIC as a representative case. Our approach, which we term \emph{Summary Visualization}, highlights a promising direction: combining LLMs with formal, user-customized visualizations to enhance protocol comprehension and support robust implementations."
2509.10338,"Quantum Key Distribution (QKD) provides information-theoretic security, but is limited by distance in optical networks, thereby requiring repeater nodes to extend coverage. Existing works usually assume all repeater nodes and associated Key Management Servers (KMSs) to be Trusted Repeater Nodes (TRNs), while ignoring risks from software exploits and insider threats. In this paper, we propose a reliability-aware TRN placement framework for metro optical networks, which assigns each node a trust score and integrates it into the Dijkstra algorithm via weighted links. We then rank the nodes using a composite score, which is a weighted combination of betweenness centrality and eigenvector centrality to enable a secure and scalable TRN deployment. Simulation results on a reference topology show that our method covers 10.77% more shortest paths compared to traditional metrics like degree centrality, using the same number (around eight) of TRNs, making it suitable for TRN selection to maximize secure connectivity."
2509.10462,"The rapid expansion of cloud computing and data center infrastructure has led to significant energy consumption, posing environmental challenges due to the growing carbon footprint. This research explores energy-aware management strategies aimed at creating sustainable data center operations. By integrating advanced energy-efficient technologies and optimizing resource utilization, this study proposes a framework to minimize power usage while maintaining high performance. Key elements include dynamic workload allocation, renewable energy integration, and intelligent cooling systems, all of which contribute to reducing overall energy consumption. The study also examines the impact of these strategies on operational costs and performance efficiency, demonstrating how sustainable practices can be both environmentally and economically beneficial. Through simulations and case studies, the research offers practical insights into reducing carbon emissions in data centers, supporting the transition towards greener cloud infrastructure. The findings highlight the potential for scalable, energy-aware data center designs that significantly lower environmental impact while ensuring optimal functionality, contributing to the global effort of mitigating climate change."
2509.10475,"This study investigates the trade-off between system stability and offloading cost in collaborative edge computing. While collaborative offloading among multiple edge servers enhances resource utilization, existing methods often overlook the role of queue stability in overall system performance. To address this, a multi-hop data transmission model is developed, along with a cost model that captures both energy consumption and delay. A time-varying queue model is then introduced to maintain system stability. Based on Lyapunov optimization, a dynamic offloading algorithm (LDSO) is proposed to minimize offloading cost while ensuring long-term stability. Theoretical analysis and experimental results verify that the proposed LDSO achieves significant improvements in both cost efficiency and system stability compared to the state-of-the-art."
2509.10478,"The management of future AI-native Next-Generation (NextG) Radio Access Networks (RANs), including 6G and beyond, presents a challenge of immense complexity that exceeds the capabilities of traditional automation. In response, we introduce the concept of the LLM-RAN Operator. In this paradigm, a Large Language Model (LLM) is embedded into the RAN control loop to translate high-level human intents into optimal network actions. Unlike prior empirical studies, we present a formal framework for an LLM-RAN operator that builds on earlier work by making guarantees checkable through an adapter aligned with the Open RAN (O-RAN) standard, separating strategic LLM-driven guidance in the Non-Real-Time (RT) RAN intelligent controller (RIC) from reactive execution in the Near-RT RIC, including a proposition on policy expressiveness and a theorem on convergence to stable fixed points. By framing the problem with mathematical rigor, our work provides the analytical tools to reason about the feasibility and stability of AI-native RAN control. It identifies critical research challenges in safety, real-time performance, and physical-world grounding. This paper aims to bridge the gap between AI theory and wireless systems engineering in the NextG era, aligning with the AI4NextG vision to develop knowledgeable, intent-driven wireless networks that integrate generative AI into the heart of the RAN."
2509.10479,"Busy period is a fundamental concept in classical deterministic real-time scheduling analysis. In this deterministic context, only one busy period - which starts at the critical instant - needs to be considered, which identifies the worst-case scenario and thus paves the way for the development of efficient and safe analysis techniques. However, a recent work has revealed that, in the context of \textit{probabilistic} real-time scheduling analysis, only considering critical instant is not safe. In this paper, we address this gap by systematically analyzing deadline miss probabilities across varying busy period starting points. We propose a novel method of Worst-Case Deadline Failure Probability (WCDFP) for probabilistic fixed-priority preemptive scheduling. Experimental results demonstrate significant improvements over state-of-the-art methods achieved by our proposed method."
2509.10481,"Wireless communication is evolving into an agent era, where large-scale agents with inherent embodied intelligence are not just users but active participants. The perfect combination of wireless communication and embodied intelligence can achieve a synergetic empowerment and greatly facilitate the development of agent communication. An overview of this synergetic empowerment is presented, framing it as a co-evolutionary process that transforms wireless communication from a simple utility into the digital nervous system of a collective intelligence, while simultaneously elevating isolated agents into a unified superorganism with emergent capabilities far exceeding individual contributions. Moreover, we elaborate how embodied intelligence and wireless communication mutually benefit each other through the lens of the perception-cognition-execution (PCE) loop, revealing a fundamental duality where each PCE stage both challenges network capacity and creates unprecedented opportunities for system-wide optimization. Furthermore, critical open issues and future research directions are identified."
2509.10486,"With the advent of 5G, the internet has entered a new video-centric era. From short-video platforms like TikTok to long-video platforms like Bilibili, online video services are reshaping user consumption habits. Adaptive Bitrate (ABR) control is widely recognized as a critical factor influencing Quality of Experience (QoE). Recent learning-based ABR methods have attracted increasing attention. However, most of them rely on limited network trace sets during training and overlook the wide-distribution characteristics of real-world network conditions, resulting in poor generalization in out-of-distribution (OOD) scenarios. To address this limitation, we propose SABR, a training framework that combines behavior cloning (BC) pretraining with reinforcement learning (RL) fine-tuning. We also introduce benchmarks, ABRBench-3G and ABRBench-4G+, which provide wide-coverage training traces and dedicated OOD test sets for assessing robustness to unseen network conditions. Experimental results demonstrate that SABR achieves the best average rank compared with Pensieve, Comyco, and NetLLM across the proposed benchmarks. These results indicate that SABR enables more stable learning across wide distributions and improves generalization to unseen network conditions."
2509.10493,"The deployment of large-scale LoRaWAN networks requires jointly optimizing conflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE) by dynamically allocating transmission parameters, including Carrier Frequency, Spreading Factor, and Transmission Power. Existing methods often oversimplify this challenge, focusing on a single metric or lacking the adaptability needed for dynamic channel environments, leading to suboptimal performance. To address this, we propose two online learning-based resource allocation frameworks that intelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa, is a fully distributed framework that models the problem as a Combinatorial Multi-Armed Bandit. By decomposing the joint parameter selection and employing specialized, disaggregated reward functions, D-LoRa dramatically reduces learning complexity and enables nodes to autonomously adapt to network dynamics. To further enhance performance in LoRaWAN networks, we introduce CD-LoRa, a hybrid framework that integrates a lightweight, centralized initialization phase to perform a one-time, quasi-optimal channel assignment and action space pruning, thereby accelerating subsequent distributed learning. Extensive simulations and real-world field experiments demonstrate the superiority of our frameworks, showing that D-LoRa excels in non-stationary environments while CD-LoRa achieves the fastest convergence in stationary conditions. In physical deployments, our methods outperform state-of-the-art baselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their practical effectiveness for scalable and efficient LoRaWAN networks."
2509.10499,"Open Radio Access Network (O-RAN) architectures enable flexible, scalable, and cost-efficient mobile networks by disaggregating and virtualizing baseband functions. However, this flexibility introduces significant challenges for resource management, requiring joint optimization of functional split selection and virtualized unit placement under dynamic demands and complex topologies. Existing solutions often address these aspects separately or lack scalability in large and real-world scenarios. In this work, we propose a novel Graph-Augmented Proximal Policy Optimization (GPPO) framework that leverages Graph Neural Networks (GNNs) for topology-aware feature extraction and integrates action masking to efficiently navigate the combinatorial decision space. Our approach jointly optimizes functional split and placement decisions, capturing the full complexity of O-RAN resource allocation. Extensive experiments on both small-and large-scale O-RAN scenarios demonstrate that GPPO consistently outperforms state-of-the-art baselines, achieving up to 18% lower deployment cost and 25% higher reward in generalization tests, while maintaining perfect reliability. These results highlight the effectiveness and scalability of GPPO for practical O-RAN deployments."
2509.10507,"Internet of Intelligent Things (IoIT), an emerging field, combines the utility of Internet of Things (IoT) devices with the innovation of embedded AI algorithms. However, it does not come without challenges, and struggles regarding available computing resources, energy supply, and storage limitations. In particular, many impediments to IoIT are linked to the energy-efficient deployment of machine learning (ML)/deep learning (DL) models in embedded devices. Research has been conducted to design energy-efficient IoIT platforms, but these papers often focus on centralized systems, in which some central entity processes all the data and coordinates actions. This can be problematic, e.g., serve as bottleneck or lead to security concerns. In a decentralized system, nodes/devices would self-organize and make their own decisions. Therefore, to address such issues, we propose a heterogeneous, decentralized sensing and monitoring IoIT peer-to-peer mesh network system model. Nodes in the network will coordinate towards several optimization goals: reliability, energy efficiency, and latency. The system employs federated learning to train nodes in a distributed manner, metaheuristics to optimize task allocation and routing paths, and multi-objective optimization to balance conflicting performance goals."
2509.10508,"Heterogeneous Vehicular Networks (HetVNets) play a key role by stacking different communication technologies such as sub-6GHz, mm-wave and DSRC to meet diverse connectivity needs of 5G/B5G vehicular networks. HetVNet helps address the humongous user demands-but maintaining a steady connection in a highly mobile, real-world conditions remain a challenge. Though there has been ample of studies on beam prediction models a dedicated solution for HetVNets is sparsely explored. Hence, it is the need of the hour to develop a reliable beam prediction solution, specifically for HetVNets. This paper introduces a lightweight deep learning-based solution termed-""CAR-BRAINet"" which consists of convolutional neural networks with a powerful multi-head attention (MHA) mechanism. Existing literature on beam prediction is largely studied under a limited, idealised vehicular scenario, often overlooking the real-time complexities and intricacies of vehicular networks. Therefore, this study aims to mimic the complexities of a real-time driving scenario by incorporating key factors such as prominent MAC protocols-3GPP-C-V2X and IEEE 802.11BD, the effect of Doppler shifts under high velocity and varying distance and SNR levels into three high-quality dynamic datasets pertaining to urban, rural and highway vehicular networks. CAR-BRAINet performs effectively across all the vehicular scenarios, demonstrating precise beam prediction with minimal beam overhead and a steady improvement of 17.9422% on the spectral efficiency over the existing methods. Thus, this study justifies the effectiveness of CAR-BRAINet in complex HetVNets, offering promising performance without relying on the location angle and antenna dimensions of the mobile users, and thereby reducing the redundant sensor-latency."
2509.10533,"Network slicing is a key 5G technology that enables multiple virtual networks to share physical infrastructure, optimizing flexibility and resource allocation. This involves Mobile Network Operators (MNO), Mobile Virtual Network Operators (MVNOs), and end users, where MNO leases network slices to MVNOs, and then provides customized services. This work considers end-to-end network slicing with a focus on fair sharing and financial-related power efficiency, modeled as a two level hierarchical combinatorial auction. At the upper level, an MNO auctions slices to competing MVNOs, while at the lower level, MVNOs allocate resources to end users through their own auctions. Dynamic user requests add complexity to the process. Our model optimizes resource allocation and revenue generation using a pair-bid mechanism and Vickrey-Clarke-Groves (VCG) pricing. The pair-bid approach enhances competition and efficiency, while VCG ensures truthful bidding based on marginal system impact. Simulations validate the model's effectiveness in resource distribution and financial performance, showing a 12.5% revenue improvement over the baseline."
2509.10544,"We propose ASL360, an adaptive deep reinforcement learning-based scheduler for on-demand 360 video streaming to mobile VR users in next generation wireless networks. We aim to maximize the overall Quality of Experience (QoE) of the users served over a UAV-assisted 5G wireless network. Our system model comprises a macro base station (MBS) and a UAV-mounted base station which both deploy mm-Wave transmission to the users. The 360 video is encoded into dependent layers and segmented tiles, allowing a user to schedule downloads of each layer's segments. Furthermore, each user utilizes multiple buffers to store the corresponding video layer's segments. We model the scheduling decision as a Constrained Markov Decision Process (CMDP), where the agent selects Base or Enhancement layers to maximize the QoE and use a policy gradient-based method (PPO) to find the optimal policy. Additionally, we implement a dynamic adjustment mechanism for cost components, allowing the system to adaptively balance and prioritize the video quality, buffer occupancy, and quality change based on real-time network and streaming session conditions. We demonstrate that ASL360 significantly improves the QoE, achieving approximately 2 dB higher average video quality, 80% lower average rebuffering time, and 57% lower video quality variation, relative to competitive baseline methods. Our results show the effectiveness of our layered and adaptive approach in enhancing the QoE in immersive videostreaming applications, particularly in dynamic and challenging network environments."
2509.10559,"AI-native 6G networks are envisioned to tightly embed artificial intelligence (AI) into the wireless ecosystem, enabling real-time, personalized, and privacy-preserving intelligence at the edge. A foundational pillar of this vision is federated learning (FL), which allows distributed model training across devices without sharing raw data. However, implementing classical FL methods faces several bottlenecks in heterogeneous dynamic wireless networks, including limited device compute capacity, unreliable connectivity, intermittent communications, and vulnerability to model security and data privacy breaches. This article investigates the integration of quantum federated learning (QFL) into AI-native 6G networks, forming a transformative paradigm capable of overcoming these challenges. By leveraging quantum techniques across computing, communication, and cryptography within FL workflows, QFL offers new capabilities along three key dimensions: (i) edge intelligence, (ii) network optimization, and (iii) security and privacy, which are studied in this work. We further present a case study demonstrating that a QFL framework employing the quantum approximate optimization algorithm outperforms classical methods in model convergence. We conclude the paper by identifying practical challenges facing QFL deployment, such as quantum state fragility, incompatibility with classical protocols, and hardware constraints, and then outline key research directions toward its scalable real-world adoption."
2509.10617,"Industrial URLLC workloads-coordinated robotics, automated guided vehicles, machine-vision collaboration require sub-5 ms latency and five-nines reliability. In standardized 5G Multicast/Broadcast Services, intra-cell group traffic remains anchored in the core using MB-SMF/MB-UPF, and the Application Function. This incurs a core network path and packet delay that is avoidable when data transmitters and receivers share a cell. We propose a gNB-local multicast breakout that pivots eligible uplink flows to a downlink point-to-multipoint bearer within the gNB, while maintaining authorization, membership, and policy in the 5G core. The design specifies an eligibility policy, configured-grant uplink. 3GPP security and compliance are preserved via unchanged control-plane anchors. A latency budget and simulation indicate that removing the backhaul/UPF/AF segment reduces end-to-end latency from approximate 6.5-11.5 ms (anchored to the core) to 1.5-4.0 ms (local breakout), producing sub-2 ms averages and a stable gap approximate 10 ms between group sizes. The approach offers a practical, standards-aligned path to deterministic intra-cell group dissemination in private 5G. We outline multi-cell and prototype validation as future work."
2509.10914,"Collaboration opportunities for devices are facilitated with Federated Learning (FL). Edge computing facilitates aggregation at edge and reduces latency. To deal with model poisoning attacks, model-based outlier detection mechanisms may not operate efficiently with hetereogenous models or in recognition of complex attacks. This paper fosters the defense line against model poisoning attack by exploiting device-level traffic analysis to anticipate the reliability of participants. FL is empowered with a topology mutation strategy, as a Moving Target Defence (MTD) strategy to dynamically change the participants in learning. Based on the adoption of recurrent neural networks for time-series analysis of traffic and a 6G wireless model, optimization framework for MTD strategy is given. A deep reinforcement mechanism is provided to optimize topology mutation in adaption with the anticipated Byzantine status of devices and the communication channel capabilities at devices. For a DDoS attack detection application and under Botnet attack at devices level, results illustrate acceptable malicious models exclusion and improvement in recognition time and accuracy."
2509.10978,"This paper presents a detailed and flexible power consumption model for Radio Units (RUs) in O-RAN using the ns3-oran simulator. This is the first ns3-oran model supporting xApp control to perform the RU power modeling. In contrast to existing frameworks like EARTH or VBS-DRX, the proposed framework is RU-centric and is parameterized by hardware-level features, such as the number of transceivers, the efficiency of the power amplifier, mmWave overheads, and standby behavior. It enables simulation-driven assessment of energy efficiency at various transmit power levels and seamlessly integrates with ns-3's energy tracking system. To help upcoming xApp-driven energy management strategies in O-RAN installations, numerical research validates the model's capacity to represent realistic nonlinear power scaling. It identifies ideal operating points for effective RU behavior."
2509.11112,"Beamforming techniques are utilized in millimeter wave (mmWave) communication to address the inherent path loss limitation, thereby establishing and maintaining reliable connections. However, adopting standard defined beamforming approach in highly dynamic vehicular environments often incurs high beam training overheads and reduces the available airtime for communications, which is mainly due to exchanging pilot signals and exhaustive beam measurements. To this end, we present a multi-modal sensing and fusion learning framework as a potential alternative solution to reduce such overheads. In this framework, we first extract the features individually from the visual and GPS coordinates sensing modalities by modality specific encoders, and subsequently fuse the multimodal features to obtain predicted top-k beams so that the best line-of-sight links can be proactively established. To show the generalizability of the proposed framework, we perform a comprehensive experiment in four different vehicle-to-vehicle (V2V) scenarios from real-world multi-modal sensing and communication dataset. From the experiment, we observe that the proposed framework achieves up to 77.58% accuracy on predicting top-15 beams correctly, outperforms single modalities, incurs roughly as low as 2.32 dB average power loss, and considerably reduces the beam searching space overheads by 76.56% for top-15 beams with respect to standard defined approach."
2509.1114,"Hardware acceleration in modern networks creates monitoring blind spots by offloading flows to a non-observable state, hindering real-time service degradation (SD) detection. To address this, we propose and formalize a novel inter-flow correlation framework, built on the hypothesis that observable flows can act as environmental sensors for concurrent, non-observable flows. We conduct a comprehensive statistical analysis of this inter-flow landscape, revealing a fundamental trade-off: while the potential for correlation is vast, the most explicit signals (i.e., co-occurring SD events) are sparse and rarely perfectly align. Critically, however, our analysis shows these signals frequently precede degradation in the target flow, validating the potential for timely detection. We then evaluate the framework using a standard machine learning model. While the model achieves high classification accuracy, a feature-importance analysis reveals it relies primarily on simpler intra-flow features. This key finding demonstrates that harnessing the complex contextual information requires more than simple models. Our work thus provides not only a foundational analysis of the inter-flow problem but also a clear outline for future research into the structure-aware models needed to solve it."
2509.11157,"In recent years, sequence features such as packet length have received considerable attention due to their central role in encrypted traffic analysis. Existing sequence modeling approaches can be broadly categorized into flow-level and trace-level methods: the former suffer from high feature redundancy, limiting their discriminative power, whereas the latter preserve complete information but incur substantial computational and storage overhead. To address these limitations, we propose the \textbf{U}p-\textbf{D}own \textbf{F}low \textbf{S}equence (\textbf{UDFS}) representation, which compresses an entire trace into a two-dimensional sequence and characterizes each flow by the aggregate of its upstream and downstream traffic, reducing complexity while maintaining high discriminability. Furthermore, to address the challenge of class-specific discriminability differences, we propose an adaptive threshold mechanism that dynamically adjusts training weights and rejection boundaries, enhancing the model's classification performance. Experimental results demonstrate that the proposed method achieves superior classification performance and robustness on both coarse-grained and fine-grained datasets, as well as under concept drift and open-world scenarios. Code and Dataset are available atthis https URL."
2509.11239,"Delay Tolerant Networks (DTNs) are critical for emergency communication in highly dynamic and challenging scenarios characterized by intermittent connectivity, frequent disruptions, and unpredictable node mobility. While some protocols are widely adopted for simplicity and low overhead, their static replication strategy lacks the ability to adaptively distinguish high-quality relay nodes, often leading to inefficient and suboptimal message dissemination. To address this challenge, we propose a novel intelligent routing enhancement that integrates machine learning-based node evaluation into the Spray and Wait framework. Several dynamic, core features are extracted from simulation logs and are used to train multiple classifiers - Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), and Random Forest (RF) - to predict whether a node is suitable as a relay under dynamic conditions. The trained models are deployed via a lightweight Flask-based RESTful API, enabling real-time, adaptive predictions. We implement the enhanced router MLPBasedSprayRouter, which selectively forwards messages based on the predicted relay quality. A caching mechanism is incorporated to reduce computational overhead and ensure stable, low-latency inference. Extensive experiments under realistic emergency mobility scenarios demonstrate that the proposed framework significantly improves delivery ratio while reducing average latency compared to the baseline protocols. Among all evaluated classifiers, MLP achieved the most robust performance, consistently outperforming both SVM and RF in terms of accuracy, adaptability, and inference speed. These results confirm the novelty and practicality of integrating machine learning into DTN routing, paving the way for resilient and intelligent communication systems in smart cities, disaster recovery, and other dynamic environments."
2509.11289,"6th Generation (6G) mobile networks are envisioned to support several new capabilities and data-centric applications for unprecedented number of users, potentially raising significant energy efficiency and sustainability concerns. This brings focus on sustainability as one of the key objectives in the their design. To move towards sustainable solution, research and standardization community is focusing on several key issues like energy information monitoring and exposure, use of renewable energy, and use of Artificial Intelligence/Machine Learning (AI/ML) for improving the energy efficiency in 6G networks. The goal is to build energy-aware solutions that takes into account the energy information resulting in energy efficient networks. Design of energy-aware 6G networks brings in new challenges like increased overheads in gathering and exposing of energy related information, and the associated user consent management. The aim of this paper is to provide a comprehensive survey of methods used for design of energy efficient 6G networks, like energy harvesting, energy models and parameters, classification of energy-aware services, and AI/ML-based solutions. The survey also includes few use cases that demonstrate the benefits of incorporating energy awareness into network decisions. Several ongoing standardization efforts in 3GPP, ITU, and IEEE are included to provide insights into the ongoing work and highlight the opportunities for new contributions. We conclude this survey with open research problems and challenges that can be explored to make energy-aware design feasible and ensure optimality regarding performance and energy goals for 6G networks."
2509.11421,"The rollout of 6G networks introduces unprecedented demands for autonomy, reliability, and scalability. However, the transmission of sensitive telemetry data to central servers raises concerns about privacy and bandwidth. To address this, we propose a federated edge learning framework for predictive maintenance in 6G small cell networks. The system adopts a Knowledge Defined Networking (KDN) architecture in Data, Knowledge, and Control Planes to support decentralized intelligence, telemetry-driven training, and coordinated policy enforcement. In the proposed model, each base station independently trains a failure prediction model using local telemetry metrics, including SINR, jitter, delay, and transport block size, without sharing raw data. A threshold-based multi-label encoding scheme enables the detection of concurrent fault conditions. We then conduct a comparative analysis of centralized and federated training strategies to evaluate their performance in this context. A realistic simulation environment is implemented using the ns-3 mmWave module, incorporating hybrid user placement and base station fault injection across various deployment scenarios. The learning pipeline is orchestrated via the Flower framework, and model aggregation is performed using the Federated Averaging (FedAvg) algorithm. Experimental results demonstrate that the federated model achieves performance comparable to centralized training in terms of accuracy and per-label precision, while preserving privacy and reducing communication overhead."
2509.1181,"Digital twins have been introduced as supporters to city operations, yet existing scene-descriptor formats and digital twin platforms often lack the integration, federation, and adaptable connectivity that urban environments demand. Modern digital twin platforms decouple data streams and representations into separate architectural planes, fusing them only at the visualization layer and limiting potential for simulation or further processing of the combined assets. At the same time, geometry-centric file standards for digital twin description, and services built on top of them, focus primarily on explicitly declaring geometry and additional structural or photorealistic parameters, making integration with evolving context information a complicated process while limiting compatibility with newer representation methods. Additionally, multi-provider federation, critical in smart city services where multiple stakeholders may control distinct infrastructure or representation assets, is sparsely supported. Consequently, most pilots isolate context and representation, fusing them per use case with ad hoc components and custom description files or glue code, which hinders interoperability. To address these gaps, this paper proposes a novel concept, the 'Digital Twin Descriptor Service (DTDS)' that fuses abstracted references to geometry assets and context information within a single, extensible descriptor service through NGSI-LD. The proposed DTDS provides dynamic and federated integration of context data, representations, and runtime synchronization across heterogeneous engines and simulators. This concept paper outlines the DTDS architectural components and description ontology that enable digital-twin processes in the modern smart city."
2509.11969,"Reconfigurable Intelligent Surfaces (RISs) transform the wireless environment by modifying the amplitude, phase, and polarization of incoming waves, significantly improving coverage performance. Notably, optimizing the deployment of RISs becomes vital, but existing optimization methods face challenges such as high computational complexity, limited adaptability to changing environments, and a tendency to converge on local optima. In this paper, we propose to optimize the deployment of large-scale 3D RISs using a diffusion model based on probabilistic generative learning. We begin by dividing the target area into fixed grids, with each grid corresponding to a potential deployment location. Then, a multi-RIS deployment optimization problem is formulated, which is difficult to solve directly. By treating RIS deployment as a conditional generation task, the well-trained diffusion model can generate the distribution of deployment strategies, and thus, the optimal deployment strategy can be obtained by sampling from this distribution. Simulation results demonstrate that the proposed diffusion-based method outperforms traditional benchmark approaches in terms of exceed ratio and generalization."
2509.12307,"This letter addresses a critical challenge in the context of 6G and beyond wireless networks, the joint optimization of power and bandwidth resource allocation for aerial intelligent platforms, specifically uncrewed aerial vehicles (UAVs), operating in highly dynamic environments with mobile ground user equipment (UEs). We introduce FLARE (Flying Learning Agents for Resource Efficiency), a learning-enabled aerial intelligence framework that jointly optimizes UAV positioning, altitude, transmit power, and bandwidth allocation in real-time. To adapt to UE mobility, we employ Silhouette-based K-Means clustering, enabling dynamic grouping of users and UAVs' deployment at cluster centroids for efficient service delivery. The problem is modeled as a multi-agent control task, with bandwidth discretized into resource blocks and power treated as a continuous variable. To solve this, our proposed framework, FLARE, employs a hybrid reinforcement learning strategy that combines Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Deep Q-Network (DQN) to enhance learning efficiency. Simulation results demonstrate that our method significantly enhances user coverage, achieving a 73.45% improvement in the number of served users under a 5 Mbps data rate constraint, outperforming MADDPG baseline."
2509.12441,"Network planning seeks to determine base station parameters that maximize coverage and capacity in cellular networks. However, achieving optimal planning remains challenging due to the diversity of deployment scenarios and the significant simulation-to-reality discrepancy. In this paper, we propose \emph{AutoPlan}, a new automatic network planning framework by leveraging digital radio twin (DRT) techniques. We derive the DRT by finetuning the parameters of building materials to reduce the sim-to-real discrepancy based on crowdsource real-world user data. Leveraging the DRT, we design a Bayesian optimization based algorithm to optimize the deployment parameters of base stations efficiently. Using the field measurement from Husker-Net, we extensively evaluate \emph{AutoPlan} under various deployment scenarios, in terms of both coverage and capacity. The evaluation results show that \emph{AutoPlan} flexibly adapts to different scenarios and achieves performance comparable to exhaustive search, while requiring less than 2\% of its computation time."
2509.12499,"Digital Twin (DT) technology enables real-time monitoring and optimization of complex network infrastructures by creating accurate virtual replicas of physical systems. In millimeter-wave (mmWave) 5G/6G networks, the deployment of Integrated Access and Backhaul (IAB) nodes faces highly dynamic urban environments, necessitating intelligent DT-enabled optimization frameworks. Traditional IAB deployment optimization approaches struggle with the combinatorial complexity of jointly optimizing coverage, connectivity, and resilience, often leading to suboptimal solutions that are vulnerable to network disruptions. With this consideration, we propose a novel Graph Attention Network v2 (GATv2)-based reinforcement learning approach for resilient IAB deployment in urban mmWave networks. Specifically, we formulate the deployment problem as a Markov Decision Process (MDP) with explicit resilience constraints and employ edge-conditioned GATv2 to capture complex spatial dependencies between heterogeneous node types and dynamic connectivity patterns. The attention mechanism enables the model to focus on critical deployment locations to maximize coverage and ensure fault tolerance through redundant backhaul connections. To address the inherent vulnerability of mmWave links, we train the GATv2 policy using Proximal Policy Optimization (PPO) with a carefully designed balance between coverage, cost, and resilience. Comprehensive simulations across three urban scenarios demonstrate that our method achieves 98.5-98.7 percent coverage with 14.3-26.7 percent fewer nodes than baseline approaches, while maintaining 87.1 percent coverage retention under 30 percent link failures, representing 11.3-15.4 percent improvement in fault tolerance compared to state-of-the-art methods."
2509.12664,"Several wireless networking problems are often posed as 0-1 mixed optimization problems, which involve binary variables (e.g., selection of access points, channels, and tasks) and continuous variables (e.g., allocation of bandwidth, power, and computing resources). Traditional optimization methods as well as reinforcement learning (RL) algorithms have been widely exploited to solve these problems under different network scenarios. However, solving such problems becomes more challenging when dealing with a large network scale, multi-dimensional radio resources, and diversified service requirements. To this end, in this paper, a unified framework that combines RL and optimization theory is proposed to solve 0-1 mixed optimization problems in wireless networks. First, RL is used to capture the process of solving binary variables as a sequential decision-making task. During the decision-making steps, the binary (0-1) variables are relaxed and, then, a relaxed problem is solved to obtain a relaxed solution, which serves as prior information to guide RL searching policy. Then, at the end of decision-making process, the search policy is updated via suboptimal objective value based on decisions made. The performance bound and convergence guarantees of the proposed framework are then proven theoretically. An extension of this approach is provided to solve problems with a non-convex objective function and/or non-convex constraints. Numerical results show that the proposed approach reduces the convergence time by about 30% over B&B in small-scale problems with slightly higher objective values. In large-scale scenarios, it can improve the normalized objective values by 20% over RL with a shorter convergence time."
2509.12716,"Despite the widespread deployment of terrestrial networks, providing reliable communication services to remote areas and maintaining connectivity during emergencies remains challenging. Low Earth orbit (LEO) satellite constellations offer promising solutions with their global coverage capabilities and reduced latency, yet struggle with intermittent coverage and limited communication windows due to orbital dynamics. This paper introduces an age of information (AoI)-aware space-air-ground integrated network (SAGIN) architecture that leverages a high-altitude platform (HAP) as intelligent relay between the LEO satellites and ground terminals. Our three-layer design employs hybrid free-space optical (FSO) links for high-capacity satellite-to-HAP communication and reliable radio frequency (RF) links for HAP-to-ground transmission, and thus addressing the temporal discontinuity in LEO satellite coverage while serving diverse user priorities. Specifically, we formulate a joint optimization problem to simultaneously minimize the AoI and satellite handover frequency through optimal transmit power distribution and satellite selection decisions. This highly dynamic, non-convex problem with time-coupled constraints presents significant computational challenges for traditional approaches. To address these difficulties, we propose a novel diffusion model (DM)-enhanced dueling double deep Q-network with action decomposition and state transformer encoder (DD3QN-AS) algorithm that incorporates transformer-based temporal feature extraction and employs a DM-based latent prompt generative module to refine state-action representations through conditional denoising. Simulation results highlight the superior performance of the proposed approach compared with policy-based methods and some other deep reinforcement learning (DRL) benchmarks."
2509.1286,"Digital twins (DTs) enable smarter, self-optimizing mobile networks, but they rely on a steady supply of real world data. Collecting and transferring complete traces in real time is a significant challenge. We present a compact traffic generator that combines hidden Markov model, capturing the broad rhythms of buffering, streaming and idle periods, with a small feed forward mixture density network that generates realistic payload sizes and inter-arrival times to be fed to the DT. This traffic generator trains in seconds on a server GPU, runs in real time and can be fine tuned inside the DT whenever the statistics of the generated data do not match the actual traffic. This enables operators to keep their DT up to date without causing overhead to the operational network. The results show that the traffic generator presented is able to derive realistic packet traces of payload length and inter-arrival time across various metrics that assess distributional fidelity, diversity, and temporal correlation of the synthetic trace."
2509.13208,"Formal specifications have numerous benefits for both designers and users of network protocols. They provide clear, unambiguous representations, which are useful as documentation and for testing. They can help reveal disagreements about what a protocol ""is"" and identify areas where further work is needed to resolve ambiguities or internal inconsistencies. They also provide a foundation for formal reasoning, making it possible to establish important security and correctness guarantees on all inputs and in every environment. Despite these advantages, formal methods are not widely used to design, implement, and validate network protocols today. Instead, Internet protocols are usually described in informal documents, such as IETF Requests for Comments (RFCs) or IEEE standards. These documents primarily consist of lengthy prose descriptions, accompanied by pseudocode, header descriptions, state machine diagrams, and reference implementations which are used for interoperability testing. So, while RFCs and reference implementations were only intended to help guide the social process used by protocol designers, they have evolved into the closest things to formal specifications the Internet community has. In this paper, we discuss the different roles that specifications play in the networking and formal methods communities. We then illustrate the potential benefits of specifying protocols formally, presenting highlights from several recent success stories. Finally, we identify key differences between how formal specifications are understood by the two communities and suggest possible strategies to bridge the gaps."
2509.1349,"This paper presents the identification of congestion control protocols TCP Reno, TCP Cubic, TCP Vegas, and BBR on the Marist University campus, with an accuracy of 97.04% using a GRU-based learning model. We used a faster neural network architecture on a more complex and competitive network in comparison to existing work and achieved comparably high accuracy."
2509.13511,"Network slicing plays a crucial role in realizing 5G/6G advances, enabling diverse Service Level Agreement (SLA) requirements related to latency, throughput, and reliability. Since network slices are deployed end-to-end (E2E), across multiple domains including access, transport, and core networks, it is essential to efficiently decompose an E2E SLA into domain-level targets, so that each domain can provision adequate resources for the slice. However, decomposing SLAs is highly challenging due to the heterogeneity of domains, dynamic network conditions, and the fact that the SLA orchestrator is oblivious to the domain's resource optimization. In this work, we propose Odin, a Bayesian Optimization-based solution that leverages each domain's online feedback for provably-efficient SLA decomposition. Through theoretical analyses and rigorous evaluations, we demonstrate that Odin's E2E orchestrator can achieve up to 45% performance improvement in SLA satisfaction when compared with baseline solutions whilst reducing overall resource costs even in the presence of noisy feedback from the individual domains."
2509.13604,"The World Wide Web has come to be a great part of our daily life, yet user observed latency is still a problem that needs a proper means of handling. Even though earlier attempts focused on caching as the chief solution to tackling this issue, its success was extremely limited. Prefetching has come to be the primary technique in supplementing caching towards soothing the latency problem associated with the contemporary Internet. However, existing approaches in prefetching are extremely limited in their ability to employ application level web document relationship which is often visible only to the content developer. This is because most approaches are access history based schemes that make future users' access prediction only based on past user access. Attempts to incorporate prefetching schemes that utilize semantic information with those that use users past access history are extremely limited in their extensibility. In this work we present a novel framework that enables integration of schemes from both worlds of prefetching without the need for a major modification to the algorithms. When there is a need/possibility to capture new application level context, a new algorithm could be developed to do so and then it can be integrated into the framework. Since each participating scheme is merely viewed as an algorithm that produces a list of candidate objects that are likely to be accessed in the near future, the framework can entertain any one of the existing prefetching schemes. With its adaptive weight management technique the framework adjusts the effect of each algorithm in the overall prediction to parallel with its observed performance so far. We have found this formwork to be less aggressive than its contemporary counterparts which is extremely important for resource constrained mobile devices that have come to be the major means of access by users of the current web."
2509.13714,"The emergence of ultra-low latency applications, such as financial transactions, has driven the development of hybrid backbone networks that rely on fiber, satellite, and microwave links. Despite providing low latencies, these hybrid networks suffer from occasional environmental packet loss caused by poor weather, construction, and line of sight blockage. Paradoxically, today's hybrid backbones rely on conventional transport protocols that take packet loss to signal network congestion, as opposed to transient environmental obstacles. A common approach to address this challenge is to use network coding (NC) between the end hosts to recover from these occasional packet loss events. However, current NC proposals assume full access to the end-hosts' stack to perform end-to-end encoding/decoding operations. In this paper, we introduce LINC, a novel system that provides in-network NC capabilities to mitigate environmental packet loss events without requiring cooperation from the end hosts. LINC uses a systematic block coding approach on a link-by-link basis, encoding and decoding packets inside the network. We model the tradeoff in goodput between end-to-end retransmissions and redundant packets introduced by LINC, and propose an optimization formulation to determine the optimal choice of coding parameters. Our simulations on real-world backbone topologies demonstrate that LINC reduces the end-to-end latency by up to 18% by eliminating unnecessary retransmissions."
2509.13724,"Mission-critical voice (MCV) communications systems have been a critical tool for the public safety community for over eight decades. Public safety users expect MCV systems to operate reliably and consistently, particularly in challenging conditions. Because of these expectations, the Public Safety Communications Research (PSCR) Division of the National Institute of Standards and Technology (NIST) has been interested in correlating impairments in MCV communication systems and public safety user quality of experience (QoE). Previous research has studied MCV voice quality and intelligibility in a controlled environment. However, such research has been limited by the challenges inherent in emulating real-world environmental conditions. Additionally, there is the question of the best metric to use to reflect QoE accurately.This paper describes our efforts to develop the methodology and tools for human-subject experiments with MCV. We illustrate their use in human-subject experiments in emulated real-world environments. The tools include a testbed for emulating real-world MCV systems and an automated speech recognition (ASR) robot approximating human subjects in transcription tasks. We evaluate QoE through a Levenshtein Distance-based metric, arguing it is a suitable proxy for measuring comprehension and the QoE. We conducted human-subject studies with Amazon MTurk volunteers to understand the influence of selected system parameters and impairments on human subject performance and end-user QoE. We also compare the performance of several ASR system configurations with human-subject performance. We find that humans generally perform better than ASR in accuracy-related MCV tasks and that the codec significantly influences the end-user QoE and ASR performance."
2509.13901,"GitOps has emerged as a foundational paradigm for managing cloud-native infrastructures by enabling declarative configuration, version-controlled state, and automated reconciliation between intents and runtime deployments. Despite its widespread adoption, the performance and scalability of GitOps tools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated. This paper presents a reproducible, metric-driven benchmarking, assessing the latency and resource overheads of three widely used GitOps operators: Argo CD, Flux CD, and ConfigSync. We conduct controlled experiments under both single- and multi-intent scenarios, capturing key performance indicators such as latency and resource consumption. Our results highlight trade-offs between the tools in terms of determinism, resource efficiency, and responsiveness. We further investigate a realistic orchestration scenario, using Nephio as our orchestrator, to quantify the processing latency and overhead in declarative end-to-end deployment pipelines. Our findings can offer valuable insights for tool selection and optimisation in future autonomous network orchestration systems."
2509.13954,"To some organizations, building campus network is sometimes considered to be very expensive; and this has made the project uneasy to perform. Moreover, if the organization without sufficient IT knowledge does not have capable IT engineers, leaving this project to third parties without supervision would lead to unexpected larger expenses. For this reason, in the year of 2003, YARSI University formed CMIS (Center for Management Infor-mation System) to perform tasks in designing, operations and maintenance of campus network and its services. By combining Open Source operating system run on a local assembled personal computer as gateway and router, and switching technology from Cisco, we designed a low-cost UTP-based campus network which covering rooms and buildings in YARSI environment. Meanwhile the internet access through several broadband connections and dedicated wireless was shared to more than 100 simultaneous users by a captive portal system. With this strategy, we can significantly reduce cost for purchasing, maintenance and operations of network infrastructure and internet access. Our model in designing low-cost campus network and internet connections could be adopted by rural community or organizations that have limited budget to have internet access."
2509.13993,"Proposed Bell pair swapping protocols, an essential component of the Quantum Internet, are planned-path: specific, structured, routing paths are reserved prior to the execution of the swapping process. This makes sense when one assumes the state used in the swapping process is expensive, fragile, and unstable. However, lessons from classical networking have shown that while reservations seem promising in concept, flexible, reservation-light or free approaches often outperform their more restrictive counterparts in well-provisioned networks. In this paper, we propose that a path-oblivious approach is more amenable to supporting swapping as quantum state evolves into a cheaper, more robust form. We formulate the swapping process as a linear program and present and evaluate a fairly naive baseline swapping protocol that tries to balance Bell pairs throughout the network. Preliminary results show that while naive balancing leaves room for improvement, investigating path-oblivious swapping is a promising direction."
2509.14002,"Recently, content-aware methods have been employed to reduce bandwidth and enhance the quality of Internet video delivery. These methods involve training distinct content-aware super-resolution (SR) models for each video chunk on the server, subsequently streaming the low-resolution (LR) video chunks with the SR models to the client. Prior research has incorporated additional partial parameters to customize the models for individual video chunks. However, this leads to parameter accumulation and can fail to adapt appropriately as video lengths increase, resulting in increased delivery costs and reduced performance. In this paper, we introduce RepCaM++, an innovative framework based on a novel Re-parameterization Content-aware Modulation (RepCaM) module that uniformly modulates video chunks. The RepCaM framework integrates extra parallel-cascade parameters during training to accommodate multiple chunks, subsequently eliminating these additional parameters through re-parameterization during inference. Furthermore, to enhance RepCaM's performance, we propose the Transparent Visual Prompt (TVP), which includes a minimal set of zero-initialized image-level parameters (e.g., less than 0.1%) to capture fine details within video chunks. We conduct extensive experiments on the VSD4K dataset, encompassing six different video scenes, and achieve state-of-the-art results in video restoration quality and delivery bandwidth compression."
2509.14523,"We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed for distributed Vehicle-to-Vehicle (V2V) communication. The platform bridges the gap between network simulation and deployment by providing a modular codebase configured for cost-effective ADALM-Pluto SDRs. Any device capable of running a Docker with ROS, executing Matlab and interface with a Pluto via USB can act as a communication node. To demonstrate collaborative sensing, we share LiDAR point clouds between nodes and fuse them into a collective perception environment. We evaluated a theoretical model for leveraging decentralized storage systems (IPFS and Filecoin), analyzing constraints such as node storage convergence, latency, and scalability. In addition, we provide a channel quality study."
2509.14628,"Next-generation cellular networks are envisioned to integrate sensing capabilities with communication, particularly in the millimeter-wave (mmWave) spectrum, where beamforming using large-scale antenna arrays enables directional signal transmissions for improved spatial multiplexing. In current 5G networks, however, beamforming is typically designed either for communication or sensing (e.g., beam training during link establishment). In this paper, we present Chameleon, a novel framework that augments and rapidly switches beamformers during each demodulation reference signal (DMRS) symbol to achieve integrated sensing and communication (ISAC) in 5G mmWave networks. Each beamformer introduces an additional sensing beam toward target angles while maintaining the communication beams toward multiple users. We implement Chameleon on a 28 GHz software-defined radio testbed supporting over-the-air 5G physical downlink shared channel (PDSCH) transmissions. Extensive experiments in open environments show that Chameleon achieves multi-user communication with a sum data rate of up to 0.80 Gbps across two users. Simultaneously, Chameleon employs a beamformer switching interval of only 0.24 {\mu}s, therefore producing a 31x31-point 2D imaging within just 0.875 ms. Leveraging machine learning, Chameleon further enables object localization with median errors of 0.14 m (distance) and 0.24 (angle), and material classification with 99.0% accuracy."
2509.14731,"We introduce the concept of 1Q, the first wireless generation of integrated classical and quantum communication. 1Q features quantum base stations (QBSs) that support entanglement distribution via free-space optical links alongside traditional radio communications. Key new components include quantum cells, quantum user equipment (QUEs), and hybrid resource allocation spanning classical time-frequency and quantum entanglement domains. Several application scenarios are discussed and illustrated through system design requirements for quantum key distribution, blind quantum computing, and distributed quantum sensing. A range of unique quantum constraints are identified, including decoherence timing, fidelity requirements, and the interplay between quantum and classical error probabilities. Protocol adaptations extend cellular connection management to incorporate entanglement generation, distribution, and handover procedures, expanding the Quantum Internet to the cellular wireless."
2509.14877,"While simulators exist for vehicular IoT nodes communicating with the Cloud through Edge nodes in a fully-simulated osmotic architecture, they often lack support for dynamic agent planning and optimisation to minimise vehicular battery consumption while ensuring fair communication times. Addressing these challenges requires extending current simulator architectures with AI algorithms for both traffic prediction and dynamic agent planning. This paper presents an extension of SimulatorOrchestrator (SO) to meet these requirements. Preliminary results over a realistic urban dataset show that utilising vehicular planning algorithms can lead to improved battery and QoS performance compared with traditional shortest path algorithms. The additional inclusion of desirability areas enabled more ambulances to be routed to their target destinations while utilising less energy to do so, compared to traditional and weighted algorithms without desirability considerations."
2509.15501,"WiFi probe request (PR) frames encode fine-grained device interactions and serve as a critical basis for mobility and crowd analytics. However, pervasive MAC address randomization and the scarcity of labeled datasets hinder progress in PR-based studies. We introduce WiFiSim, a simulation framework that reconstructs PR generation through Android Open Source Project (AOSP) protocol analysis and finite-state device behavior modeling. WiFiSim identifies the key determinants of PR structure and timing while capturing realistic user-driven state transitions. Experiments show that WiFiSim achieves less than 5% deviation from real measurements in both distributional and temporal dynamics, scales to large-scale dataset synthesis, and enables reliable evaluation of downstream applications. Source code and sample datasets are publicly released to foster reproducible research."
2509.15856,"Routing-driven timely data collection in Underwater Acoustic Sensor Networks (UASNs) is crucial for marine environmental monitoring, disaster warning and underwater resource exploration, etc. However, harsh underwater conditions, including high delays, limited bandwidth, and dynamic topologies - make efficient routing decisions challenging in UASNs. In this paper, we propose a smart interrupted routing scheme for UASNs to address dynamic underwater challenges. We first model underwater noise influences from real underwater routing features, e.g., turbulence and storms. We then propose a Software-Defined Networking (SDN)-based Interrupted Software-defined UASNs Reinforcement Learning (ISURL) framework which ensures adaptive routing through dynamically failure handling (e.g., energy depletion of sensor nodes or link instability) and real-time interrupted recovery. Based on ISURL, we propose MA-MAPPO algorithm, integrating multi-head attention mask mechanism with MAPPO to filter out infeasible actions and streamline training. Furthermore, to support interrupted data routing in UASNs, we introduce MA-MAPPO_i, MA-MAPPO with interrupted policy, to enable smart interrupted routing decision in UASNs. The evaluations demonstrate that our proposed routing scheme achieves exact underwater data routing decision with faster convergence speed and lower routing delays than existing approaches."
2509.1593,"Time-Sensitive Networking (TSN) is a toolbox of technologies that enable deterministic communication over Ethernet. A key area has been TSN's time-aware traffic shaping (TAS), which supports stringent end-to-end latency and reliability requirements. Configuration of TAS requires the computation of a network-wide traffic schedule, which is particularly challenging with integrated wireless networks (e.g., 5G, Wi-Fi) due to the stochastic nature of wireless links. This paper introduces a novel method for configuring TAS, focusing on cyclic traffic patterns and jitter of wireless links. We formulate a linear program that computes a network-wide time-aware schedule, robust to wireless performance uncertainties. The given method enables robust scheduling of multiple TSN frames per transmission window using a tunable robustness parameter ({\Gamma}). To reduce computational complexity, we also propose a sequential batch-scheduling heuristic that runs in polynomial time. Our approach is evaluated by using different network topologies and wireless link characteristics, demonstrating that the heuristic can schedule 90% of 6500 requested TSN streams in a large topology."
2509.16485,"Efficient encoding of network flow spaces while preserving spatial locality is essential for intelligent Software-Defined Networking (SDN) applications, particularly those employing reinforcement learning (RL) methods in a reactive manner. In this work, we introduce a spatially aware Bloom Filter-based approach to encode IP flow pairs, leveraging their inherent geographical locality. Through controlled experiments using IoT traffic data, we demonstrate that Bloom Filters effectively preserve spatial relationships among flows. Our findings show that Bloom Filters degrade gracefully, maintaining predictable spatial correlations critical for RL state representation. We integrate this encoding into a DQN-based eviction strategy for reactive SDN forwarding. Experiments show that Bloom Filter-encoded, spatially aware flow representation enables up to 7% and 8% reduction in normalized miss rate over LRU and LFU, respectively, across 10 hours of traffic, demonstrating potential for low-latency applications. This experiment justifies the usefulness of preserving spatial correlation by encoding the flow space into a manageable size, opening a novel research direction for RL-based SDN applications."
2509.167,"Integrated sensing and communication (ISAC) is a key enabler for next-generation wireless networks, offering spectrum efficiency and reduced hardware complexity. While monostatic ISAC has been well studied, its limited spatial diversity reduces reliability in high-mobility scenarios. Distributed ISAC alleviates this via cooperative nodes, but conventional OFDM-based designs remain vulnerable to Doppler shifts and multipath fading. Orthogonal time frequency space (OTFS) modulation has recently emerged as a resilient alternative, as its delay-Doppler domain representation enables robust communication and high-resolution sensing. Motivated by this, we extend OTFS to distributed ISAC and address the underexplored problem of spatial node deployment. We propose a triangulation-based framework that leverages spatial diversity to improve target localization, velocity estimation, and communication rates, and analytically characterize the role of deployment geometry in minimizing estimation error. Furthermore, we integrate Kalman filtering (KF) into distributed OTFS-ISAC to enhance tracking of moving targets, and design novel algorithms for active sensing, passive sensing, and joint sensing-communication. Closed-form expressions are derived for localization error under general topologies, and a near-optimal deployment strategy is identified by aligning receivers along orthogonal axes. Numerical evaluations show significant reductions in localization error and bit error rate (BER), while capturing the trade-offs between sensing accuracy and communication reliability. These results highlight the potential of KF-assisted node placement in distributed OTFS-ISAC for reliable, high-performance operation in dynamic wireless environments."
2509.16856,"Service Function Chains (SFCs) enable programmatic control of the functions and services in a computer network. By leveraging Software Defined Networking to control the links between virtualised network functions, SFCs provide a scalable approach to dealing with the increased pressures on network operation and management. Unfortunately, the challenge of embedding SFCs onto the underlying physical network and compute infrastructure is an NP-hard problem. Genetic Algorithms (GAs) have been used to address this issue, but they require significant time to evaluate solution quality (fitness) \textit{online}, with most solutions instead adopting \textit{offline} simulations or analytical evaluations.To enable online use of GAs in solving the SFC embedding problem, we introduce a hybrid online-offline approach to evaluate generated solutions. At the core of this is BeNNS--a topology, traffic, and SFC-embedding agnostic surrogate model that approximates fitness. We evaluate our approach across six experiments, varying available resources and traffic loads. Our results demonstrate that our approach is capable of exploring thousands of potential configurations and generating deployable solutions in 36.8 minutes on average, compared to online-only approaches, which take 17.9 hours on average to explore tens of solutions, which do not converge on an optimal solution."
2509.16917,"This paper analyzes the functional requirements and architectural considerations for Integrated Sensing and Communication ( ISAC) in a 5G Open Radio Access Network (OpenRAN) environment, with emphasis on secure and modular deployment. Focusing on a mono-static, half-duplex sensing approach, it evaluates radar setup options, signal types, and processing placement within the Radio Access Network ( RAN), considering performance and security implications. The proposed architecture minimizes hardware modifications by leveraging sniffer Radio Units (RU s) and existing OpenRAN fronthaul interfaces, while protecting sensitive In-phase and Quadrature (I/Q) data and control traffic against potential attacks. Security threats, such as passive sensing, spoofing, and privacy violations, are mapped to mitigation strategies within the OpenRAN framework. The result is a deployment blueprint applicable to both Public Land Mobile Networks ( PLMNs) and Non-Public Networks (NPNs), supporting future 6G ISAC capabilities in a standards-compliant manner."
2509.16984,"Prevailing network control strategies, which rely on static shortest-path logic, suffer from catastrophic ""stress concentration"" on critical nodes. This paper introduces the System Relaxation Algorithm (SRA), a new control paradigm inspired by physical relaxation that guides a network toward an emergent equilibrium of load balance. SRA is an interpretable, 'white-box' dynamical system whose behavior is profoundly topology-dependent: in heterogeneous networks, it acts as a proactive performance optimizer, reducing peak centrality by over 80\% and increasing high-load throughput by more than 45\%; in homogeneous topologies, its objective intelligently shifts to resilience enhancement. We rigorously prove its global convergence and practical stability using the theory of non-smooth dynamical systems, establishing a predictable paradigm for network governance that intelligently trades off performance and resilience."
2509.17028,"For packet-switched networks, when the packetization effect is overlooked, network calculus analysis can produce faulty results. To exemplify, network calculus analysis is applied in this paper to two basic systems that are fundamental or default settings in Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet). Through counterexamples, it is revealed that for the two fundamental settings, some widely adopted, network calculus-based service characterization results, known as service curves, which ignore packetization, are faulty. In addition, for performance bounds derived from the faulty service curves, it is shown that the validity of the bounds can be arguable. In particular, the output bound, backlog bound and concatenation service curve results are shown to be also faulty: counterexamples can be constructed. By factoring the packetization effect directly into the service models, corrected service curves and performance bounds are derived for the two basic systems. These results remind that special care is needed when applying network calculus analysis to packet-switched networks."
2509.17398,"To enable training of large artificial intelligence (AI) models at the network edge, split federated learning (SFL) has emerged as a promising approach by distributing computation between edge devices and a server. However, while unstable network environments pose significant challenges to SFL, prior schemes often overlook such an effect by assuming perfect client participation, rendering them impractical for real-world scenarios. In this work, we develop an optimization framework for SFL with unstable client participation. We theoretically derive the first convergence upper bound for SFL with unstable client participation by considering activation uploading failures, gradient downloading failures, and model aggregation failures. Based on the theoretical results, we formulate a joint optimization problem for client sampling and model splitting to minimize the upper bound. We then develop an efficient solution approach to solve the problem optimally. Extensive simulations on EMNIST and CIFAR-10 demonstrate the superiority of our proposed framework compared to existing benchmarks."
2509.17676,"Long Range (LoRa) based low-power wide area networks (LPWANs) are crucial for enabling next-generation IoT (NG-IoT) applications in 5G/6G ecosystems due to their long-range, low-power, and low-cost characteristics. However, achieving high energy efficiency in such networks remains a critical challenge, particularly in large-scale or dynamically changing environments. Traditional terrestrial LoRa deployments often suffer from coverage gaps and non-line-of-sight (NLoS) propagation losses, while satellite-based IoT solutions consume excessive energy and introduce high latency, limiting their suitability for energy-constrained and delay-sensitive applications. To address these limitations, we propose a novel architecture using multiple unmanned aerial vehicles (UAVs) as flying LoRa gateways to dynamically collect data from ground-based LoRa end devices. Our approach aims to maximize the system's weighted global energy efficiency by jointly optimizing spreading factors, transmission powers, UAV trajectories, and end-device associations. Additionally, we formulate this complex optimization problem as a partially observable Markov decision process (POMDP) and propose green LoRa multi-agent proximal policy optimization (GLo-MAPPO), a multi-agent reinforcement learning (MARL) framework based on centralized training with decentralized execution (CTDE). Simulation results show that GLo-MAPPO significantly outperforms benchmark algorithms, achieving energy efficiency improvements of 71.25%, 18.56%, 67.00%, 59.73%, and 49.95% for networks with 10, 20, 30, 40, and 50 LoRa end devices, respectively."
2509.18007,"Recent advancements in deep learning have significantly enhanced the performance and efficiency of traffic classification in networking systems. However, the lack of transparency in their predictions and decision-making has made network operators reluctant to deploy DL-based solutions in production networks. To tackle this challenge, we propose Traffic-Explainer, a model-agnostic and input-perturbation-based traffic explanation framework. By maximizing the mutual information between predictions on original traffic sequences and their masked counterparts, Traffic-Explainer automatically uncovers the most influential features driving model predictions. Extensive experiments demonstrate that Traffic-Explainer improves upon existing explanation methods by approximately 42%. Practically, we further apply Traffic-Explainer to identify influential features and demonstrate its enhanced transparency across three critical tasks: application classification, traffic localization, and network cartography. For the first two tasks, Traffic-Explainer identifies the most decisive bytes that drive predicted traffic applications and locations, uncovering potential vulnerabilities and privacy concerns. In network cartography, Traffic-Explainer identifies submarine cables that drive the mapping of traceroute to physical path, enabling a traceroute-informed risk analysis."
2509.1804,"The ability to centrally control network infrastructure using a programmable middleware has made Software-Defined Networking (SDN) ideal for emerging applications, such as immersive environments. However, such flexibility introduces new vulnerabilities, such as switch misreporting led load imbalance, which in turn make such immersive environment vulnerable to severe quality degradation. In this paper, we present a hybrid machine learning (ML)-based network anomaly detection framework that identifies such stealthy misreporting by capturing temporal inconsistencies in switch-reported loads, and thereby counter potentially catastrophic quality degradation of hosted immersive application. The detection system combines unsupervised anomaly scoring with supervised classification to robustly distinguish malicious behavior. Data collected from a realistic testbed deployment under both benign and adversarial conditions is used to train and evaluate the model. Experimental results show that the framework achieves high recall in detecting misreporting behavior, making it effective for early and reliable detection in SDN environments."
2509.18411,"This article describes the implementation of a technological solution aimed at improving the recording of physiological signals in the elderly population residing in geriatric facilities. The developed system consists of a smart device equipped with sensors for body temperature, heart rate, and blood oxygen levels. This device establishes an Internet connection to transmit data to a cloud-based platform for storage. Within this platform, a dashboard has been created to visualize real-time values captured by the sensors, along with additional functionalities such as user management and the configuration of personalized alerts, which are transmitted to the solution's users through the instant messaging system called Telegram."
2509.18443,"The disaggregated, cloud-native design of the 5G Core (5GC) enables flexibility and scalability but introduces significant challenges. Control-plane procedures involve complex interactions across multiple Virtual Network Functions (VNFs), while the user plane must sustain diverse and resource-intensive traffic. Existing tools often benchmark these dimensions in isolation, rely on synthetic workloads, or lack visibility into fine-grained resource usage. This paper presents 5GC-Bench, a modular framework for stress-testing the 5GC under realistic workloads. 5GC-Bench jointly emulates signaling and service traffic, supporting both VNF profiling and end-to-end service-chain analysis. By characterizing bottlenecks and resource demands, it provides actionable insights for capacity planning and performance optimization. We integrated 5GC-Bench with the OpenAirInterface (OAI) 5GC and deployed it on a real 5G testbed, demonstrating its ability to uncover resource constraints and expose cross-VNF dependencies under scenarios that mirror operational 5G deployments. To foster reproducibility and further research, we release publicly all the artifacts."
2509.18465,"We consider a spectrum sharing problem where two users attempt to communicate over N channels. The Primary User (PU) has prioritized transmissions and its occupancy on each channel over time can be modeled as a Markov chain. The Secondary User (SU) needs to determine which channels are free at each time-slot and attempt opportunistic transmissions. The goal of the SU is to maximize its own throughput, while simultaneously minimizing collisions with the PU, and satisfying spectrum access constraints. To solve this problem, we first decouple the multiple-channel problem into N single-channel problems. For each decoupled problem, we prove that there exists an optimal threshold policy that depends on the last observed PU occupancy and the freshness of this occupancy information. Second, we establish the indexability of the decoupled problems by analyzing the structure of the optimal threshold policy. Using this structure, we derive a Whittle index-based scheduling policy that allocates SU transmissions using the Age of Information (AoI) of accessed channels. We also extend our insights to PU occupancy models that are correlated across channels and incorporate learning of unknown Markov transition matrices into our policies. Finally, we provide detailed numerical simulations that demonstrate the performance gains of our approach."
2509.18519,"We present Whack-a-Mole, a deterministic packet spraying algorithm for distributing packets across multiple network paths with provably tight discrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML training and inference workloads, where collective completion time (CCT) and effective training time ratio (ETTR) are highly sensitive to tail latency and transport imbalance. Whack-a-Mole represents the path profile as a discrete allocation of $m$ selection units across $n$ paths and uses a bit-reversal counter to choose a path for each packet. We prove that the discrepancy between expected and actual packet counts per path is bounded by $O(\log m)$ over any contiguous packet sequence. The algorithm responds quickly to congestion feedback by reducing allocations to degraded paths and redistributing load to healthier ones. This combination of deterministic distribution, low per-packet overhead, and compatibility with erasure-coded transport makes Whack-a-Mole an effective building block for multipath transport protocols that aim to minimize CCT and maximize GPU utilization."
2509.18545,"Cellular networks are increasingly realized through software-based entities, with core functions deployed as Virtual Network Functions (VNFs) on Commercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key enabler of 5G by providing logically isolated Quality of Service (QoS) guarantees for diverse applications. With the adoption of cloud-native infrastructures, the placement of network slices across heterogeneous multi-cloud environments poses new challenges due to variable resource capabilities and slice-specific requirements. This paper introduces a modular framework for autonomous and near-optimal VNF placement based on a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework incorporates real traffic profiles to estimate slice resource demands and employs a MARL-based scheduler to minimize deployment cost while meeting QoS constraints. Experimental evaluation on a multi-cloud testbed shows a 19x speed-up compared to combinatorial optimization, with deployment costs within 7.8% of the optimal. While the method incurs up to 2.42x more QoS violations under high load, the trade-off provides significantly faster decision-making and reduced computational complexity. These results suggest that MARL-based approaches offer a scalable and cost-efficient solution for real-time network slice placement in heterogeneous infrastructures."
2509.18654,"We consider a real-time monitoring system where a source node (with energy limitations) aims to keep the information status at a destination node as fresh as possible by scheduling status update transmissions over a set of channels. The freshness of information at the destination node is measured in terms of the Age of Information (AoI) metric. In this setting, a natural tradeoff exists between the transmission cost (or equivalently, energy consumption) of the source and the achievable AoI performance at the destination. This tradeoff has been optimized in the existing literature under the assumption of having a complete knowledge of the channel statistics. In this work, we develop online learning-based algorithms with finite-time guarantees that optimize this tradeoff in the practical scenario where the channel statistics are unknown to the scheduler. In particular, when the channel statistics are known, the optimal scheduling policy is first proven to have a threshold-based structure with respect to the value of AoI (i.e., it is optimal to drop updates when the AoI value is below some threshold). This key insight was then utilized to develop the proposed learning algorithms that surprisingly achieve an order-optimal regret (i.e., $O(1)$) with respect to the time horizon length."
2509.18933,"Wireless communications are characterized by their unpredictability, posing challenges for maintaining consistent communication quality. This paper presents a comprehensive analysis of various prediction models, with a focus on achieving accurate and efficient Wi-Fi link quality forecasts using machine learning techniques. Specifically, the paper evaluates the performance of data-driven models based on the linear combination of exponential moving averages, which are designed for low-complexity implementations and are then suitable for hardware platforms with limited processing resources. Accuracy of the proposed approaches was assessed using experimental data from a real-world Wi-Fi testbed, considering both channel-dependent and channel-independent training data. Remarkably, channel-independent models, which allow for generalized training by equipment manufacturers, demonstrated competitive performance. Overall, this study provides insights into the practical deployment of machine learning-based prediction models for enhancing Wi-Fi dependability in industrial environments."
2509.19034,"In this paper, we introduce the Internet Quality Barometer (IQB), a framework aiming to redefine Internet quality beyond ``speed''. IQB (i) defines Internet quality in a user-centric way by considering popular use cases, (ii) maps network requirements to use cases through a set of weights and quality thresholds, and (iii) leverages publicly available Internet performance datasets, to calculate the IQB score, a composite metric that reflects the quality of Internet experience."
2509.19337,"Differentiable ray tracing has recently challenged the status quo in radio propagation modelling and digital twinning. Promising unprecedented speed and the ability to learn from real-world data, it offers a real alternative to conventional deep learning (DL) models. However, no experimental evaluation on production-grade networks has yet validated its assumed scalability or practical benefits. This leaves mobile network operators (MNOs) and the research community without clear guidance on its applicability. In this paper, we fill this gap by employing both differentiable ray tracing and DL models to emulate radio coverage using extensive real-world data collected from the network of a major MNO, covering 13 cities and more than 10,000 antennas. Our results show that, while differentiable ray-tracing simulators have contributed to reducing the efficiency-accuracy gap, they struggle to generalize from real-world data at a large scale, and they remain unsuitable for real-time applications. In contrast, DL models demonstrate higher accuracy and faster adaptation than differentiable ray-tracing simulators across urban, suburban, and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental results aim to provide timely insights into a fundamental open question with direct implications on the wireless ecosystem and future research."
2509.19341,"6G networks are envisioned to support on-demand AI model downloading to accommodate diverse inference requirements of end users. By proactively caching models at edge nodes, users can retrieve the requested models with low latency for on-device AI inference. However, the substantial size of contemporary AI models poses significant challenges for edge caching under limited storage capacity, as well as for the concurrent delivery of heterogeneous models over wireless channels. To address these challenges, we propose a fine-grained AI model caching and downloading system that exploits parameter reusability, stemming from the common practice of fine-tuning task-specific models from a shared pre-trained model with frozen parameters. This system selectively caches model parameter blocks (PBs) at edge nodes, eliminating redundant storage of reusable parameters across different cached models. Additionally, it incorporates coordinated multipoint (CoMP) broadcasting to simultaneously deliver reusable PBs to multiple users, thereby enhancing downlink spectrum utilization. Under this arrangement, we formulate a model downloading delay minimization problem to jointly optimize PB caching, migration (among edge nodes), and broadcasting beamforming. To tackle this intractable problem, we develop a distributed multi-agent learning framework that enables edge nodes to explicitly learn mutual influence among their actions, thereby facilitating cooperation. Furthermore, a data augmentation approach is proposed to adaptively generate synthetic training samples through a predictive model, boosting sample efficiency and accelerating policy learning. Both theoretical analysis and simulation experiments validate the superior convergence performance of the proposed learning framework."
2509.1935,"Autonomic Computing (AC) is a promising approach for developing intelligent and adaptive self-management systems at the deep network edge. In this paper, we present the problems and challenges related to the use of AC for IoT devices. Our proposed hybrid approach bridges bottom-up intelligence (TinyML and on-device learning) and top-down guidance (LLMs) to achieve a scalable and explainable approach for developing intelligent and adaptive self-management tiny systems. Moreover, we argue that TinyAC systems require self-adaptive features to handle problems that may occur during their operation. Finally, we identify gaps, discuss existing challenges and future research directions."
2509.19392,"The development of the Open RAN (O-RAN) framework helps enable network slicing through its virtualization, interoperability, and flexibility. To improve spectral efficiency and better meet users' dynamic and heterogeneous service demands, O-RAN's flexibility further presents an opportunity for resource reselling of unused physical resource blocks (PRBs) across users. In this work, we propose a novel game-based user-to-user PRB reselling model in the O-RAN setting, which models the carryover of unmet demand across time slots, along with how users' internal buffer states relate to any PRBs purchased. We formulate the interplay between the users as a strategic game, with each participant aiming to maximize their own payoffs, and we prove the existence and uniqueness of the Nash equilibrium (NE) in the game. We furthermore propose an iterative bidding mechanism that converges to this NE. Extensive simulations show that our best approach reduces data loss by 30.5% and spectrum resource wastage by 50.7% while significantly improving social welfare, compared to its absence."
2509.19398,"Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. We focus on a typical multi-server FL architecture, where the regions covered by different edge servers (ESs) may overlap. A key observation of this architecture is that clients located in the overlapping areas can access edge models from multiple ESs. Building on this insight, we propose FedOC (Federated learning with Overlapping Clients), a novel framework designed to fully exploit the potential of these overlapping clients. In FedOC, overlapping clients could serve dual roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models between neighboring ESs in real time to facilitate model sharing among different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically select their initial model for local training based on the edge model delivery time, which enables indirect data fusion among different regions of ESs. The overall FedOC workflow proceeds as follows: in every round, each client trains local model based on the earliest received edge model and transmits to the respective ESs for model aggregation. Then each ES transmits the aggregated edge model to neighboring ESs through ROC relaying. Upon receiving the relayed models, each ES performs a second aggregation and subsequently broadcasts the updated model to covered clients. The existence of ROCs enables the model of each ES to be disseminated to the other ESs in a decentralized manner, which indirectly achieves intercell model and speeding up the training process, making it well-suited for latency-sensitive edge environments. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods."
2509.19405,"Accurate outdoor positioning in cellular networks is hindered by sparse, heterogeneous measurement collections and the high cost of exhaustive site surveys. This paper introduces a lightweight, modular mobile data augmentation framework designed to enhance multi-cell fingerprinting-based positioning using operator-collected minimization of drive test (MDT) records. The proposed approach decouples spatial and radio-feature synthesis: kernel density estimation (KDE) models the empirical spatial distribution to generate geographically coherent synthetic locations, while a k-nearest-neighbor (KNN)-based block produces augmented per-cell radio fingerprints. The architecture is intentionally training-free, interpretable, and suitable for distributed or on-premise operator deployments, supporting privacy-aware workflows. We both validate each augmentation module independently and assess its end-to-end impact on fingerprinting-based positioning using a real-world MDT dataset provided by an Italian mobile network operator across diverse urban and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation consistently improves positioning performance, with the largest benefits in sparsely sampled or structurally complex regions; we also observe region-dependent saturation effects as augmentation increases. The framework offers a practical, low-complexity path to enhance operator positioning services using existing mobile data traces."
2509.19411,"The Internet Yellow Pages (IYP) aggregates information from multiple sources about Internet routing into a unified, graph-based knowledge base. However, querying it requires knowledge of the Cypher language and the exact IYP schema, thus limiting usability for non-experts. In this paper, we propose ChatIYP, a domain-specific Retrieval-Augmented Generation (RAG) system that enables users to query IYP through natural language questions. Our evaluation demonstrates solid performance on simple queries, as well as directions for improvement, and provides insights for selecting evaluation metrics that are better fit for IYP querying AI agents."
2509.19646,"As the fifth-generation (5G) mobile communication system continues its global deployment, both industry and academia have started conceptualizing the 6th generation (6G) to address the growing need for a progressively advanced and digital society. Even while 5G offers considerable advancements over LTE, it could struggle to be sufficient to meet all of the requirements, including ultra-high reliability, seamless automation, and ubiquitous coverage. In response, 6G is supposed to bring out a highly intelligent, automated, and ultra-reliable communication system that can handle a vast number of connected devices. This paper offers a comprehensive overview of 6G, beginning with its main stringent requirements while focusing on key enabling technologies such as terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO and AI-driven networking that will shape the 6G networks. Furthermore, the paper lists various 6G applications and usage scenarios that will benefit from these advancements. At the end, we outline the potential challenges that must be addressed to achieve the 6G promises."
2509.19651,"Low-altitude wireless networks (LAWNs) have become effective solutions for collecting data from low-power Internet-of-Things devices (IoTDs) in remote areas with limited communication infrastructure. However, some outdoor IoTDs deployed in such areas face both energy constraints and low-channel quality challenges, making it challenging to ensure timely data collection from these IoTDs in LAWNs. In this work, we investigate a reconfigurable intelligent surface (RIS)-assisted uncrewed aerial vehicle (UAV)-enabled data collection and wireless power transfer system in LAWN. Specifically, IoTDs first harvest energy from a low-altitude UAV, and then upload their data to the UAV by applying the time division multiple access (TDMA) protocol, supported by an RIS to improve the channel quality. To maintain satisfactory data freshness of the IoTDs and save energy for an energy-constrained UAV, we aim to minimize the age of information (AoI) and energy consumption of the UAV by jointly optimizing the RIS phase shits, UAV trajectory, charging time allocation, and binary IoTD scheduling. We propose a deep reinforcement learning (DRL)-based approach, namely the alternating optimization-improved parameterized deep Q-network (AO-IPDQN). Specifically, considering that RIS typically contains a large number of reflecting elements, we first adopt an alternating optimization (AO) method to optimize the RIS phase shifts to reduce the dimension of the action space. Then, we propose the improved parameterized deep Q-network (IPDQN) method to deal with the hybrid action space. Simulation results indicate that AO-IPDQN approach achieves excellent performance relative to multiple comparison methods across various simulation scenarios."
2509.19669,"To tap into the growing market of cloud gaming, whereby game graphics is rendered in the cloud and streamed back to the user as a video feed, network operators are creating monetizable assurance services that dynamically provision network resources. However, without accurately measuring cloud gaming user experience, they cannot assess the effectiveness of their provisioning methods. Basic measures such as bandwidth and frame rate by themselves do not suffice, and can only be interpreted in the context of the game played and the player activity within the game. This paper equips the network operator with a method to obtain a real-time measure of cloud gaming experience by analyzing network traffic, including contextual factors such as the game title and player activity stage. Our method is able to classify the game title within the first five seconds of game launch, and continuously assess the player activity stage as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud gaming servers for the region. We provide insights from hundreds of thousands of cloud game streaming sessions over a three-month period into the dependence of bandwidth consumption and experience level on the gameplay contexts."
2509.19913,"Next-generation real-time compute-intensive applications, such as extended reality, multi-user gaming, and autonomous transportation, are increasingly composed of heterogeneous AI-intensive functions with diverse resource requirements and stringent latency constraints. While recent advances have enabled very efficient algorithms for joint service placement, routing, and resource allocation for increasingly complex applications, current models fail to capture the non-linear relationship between delay and resource usage that becomes especially relevant in AI-intensive workloads. In this paper, we extend the cloud network flow optimization framework to support queuing-delay-aware orchestration of distributed AI applications over edge-cloud infrastructures. We introduce two execution models, Guaranteed-Resource (GR) and Shared-Resource (SR), that more accurately capture how computation and communication delays emerge from system-level resource constraints. These models incorporate M/M/1 and M/G/1 queue dynamics to represent dedicated and shared resource usage, respectively. The resulting optimization problem is non-convex due to the non-linear delay terms. To overcome this, we develop SPARQ, an iterative approximation algorithm that decomposes the problem into two convex sub-problems, enabling joint optimization of service placement, routing, and resource allocation under nonlinear delay constraints. Simulation results demonstrate that the SPARQ not only offers a more faithful representation of system delays, but also substantially improves resource efficiency and the overall cost-delay tradeoff compared to existing state-of-the-art methods."
2509.20068,"Secure monitoring and dynamic control in an IIoT environment are major requirements for current development goals. We believe that dynamic, secure monitoring of the IIoT environment can be achieved through integration with the Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current literature lacks implementation details for SDN-based DT and time-aware intelligent model training for short-term anomaly detection against IIoT threats. Therefore, we have proposed a novel framework for short-term anomaly detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware labeling of features, and a comprehensive evaluation of various machine learning models, we propose a novel SD-TWIN-based anomaly detection algorithm. According to the performance of a new real-time SD-TWIN deployment, the GPU- accelerated LightGBM model is particularly effective, achieving a balance of high recall and strong classification performance."
2509.20123,"Societal events shape the Internet's behavior. The death of a prominent public figure, a software launch, or a major sports match can trigger sudden demand surges that overwhelm peering points and content delivery networks. Although these events fall outside regular traffic patterns, forecasting systems still rely solely on those patterns and therefore miss these critical anomalies.Thus, we argue for socio-technical systems that supplement technical measurements with an active understanding of the underlying drivers, including how events and collective behavior shape digital demands. We propose traffic forecasting using signals from public discourse, such as headlines, forums, and social media, as early demand indicators.To validate our intuition, we present a proof-of-concept system that autonomously scrapes online discussions, infers real-world events, clusters and enriches them semantically, and correlates them with traffic measurements at a major Internet Exchange Point. This prototype predicted between 56-92% of society-driven traffic spikes after scraping a moderate amount of online discussions.We believe this approach opens new research opportunities in cross-domain forecasting, scheduling, demand anticipation, and society-informed decision making."
2509.206,"Traditional approaches to network management have been accessible only to a handful of highly-trained network operators with significant expert knowledge. This creates barriers for lay users to easily manage their networks without resorting to experts. With recent development of powerful large language models (LLMs) for language comprehension, we design a system to make network management accessible to a broader audience of non-experts by allowing users to converse with networks in natural language. To effectively leverage advancements in LLMs, we propose an agentic framework that uses an intermediate representation to streamline configuration across diverse vendor equipment, retrieves the network state from memory in real-time, and provides an interface for external feedback. We also conduct pilot studies to collect real user data of natural language utterances for network control, and present a visualization interface to facilitate dialogue-driven user interaction and enable large-scale data collection for future development. Preliminary experiments validate the effectiveness of our proposed system components with LLM integration on both synthetic and real user utterances. Through our data collection and visualization efforts, we pave the way for more effective use of LLMs and democratize network control for everyday users."
2509.20692,"The integration of satellite communication into 5G has been formalized in 3GPP Release 17 through the specification of Non-Terrestrial Networks (NTN), marking a significant step toward achieving global connectivity. However, the early-stage maturity of 5G NTN standards and the lack of commercial NTN-capable equipment hinder extensive performance validation and system prototyping. To address this gap, this paper proposes a software-defined radio (SDR) test platform with General-Purpose Processor (GPP) processing, leveraging Amarisoft's 5G NTN protocol stack software while performing custom system integration and adaptation for real satellite operation. The platform supports bidirectional communication between an SDR-based NTN gNB and UE emulator through a Geostationary Earth Orbit (GEO) satellite link, with full compliance to 3GPP NTN specifications. We provide detailed insights into the system architecture, SDR hardware-software co-design, and satellite gateway adaptations. Through field trials, we evaluate the performance metrics including downlink throughput and round-trip time. Results validate the feasibility and effectiveness of SDR-based platforms for NTN testing, and highlight their potential in bridging current implementation gaps before widespread commercial deployment."
2509.2083,"Semantic communication (SemCom) has the potential to significantly reduce communication delay in vehicle-to-everything (V2X) communications within vehicular networks (VNs). However, the deployment of vehicular SemCom networks (VN-SemComNets) faces critical trust challenges in information transmission, semantic encoding, and communication entity reliability. This paper proposes an innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we introduce a semantic camouflage transmission mechanism leveraging defensive adversarial noise for active eavesdropping defense, a robust federated encoder-decoder training framework to mitigate encoder-decoder poisoning attacks, and an audit game-based distributed vehicle trust management mechanism to deter untrustworthy vehicles. A case study validates the effectiveness of the proposed solutions. Lastly, essential future research directions are pointed out to advance this emerging field."
2509.20974,"Peer-to-peer networks, as a key enabler of modern networked and distributed systems, rely on peer-selection algorithms to optimize their scalability and performance. Peer-selection methods have been studied extensively in various aspects, including routing mechanisms and communication overhead. However, many state-of-the-art algorithms are oblivious to application-specific data traffic. This mismatch between design and demand results in underutilized connections, which inevitably leads to longer paths and increased latency. In this work, we propose a novel demand-aware peer-selection algorithm, called Binary Search in Buckets (BSB). Our demand-aware approach adheres to a local and greedy XOR-based routing mechanism, ensuring compatibility with existing protocols and mechanisms. We evaluate our solution against two prior algorithms by conducting simulations on real-world and synthetic communication network traces. The results of our evaluations show that BSB can offer up to a 43% improvement compared to two selected algorithms from the literature."
2509.21026,"The transition to Sixth Generation (6G) networks presents challenges in managing quality of service (QoS) of diverse applications and achieving Service Level Agreements (SLAs) under varying network conditions. Hence, network management must be automated with the help of Machine Learning (ML) and Artificial Intelligence (AI) to achieve real-time requirements. Zero touch network (ZTN) is one of the frameworks to automate network management with mechanisms such as closed loop control to ensure that the goals are met perpetually. Intent- Based Networking (IBN) specifies the user intents with diverse network requirements or goals which are then translated into specific network configurations and actions. This paper presents a novel architecture for integrating IBN and ZTN to serve the intent goals. Users provides the intent in the form of natural language, e.g., English, which is then translated using natural language processing (NLP) techniques (e.g., retrieval augmented generation (RAG)) into Network Intent LanguagE (Nile). The Nile intent is then passed on to the BiLSTM and Q-learning based ZTN closed loop framework as a goal which maintains the intent under varying network conditions. Thus, the proposed architecture can work autonomously to ensure the network performance goal is met by just specifying the user intent in English. The integrated architecture is also implemented on a testbed using OpenAirInterface (OAI). Additionally, to evaluate the architecture, an optimization problem is formulated which evaluated with Monte Carlo simulations. Results demonstrate how ZTN can help achieve the bandwidth goals autonomously set by user intent. The simulation and the testbed results are compared and they show similar trend. Mean Opinion Score (MOS) for Quality of Experience (QoE) is also measured to indicate the user satisfaction of the intent."
2509.21074,"Reproducing networking research is a critical but challenging task due to the scarcity of open-source code. While Large Language Models (LLMs) can automate code generation, current approaches lack the generalizability required for the diverse networking field. To address this, we propose RePro, a semi-automated reproduction framework that leverages advanced prompt engineering to reproduce network systems from their research papers. RePro combines few-shot in-context learning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques to systematically translate a paper's description into an optimized, executable implementation. The framework operates through a three-stage pipeline: system description extraction, structural code generation, and code optimization. Our evaluation with five state-of-the-art LLMs across diverse network sub-domains demonstrates that RePro significantly reduces reproduction time compared to manual efforts while achieving comparable system performance, validating its effectiveness and efficiency."
2509.21201,"The vision of 6G networks aims to enable edge inference by leveraging ubiquitously deployed artificial intelligence (AI) models, facilitating intelligent environmental perception for a wide range of applications. A critical operation in edge inference is for an edge node (EN) to aggregate multi-view sensory features extracted by distributed agents, thereby boosting perception accuracy. Over-the-air computing (AirComp) emerges as a promising technique for rapid feature aggregation by exploiting the waveform superposition property of analog-modulated signals, which is, however, incompatible with existing digital communication systems. Meanwhile, hybrid reconfigurable intelligent surface (RIS), a novel RIS architecture capable of simultaneous signal amplification and reflection, exhibits potential for enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital AirComp (HRD-AirComp) scheme, which employs vector quantization to map high-dimensional features into discrete codewords that are digitally modulated into symbols for wireless transmission. By judiciously adjusting the AirComp transceivers and hybrid RIS reflection to control signal superposition across agents, the EN can estimate the aggregated features from the received signals. To endow HRD-AirComp with a task-oriented design principle, we derive a surrogate function for inference accuracy that characterizes the impact of feature quantization and over-the-air aggregation. Based on this surrogate, we formulate an optimization problem targeting inference accuracy maximization, and develop an efficient algorithm to jointly optimize the quantization bit allocation, agent transmission coefficients, EN receiving beamforming, and hybrid RIS reflection beamforming. Experimental results demonstrate that the proposed HRD-AirComp outperforms baselines in terms of both inference accuracy and uncertainty."
2509.21259,"Real-time urban traffic surveillance is vital for Intelligent Transportation Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle trajectories, and prevent collisions in smart cities. Deploying edge cameras across urban environments is a standard practice for monitoring road conditions. However, integrating these with intelligent models requires a robust understanding of dynamic traffic scenarios and a responsive interface for user interaction. Although multimodal Large Language Models (LLMs) can interpret traffic images and generate informative responses, their deployment on edge devices is infeasible due to high computational demands. Therefore, LLM inference must occur on the cloud, necessitating visual data transmission from edge to cloud, a process hindered by limited bandwidth, leading to potential delays that compromise real-time performance. To address this challenge, we propose a semantic communication framework that significantly reduces transmission overhead. Our method involves detecting Regions of Interest (RoIs) using YOLOv11, cropping relevant image segments, and converting them into compact embedding vectors using a Vision Transformer (ViT). These embeddings are then transmitted to the cloud, where an image decoder reconstructs the cropped images. The reconstructed images are processed by a multimodal LLM to generate traffic condition descriptions. This approach achieves a 99.9% reduction in data transmission size while maintaining an LLM response accuracy of 89% for reconstructed cropped images, compared to 93% accuracy with original cropped images. Our results demonstrate the efficiency and practicality of ViT and LLM-assisted edge-cloud semantic communication for real-time traffic surveillance."
2509.2149,"Bluetooth-based mesh networks offer a promising infrastructure for offline communication in emergency and resource constrained scenarios. However, traditional routing strategies such as Ad hoc On-Demand Distance Vector (AODV) often degrade under congestion and dynamic topological changes. This study proposes a hybrid intelligent routing framework that augments AODV with supervised machine learning to improve next-hop selection under varied network constraints. The framework integrates four predictive models: a delivery success classifier, a TTL regressor, a delay regressor, and a forwarder suitability classifier, into a unified scoring mechanism that dynamically ranks neighbors during multi-hop message transmission. A simulation environment with stationary node deployments was developed, incorporating buffer constraints and device heterogeneity to evaluate three strategies: baseline AODV, a partial hybrid ML model (ABC), and the full hybrid ML model (ABCD). Across ten scenarios, the Hybrid ABCD model achieves approximately 99.97 percent packet delivery under these controlled conditions, significantly outperforming both the baseline and intermediate approaches. The results demonstrate that lightweight, explainable machine learning models can enhance routing reliability and adaptability in Bluetooth mesh networks, particularly in infrastructure-less environments where delivery success is prioritized over latency constraints."
2509.2155,"Transport protocols are fundamental to network communications, continuously evolving to meet the demands of new applications, workloads, and network architectures while running in a wide range of execution environments (a.k.a targets). We argue that this diversity across protocols and targets calls for a high-level, target-agnostic programming abstraction for the transport layer. Specifically, we propose to specify transport protocols as high-level programs that take an event and flow state as input, and using constrained C-like constructs, produce the updated state along with target-agnostic instructions for key transport operations such as data reassembly, packet generation and scheduling, and timer manipulations.We show the benefits of our high-level transport programs by developing multiple transport protocols in our programming framework called TINF, developing two TINF- compliant backends, one in DPDK and one in Linux eXpress DataPath, and deploying TINF programs for multiple protocols across both backends. Inspired by the benefits unlocked by L2/L3 packet-processing languages like P4, we believe target-agnostic transport programs can reduce the development effort for transport protocols, enable automated analysis and formal verification of the transport layer, and further research in programmable targets for transport protocols."
2509.21649,"Reinforcement Learning (RL) agents have been widely used to improve networking tasks. However, understanding the decisions made by these agents is essential for their broader adoption in networking and network management. To address this, we introduce eXplaNet - a pipeline grounded in explainable artificial intelligence - designed to help networking researchers and practitioners gain deeper insights into the decision-making processes of RL-based solutions. We demonstrate how eXplaNet can be applied to refine a routing solution powered by a Q-learning agent, specifically by improving its reward function. In addition, we discuss the opportunities and challenges of incorporating explainability into RL to better optimize network performance."
2509.21656,"With the advent of programmable network hardware, more and morefunctionality can be moved from software running on general purpose CPUs tothe NIC. Early NICs only allowed offloading fixed functions like checksumcomputation. Recent NICs like the Nvidia Bluefield-3 allow a fullyprogrammable dataplane. In this paper, we present our first steps towards aload balancer named XenoFlow running on the Bluefield-3. Furthermore, weshow the capabilities and limitations of the Bluefield-3 eSwitch. Ourresults show that the Bluefield-3 will not achieve line rate with only 2entries in a Flow Pipe. However, we also show the adventages of hardwareoffloading on the NIC and being closer to the network. With XenoFlow, weachieve an 44% lower latency compared to a comparable eBPF-based loadbalancer running on the host. Furthermore, XenoFlow achieves this lowlatency even under high load."
2509.21949,"Large Language Models (LLMs) have shown remarkable capabilities across various fields. However, their performance in technical domains such as telecommunications remains underexplored. This paper evaluates two open-source LLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions derived from advanced wireless communications material. We construct a benchmark of 105 question-answer pairs and assess performance using lexical metrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze consistency, judgment reliability, and hallucination through source attribution and score variance. Results show that Gemma excels in semantic fidelity and LLM-rated correctness, while DeepSeek demonstrates slightly higher lexical consistency. Additional findings highlight current limitations in telecom applications and the need for domain-adapted models to support trustworthy Artificial Intelligence (AI) assistants in engineering."
2509.22547,"Efficient handover (HO) strategies are essential for maintaining the stringent performance requirements of ultra-reliable communication (URC) systems. This work introduces a novel HO framework designed from a physical-layer perspective, where the decision-making process focuses on determining the optimal time and location for performing HOs. Leveraging extreme value theory (EVT) and statistical radio maps, the proposed method predicts signal behaviour and enables efficient resource allocation. The framework ensures seamless HOs and improved system performance by facilitating effective resource transitions and coordination across spatial locations while incorporating mechanisms to mitigate the ping-pong effect. Comparative evaluations demonstrate that this strategy provides superior service availability and energy efficiency than traditional HO mechanisms, highlighting its effectiveness in URC environments."
2509.22568,"During large-scale crises disrupting cellular and Internet infrastructure, civilians lack reliable methods for communication, aid coordination, and access to trustworthy information. This paper presents a unified emergency communication system integrating a low-power, long-range network with a crisis-oriented smartphone application, enabling decentralized and off-grid civilian communication. Unlike previous solutions separating physical layer resilience from user layer usability, our design merges these aspects into a cohesive crisis-tailored framework.The system is evaluated in two dimensions: communication performance and application functionality. Field experiments in urban Zrich demonstrate that the 868 MHz band, using the LongFast configuration, achieves a communication range of up to 1.2 km with 92% Packet Delivery Ratio, validating network robustness under real-world infrastructure degraded conditions. In parallel, a purpose-built mobile application featuring peer-to-peer messaging, identity verification, and community moderation was evaluated through a requirements-based analysis."
2509.22834,"Intent-Based Networking (IBN) aims to simplify network management by enabling users to specify high-level goals that drive automated network design and configuration. However, translating informal natural-language intents into formally correct optical network topologies remains challenging due to inherent ambiguity and lack of rigor in Large Language Models (LLMs). To address this, we propose a novel hybrid pipeline that integrates LLM-based intent parsing, formal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching design decisions with domain-specific optical standards and systematically incorporating symbolic reasoning and verification techniques, our pipeline generates explainable, verifiable, and trustworthy optical network designs. This approach significantly advances IBN by ensuring reliability and correctness, essential for mission-critical networking tasks."
2509.23125,"In WSN/IoT, node localization is essential to long-running applications for accurate environment monitoring and event detection, often covering a large area in the field. Due to the lower time resolution of typical WSN/IoT platforms (e.g., 1 microsecond on ESP32 platforms) and the jitters in timestamping, packet-level localization techniques cannot provide meter-level resolution. For high-precision localization as well as world-wide interoperability via 2.4-GHz ISM band, a new variant of LoRa, called LoRa 2.4 GHz, was proposed by semtech, which provides a radio frequency (RF) time of flight (ToF) ranging method for meter-level localization. However, the existing datasets reported in the literature are limited in their coverages and do not take into account varying environmental factors such as temperature and humidity. To address these issues, LoRa 2.4 GHz RF ToF ranging data was collected on a sports field at the XJTLU south campus, where three LoRa nodes logged samples of ranging with a LoRa base station, together with temperature and humidity, at reference points arranged as a 3x3 grid covering 400 square meter over three weeks and uploaded all measurement records to the base station equipped with an ESP32-based transceiver for machine and user communications. The results of a preliminary investigation based on a simple deep neural network (DNN) model demonstrate that the environmental factors, including the temperature and humidity, significantly affect the accuracy of ranging, which calls for advanced methods of compensating for the effects of environmental factors on LoRa RF ToF ranging outdoors."
2509.23216,"Based on the License-Assisted Access (LAA) small cell architecture, the LAA coexisting with Wi-Fi heterogeneous networks provides LTE mobile users with high bandwidth efficiency as the unlicensed channels are shared among LAA and Wi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use the same unlicensed channel in heterogeneous networks. In such a network, unlicensed band allocation for LAA and Wi-Fi is an important issue that may affect the quality of service (QoS) of both systems significantly. In this paper, we propose an analytical model and conduct simulation experiments to study four allocations for the unlicensed band: unlicensed full allocation (UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering mechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance of these unlicensed band allocation schemes in terms of the acceptance rate of both LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides guidelines for designing the channel occupation phase and the buffer size of the LAA small cell."
2509.23217,"In this letter, we propose an analytical model and conduct simulation experiments to study listen-before-talk-based unlicensed band allocation with the buffering mechanism for the License-Assisted Access (LAA) packets in the heterogeneous networks. In such a network, unlicensed band allocation for LAA and Wi-Fi is an important issue, which may affect the quality of service for both systems significantly. We evaluate the performance of these unlicensed band allocations in terms of the acceptance rate of both LAA and Wi-Fi packets. This letter provides the guidelines for designing the channel occupation phase and buffer threshold of the LAA systems."
2509.23218,"In this paper, a novel analytical model for resource allocation is proposed for a device-to-device (D2D) assisted cellular network. The proposed model can be applied to underlay and overlay D2D systems for sharing licensed bands and offloading cellular traffic. The developed model also takes into account the problem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a global system state reflects the interaction among D2D, conventional cellular, and Wi-Fi packets. Under the standard traffic model assumptions, a threshold-based flow control is proposed for guaranteeing the quality-of-service (QoS) of Wi-Fi. The packet blockage probability is then derived. Simulation results show the proposed scheme sacrifices conventional cellular performance slightly to improve overlay D2D performance significantly while maintaining the performance for Wi-Fi users. Meanwhile, the proposed scheme has more flexible adjustments between D2D and Wi-Fi than the underlay scheme."
2509.23389,"The convergence of Information Technology (IT) and Operational Technology (OT) is a critical enabler for achieving autonomous and intelligent industrial systems. However, the increasing complexity, heterogeneity, and real-time demands of industrial environments render traditional rule-based or static management approaches insufficient. In this paper, we present a modular framework based on the Knowledge-Defined Networking (KDN) paradigm, enabling adaptive and autonomous control across IT-OT infrastructures. The proposed architecture is composed of four core modules: Telemetry Collector, Knowledge Builder, Decision Engine, and Control Enforcer. These modules operate in a closed control loop to continuously observe system behavior, extract contextual knowledge, evaluate control actions, and apply policy decisions across programmable industrial endpoints. A graph-based abstraction is used to represent system state, and a utility-optimization mechanism guides control decisions under dynamic conditions. The framework's performance is evaluated using three key metrics: decision latency, control effectiveness, and system stability, demonstrating its capability to enhance resilience, responsiveness, and operational efficiency in smart industrial networks."
2509.23398,"The increasing complexity, dynamism, and heterogeneity of 6G networks demand management systems that can reason proactively and generalize beyond pre-defined cases. In this paper, we propose a modular, knowledge-defined architecture that integrates Digital Twin models with semantic reasoning and zero-shot learning to enable autonomous decision-making for previously unseen network scenarios. Real-time data streams are used to maintain synchronized virtual replicas of the physical network, which also forecast short-term state transitions. These predictions feed into a knowledge plane that constructs and updates a graph-based abstraction of the network, enabling context-aware intent generation via graph neural reasoning. To ensure adaptability without retraining, the management plane performs zero-shot policy matching by semantically embedding candidate intents and selecting suitable pre-learned actions. The selected decisions are translated and enforced through the control plane, while a closed-loop feedback mechanism continuously refines predictions, knowledge, and policies over time. Simulation results confirm that the proposed framework observes notable improvements in policy response time, SLA compliance rate, and intent matching accuracy."
2509.23401,"This study presents a simulation model for underwater 6G networks, focusing on the optimized placement of sensors, AUVs, and hubs. The network architecture consists of fixed hub stations, mobile autonomous underwater vehicles (AUVs), and numerous sensor nodes. Environmental parameters such as temperature, salinity, and conductivity are considered in the transmission of electromagnetic signals; signal attenuation and transmission delays are calculated based on physical models. The optimization process begins with K-Means clustering, followed by sequential application of Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) to refine the cluster configurations. The simulation includes key network dynamics such as multi-hop data transmission, cluster leader selection, queue management, and traffic load balancing. To compare performance, two distinct scenarios -- one with cluster leaders and one without -- are modeled and visualized through a PyQt5-based real-time graphical interface. The results demonstrate that 6G network architectures in underwater environments can be effectively modeled and optimized by incorporating environmental conditions."
2509.23522,"Network traffic classification (NTC) is vital for efficient network management, security, and performance optimization, particularly with 5G/6G technologies. Traditional methods, such as deep packet inspection (DPI) and port-based identification, struggle with the rise of encrypted traffic and dynamic port allocations. Supervised learning methods provide viable alternatives but rely on large labeled datasets, which are difficult to acquire given the diversity and volume of network traffic. Meanwhile, unsupervised learning methods, while less reliant on labeled data, often exhibit lower accuracy. To address these limitations, we propose a novel framework that first leverages Self-Supervised Learning (SSL) with techniques such as autoencoders or Tabular Contrastive Learning (TabCL) to generate pseudo-labels from extensive unlabeled datasets, addressing the challenge of limited labeled data. We then apply traffic-adopted Confident Learning (CL) to refine these pseudo-labels, enhancing classification precision by mitigating the impact of noise. Our proposed framework offers a generalizable solution that minimizes the need for extensive labeled data while delivering high accuracy. Extensive simulations and evaluations, conducted using three datasets (ISCX VPN-nonVPN, self-generated dataset, and UCDavis--QUIC), and demonstrate that our method achieves superior accuracy compared to state-of-the-art techniques in classifying network traffic."
2509.23528,"Following state-of-the-art research results, which showed the potential for significant performance gains by applying AI/ML techniques in the cellular Radio Access Network (RAN), the wireless industry is now broadly pushing for the adoption of AI in 5G and future 6G technology. Despite this enthusiasm, AI-based wireless systems still remain largely untested in the field. Common simulation methods for generating datasets for AI model training suffer from ""reality gap"" and, as a result, the performance of these simulation-trained models may not carry over to practical cellular systems. Additionally, the cost and complexity of developing high-performance proof-of-concept implementations present major hurdles for evaluating AI wireless systems in the field. In this work, we introduce a methodology which aims to address the challenges of bringing AI to real networks. We discuss how detailed Digital Twin simulations may be employed for training site-specific AI Physical (PHY) layer functions. We further present a powerful testbed for AI-RAN research and demonstrate how it enables rapid prototyping, field testing and data collection. Finally, we evaluate an AI channel estimation algorithm over-the-air with a commercial UE, demonstrating that real-world throughput gains of up to 40% are achievable by incorporating AI in the physical layer."
2509.23794,"Unmanned Aerial Vehicles (UAVs), commonly known as drones, have experienced expanding use in urban environments in recent years. However, the growing density of drones raises significant challenges, such as avoiding collisions and managing air traffic efficiently, especially in congested areas. To address these issues, a structured road system and an effective guidance algorithm are essential. In this paper, we introduce a markup language allowing to describe drone road systems (DRS), in which a road system is given by a set of individual roads, each of which can have a varying number of lanes. Roads can be linked through connecting lanes. Furthermore, we propose a novel short-term decentralized greedy (STDG) guidance algorithm that uses only the position and speed information of nearby drones -- communicated via periodically transmitted beacons -- to make real-time decisions such as stopping, changing lanes, or adjusting speed for the next few seconds. Unlike existing methods that rely on centralized coordination, our algorithm enables drones to operate independently while ensuring safety and efficiency. We present simulation results showing the impact of key wireless and algorithm parameters on performance metrics like the drone collision rate, average speed and throughput of the drone road system."
2509.2381,"The rapid development of the Low-Altitude Economy (LAE) has created opportunities for emerging services such as autonomous aerial transportation, aerial sensing, and emergency response, all of which rely on efficient and intelligent communications. However, LAE intelligent communications face several challenges, including the limited computational capacity of aerial nodes, the lack of cross-scenario generalization, and the complexity of heterogeneous demands. Meanwhile, Computing Power Networks (CPNs) have emerged as a new paradigm for integrating distributed computing, networking, and storage resources, but they are also constrained by static deployment and limited adaptability. In this survey, we explore the synergy between LAE intelligent communications and CPNs. We first analyze how CPNs can support LAE intelligent communications in areas such as air-ground collaborative control, AI training, communication-computation co-ptimization, and ubiquitous low-altitude information processing. Conversely, we discuss how LAE intelligent communications can enhance CPNs through mobility-assisted control, distributed intelligent training, dynamic routing, and in-network aerial computing. Finally, based on these insights, we outline design principles and future research directions for integrated CPN-LAE systems. This work provides a comprehensive foundation for building flexible, adaptive, and resilient architectures that leverage the synergy between CPNs and LAE to deliver high-quality and sustainable low-altitude services."
2509.23913,"Deep reinforcement learning (DRL) has been successfully used to design forwarding strategies for multi-hop mobile wireless networks. While such strategies can be used directly for networks with varied connectivity and dynamic conditions, developing generalizable approaches that are effective on scenarios significantly different from the training environment remains largely unexplored. In this paper, we propose a framework to address the challenge of generalizability by (i) developing a generalizable base model considering diverse mobile network scenarios, and (ii) using the generalizable base model for new scenarios, and when needed, fine-tuning the base model using a small amount of data from the new scenarios. To support this framework, we first design new features to characterize network variation and feature quality, thereby improving the information used in DRL-based forwarding decisions. We then develop a continual learning (CL) approach able to train DRL models across diverse network scenarios without ``catastrophic forgetting.'' Using extensive evaluation, including real-world scenarios in two cities, we show that our approach is generalizable to unseen mobility scenarios. Compared to a state-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction in delay, 24% improvement in delivery rate, and comparable or slightly higher number of forwards."
2509.23921,"We conduct a comprehensive evaluation of the performance of the uplink of OFDMA-based MU-MIMO systems with multi-antenna users, for three Zero-Forcing (ZF) Beamforming (BF) strategies: Coordinated-Transmit-Receive-1 (CTR1), where only the strongest data stream is enabled per scheduled user; Block Diagonalization (BD), where all possible streams are enabled per scheduled user; Coordinated-Transmit-Receive-Flexible (CTRF), which allows a flexible stream allocation per user. The Radio Resource Management (RRM) of the uplink of all OFDMA-based systems must be done over an entire Time-Slot (TS) due to power management, making it challenging. To enable this study, we propose an efficient heuristic based on greedy-up searches for stream-sets that provides feasible solutions. It operates over the TS and considers fairness, practical Modulation and Coding Schemes and all RRM processes. The results show that, for Rural Macro scenarios, BD (resp. CTR1) could replace the more complex CTRF if the number of users is small (resp. large), while for Urban Macro scenarios, CTR1 emerges as an alternative to CTRF due to its similar performance. We also show that the system parameters can substantially impact the performance of the ZF strategies and that BD performance is more impaired with a simpler power management scheme than CTR1 and CTRF."
2509.24038,"Resilience in optical networks has traditionally relied on redundancy and pre-planned recovery strategies, both of which assume a certain level of disaster predictability. However, recent environmental changes such as climate shifts, the evolution of communication services, and rising geopolitical risks have increased the unpredictability of disasters, reducing the effectiveness of conventional resilience approaches. To address this unpredictability, this paper introduces the concept of agile resilience, which emphasizes dynamic adaptability across multiple operators and layers. We identify key requirements and challenges, and present enabling technologies for the realization of agile resilience. Using a field-deployed transmission system, we demonstrate rapid system characterization, optical path provisioning, and database migration within six hours. These results validate the effectiveness of the proposed enabling technologies and confirm the feasibility of agile resilience."
2509.24417,"The collaborative research project 6G-ANNA develops concepts for the 6G radio access network (RAN) architecture and technology components. Previous RAN generations have become inherently more complex and reach their limits in handling foreseen future traffic demands with their diverse characteristics in an efficient manner, e.g., for the use-case of mobile eXtended Reality (XR) on a massive scale. One main objective of 6G is to regain both operational and energy efficiency, i.e., by simplification and automation. To achieve this, in this paper a flexible 6G RAN functional architecture and protocol stack as well as implementation and deployment options are described. We outline how performance is optimized by distributed Multiple Input Multiple Output (MIMO) and distributed Carrier Aggregation (CA), and furthermore, how adaptiveness and scalability is enabled by Cloud RAN and service orchestration. Finally, the proposed zero-trust framework mitigates security risks in the described 6G RAN architecture."
2509.24446,"Internet service providers monitor their networks to detect, triage, and remediate service impairments. When an incident is detected, it is important to determine whether similar incidents have occurred in the past or are happening concurrently elsewhere in the network. Manual correlation of such incidents is infeasible due to the scale of the networks under observation, making automated correlation a necessity. This paper presents a self-supervised learning method for similarity-based correlation of network situations. Using this method, a deep neural network is trained on a large unlabeled dataset of network situations using contrastive learning. High precision achieved in experiments on real-world network monitoring data suggests that contrastive learning is a promising approach to network incident correlation."
2509.24846,"Edge computing brings computation near end users, enabling the provisioning of novel use cases. To satisfy end-user requirements, the concept of edge federation has recently emerged as a key mechanism for dynamic resources and services sharing across edge systems managed by different administrative domains. However, existing federation solutions often rely on pre-established agreements and face significant limitations, including operational complexity, delays caused by manual operations, high overhead costs, and dependence on trusted third parties. In this context, blockchain can create dynamic federation agreements that enable service providers to securely interact and share services without prior trust.This article first describes the problem of edge federation, using the standardized ETSI multi-access edge computing framework as a reference architecture, and how it is being addressed. Then, it proposes a novel solution using blockchain and smart contracts to enable distributed MEC systems to dynamically negotiate and execute federation in a secure, automated, and scalable manner. We validate our framework's feasibility through a performance evaluation using a private Ethereum blockchain, built on the open-source Hyperledger Besu platform. The testbed includes a large number of MEC systems and compares two blockchain consensus algorithms. Experimental results demonstrate that our solution automates the entire federation lifecycle-from negotiation to deployment-with a quantifiable overhead, achieving federation in approximately 18 seconds in a baseline scenario. The framework scales efficiently in concurrent request scenarios, where multiple MEC systems initiate federation requests simultaneously. This approach provides a promising direction for addressing the complexities of dynamic, multi-domain federations across the edge-to-cloud continuum."
2509.24944,"An experimental study on magnetic near-field (NF) scanning of printed circuit board (PCB) emission radiation is developed in this paper. The design and installation of the electromagnetic (EM) NF scanner is introduced. The test bed of magnetic NF emission in the microwave frequency range is described. The methodology of the microstrip magnetic NF probe is discussed. The probe calibration process was performed following the IEC 61967-1 NF scanning standard. The NF scanner functioning is tested with passive microstrip circuit square loop probe and device under test (DUT) PCB radiation in the test plan positioned at 1-mm above the ground plane. Based on the standard test with I-shape 50-$\Omega$ transmission line (TL), the calibration process of radiated magnetic field was validated by comparison between HFSS__ simulation and experimentation in very wideband frequency from 0.1-GHz to 3-GHz. Then, a nonstandard TL based DUT was experimented. Accordingly, the cartographies of scanned magnetic NF at two different test frequencies, 2 GHz and 3 GHz, are discussed. The NF scanner is under development for targeting the EMC radiated emission of PCB dedicated to operate in 6G wireless communication."
2509.25683,"Incentive-driven computing resource sharing is crucial for meeting the ever-growing demands of emerging mobile applications. Although conventional spot trading offers a solution, it frequently leads to excessive overhead due to the need for real-time trading related interactions. Likewise, traditional futures trading, which depends on historical data, is susceptible to risks from network dynamics. This paper explores a dynamic and uncertain edge network comprising a computing platform, e.g., an edge server, that offers computing services as resource seller, and various types of mobile users with diverse resource demands as buyers, including fixed buyers (FBs) and uncertain occasional buyers (OBs) with fluctuating needs. To facilitate efficient and timely computing services, we propose an overbooking- and hybrid trading-empowered resource scheduling mechanism with reputation update, termed Oh-Trust. Particularly, our Oh-Trust incentivizes FBs to enter futures trading by signing long-term contracts with the seller, while simultaneously attracting OBs to spot trading, enhancing resource utilization and profitability for both parties. Crucially, to adapt to market fluctuations, a smart reputation updating mechanism is integrated, allowing for the timely renewal of long-term contracts to optimize trading performance. Extensive simulations using real-world datasets demonstrate the effectiveness of Oh-Trust across multiple evaluation metrics."
2509.25828,"The rapid growth of communications and networking research has created an unprecedented demand for high-quality survey and tutorial papers that can synthesize vast bodies of literature into coherent understandings and actionable insights. However, writing impactful survey papers presents multifaceted challenges that demand substantial effort beyond traditional research article composition. This article provides a systematic, practical roadmap for prospective authors in the communications research community, drawing upon extensive editorial experience from premier venues such as the IEEE Communications Surveys & Tutorials. We present structured guidelines covering seven essential aspects: strategic topic selection with novelty and importance, systematic literature collection, effective structural organization, critical review writing, tutorial content development with emphasis on case studies, comprehensive illustration design that enhances comprehension, and identification of future directions. Our goal is to enable junior researchers to craft exceptional survey and tutorial articles that enhance understanding and accelerate innovation within the communications and networking research ecosystem."
2509.25855,"IEEE 802.11be (Wi-Fi 7) introduces Multi-Link Operation (MLO) as a While MLO offers significant parallelism and capacity, realizing its full potential in guaranteeing strict delay bounds and optimizing Quality of Service (QoS) for diverse, heterogeneous traffic streams in complex multi-link scenarios remain a significant challenge. This is largely due to the limitations of static Enhanced Distributed Channel Access (EDCA) parameters and the complexity inherent in cross-link traffic management. To address this, this paper investigates the correlation between overall MLO QoS indicators and the configuration of EDCA parameters and Acess Catagory (AC) traffic allocation among links. Based on this analysis, we formulate a constrained optimization problem aiming to minimize the sum of overall packet loss rates for all access categories while satisfying their respective overall delay violation probability constraints. A Genetic Algorithm (GA)-based MLO EDCA QoS optimization algorithm is designed to efficiently search the complex configuration space of AC assignments and EDCA parameters. Experimental results demonstrate that the proposed approach's efficacy in generating adaptive MLO configuration strategies that align with diverse service requirements. The proposed solution significantly improves delay distribution characteristics, and enhance QoS robustness and resource utilization efficiency in high-load MLO environments."
2509.25905,"Future 6G networks are envisioned to facilitate edge-assisted mobile augmented reality (MAR) via strengthening the collaboration between MAR devices and edge servers. In order to provide immersive user experiences, MAR devices must timely upload camera frames to an edge server for simultaneous localization and mapping (SLAM)-based device pose tracking. In this paper, to cope with user-specific and non-stationary uplink data traffic, we develop a digital twin (DT)-based approach for user-centric communication service provision for MAR. Specifically, to establish DTs for individual MAR devices, we first construct a data model customized for MAR that captures the intricate impact of the SLAM-based frame uploading mechanism on the user-specific data traffic pattern. We then define two DT operation functions that cooperatively enable adaptive switching between different data-driven models for capturing non-stationary data traffic. Leveraging the user-oriented data management introduced by DTs, we propose an algorithm for network resource management that ensures the timeliness of frame uploading and the robustness against inherent inaccuracies in data traffic modeling for individual MAR devices. Trace-driven simulation results demonstrate that the user-centric communication service provision achieves a 14.2% increase in meeting the camera frame uploading delay requirement in comparison with the slicing-based communication service provision widely used for 5G."
2509.25974,"OpenID Connect for Agents (OIDC-A) 1.0 is an extension to OpenID Connect Core 1.0 that provides a comprehensive framework for representing, authenticating, and authorizing LLM-based agents within the OAuth 2.0 ecosystem. As autonomous AI agents become increasingly prevalent in digital systems, there is a critical need for standardized protocols to establish agent identity, verify agent attestation, represent delegation chains, and enable fine-grained authorization based on agent attributes. This specification defines standard claims, endpoints, and protocols that address these requirements while maintaining compatibility with existing OAuth 2.0 and OpenID Connect infrastructure. The proposed framework introduces mechanisms for agent identity representation, delegation chain validation, attestation verification, and capability-based authorization, providing a foundation for secure and trustworthy agent-to-service interactions in modern distributed systems."
2509.26013,"The integration of satellite networks into next-generation mobile communication systems has gained considerable momentum with the advent of 5G Non-Terrestrial Networks (5G-NTN). Since established technologies like DVB-S2/RCS2 are already widely used for satellite broadband, a detailed comparison with emerging 5G NTN solutions is necessary to understand their relative merits and guide deployment decisions. This paper presents a user-centric, end-to-end evaluation of these technologies under realistic traffic conditions, showing how differences in architecture and protocols impact application-layer performance. Utilizing the 6G Sandbox platform, we employ OpenAirInterface to emulate 5G NTN and OpenSAND for DVB-S2/RCS2, replicating transparent payload GEO satellite scenarios under uniform downlink conditions. A range of real-world applications, such as web browsing, file downloads, and video streaming, are tested across both systems and systematically analyzed. While the emulation lacks real-time capability, it reveals key strengths and limitations of each approach, helping identify suitable deployment scenarios for 5G NTN and DVB-S2/RCS2."
2509.26075,"6G networks are expected to revolutionize connectivity, offering significant improvements in speed, capacity, and smart automation. However, existing network designs will struggle to handle the demands of 6G, which include much faster speeds, a huge increase in connected devices, lower energy consumption, extremely quick response times, and better mobile broadband. To solve this problem, incorporating the artificial intelligence (AI) technologies has been proposed. This idea led to the concept of Knowledge-Defined Networking (KDN). KDN promises many improvements, such as resource management, routing, scheduling, clustering, and mobility prediction. The main goal of this study is to optimize resource management using Reinforcement Learning."
2509.26086,"Six-dimensional movable antenna (6DMA) has emerged as a promising new technology for future wireless networks, which can adaptively adjust the three-dimensional (3D) positions and 3D rotations of antennas/antenna arrays for performance enhancement. This paper proposes a novel cost-effective 6DMA-based base station (BS) architecture, termed the \textit{flexible-sector} BS, which allows the deployed antennas to flexibly rotate and move along a circular track, thus enabling common sector rotation and flexible antenna allocation across sectors to adapt to the spatial user distribution efficiently. In particular, we focus on the uplink transmission in a single-cell system, where the flexible-sector BS receives independent messages from multiple users. We introduce an angular-domain user distribution model, which captures the users' spatial clustering or hot-spot distribution effectively. Assuming the zero-forcing (ZF) based receiver applied at the BS to decode multiuser signals, we derive the average sum rate achievable for the users as a function of the common rotation of sectors and the antenna allocation over them. Moreover, we develop a two-step algorithm to jointly optimize the common sector rotation and antenna allocation to maximize the average sum rate of all users. It is shown that the optimal antenna number in each sector linearly increases with the number of users in it. It is also revealed that under the most favorable user distribution, the achievable sum rate gain increases in the order of $\log_{2}(B)$ in the regime of asymptotically large number of antennas, where $B$ denotes the number of sectors. Numerically results also show that as $B$ increases, the proposed flexible-sector BS achieves higher sum rate, and it outperforms other benchmark schemes, such as the traditional fixed-sector BS as well as the BS with sector rotation or antenna allocation optimization only."
2509.262,"This paper introduces a novel framework for proactive cross-domain resource orchestration in 6G RAN-Edge networks, featuring large language model (LLM)-augmented agents. The system comprises specialized RAN (energy efficiency) and Edge (latency assurance) agents that engage in iterative negotiation, supported by advanced reasoning and planning capabilities. Agents dynamically interact with a digital twin (DT) to test their proposals and leverage a long-term collective memory where their joint successful and failed agreements along with the related network contexts are distilled into strategies to either follow or avoid and subsequently stored. Given that agents are subject to a plethora of cognitive distortions when retrieving those past experiences -- such as primacy, recency, confirmation and availability biases -- we propose in this work a novel unbiased memory design (A reusable mockup version of the unbiased memory source code is available for non-commercial use atthis https URL). featuring (i) semantic retrieval of past strategies via Jaccard similarity; (ii) learning from failures through amplified weighting of SLA violations and mandatory inclusion of failed negotiation cases to mitigate confirmation bias; (iii) diversity enforcement to minimize availability bias and (iv) recency and primacy weighting with slow decay to counteract temporal biases. Evaluation results showcase the impact of existing biases and how the unbiased memory allows to tackle them by learning from both successful and failed strategies, either present or old, resulting in $\times 4.5$ and $\times 3.5$ reductions of unresolved negotiations compared to non-memory and vanilla memory baselines, respectively, while totally mitigating SLA violations as well as improving latency and energy saving distributions."
2509.26245,"Time Sensitive Networking (TSN) is fundamental for the reliable, low-latency networks that will enable the Industrial Internet of Things (IIoT). Wi-Fi has historically been considered unfit for TSN, as channel contention and collisions prevent deterministic transmission delays. However, this issue can be overcome by using Target Wake Time (TWT), which enables the access point to instruct Wi-Fi stations to wake up and transmit in non-overlapping TWT Service Periods (SPs), and sleep in the remaining time. In this paper, we first formulate the TWT Acceptance and Scheduling Problem (TASP), with the objective to schedule TWT SPs that maximize traffic throughput and energy efficiency while respecting Age of Information (AoI) constraints. Then, due to TASP being NP-hard, we propose the TASP Efficient Resolver (TASPER), a heuristic strategy to find near-optimal solutions efficiently. Using a TWT simulator based on ns-3, we compare TASPER to several baselines, including HSA, a state-of-the-art solution originally designed for WirelessHART networks. We demonstrate that TASPER obtains up to 24.97% lower mean transmission rejection cost and saves up to 14.86% more energy compared to the leading baseline, ShortestFirst, in a challenging, large-scale scenario. Additionally, when compared to HSA, TASPER also reduces the energy consumption by 34% and reduces the mean rejection cost by 26%. Furthermore, we validate TASPER on our IIoT testbed, which comprises 10 commercial TWT-compatible stations, observing that our solution admits more transmissions than the best baseline strategy, without violating any AoI deadline."
2509.26368,"The growing demand for real-time, safety-critical systems has significantly increased both the adoption and complexity of Time Sensitive Networking (TSN). Configuring an optimized TSN network is highly challenging, requiring careful planning, design, verification, validation, and deployment. Large Language Models (LLMs) have recently demonstrated strong capabilities in solving complex tasks, positioning them as promising candidates for automating end-to-end TSN deployment, referred to as TSN orchestration. This paper outlines the steps involved in TSN orchestration and the associated challenges. To assess the capabilities of existing LLM models, we conduct an initial proof-of-concept case study focused on TSN configuration across multiple models. Building on these insights, we propose an LLM-assisted orchestration framework. Unlike prior research on LLMs in computer networks, which has concentrated on general configuration and management, TSN-specific orchestration has not yet been investigated. We present the building blocks for automating TSN using LLMs, describe the proposed pipeline, and analyze opportunities and limitations for real-world deployment. Finally, we highlight key challenges and research directions, including the development of TSN-focused datasets, standardized benchmark suites, and the integration of external tools such as Network Calculus (NC) engines and simulators. This work provides the first roadmap toward assessing the feasibility of LLM-assisted TSN orchestration."
2510.00477,"Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers for the Internet of Things (IoT) by offering enhanced coverage, improved connectivity and access to remote areas. A critical challenge limiting their operational capacity lies in the energy constraints of both aerial platforms and ground-based sensors. This paper explores WLPT as a transformative solution for sustainable energy provisioning in UAV-assisted IoT networks. We first systematically investigate the fundamental principles of WLPT and analysis the comparative advantages. Then, we introduce three operational paradigms for system integration, identify key challenges, and discuss corresponding potential solutions. In case study, we propose a multi-agent reinforcement learning framework to address the coordination and optimization challenges in WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate that our framework significantly improves energy sustainability and data freshness. Finally, we discuss some future directions."
2510.00481,"In 2025, Large Language Model (LLM) services have launched a new feature -- AI video chat -- allowing users to interact with AI agents via real-time video communication (RTC), just like chatting with real people. Despite its significance, no systematic study has characterized the performance of existing AI video chat systems. To address this gap, this paper proposes a comprehensive benchmark with carefully designed metrics across four dimensions: quality, latency, internal mechanisms, and system overhead. Using custom testbeds, we further evaluate five mainstream AI video chatbots with this benchmark. This work provides the research community a baseline of real-world performance and identifies unique system bottlenecks. In the meantime, our benchmarking results also open up several research questions for future optimizations of AI video chatbots."
2510.00588,"Wireless Sensor Networks (WSNs) are extensively utilized in critical applications, including remote monitoring, target tracking, healthcare systems, industrial automation, and smart control in both residential and industrial settings. One of the primary challenges in these systems is maintaining energy efficiency, given that most sensor nodes rely on limited battery resources. To tackle this problem, this study introduces an energy-saving strategy designed for tree-structured networks with dynamic traffic patterns. The approach focuses on lowering power usage by decreasing the length and occurrence of idle listening state where nodes remain active unnecessarily while waiting for data transmissions that may never occur. By reducing this form of energy waste, the proposed approach is designed to extend the operational lifetime and enhance the throughput of the wireless sensor network. Simulation results obtained using the OMNeT++ simulator with the MiXiM framework demonstrate that the solution significantly reduces energy consumption, increases data throughput, and improves overall network efficiency and longevity."
2510.00735,"From hardware offloads like RDMA to software ones like eBPF, offloads are everywhere and their value is in performance. However, there is evidence that fully offloading -- even when feasible -- does not always give the expected speedups. Starting from the observation that this is due to changes the offloads make -- by moving tasks from the application/CPU closer to the network/link layer -- we argue that to further accelerate offloads, we need to make offloads reversible by unloading them -- moving back part of the offloaded tasks.Unloading comes with a set of challenges that we start answering in this paper by focusing on (offloaded) RDMA writes: which part of the write operation does it make sense to unload? how do we dynamically decide which writes to execute on the unload or offload path to improve performance? how do we maintain compatibility between the two paths? Our current prototype shows the potential of unloading by accelerating RDMA writes by up to 31%."
2510.00904,"Efficient data transmission in resource-constrained Internet of Things (IoT) systems requires semantics-aware management that maximizes the delivery of timely and informative data. This paper investigates the optimization of the semantic metric Version Age of Information (VAoI) in a status update system comprising an energy-harvesting (EH) sensor and a destination monitoring node. We consider three levels of knowledge about the system model -- fully known, partially known, and unknown -- and propose corresponding optimization strategies: model-based, estimation-based, and model-free methods. By employing Markov Decision Process (MDP) and Reinforcement Learning (RL) frameworks, we analyze performance trade-offs under varying degrees of model information. Our findings provide guidance for designing efficient and adaptive semantics-aware policies in both known and unknown IoT environments."
2510.00939,"Vehicular Ad-hoc Networks (VANETs), a subclass of Mobile Ad-hoc Networks (MANETs), are expected to play a crucial role in the future of intelligent transportation systems (ITSs). A key objective of VANETs is to enable efficient and cost-effective communication among vehicles while supporting a large number of network participants and minimizing infrastructure dependency. However, the highly dynamic nature of vehicular networks poses significant challenges to their deployment. Clustering techniques are employed to address these challenges, with a strong emphasis on stability, as they directly influence the routing process and enhance the quality of service (QoS). This paper explores the feasibility of reducing reliance on roadside units (RSUs) in metropolitan areas while improving cluster stability. We propose an efficient clustering algorithm tailored for urban environments, leveraging existing metropolitan infrastructure to compensate for the absence of RSUs. Our approach designates public transportation buses as primary cluster heads (CHs), minimizing reliance on additional infrastructure, while stand-alone vehicles (SAVs) dynamically select additional CHs. Through comprehensive case studies and comparative analysis with existing algorithms, our results demonstrate the superior performance of the proposed method across different transmission ranges (TRs)."
2510.00956,"Machine Learning (ML)-based network models provide fast and accurate predictions for complex network behaviors but require substantial training data. Collecting such data from real networks is often costly and limited, especially for critical scenarios like failures. As a result, researchers commonly rely on simulated data, which reduces accuracy when models are deployed in real environments. We propose a hybrid approach leveraging transfer learning to combine simulated and real-world data. Using RouteNet-Fermi, we show that fine-tuning a pre-trained model with a small real dataset significantly improves performance. Our experiments with OMNeT++ and a custom testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and with 50 scenarios, by 48%."
2510.01579,"Physics-inspired and quantum compute based methods for processing in the physical layer of next-generation cellular radio access networks have demonstrated theoretical advances in spectral efficiency in recent years, but have stopped short of practical realization on commodity processors, leaving a gap between the throughput practical systems can achieve and the projected throughput the state-of-the-art should achieve. To fill this gap, this paper proposes MMGaP, an uplink multi-user MIMO detector and downlink Vector perturbation precoder for next-generation cellular networks. MMGaP realizes these large MIMO processing algorithms for the first time on bare-metal CUDA kernels that scale to run on large GPU processing platforms, and can be packaged as TensorFlow modules, allowing easy integration with a variety of systems. We integrate MMGaP with NVIDIA's software-defined, GPU-accelerated 5G platform and evaluate its performance against the state-of-the-art. In a 5G cellular network using 100 MHz of radio bandwidth, eight antennas at the base station and eight concurrent users, we show that MMGaP improves uplink throughput by approximately 50 Mbps per user and downlink throughput by 100 Mbps per user over a wide range of SNR. We further show that MMGaP can also support larger MIMO sizes: for 16 antennas at the base station and 16 concurrent users, MMGaP provides more than 50 Mbps higher uplink throughput per user. We measure the execution time of MMGaP on different NVIDIA GPUs and show that it can operate at line-rate and meet the timing requirements of state-of-the-art 5G systems."
2510.02487,"The advancement of 6G technology has the potential to revolutionize the transportation sector and significantly improve how we travel. 6G-enabled Intelligent Transportation Systems (ITS) promise to offer high-speed, low-latency communication and advanced data analytics capabilities, supporting the development of safer, more efficient, and more sustainable transportation solutions. However, various security and privacy challenges were identified in the literature that must be addressed to enable the safe and secure deployment of 6G-ITS and ensure people's trust in using these technologies. This paper reviews the opportunities and challenges of 6G-ITS, particularly focusing on trust, security, and privacy, with special attention to quantum technologies that both enhance security through quantum key distribution and introduce new vulnerabilities. It discusses the potential benefits of 6G technology in the transportation sector, including improved communication, device interoperability support, data analytic capabilities, and increased automation for different components, such as transportation management and communication systems. A taxonomy of different attack models in 6G-ITS is proposed, and a comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with potential mitigating solutions. This research highlights the urgent need for a comprehensive, multi-layered security framework spanning physical infrastructure protection, network protocol security, data management safeguards, application security measures, and trust management systems to effectively mitigate emerging security and privacy risks and ensure the integrity and resilience of future transportation ecosystems."
2510.02682,"Design for low latency networking is essential for tomorrow's interactive applications, but it is essential to deploy incrementally and universally at the network's last mile. While wired broadband ISPs are rolling out the leading queue occupancy signaling mechanisms, the cellular Radio Access Network (RAN), another important last mile to many users, lags behind these efforts. This paper proposes a new RAN design, L4Span, that abstracts the complexities of RAN queueing in a simple interface, thus tying the queue state of the RAN to end-to-end low-latency signaling all the way back to the content server. At millisecond-level timescales, L4Span predicts the RAN's queuing occupancy and performs ECN marking for both low-latency and classic flows. L4Span is lightweight, requiring minimal RAN modifications, and remains 3GPP and O-RAN compliant for maximum ease of deployment. We implement a prototype on the srsRAN open-source software in C++. Our evaluation compares the performance of low-latency as well as classic flows with or without the deployment of L4Span in various wireless channel conditions. Results show that L4Span reduces the one-way delay of both low-latency and classic flows by up to 98 %, while simultaneously maintaining near line-rate throughput. The code is available atthis https URL."
2510.028,"The proliferation of Low Earth Orbit (LEO) satellites for universal IoT applications and the growing use of drones in emergency services, agriculture, and military operations highlight the transformative potential of non-terrestrial networks (NTN). However, these networks face two key challenges: (1) large coverage footprints that create frequent collisions and (2) moving gateways that cause dynamic links and demand synchronization-free, link-aware transmissions. Existing random access schemes such as ALOHA, CSMA, and BSMA fail in this setting, suffering from high collision rates, hidden terminals, or excessive gateway energy overhead. We propose Free Signal Multiple Access (FSMA), a gateway-controlled protocol that introduces a lightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes transmit only when the channel is idle and when links are reliable, thereby reducing collisions and enabling link-aware access without the need for synchronization or complex scheduling. We evaluate FSMA using 25 commercial LoRa devices with a drone-mounted moving gateway and demonstrate up to 2x higher throughput, 2x to 5x better packet reception ratio, and 5x improved energy efficiency compared to the baselines. Large-scale simulations with a custom Satellite IoT Simulator further show that FSMA scales to 5000+ devices per satellite pass. These results establish FSMA as a practical step toward scalable, energy-efficient, and reliable NTN IoT networks."
2510.02895,"We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a pure-quantum protocol for fair and anonymous allocation of scarce entanglement across wide-area quantum networks composed of many quantum LANs (QLANs). Prior Dicke-state-based pure-quantum MACs resolve contention by local measurements without classical signaling, but they mainly target a single QLAN under static conditions; extending them to wide-area, dynamic settings while avoiding post-selection reconciliation remains open. DH-EAC adopts a two-layer pure-quantum lottery: the outer layer selects winning QLANs and the inner layer selects winning nodes within each winning QLAN. A key design principle is that both the winning set and the per-QLAN quota are fixed by measurements alone, so the contention loop requires no classical round trip. The protocol thus aims to jointly satisfy anonymity (no node IDs revealed until decisions are fixed) and fairness (bias suppression under heterogeneous QLAN sizes). We also provide analytical models for success probability and latency under a standard i.i.d. loss model, and we evaluate DH-EAC against two baselines - single-layer Dicke within one QLAN and a classical GO-driven allocator - using a minimal, reproducible set of scenarios. Metrics include success probability, end-to-end latency, throughput, and Jain's fairness index. The results indicate that DH-EAC offers an implementable design point in the space of entanglement access control, balancing pure-quantum contention resolution, anonymity, and scalability for multi-QLAN networks."
2510.02958,"Efficient handover management remains a critical challenge in dense urban cellular networks, where high cell density, user mobility, and diverse service demands increase the likelihood of unnecessary handovers and ping-pong effects. This paper leverages a real-world, multi-operator drive-test dataset of 30,925 labelled records collected within a 2 km area around Sunway City to investigate sequence-based deep learning approaches for handover detection and avoidance. We formulate handover prediction as a sequence problem and evaluate Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer architectures under Reference Signal Received Power (RSRP)-only and all-feature settings. The integration of multi-dimensional features significantly enhanced handover performance in dense urban cellular networks. The proposed GRU-based model achieved a remarkable 98% reduction in ping-pong handovers, alongside a 46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only approach which yielded a 22.19% reduction. Furthermore, the model demonstrated a 46% improvement in Time of Stay (ToS), indicating more stable user connections. With an inference time of just 0.91 seconds, the solution proves highly efficient and well-suited for real-time edge deployment scenarios. Compared to the conventional 3GPP A3 algorithm, these improvements demonstrate significant gains in mobility robustness and user Quality of Experience (QoE) improvement. The dataset is released to foster reproducibility and further research in intelligent mobility management for 5G and beyond."
2510.03205,"The increased use of software in the operation and management of telecommunication networks has moved the industry one step closer to realizing autonomous network operation. One consequence of this shift is the significantly increased need for testing and validation before such software can be deployed. Complementing existing simulation or hardware-based approaches, digital twins present an environment to achieve this testing; however, they require significant time and human effort to configure and execute. This paper explores the automatic generation of digital twins to provide efficient and accurate validation tools, aligned to the ITU-T autonomous network architecture's experimentation subsystem. We present experimental results for an initial use case, demonstrating that the approach is feasible in automatically creating efficient digital twins with sufficient accuracy to be included as part of existing validation pipelines."
2510.03438,"Effective ground station selection is critical for low Earth orbiting (LEO) satellite constellations to minimize operational costs, maximize data downlink volume, and reduce communication gaps between access windows. Traditional ground station selection typically begins by choosing from a fixed set of locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps reduce the problem scope to optimizing locations over existing infrastructure. However, finding a globally optimal solution for stations using existing mixed-integer programming methods quickly becomes intractable at scale, especially when considering multiple providers and large satellite constellations. To address this issue, we introduce a scalable, hierarchical framework that decomposes the global selection problem into single-satellite, short time-window subproblems. Optimal station choices from each subproblem are clustered to identify consistently high-value locations across all decomposed cases. Cluster-level sets are then matched back to the closest GSaaS candidate sites to produce a globally feasible solution. This approach enables scalable coordination while maintaining near-optimal performance. We evaluate our method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10 stations), achieving solutions within 95% of the global IP optimum for all test cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and Planet's Flock (96) show that while exact IP solutions fail to scale, our framework continues to deliver high-quality site selections."
2510.03491,"Efficient collective communication is critical for many distributed ML and HPC applications. In this context, it is widely believed that the Ring algorithm for the AllReduce collective communication operation is optimal only for large messages, while Recursive Doubling is preferable for small ones due to its logarithmic number of steps compared to the linear number for Ring. In this paper, we challenge this long-held assumption and show that the Ring algorithm can remain optimal even for short messages in ring-based GPU-to-GPU topologies, once realistic propagation delays and link capacity constraints are accounted for. We find that the total propagation delay for both Ring and Recursive Doubling essentially sums to the same value, but the latter incurs significantly higher congestion due to longer hop counts, leading to increased completion times. This surprising result motivates our case for in-collective adaptive topologies, particularly in the context of emerging photonic interconnects, which can break through the limitations of static topology designs at the collective communication granularity. We design a \emph{simple and fast} heuristic for circuit-switching that enables Recursive Doubling to exploit dynamically reconfigurable photonic paths, carefully balancing reconfiguration delays, propagation latencies, and link congestion to minimize overall completion time. Our preliminary evaluations, using realistic reconfiguration delays, show that our circuit-switching schedules enable faster completion times for Recursive Doubling, even compared to Ring AllReduce on static ring topologies. We conclude by highlighting key challenges and future research directions for realizing practical, in-collective photonic switching."
2510.03524,"Fog computing integrates cloud and edge resources. According to an intelligent and decentralized method, this technology processes data generated by IoT sensors to seamlessly integrate physical and cyber environments. Internet of Things uses wireless and smart objects. They communicate with each other, monitor the environment, collect information, and respond to user requests. These objects have limited energy resources since they use batteries to supply energy. Also, they cannot replace their batteries. As a result, the network lifetime is limited and short. Thus, reducing energy consumption and accelerating the data transmission process are very important challenges in IoT networks to reduce the response time. In the data transmission process, selecting an appropriate cluster head node is very important because it can reduce the delay when sending data to the fog. In this paper, cluster head nodes are selected based on several important criteria such as distance, residual energy, received signal strength, and link expiration time. Then, objects send the processed data to the server hierarchically through a balanced tree. The simulation results show that the proposed method outperforms the energy-efficient centroid-based routing protocol (EECRP) and the Emergency Response IoT based on Global Information Decision (ERGID) in terms of packet delivery rate, delay, response time, and network lifetime."
2510.03533,"Due to the big data exchange on the Internet of Things, proper routing and selecting the best routes for fast data transmission improve network performance. There are major challenges, like high delay, when cloud computing is used. Therefore, one solution is to use other schemes, such as fog computing. In fog computing, all data is not sent to the cloud and the fog nodes close to objects are used for data processing. This reduces the network delay. In this paper, we propose an overlapping clustering method called MFCT-IoT to select the best cluster head nodes to guarantee the fast data transfer from objects to fog nodes. The selected cluster head nodes are responsible for sending the collected data to the closest fog nodes in the network edge. Upon receiving the data, the fog nodes process it, and if a response is ready, they respond immediately to the object. Otherwise, they merge and transmit the data to the cloud servers, which are considered as the root node of the proposed hierarchical tree. After processing, the merged data is sent to the object. We compare the proposed scheme with two schemes, including ERGID and EECRP. These schemes are evaluated based on various criteria, including the response time, packet delivery ratio, end-to-end delay, network lifetime, and energy consumption. The results indicate that the proposed method outperforms others in terms of all criteria."
2510.03714,"Although LoRa is predominantly employed with the single-hop LoRaWAN protocol, recent advancements have extended its application to multi-hop mesh topologies. Designing efficient routing for LoRa mesh networks remains challenging due to LoRa's low data rate and ALOHA-based MAC. Prior work often adapts conventional protocols for low-traffic, aboveground networks with strict duty cycle constraints or uses flooding-based methods in subterranean environments. However, these approaches inefficiently utilize the limited available network bandwidth in these low-data-rate networks due to excessive control overhead, acknowledgments, and redundant retransmissions. In this paper, we introduce a novel position- and energy-aware routing strategy tailored for subterranean LoRa mesh networks aimed at enhancing maximum throughput and power efficiency while also maintaining high packet delivery ratios. Our mechanism begins with a lightweight position learning phase, during which LoRa repeaters ascertain their relative positions and gather routing information. Afterwards, the network becomes fully operational with adaptive routing, leveraging standby LoRa repeaters for recovery from packet collisions and losses, and energy-aware route switching to balance battery depletion across repeaters. The simulation results on a representative subterranean network demonstrate a 185% increase in maximum throughput and a 75% reduction in energy consumption compared to a previously optimized flooding-based approach for high traffic."
2510.03807,"Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT) technology face critical limitations in achieving real-time performance for mission-critical industrial applications. Existing 5G-enabled systems suffer from latencies exceeding 10ms, which are inadequate for applications requiring sub-millisecond response times, such as autonomous industrial control and predictive maintenance. This research aims to develop and validate a 6G-enabled Digital Twin framework that achieves ultra-low latency communication and real-time synchronization between physical industrial assets and their digital counterparts, specifically targeting bearing fault detection as a critical industrial use case. The proposed framework integrates terahertz communications (0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence within a five-layer architecture. Experimental validation was conducted using the Case Western Reserve University (CWRU) bearing dataset, implementing comprehensive feature extraction (15 time and frequency domain features) and Random Forest classification algorithms. The system performance was evaluated against traditional WiFi-6 and 5G networks across multiple metrics, including classification accuracy, end-to-end latency, and scalability. It achieved 97.7% fault classification accuracy with 0.8ms end-to-end latency, representing a 15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms) networks. The system demonstrated superior scalability with sub-linear processing time growth and maintained consistent performance across four bearing fault categories (normal, inner race, outer race, and ball faults) with macro-averaged F1-scores exceeding 97%."
2510.03829,"This position paper presents A4FN, an Agentic Artificial Intelligence (AI) architecture for intent-driven automation in Flying Networks (FNs) using Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI and Large Language Models (LLMs) to enable real-time, context-aware network control via a distributed agentic system. It comprises two components: the Perception Agent (PA), which semantically interprets multimodal input -- including imagery, audio, and telemetry data -- from UAV-mounted sensors to derive Service Level Specifications (SLSs); and the Decision-and-Action Agent (DAA), which reconfigures the network based on inferred intents. A4FN embodies key properties of Agentic AI, including autonomy, goal-driven reasoning, and continuous perception-action cycles. Designed for mission-critical, infrastructure-limited scenarios such as disaster response, it supports adaptive reconfiguration, dynamic resource management, and interoperability with emerging wireless technologies. The paper details the A4FN architecture, its core innovations, and open research challenges in multi-agent coordination and Agentic AI integration in next-generation FNs."
2510.04035,"The paper addresses optimizing two of the most important performance parameters, packet loss, and delay, in the critical path optimization of LTE and 5G networks using metaheuristic algorithms to play a vital role in the smartphone user experience. In this context, nine metaheuristic algorithms, such as WOA, PSO, and ABC, have been studied for their effectiveness in various slices of networks: eMBB, URLLC, and mMTC. It can be seen from the results that WOA performed the best: it reduced packet loss by 31% and delay by 6.3 ms; PSO followed closely with a 30% packet loss reduction with a decrease of 6.1 ms in delay. In most scenarios, ABC accomplished good results with a packet loss reduction of 29% and a delay decrease of 6 ms in mMTC scenarios. These results emphasize how selecting appropriate algorithms based on the intended network slice is crucial for optimizing resource utilization and network efficiency. It provides a quantitative framework for assessing and improving the reliability and responsiveness of an LTE/5G network. It encourages more research in hybrid optimization techniques and real-time adaptation mechanisms for further improvements"
2510.04052,"Portable service mesh implementations enable Layer 4 to Layer 7 policy enforcement across heterogeneous infrastructures, yet they depend on the underlying network's connectivity and policies. Layer 3 network policies govern IP traffic regardless of whether upper layers authorize the flow. While these policies are integral to security, correct enforcement often requires coordination across multiple teams, and achieving consistent policy behavior across heterogeneous environments is challenging. Studies show that most Kubernetes clusters do not enforce any network policies. We propose integrating Layer 3 network policy enforcement with service meshes to protect data-plane traffic in a portable, infrastructure-agnostic manner. This integration allows developers to define Layer 3-7 policies and to ensure enforcement across any infrastructure. Our solution builds an overlay Layer 3 network and enforces Layer 3 policies by routing traffic through specific policy enforcement points and applying default-deny principles with authorization keys. We prototyped our approach using Kubernetes and Istio and found that it adds less than 1ms of latency while supporting complex policies comparable to native Kubernetes network policies."
2510.04183,"Beamforming techniques use massive antenna arrays to formulate narrow Line-of-Sight signal sectors to address the increased signal attenuation in millimeter Wave (mmWave). However, traditional sector selection schemes involve extensive searches for the highest signal-strength sector, introducing extra latency and communication overhead. This paper introduces a dynamic layer-wise and clustering-based federated learning (FL) algorithm for beam sector selection in autonomous vehicle networks called enhanced Dynamic Adaptive FL (eDAFL). The algorithm detects and selects the most important layers of a machine learning model for aggregation in the FL process, significantly reducing network overhead and failure risks. eDAFL also considers intra-cluster and inter-cluster approaches to reduce overfitting and increase the abstraction level. We evaluate eDAFL on a real-world multi-modal dataset, demonstrating improved model accuracy by approximately 6.76% compared to existing methods, while reducing inference time by 84.04% and model size by up to 52.20%."
2510.04346,"Indoor LoRaWAN propagation is shaped by structural and time-varying context factors, which challenge log-distance models and the assumption of log-normal shadowing. We present an environment-aware, statistically disciplined path loss framework evaluated using leakage-safe cross-validation on a 12-month campaign in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is augmented with environmental covariates (relative humidity, temperature, carbon dioxide, particulate matter, and barometric pressure), as well as the signal-to-noise ratio. We compare multiple linear regression with regularized variants, Bayesian linear regression, and a selective second-order polynomial applied to continuous drivers. Predictor relevance is established using heteroscedasticity-robust Type II and III analysis of variance and nested partial F tests. Shadow fading is profiled with kernel density estimation and non-parametric families, including Normal, Skew-Normal, Student's t, and Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07 to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are non-Gaussian; a 3-component mixture captures a sharp core with a light, broad tail. We convert accuracy into reliability by prescribing the fade margin as the upper-tail quantile of cross-validated residuals, quantifying uncertainty via a moving-block bootstrap, and validating on a held-out set. At 99% packet delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7 to 27.9 dB for linear baselines. This result presents a deployment-ready, interpretable workflow with calibrated reliability control for indoor Internet of Things planning, aligned with 6G targets."
2510.04516,"HTTP underpins modern Internet services, and providers enforce quotas to regulate HTTP API traffic for scalability and reliability. When requests exceed quotas, clients are throttled and must retry. Server-side enforcement protects the service. However, when independent clients' usage counts toward a shared quota, server-only controls are inefficient; clients lack visibility into others' load, causing their retry attempts to potentially fail. Indeed, retry timing is important since each attempt incurs costs and yields no benefit unless admitted. While centralized coordination could address this, practical limitations have led to widespread adoption of simple client-side strategies like exponential backoff. As we show, these simple strategies cause excessive retries and significant costs. We design adaptive client-side mechanisms requiring no central control, relying only on minimal feedback. We present two algorithms: ATB, an offline method deployable via service workers, and AATB, which enhances retry behavior using aggregated telemetry data. Both algorithms infer system congestion to schedule retries. Through emulations with real-world traces and synthetic datasets with up to 100 clients, we demonstrate that our algorithms reduce HTTP 429 errors by up to 97.3% compared to exponential backoff, while the modest increase in completion time is outweighed by the reduction in errors."
2510.0462,"The internet faces a sovereignty crisis due to power concentration and data growth among a few hyperscalers, leading to centralization and loss of user control. This consolidation risks censorship and creates single points of failure. While Web3 offers decentralized solutions, they often sacrifice either scalability, decentralization, or security, which are key elements in the blockchain trilemma. These solutions also struggle with limited access to enterprise-grade hardware and frequently rely on centralized infrastructure. The Impossible Cloud Network (ICN) addresses these issues by creating a multi-tiered, decentralized infrastructure layer. ICN offers a composable service layer, an enterprise-grade hardware resource layer, and a transparent, permissionless HyperNode network for performance enforcement. By strategically decoupling and decentralizing each layer, ICN aims to provide an open, extensively scalable infrastructure that ensures digital sovereignty, eliminates single points of trust, enables service programmability, and offers a decoupled architecture for limitless possibilities in the future internet."
2510.04651,"Low Earth orbit (LEO) satellites and satellite direct-to-device (D2D) technology are at the heart of the next-generation global connectivity which promises direct access to space-based broadband services for unmodified 3GPP-compliant handsets. With a rapidly evolving ecosystem, it is important to evaluate the feasibility, cost-effectiveness, and profitability of these services. By assessing the technological aspects as well as economic implications, stakeholders can make informed decisions about investment, development, and deployment strategies. This paper presents a comprehensive techno-economic analysis (TEA) framework for evaluating LEO-based satellite D2D systems. The framework integrates a global satellite constellation model, radio propagation aspects including atmospheric and rainfall attenuation models compliant with ITU-R recommendations, 3GPP-compliant capacity calculations, realistic global population data, and an all-encompassing cost model accounting for both capital and operational expenses associated with space and ground segments. Further, the framework evaluates three different architectural options for realizing a global non-terrestrial network (NTN) for satellite D2D services. With an emphasis on reproducibility, the framework has been implemented through significant enhancements to an open-source tool. The economic assessment reveals that global satellite D2D services can be provided at a monthly cost per subscriber which is comparable to terrestrial services while achieving a positive return on investment (ROI). Moreover, the results show the potential of Open RAN technology for realizing cost-effective satellite D2D services."
2510.04731,"IEEE 802.11ax (Wi-Fi 6) introduced Orthogonal Frequency Division Multiple Access (OFDMA), which enables simultaneous transmissions through centralized resource allocation. However, effective uplink scheduling requires the Access Point (AP) to identify which stations (STAs) have data to transmit. This typically necessitates polling for buffer status reports, a process that becomes increasingly inefficient and unscalable with growing device density. In this paper, we study how the Uplink OFDMA-based Random Access (UORA) feature improves the scalability and delay experienced by latency-sensitive data streams. We show that UORA enables efficient uplink scheduling while opportunistically identifying buffered traffic from unscheduled STAs, striking a balance between coordination and scalability. Performance evaluation of different polling strategies is done by means of simulation in ns-3. The results indicate that UORA-based polling outperforms alternative schemes in densely deployed network environments with heterogeneous uplink traffic patterns. Furthermore, under highly sparse and sporadic traffic conditions, UORA-based polling yields over 40% delay reduction compared to Scheduled Access (SA) OFDMA."
2510.05255,"In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive control is preferable. A key open challenge is delivering control-grade predictions within Near-Real-Time (Near-RT) latency and computational constraints under multi-timescale dynamics. We therefore cast RAN Intelligent Controller (RIC) analytics as an agentic perceive-predict xApp that turns noisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE) key performance indicator (KPI) forecasts to drive anticipatory control. In this regard, Transformers are powerful for sequence learning and time-series forecasting, but they are memory-intensive, which limits Near-RT RIC use. Therefore, we need models that maintain accuracy while reducing latency and data movement. To this end, we propose a lightweight Multi-Scale Structured State-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture multi-timescale radio dynamics. We develop stable discrete state-space models (SSMs) via bilinear (Tustin) discretization and apply their causal impulse responses as per-feature depthwise convolutions. Squeeze-and-Excitation gating dynamically reweights KPI channels as conditions change, and a compact gated channel-mixing layer models cross-feature nonlinearities without Transformer-level cost. The model is KPI-agnostic -- Reference Signal Received Power (RSRP) serves as a canonical use case -- and is trained on sliding windows to predict the immediate next step. Empirical evaluations conducted using our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across 13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s per-inference latency with 0.70M parameters, yielding 3-10x lower latency than the Transformer baselines evaluated on the same hardware, while maintaining competitive accuracy."
2510.0529,"Time-Sensitive Networking (TSN) is a collection of mechanisms to enhance the realtime transmission capability of Ethernet networks. TSN combines priority queuing, traffic scheduling, and the Time-Aware Shaper (TAS) to carry periodic traffic with ultra-low latency and jitter. That is, so-called Talkers send periodic traffic with highest priority according to a schedule. The schedule is designed such that the scheduled traffic is forwarded by the TSN bridges with no or only little queuing delay. To protect that traffic against other frames, the TAS is configured on all interfaces such that lower-priority queues can send only when high-priority traffic is not supposed to be forwarded. In the literature on scheduling algorithms for the TAS there is mostly the explicit or implicit assumption that the TAS also limits transmission slots of high-priority traffic.In this paper we show that this assumption can lead to tremendous problems like very long queuing delay or even packet loss in case of faulty frames. A faulty frame arrives too early or too late according to the schedule, it is missing or additional. We construct minimal examples to illustrate basic effects of faulty frames on a single link and demonstrate how this effect can propagate through the networks and cause remote problems. We further show using simulations that a single slightly delayed frame may lead to frame loss on multiple links. We show that these problems can be alleviated or avoided when TAS-based transmission slots for high-priority traffic are configured longer than needed or if they are not limited at all."
2510.05625,"The rapid development of Generative Artificial Intelligence (GenAI) has catalyzed a transformative technological revolution across all walks of life. As the backbone of wideband communication, optical networks are expecting high-level autonomous operation and zero-touch management to accommodate their expanding network scales and escalating transmission bandwidth. The integration of GenAI is deemed as the pivotal solution for realizing zero-touch optical networks. However, the lifecycle management of optical networks involves a multitude of tasks and necessitates seamless collaboration across multiple layers, which poses significant challenges to the existing single-agent GenAI systems. In this paper, we propose a GenAI-driven hierarchical multi-agent framework designed to streamline multi-task autonomous execution for zero-touch optical networks. We present the architecture, implementation, and applications of this framework. A field-deployed mesh network is utilized to demonstrate three typical scenarios throughout the lifecycle of optical network: quality of transmission estimation in the planning stage, dynamic channel adding/dropping in the operation stage, and system capacity increase in the upgrade stage. The case studies, illustrate the capabilities of multi-agent framework in multi-task allocation, coordination, execution, evaluation, and summarization. This work provides a promising approach for the future development of intelligent, efficient, and collaborative network management solutions, paving the way for more specialized and adaptive zero-touch optical networks."
2510.05686,"The Transport Control Protocol has long been the primary transport protocol for applications requiring performance and reliability over the Internet. Unfortunately, due its retransmission mechanism, TCP incurs high packet delivery delays when segments are lost. To address this issue, previous research proposed to use a novel network function, namely Transport Assistant, deployed within the network to cache and retransmit lost packets, thus reducing retransmission delays. In this paper, we propose to jointly route the flows and deploy TAs in order to minimize packet delivery delays in best-effort networks (scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based networks (scenario 2). We hence formulate the joint routing and TA deployment problem as Integer Linear Program for the two scenarios and propose a heuristic solution for large-scale instances of the problem. Through extensive simulations, we demonstrate the benefits of performing joint routing flows and TA deployment in reducing packet delivery delays (up to 16.4%) while minimizing deployment costs (up to 60.98%)."
2510.05762,"The impact of Radio link failure (RLF) has been largely ignored in designing handover algorithms, although RLF is a major contributor towards causing handover failure (HF). RLF can cause HF if it is detected during an ongoing handover. The objective of this work is to propose an efficient power control mechanism based on Deep Q-Network (DQN), considering handover parameters (i.e., time-to-preparation, time-to-execute, preparation offset, execution offset) and radio link monitoring parameters (T310 and N310) as input. The proposed DRL based power control algorithm decides on a possible increase of transmitting power to avoid RLF driven HF. Simulation results show that the traditional conditional handover, when equipped with the proposed DRL based power control algorithm can significantly reduce both RLFs and subsequent HFs, as compared to the existing state of the art approaches."
2510.05797,"The traditional role of the network layer is to create an end-to-end route, through which the intermediate nodes replicate and forward the packets towards the destination. This role can be radically redefined by exploiting the power of Generative AI (GenAI) to pivot towards a prediction-based network layer, which addresses the problems of throughput limits and uncontrollable latency. In the context of real-time delivery of image content, the use of GenAI-aided network nodes has been shown to improve the flow arriving at the destination by more than 100%. However, to successfully exploit GenAI nodes and achieve such transition, we must provide solutions for the problems which arise as we scale the networks to include large amounts of users and multiple data modalities other than images. We present three directions that play a significant role in enabling the use of GenAI as a network layer tool at a large scale. In terms of design, we emphasize the need for initialization protocols to select the prompt size efficiently. Next, we consider the use case of GenAI as a tool to ensure timely delivery of data, as well as an alternative to traditional TCP congestion control algorithms."
2510.06901,"Effective path planning is fundamental to the coordination of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) systems, particularly in applications such as surveillance, navigation, and emergency response. Combining UAVs' broad field of view with UGVs' ground-level operational capability greatly improve the likelihood of successfully achieving task objectives such as locating victims, monitoring target areas, or navigating hazardous terrain. In complex environments, UAVs need to provide precise environmental perception information for UGVs to optimize their routing policy. However, due to severe interference and non-line-of-sight conditions, wireless communication is often unstable in such complex environments, making it difficult to support timely and accurate path planning for UAV-UGV coordination. To this end, this paper proposes a semantic communication (SemCom) framework to enhance UAV/UGV cooperative path planning under unreliable wireless conditions. Unlike traditional methods that transmit raw data, SemCom transmits only the key information for path planning, reducing transmission volume without sacrificing accuracy. The proposed framework is developed by defining key semantics for path planning and designing a transceiver for meeting the requirements of UAV-UGV cooperative path planning. Simulation results show that, compared to conventional SemCom transceivers, the proposed transceiver significantly reduces data transmission volume while maintaining path planning accuracy, thereby enhancing system collaboration efficiency."
2510.06916,"The primary challenge in autonomous lunar landing missions lies in the unreliable local control system, which has limited capacity to handle high-dynamic conditions, severely affecting landing precision and safety. Recent advancements in lunar satellite communication make it possible to establish a wireless link between lunar orbit satellites and the lunar lander. This enables satellites to run high-performance autonomous landing algorithms, improving landing accuracy while reducing the lander's computational and storage load. Nevertheless, traditional communication paradigms are not directly applicable due to significant temperature fluctuations on the lunar surface, intense solar radiation, and severe interference caused by lunar dust on hardware. The emerging technique of semantic communication (SemCom) offers significant advantages in robustness and resource efficiency, particularly under harsh channel conditions. In this paper, we introduce a novel SemCom framework for transmitting images from the lander to satellites operating the remote landing control system. The proposed encoder-decoder dynamically adjusts the transmission strategy based on real-time feedback from the lander's control algorithm, ensuring the accurate delivery of critical image features and enhancing control reliability. We provide a rigorous theoretical analysis of the conditions that improve the accuracy of the control algorithm and reduce end-to-end transmission time under the proposed framework. Simulation results demonstrate that our SemCom method significantly enhances autonomous landing performance compared to traditional communication methods."
2510.07292,"In recent years, Unmanned Aerial Vehicles (UAVs) have brought a new true revolution to military tactics. While UAVs already constitute an advantage when operating alone, multi-UAV swarms expand the available possibilities, allowing the UAVs to collaborate and support each other as a team to carry out a given task. This entails the capability to exchange information related with situation awareness and action coordination by means of a suitable wireless communication technology. In such scenario, the adversary is expected to disrupt communications by jamming the communication channel. The latter becomes the Achilles heel of the swarm. While anti-jamming techniques constitute a well covered topic in the literature, the use of intelligent swarm behaviors to leverage those techniques is still an open research issue.This paper explores the use of Genetic Algorithms (GAs) to jointly optimize UAV swarm formation, beam-steering antennas and traffic routing in order to mitigate the effect of jamming in the main coordination channel, under the assumption that a more robust and low data rate channel is used for formation management signaling. Simulation results show the effectiveness of proposed approach. However, the significant computational cost paves the way for further research."
2510.07464,"The Internet of Things (IoT) bridges the gap between the physical and digital worlds, enabling seamless interaction with real-world objects via the Internet. However, IoT systems face significant challenges in ensuring efficient data generation, collection, and management, particularly due to the resource-constrained and unreliable nature of connected devices, which can lead to data loss. This paper presents DRACO (Data Replication and Collection), a framework that integrates a distributed hop-by-hop data replication approach with an overhead-free mobile sink-based data collection strategy. DRACO enhances data availability, optimizes replica placement, and ensures efficient data retrieval even under node failures and varying network densities. Extensive ns-3 simulations demonstrate that DRACO outperforms state-of-the-art techniques, improving data availability by up to 15% and 34%, and replica creation by up to 18% and 40%, compared to greedy and random replication techniques, respectively. DRACO also ensures efficient data dissemination through optimized replica distribution and achieves superior data collection efficiency under varying node densities and failure scenarios as compared to commonly used uncontrolled sink mobility approaches namely random walk and self-avoiding random walk. By addressing key IoT data management challenges, DRACO offers a scalable and resilient solution well-suited for emerging use cases."
2510.08001,"Channel Charting (CC) has emerged as a promising framework for data-driven radio localization, yet existing approaches often struggle to scale globally and to handle the distortions introduced by non-line-of-sight (NLoS) conditions. In this work, we propose a novel CC method that leverages Channel Impulse Response (CIR) data enriched with practical features such as Time Difference of Arrival (TDoA) and Transmission Reception Point (TRP) locations, enabling a self-supervised localization function on a global scale. The proposed framework is further enhanced with short-interval User Equipment (UE) displacement measurements, which improve the continuity and robustness of the learned positioning function. Our algorithm incorporates a mechanism to identify and mask NLoS-induced noisy measurements, leading to significant performance gains. We present the evaluations of our proposed models in a real 5G testbed and benchmarked against centimeter-accurate Real-Time Kinematic (RTK) positioning, in an O-RAN--based 5G network by OpenAirInterface (OAI) software at EURECOM. It demonstrated outperforming results against the state-of-the-art semi-supervised and self-supervised CC approaches in a real-world scenario. The results show localization accuracies of 2-4 meters in 90% of cases, across a range of NLoS ratios. Furthermore, we provide public datasets of CIR recordings, along with the true position labels used in this paper's evaluation."
2510.08072,"As chip-to-chip silicon photonics gain traction for their bandwidth and energy efficiency, collective communication has emerged as a critical bottleneck in scale-up systems. Programmable photonic interconnects offer a promising path forward: by dynamically reconfiguring the fabric, they can establish direct, high-bandwidth optical paths between communicating endpoints -- \emph{synchronously and guided by the structure of collective operations} (e.g., AllReduce). However, realizing this vision -- \emph{when light bends to the collective will} -- requires navigating a fundamental trade-off between reconfiguration delay and the performance gains of adaptive topologies.In this paper, we present a simple theoretical framework for adaptive photonic scale-up domains that makes this trade-off explicit and clarifies when reconfiguration is worthwhile. Along the way, we highlight a connection -- not surprising but still powerful -- between the Birkhoff--von Neumann (BvN) decomposition, maximum concurrent flow (a classic measure of network throughput), and the well-known $\alpha$-$\beta$ cost model for collectives. Finally, we outline a research agenda in algorithm design and systems integration that can build on this foundation."
2510.0808,"The evolution from Industry 4.0 to Industry 5.0 introduces stringent requirements for ultra reliable low latency communication (URLLC) to support human centric, intelligent, and resilient industrial systems. Sixth-generation (6G) wireless networks aim to meet these requirements through sub-millisecond end-to-end delays, microsecond level jitter, and near perfect reliability, enabled by advances such as terahertz (THz) communication, reconfigurable intelligent surfaces (RIS), multi-access edge computing (MEC), and AI driven cross layer optimization. This paper presents a comprehensive review of URLLC solutions for 6G enabled industry 5.0, organized into a structured taxonomy including application domains, key technical enablers, design challenges, and performance enhancements. The survey examines emerging approaches, including digital twin integration, AI/ML based resource orchestration, Network Function Virtualization (NFV) enabled service function chaining, and cross domain networking, while mapping them to critical industrial scenarios such as smart manufacturing, connected healthcare, autonomous mobility, remote control, and next-generation mobile networks. Performance trade-offs between latency, reliability, scalability, and energy efficiency are analyzed in the context of representative state-of-the-art studies. Finally, the paper identifies open challenges and outlines future research directions to realize deterministic, secure, and sustainable URLLC architectures for Industry 5.0."
2510.08139,"The scalability of blockchain systems is constrained by inefficient P2P broadcasting, as most existing optimizations focus only on the logical layer without considering physical network conditions. To address this, we propose BlockSDN, the first SDN-based integrated architecture for blockchain. BlockSDN employs a distributed control plane for a global network view, a graph engine for hierarchical clustering, and a hybrid macro-micro neighbor selection with hierarchical broadcasting. A dedicated simulation platform shows that BlockSDN reduces global block synchronization time by 65% and 55% compared to Gossip and Mercury,this http URLresults highlight the potential of SDN-enabled cross-layer coordination to significantly enhance blockchain scalability and performance."
2510.08752,"The integration of unmanned aerial vehicles (UAVs) into 5G-Advanced and future 6G networks presents a transformative opportunity for wireless connectivity, enabling agile deployment and improved LoS communications. However, the effective design and optimization of these aerial networks depend critically on high-quality, empirical data. This paper provides a comprehensive survey of publicly available wireless datasets collected from an airborne platform called Aerial Experimentation and Research Platform on Advanced Wireless (AERPAW). We highlight the unique challenges associated with generating reproducible aerial wireless datasets, and review the existing related works in the literature. Subsequently, for each dataset considered, we explain the hardware and software used, present the dataset format, provide representative results, and discuss how these datasets can be used to conduct additional research. The specific aerial wireless datasets presented include raw I/Q samples from a cellular network over different UAV trajectories, spectrum measurements at different altitudes, flying 4G base station (BS), a 5G-NSA Ericsson network, a LoRaWAN network, an radio frequency (RF) sensor network for source localization, wireless propagation data for various scenarios, and comparison of ray tracing and real-world propagation scenarios. References to all datasets and post-processing scripts are provided to enable full reproducibility of the results. Ultimately, we aim to guide the community toward effective dataset utilization for validating propagation models, developing machine learning algorithms, and advancing the next generation of aerial wireless systems."
2510.08769,"5G networks enable diverse services such as eMBB, URLLC, and mMTC through network slicing, necessitating intelligent admission control and resource allocation to meet stringent QoS requirements while maximizing Network Service Provider (NSP) profits. However, existing Deep Reinforcement Learning (DRL) frameworks focus primarily on profit optimization without explicitly accounting for service delay, potentially leading to QoS violations for latency-sensitive slices. Moreover, commonly used epsilon-greedy exploration of DRL often results in unstable convergence and suboptimal policy learning. To address these gaps, we propose DePSAC -- a Delay and Profit-aware Slice Admission Control scheme. Our DRL-based approach incorporates a delay-aware reward function, where penalties due to service delay incentivize the prioritization of latency-critical slices such as URLLC. Additionally, we employ Boltzmann exploration to achieve smoother and faster convergence. We implement and evaluate DePSAC on a simulated 5G core network substrate with realistic Network Slice Request (NSLR) arrival patterns. Experimental results demonstrate that our method outperforms the DSARA baseline in terms of overall profit, reduced URLLC slice delays, improved acceptance rates, and improved resource consumption. These findings validate the effectiveness of the proposed DePSAC in achieving better QoS-profit trade-offs for practical 5G network slicing scenarios."
2510.09239,"Characterizing application-layer user throughput in next-generation networks is increasingly challenging as the higher capacity of the 5G Radio Access Network (RAN) shifts connectivity bottlenecks towards deeper parts of the network. Traditional methods, such as drive tests and operator equipment counters, are costly, limited, or fail to capture end-to-end (E2E) Quality of Service (QoS) and its variability. In this work, we leverage large-scale crowdsourced measurements-including E2E, radio, contextual and network deployment features collected by the user equipment (UE)-to propose an uncertainty-aware and explainable approach for downlink user throughput estimation. We first validate prior 4G methods, improving R^2 by 8.7%, and then extend them to 5G NSA and 5G SA, providing the first benchmarks for 5G crowdsourced datasets. To address the variability of throughput, we apply NGBoost, a model that outputs both point estimates and calibrated confidence intervals, representing its first use in the field of computer communications. Finally, we use the proposed model to analyze the evolution from 4G to 5G SA, and show that throughput bottlenecks move from the RAN to transport and service layers, as seen by E2E metrics gaining importance over radio-related features."
2510.09842,"Energy efficiency has emerged as a defining constraint in the evolution of sustainable Internet of Things (IoT) networks. This work moves beyond simulation-based or device-centric studies to deliver measurement-driven, network-level smart energy analysis. The proposed system enables end-to-end visibility of energy flows across distributed IoT infrastructures, uniting Bluetooth Low Energy (BLE) and Visible Light Communication (VLC) modes with environmental sensing and E-ink display subsystems under a unified profiling and prediction platform. Through automated, time-synchronized instrumentation, the framework captures fine-grained energy dynamics across both node and gateway layers. We developed a suite of tools that generate energy datasets for IoT ecosystems, addressing the scarcity of such data and enabling AI-based predictive and adaptive energy optimization. Validated within a network-level IoT testbed, the approach demonstrates robust performance under real operating conditions."
2510.09983,"The use of Content Delivery Networks (CDNs) has significantly increased over the past decade, with approximately 55 million websites currently relying on CDN services. Emerging solutions, such as Delegated Credentials (RFC 9345), lack fine-grained definitions of many critical aspects of delegation, such as the length of delegation chains, revocation mechanism, permitted operations, and a well-defined scope for said delegation. We present Delegation Certificates (DeCerts), which modify X.509 certificate standard and add new extensions to enable fine-grained CDN delegation. DeCerts allow domain owners to specify delegated and non-delegated subdomains, and control the depth of delegation extended by CDNs, which provides flexibility in delegation management. But more importantly, DeCerts are built on a new principle which provides full autonomy to domain owners-domain owners can issue DeCerts fully independent of Certificate Authorities (CAs), and thus have greater flexibility in policy control, including revocation methods. Such level of flexibility would be hard to match if CAs where to issue such certificates. Revoking a DeCert revokes delegation. We discuss multiple revocation mechanisms for a DeCerts balancing security, performance, and delegator control. We modify Firefox to support DeCert (i.e., proper validation) as a proof-of-concept, and test it to demonstrate the feasibility, compatibility of DeCerts with browsers and TLS/HTTPS protocols. DeCerts enhance the security, scalability, and manageability of CDN delegation, offering a practical solution for Internet services."
2510.1004,"Spectrum sharing is a critical strategy for meeting escalating user demands via commercial wireless services, yet its effective regulation and technological enablement, particularly concerning coexistence with incumbent systems, remain significant challenges. Federal organizations have established regulatory frameworks to manage shared commercial use alongside mission-critical operations, such as military communications. This paper investigates the potential of machine learning (ML)-based approaches to enhance spectrum sharing capabilities within the Citizens Broadband Radio Service (CBRS) band, specifically focusing on the coexistence of commercial signals (e.g., 5G) and military radar systems. We demonstrate that ML techniques can potentially extend the Federal Communications Commission (FCC)-recommended signal-to-interference-plus-noise ratio (SINR) boundaries by improving radar detection and waveform identification in high-interference environments. Through rigorous evaluation using both synthetic and real-world signals, our findings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ) data and spectrograms, can achieve the FCC-recommended $99\%$ radar detection accuracy even when subjected to high interference from 5G signals upto -5dB SINR, exceeding the required limits of $20$ SINR. Our experimental studies distinguish this work from the state-of-the-art by significantly extending the SINR limit for $99\%$ radar detection accuracy from approximately $12$ dB down to $-5$ dB. Subsequent to detection, we further apply ML to analyze and identify radar waveforms. The proposed models also demonstrate the capability to classify six distinct radar waveform types with $93\%$ accuracy."
2510.10044,"The growing demand for effective spectrum management and interference mitigation in shared bands, such as the Citizens Broadband Radio Service (CBRS), requires robust radar detection algorithms to protect the military transmission from interference due to commercial wireless transmission. These algorithms, in turn, depend on large, diverse, and carefully labeled spectrogram datasets. However, collecting and annotating real-world radio frequency (RF) spectrogram data remains a significant challenge, as radar signals are rare, and their occurrences are infrequent. This challenge makes the creation of balanced datasets difficult, limiting the performance and generalizability of AI models in this domain.To address this critical issue, we propose a diffusion-based generative model for synthesizing realistic and diverse spectrograms of five distinct categories that integrate LTE, 5G, and radar signals within the CBRS band. We conduct a structural and statistical fidelity analysis of the generated spectrograms using widely accepted evaluation metrics Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from the training data. Furthermore, we demonstrate that pre-training on the generated spectrograms significantly improves training efficiency on a real-world radar detection task by enabling $51.5\%$ faster convergence."
2510.10158,"User mobility trajectory and mobile traffic data are essential for a wide spectrum of applications including urban planning, network optimization, and emergency management. However, large-scale and fine-grained mobility data remains difficult to obtain due to privacy concerns and collection costs, making it essential to simulate realistic mobility and traffic patterns. User trajectories and mobile traffic are fundamentally coupled, reflecting both physical mobility and cyber behavior in urban environments. Despite this strong interdependence, existing studies often model them separately, limiting the ability to capture cross-modal dynamics. Therefore, a unified framework is crucial. In this paper, we propose MSTDiff, a Multi-Scale Diffusion Transformer for joint simulation of mobile traffic and user trajectories. First, MSTDiff applies discrete wavelet transforms for multi-resolution traffic decomposition. Second, it uses a hybrid denoising network to process continuous traffic volumes and discrete location sequences. A transition mechanism based on urban knowledge graph embedding similarity is designed to guide semantically informed trajectory generation. Finally, a multi-scale Transformer with cross-attention captures dependencies between trajectories and traffic. Experiments show that MSTDiff surpasses state-of-the-art baselines in traffic and trajectory generation tasks, reducing Jensen-Shannon divergence (JSD) across key statistical metrics by up to 17.38% for traffic generation, and by an average of 39.53% for trajectory generation. The source code is available at:this https URL."
2510.10236,"Flying Ad Hoc Networks (FANETs) present unique challenges due to high node mobility, dynamic topologies, and strict resource constraints. Existing routing protocols often optimize for a single metric, such as path length or energy, while neglecting the complex dependencies between network performance, security, and MAC layer efficiency. This paper introduces a novel hardware software co design framework for secure and adaptive UAV swarm communications, featuring an energy aware protocol stack. The architecture employs a multicast, clustered organization where routing decisions integrate dynamic trust scores, historical link quality, and internodal distance. A hybrid MAC protocol combines contention based and scheduled channel access for optimized throughput. Security is ensured through a zero trust model that fuses cryptographic authentication with a behavioral reputation system, alongside hardware accelerated AES GCM encryption. Comparative analysis in an NS 3 simulation environment demonstrates the framework's superiority in packet delivery ratio, latency, resilience, and overhead, providing a scalable foundation for high performance swarm operations."
2510.10756,"In the ensuing ultra-dense and diverse environment in future \ac{6G} communication networks, it will be critical to optimize network resources via mechanisms that recognize and cater to the diversity, density, and dynamicity of system changes. However, coping with such environments cannot be done through the current network approach of compartmentalizing data as distinct from network operations. Instead, we envision a computing continuum where the content of the transmitted data is considered as an essential element in the transmission of that data, with data sources and streams analyzed and distilled to their essential elements, based on their semantic context, and then processed and transmitted over dedicated slices of network resources. By exploiting the rich content and semantics within data for dynamic and autonomous optimization of the computing continuum, this article opens the door to integrating communication, computing, cyber-physical systems, data flow, and AI, presenting new and exciting opportunities for cross-layer design. We propose semantic slicing, a two-pronged approach that builds multiple virtual divisions within a single physical and data infrastructure, each with its own distinct characteristics and needs. We view semantic slicing as a novel shift from current static slicing techniques, extending existing slicing approaches such that it can be applied dynamically at different levels and categories of resources in the computing continuum. Further it propels the advancement of semantic communication via the proposed architectural framework."
2510.11043,"Operating at petabit-scale, ByteDance's cloud gateways are deployed at critical aggregation points to orchestrate a wide array of business traffic. However, this massive scale imposes significant resource pressure on our previous-generation cloud gateways, rendering them unsustainable in the face of ever-growing cloud-network traffic. As the DPU market rapidly expands, we see a promising path to meet our escalating business traffic demands by integrating DPUs with our established Tofino-based gateways. DPUs augment these gateways with substantially larger table capacities and richer programmability without compromising previously low-latency and high-throughput forwarding. Despite compelling advantages, the practical integration of DPUs into cloud gateways remains unexplored, primarily due to underlying challenges. In this paper, we present Zephyrus, a production-scale gateway built upon a unified P4 pipeline spanning high-performance Tofino and feature-rich DPUs, which successfully overcomes these challenges. We further introduce a hierarchical co-offloading architecture (HLCO) to orchestrate traffic flow within this heterogeneous gateway, achieving > 99% hardware offloading while retaining software fallback paths for complex operations. Zephyrus outperforms LuoShen (NSDI '24) with 33% higher throughput and our evaluation further indicates 21% lower power consumption and 14% lower hardware cost. Against FPGA-based systems, Albatross (SIGCOMM '25), it doubles the throughput at a substantially lower Total Cost of Ownership (TCO), showcasing its superior performance-per-dollar. Beyond these performance gains, we also share key lessons from several years of developing and operating Zephyrus at production scale. We believe these insights provide valuable references for researchers and practitioners designing performant cloud gateways."
2510.11109,"The increase of bandwidth-intensive applications in sixth-generation (6G) wireless networks, such as real-time volumetric streaming and multi-sensory extended reality, demands intelligent multicast routing solutions capable of delivering differentiated quality-of-service (QoS) at scale. Traditional shortest-path and multicast routing algorithms are either computationally prohibitive or structurally rigid, and they often fail to support heterogeneous user demands, leading to suboptimal resource utilization. Neural network-based approaches, while offering improved inference speed, typically lack topological generalization and scalability. To address these limitations, this paper presents a graph neural network (GNN)-based multicast routing framework that jointly minimizes total transmission cost and supports user-specific video quality requirements. The routing problem is formulated as a constrained minimum-flow optimization task, and a reinforcement learning algorithm is developed to sequentially construct efficient multicast trees by reusing paths and adapting to network dynamics. A graph attention network (GAT) is employed as the encoder to extract context-aware node embeddings, while a long short-term memory (LSTM) module models the sequential dependencies in routing decisions. Extensive simulations demonstrate that the proposed method closely approximates optimal dynamic programming-based solutions while significantly reducing computational complexity. The results also confirm strong generalization to large-scale and dynamic network topologies, highlighting the method's potential for real-time deployment in 6G multimedia delivery scenarios. Code is available atthis https URL."
2510.11123,"The advent of the fifth-generation technology promises to bring about more vertical applications and emerging services that include vehicular networks and intelligent transportation systems (ITSs). To achieve their vision of real-time and safetyapplications, vehicular networks rely on short-range to medium-range communications. One emerging technology that aims to provide reliability and high-data rate in short-range communications is the visible light communications (VLC). Due to its remarkable advantages, some studies have recently investigated the integration of VLC in vehicular networks and ITSs. Despite their attractive features, such networks also face several implementation issues. This paper provides an extended tutorial on the implementation of VLC-based vehicular networks. To begin with, we present the implementation characteristics of these systems and discuss some related issues. The underlying system considers a general structure with transmitters, channels, and receivers based on photodetectors and cameras, as well as standardization efforts and types of topologies. In addition, we discuss the impact of the sun and artificial light sources, flickering, dimming, throughput enhancement, uplink security, and mobility on practical implementation. Finally, we highlight some key challenges and potential solutions and provide some directions for future research investigations that could constitute an advancement toward the development of commercial VLC-based vehicular systems."
2510.11198,"This study investigates a cognitive shared access network with energy harvesting capabilities operating under Age of Information (AoI) constraints for the primary user. Secondary transmitters are spatially distributed according to a homogeneous Poisson Point Process (PPP), while the primary user is located at a fixed position. The primary transmitter handles bursty packet arrivals, whereas secondary users operate under saturated traffic conditions. To manage interference and energy, two distinct zones are introduced: an energy harvesting zone around the primary transmitter and a guard zone around the primary receiver, within which secondary transmissions are prohibited. Secondary users access the channel probabilistically, with access decisions depending on their current battery state (charged or empty) and their location relative to the guard zone. Our objective is to analyze the primary user's AoI performance under three distinct packet management policies."
2510.11269,"Generative AI (GenAI) chatbots are now pervasive in digital ecosystems, yet their network traffic remains largely underexplored. This study presents an in-depth investigation of traffic generated by three leading chatbots (ChatGPT, Copilot, and Gemini) when accessed via Android mobile apps for both text and image generation. Using a dedicated capture architecture, we collect and label two complementary workloads: a 60-hour generic dataset with unconstrained prompts, and a controlled dataset built from identical prompts across GenAI apps and replicated via conventional messaging apps to enable one-to-one comparisons. This dual design allows us to address practical research questions on the distinctiveness of GenAI traffic, its differences from widely deployed traffic categories, and its novel implications for network usage. To this end, we provide fine-grained traffic characterization at trace, flow, and protocol levels, and model packet-sequence dynamics with Multimodal Markov Chains. Our analyses reveal app- and content-specific traffic patterns, particularly in volume, uplink/downlink profiles, and protocol adoption. We highlight the predominance of TLS, with Gemini extensively leveraging QUIC, ChatGPT exclusively using TLS 1.3, and app- and content-specific Server Name Indication (SNI) values. A payload-based occlusion analysis quantifies SNI's contribution to classification: masking it reduces F1-score by up to 20 percentage points in GenAI app traffic classification. Finally, compared with conventional messaging apps when carrying the same content, GenAI chatbots exhibit unique traffic characteristics, highlighting new stress factors for mobile networks, such as sustained upstream activity, with direct implications for network monitoring and management. We publicly release the datasets to support reproducibility and foster extensions to other use cases."
2510.11291,"This work presents a Network-Optimised Spiking (NOS) delay-aware scheduler for 6G radio access. The scheme couples a bounded two-state kernel to a clique-feasible proportional-fair (PF) grant head: the excitability state acts as a finite-buffer proxy, the recovery state suppresses repeated grants, and neighbour pressure is injected along the interference graph via delayed spikes. A small-signal analysis yields a delay-dependent threshold $k_\star(\Delta)$ and a spectral margin $\delta = k_\star(\Delta) - gH\rho(W)$ that compress topology, controller gain, and delay into a single design parameter. Under light assumptions on arrivals, we prove geometric ergodicity for $\delta>0$ and derive sub-Gaussian backlog and delay tail bounds with exponents proportional to $\delta$. A numerical study, aligned with the analysis and a DU compute budget, compares NOS with PF and delayed backpressure (BP) across interference topologies over a $5$--$20$\,ms delay sweep. With a single gain fixed at the worst spectral radius, NOS sustains higher utilisation and a smaller 99.9th-percentile delay while remaining clique-feasible on integer PRBs."
2510.11361,"We present a novel protocol that reduces worst-case packet latency in deflection-based on-chip interconnect networks. It enforces the deflection of the header of a packet but not its payload, resulting in a reduction in overall network traffic and, more importantly, worst-case packet latency due to decreased pre-injection latency."
2510.11535,"Timely delivery of delay-sensitive information over dynamic, heterogeneous networks is increasingly essential for a range of interactive applications, such as industrial automation, self-driving vehicles, and augmented reality. However, most existing network control solutions target only average delay performance, falling short of providing strict End-to-End (E2E) peak latency guarantees. This paper addresses the challenge of reliably delivering packets within application-imposed deadlines by leveraging recent advancements in Multi-Agent Deep Reinforcement Learning (MA-DRL). After introducing the Delay-Constrained Maximum-Throughput (DCMT) dynamic network control problem, and highlighting the limitations of current solutions, we present a novel MA-DRL network control framework that leverages a centralized routing and distributed scheduling architecture. The proposed framework leverages critical networking domain knowledge for the design of effective MA-DRL strategies based on the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) technique, where centralized routing and distributed scheduling agents dynamically assign paths and schedule packet transmissions according to packet lifetimes, thereby maximizing on-time packet delivery. The generality of the proposed framework allows integrating both data-driven \blue{Deep Reinforcement Learning (DRL)} agents and traditional rule-based policies in order to strike the right balance between performance and learning complexity. Our results confirm the superiority of the proposed framework with respect to traditional stochastic optimization-based approaches and provide key insights into the role and interplay between data-driven DRL agents and new rule-based policies for both efficient and high-performance control of latency-critical services."
2510.11937,"Cloud providers have recently decentralized their wide-area network traffic engineering (TE) systems to contain the impact of TE controller failures. In the decentralized design, a controller fault only impacts its slice of the network, limiting the blast radius to a fraction of the network. However, we find that autonomous slice controllers can arrive at divergent traffic allocations that overload links by 30% beyond their capacity. We present Symphony, a decentralized TE system that addresses the challenge of divergence-induced congestion while preserving the fault-isolation benefits of decentralization. By augmenting TE objectives with quadratic regularization, Symphony makes traffic allocations robust to demand perturbations, ensuring TE controllers naturally converge to compatible allocations without coordination. In parallel, Symphony's randomized slicing algorithm partitions the network to minimize blast radius by distributing critical traffic sources across slices, preventing any single failure from becoming catastrophic. These innovations work in tandem: regularization ensures algorithmic stability to traffic allocations while intelligent slicing provides architectural resilience in the network. Through extensive evaluation on cloud provider WANs, we show Symphony reduces divergence-induced congestion by 14x and blast radius by 79% compared to current practice."
2510.12064,"The proliferation of Large Language Models (LLMs) with exponentially growing parameters is making cross-data center (DC) training an inevitable trend. However, viable strategies for extending single-DC training frameworks to multi-DC environments remain underdeveloped. We experimentally demonstrate, for the first time, a high-performance geo-distributed LLMs training framework across multiple DCs interconnected by a lossless, remote direct memory access (RDMA) enabled Datacenter Optical Transport Network (DC-OTN). An enhanced pipeline parallelism scheme is implemented within the Ascend full-stack environment of Huawei, which effectively eliminates the impact of cross-DC communication overhead on training efficiency. The overlapped computation and cross-DC communication is achieved with constraint cross-DC bandwidth and High Bandwidth Memory (HBM), reducing computation bubble ratio by up to 78.91%."
2510.12458,"Network Digital Twins represent a key technology in future networks, expected to provide the capability to perform accurate analysis and predictions about the behaviour of 6G mobile networks. However, despite the availability of several theoretical works on the subject, still very few examples of actual implementations of Network Digital Twin are available. This paper provides a detailed description about the characteristics of Network Digital Twin and provides a practical example about real deployment of the technology. The considered network infrastructure is a real 5G private network running in a lab. The Network Digital Twin is built based on open source network emulation software and is available to the community as open source. Measurements on both the physical infrastructure and the related Digital Twin demonstrate a high accuracy in reproducing the state and behavior of the actual 5G system."
2510.12698,"This paper presents a protocol for enhancement of life time of WBAN network as well other protocol related issues such as throughput, path loss, and residual energy. Bio-sensors are used for deployment on human body. Poisson distribution and equilibrium model techniques have been used for attaining the required results. Multi-hop network topology and random network node deployment used to achieve minimum energy consumption and longer network lifetime."
2510.12896,"Cellular wireless networks enable mobile broadband connectivity for Internet-based applications through their radio access and core network infrastructure. While Fifth-Generation (5G) cellular systems are currently being deployed, ongoing research on cellular technologies primarily focuses on Sixth-Generation (6G) networks to set the stage for developing standards for these systems. Therefore, the time has come to articulate the visions for beyond 6G (B6G) systems. In this article, we present a visionary framework toward hyper-dimensional connectivity in B6G that enables wireless access to hyper-immersive Internet technologies. Our contributions include a conceptual framework for B6G cellular systems with jointly integrated communication, cognition, computing, and cyber-physical capabilities as core connectivity dimensions, a set of technical definitions outlining potential use cases and system-level requirements, a mapping of prospective technology enablers, and a forward-looking research agenda for B6G systems. The conceptual discussions in this article would be helpful for identifying innovation drivers, shaping long-term technical goals, and defining research agendas for the future of mobile broadband technologies."
2510.13031,"The Open Radio Access Network (O-RAN) architecture enables a flexible, vendor-neutral deployment of 5G networks by disaggregating base station components and supporting third-party xApps for near real-time RAN control. However, the concurrent operation of multiple xApps can lead to conflicting control actions, which may cause network performance degradation. In this work, we propose a framework for xApp conflict management that combines explainable machine learning and causal inference to evaluate the causal relationships between RAN Control Parameters (RCPs) and Key Performance Indicators (KPIs). We use model explainability tools such as SHAP to identify RCPs that jointly affect the same KPI, signaling potential conflicts, and represent these interactions as a causal Directed Acyclic Graph (DAG). We then estimate the causal impact of each of these RCPs on their associated KPIs using metrics such as Average Treatment Effect (ATE) and Conditional Average Treatment Effect (CATE). This approach offers network operators guided insights into identifying conflicts and quantifying their impacts, enabling more informed and effective conflict resolution strategies across diverse xApp deployments."
2510.13248,"Network protocol testing is fundamental for modern network infrastructure. However, traditional network protocol testing methods are labor-intensive and error-prone, requiring manual interpretation of specifications, test case design, and translation into executable artifacts, typically demanding one person-day of effort per test case. Existing model-based approaches provide partial automation but still involve substantial manual modeling and expert intervention, leading to high costs and limited adaptability to diverse and evolving protocols. In this paper, we propose a first-of-its-kind system called NeTestLLM that takes advantage of multi-agent Large Language Models (LLMs) for end-to-end automated network protocol testing. NeTestLLM employs hierarchical protocol understanding to capture complex specifications, iterative test case generation to improve coverage, a task-specific workflow for executable artifact generation, and runtime feedback analysis for debugging and refinement. NeTestLLM has been deployed in a production environment for several months, receiving positive feedback from domain experts. In experiments, NeTestLLM generated 4,632 test cases for OSPF, RIP, and BGP, covering 41 historical FRRouting bugs compared to 11 by current national standards. The process of generating executable artifacts also improves testing efficiency by a factor of 8.65x compared to manual methods. NeTestLLM provides the first practical LLM-powered solution for automated end-to-end testing of heterogeneous network protocols."
2510.13467,"Large Language Models (LLMs) remain static in functionality after training, and extending their capabilities requires integration with external data, computation, and services. The Model Context Protocol (MCP) has emerged as a standard interface for such extensions, but current implementations rely solely on semantic matching between users' requests and server function descriptions, which makes current deployments and simulation testbeds fragile under latency fluctuations or server failures. We address this gap by enhancing MCP tool routing algorithms with real-time awareness of network and server status. To provide a controlled test environment for development and evaluation, we construct a heterogeneous experimental platform, namely Network-aware MCP (NetMCP), which offers five representative network states and build a benchmark for latency sequence generation and MCP server datasets. On top of NetMCP platform, we analyze latency sequences and propose a Semantic-Oriented and Network-Aware Routing (SONAR) algorithm, which jointly optimizes semantic similarity and network Quality of Service (QoS) metrics for adaptive tool routing. Results show that SONAR consistently improves task success rate and reduces completion time and failure number compared with semantic-only, LLM-based baselines, demonstrating the value of network-aware design for production-scale LLM systems. The code for NetMCP is available atthis https URL."
2510.13664,"A growing class of applications demands \emph{fair ordering} of events, which ensures that events generated earlier are processed before later events. However, achieving such sequencing is challenging due to the inherent errors in clock synchronization: two events at two clients generated close together may have timestamps that cannot be compared confidently. We advocate for an approach that embraces, rather than eliminates, clock synchronization errors. Instead of attempting to remove the error from a timestamp, \systemname{}, our proposed system, leverages a statistical model to compare two noisy timestamps probabilistically by learning per-clock synchronization error distributions. Our preliminary statistical model computes the probability that one event precedes another by only relying on local clocks of clients. This serves as a foundation for a new relation: \emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents the probability that an event happened before another. The $\xrightarrow{p}$ relation provides a basis for ordering multiple events which are otherwise considered \emph{concurrent} by Lamport's \emph{happened-before} ($\rightarrow$) relation. We highlight various related challenges including the intransitivity of the $\xrightarrow{p}$ relation as opposed to the transitive $\rightarrow$ relation. We outline several research directions: online fair sequencing, stochastically fair total ordering, and handling byzantine clients."
2510.13689,"Satellite communication offers Internet connectivity to remote locations, such as villages, deserts, mountains, and at sea. However, transmitting content over satellite networks is significantly more expensive than traditional Internet. To address this issue, we propose placing content replica servers within satellite networks and optimizing replica placement for important performance metrics, such as latency, transmission, and storage cost. Our approach can support different types of satellite networks, including Low Earth Orbit (LEO), Medium Earth Orbit (MEO), Geostationary Orbit (GEO), and their combinations. An important challenge for supporting content replicas in such networks is that LEO and MEO satellites are constantly moving. We address this challenge by explicitly considering their moving trajectories and strategically optimizing not only client performance, but also the cost of transferring content from one satellite to another as needed. We demonstrate the effectiveness of our approach using both simulated traffic traces and a prototype system."
2510.1371,"Low Earth Orbit (LEO) satellite ISPs promise universal Internet connectivity, yet their interaction with content delivery remains poorly understood. We present the first comprehensive measurement study decomposing Starlink's web content delivery performance decomposed across Point of Presence (PoP), DNS, and CDN layers. Through two years of measurements combining 225K Cloudflare AIM tests, M-Lab data, and active probing from 99 RIPE Atlas and controlled Starlink probes, we collect 6.1M traceroutes and 10.8M DNS queries to quantify how satellite architecture disrupts terrestrial CDN assumptions. We identify three distinct performance regimes based on infrastructure density. Regions with local content-rich PoPs achieve near-terrestrial latencies with the satellite segment dominating 80-90% of RTT. Infrastructure-sparse regions suffer cascading penalties: remote PoPs force distant resolver selection, which triggers CDN mis-localization, pushing latencies beyond 200 ms. Dense-infrastructure regions show minimal sensitivity to PoP changes. Leveraging Starlink's infrastructure expansion in early 2025 as a natural experiment, we demonstrate that relocating PoPs closer to user location reduces median page-fetch times by 60%. Our findings reveal that infrastructure proximity, not satellite coverage, influences web performance, requiring fundamental changes to CDN mapping and DNS resolution for satellite ISPs."
2510.13732,"Pilot contamination remains a major bottleneck in realizing the full potential of distributed massive MIMO systems. We propose two dynamic and scalable pilot assignment schemes designed for practical deployment in such networks. First, we present a low-complexity centralized scheme that sequentially assigns pilots to user equipments (UEs) to minimize the global channel estimation errors across serving access points (APs). This improves the channel estimation quality and reduces interference among UEs, enhancing the spectral efficiency. Second, we develop a fully distributed scheme that uses a priority-based pilot selection approach. In this scheme, each selected AP minimizes the channel estimation error using only local information and offers candidate pilots to the UEs. Every UE then selects a suitable pilot based on its AP priority. This approach ensures consistency and minimizes interference while significantly reducing pilot contamination. The method requires no global coordination, maintains low signaling overhead, and adapts dynamically to the UE deployment. Numerical simulations demonstrate the superiority of the proposed schemes in terms of network throughput when compared to the existing state-of-the-art schemes."
2510.13819,"This paper studies user localization aided by a Reconfigurable Intelligent Surface (RIS). A feedback link from the Base Station (BS) to the user is adopted to enable dynamic power control of the user pilot transmissions in the uplink. A novel multi-agent algorithm for the joint control of the RIS phase configuration and the user transmit power is presented, which is based on a hybrid approach integrating NeuroEvolution (NE) and supervised learning. The proposed scheme requires only single-bit feedback messages for the uplink power control, supports RIS elements with discrete responses, and is numerically shown to outperform fingerprinting, deep reinforcement learning baselines and backpropagation-based position estimators."
2510.1382,"This research proposes an extensive technique for monitoring and controlling the industrial parameters using Internet of Things (IoT) technology based on wireless communication. We proposed a system based on NRF transceivers to establish a strong Wireless Sensor Network (WSN), enabling transfer of real-time data from multiple sensors to a central setup that is driven by ARDUINO microcontrollers. Different key parameters, crucial for industrial setup such as temperature, humidity, soil moisture and fire detection, are monitored and displayed on an LCD screen, enabling factory administration to oversee the industrial operations remotely over the internet. Our proposed system bypasses the need for physical presence for monitoring by addressing the shortcomings of conventional wired communication systems. Other than monitoring, there is an additional feature to remotely control these parameters by controlling the speed of DC motors through online commands. Given the rising incidence of industrial fires over the worldwide between 2020 and 2024 due to an array of hazards, this system with dual functionality boosts the overall operational efficiency and safety. This overall integration of IoT and Wireless Sensor Network (WSN) reduces the potential risks linked with physical monitoring, providing rapid responses in emergency scenarios, including the activation of firefighting equipment. The results show that innovations in wireless communication perform an integral part in industrial process automation and safety, paving the way to more intelligent and responsive operating environments. Overall, this study highlights the potential for change of IoT-enabled systems to revolutionize monitoring and control in a variety of industrial applications, resulting in increased productivity and safety."
2510.13821,"This position paper argues that the field of LLM agents requires a unified, telecom-inspired communication protocol to ensure safety, interoperability, and scalability, especially within the context of Next Generation (NextG) networks. Current ad-hoc communication methods are creating a fragmented ecosystem, reminiscent of the early ""protocol wars"" in networking, which stifles innovation and poses significant risks. Drawing inspiration from the layered, standardized protocols that underpin modern telecommunications, we propose the LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer architecture designed to ensure semantic clarity in communication, transactional integrity for complex tasks, and robust, built-in security. In this position paper, we argue that adopting a principled, universal protocol is not merely beneficial but essential for realizing the potential of distributed AI. Such a standard is critical for ensuring that multi-agent systems can operate safely and reliably in the complex, real-time applications envisioned for 6G and beyond."
2510.13823,"This work presents a simulator designed for the validation, evaluation, and demonstration of flying adhoc networks (FANETs) using 5G vehicle-to-everything (V2X) communications and the named-data networking (NDN) paradigm. The simulator integrates the ns-3 network simulator and the Zenoh NDN protocol, enabling realistic testing of applications that involve the multi-hop communication among multiple unmanned aerial vehicles (UAVs)."
2510.14111,"This paper introduces a novel framework for high-accuracy outdoor user equipment (UE) positioning that applies a conditional generative diffusion model directly to high-dimensional massive MIMO channel state information (CSI). Traditional fingerprinting methods struggle to scale to large, dynamic outdoor environments and require dense, impractical data surveys. To overcome these limitations, our approach learns a direct mapping from raw uplink Sounding Reference Signal (SRS) fingerprints to continuous geographic coordinates. We demonstrate that our DiffLoc framework achieves unprecedented sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic, ray-traced Tokyo urban macro-cell environment. This represents an order-of-magnitude improvement over existing methods, including supervised regression approaches (over 10 m error) and grid-based fusion (3 m error). Our consistency training approach reduces inference time from 200 steps to just 2 steps while maintaining exceptional accuracy even for high-speed users (15-25 m/s) and unseen user trajectories, demonstrating the practical feasibility of our framework for real-time 6G applications."
2510.14214,"In 5G networks, base station (BS) disaggregation and new services present challenges in radio access network (RAN) configuration, particularly in meeting their bandwidth and latency constraints. The BS disaggregation is enabled by functional splitting (FS), which distributes the RAN functions in processing nodes and alleviates latency and bandwidth requirements in the fronthaul (FH). Besides network performance, energy consumption is a critical concern for mobile network operators (MNO), since RAN operation constitutes a major portion of their operational expenses (OPEX). RAN configuration optimization is essential to balance service performance with cost-effective energy consumption. In this paper, we propose a mixed-integer linear programming (MILP) model formulated with three objective functions: (i) minimizing fronthaul (FH) latency, (ii) minimizing energy consumption, and (iii) a bi-objective optimization that jointly balances both latency and energy consumption. The model determines the optimal FS option, RAN function placement, and routing for eMBB, URLLC, and mMTC slices. Although prior studies have addressed RAN configuration either from an energy minimization or latency reduction perspective, few have considered both aspects in realistic scenarios. Our evaluation spans different topologies, accounts for variations in aggregated gNB demand, explores diverse FS combinations, and incorporates Time Sensitive Networking (TSN) modeling for latency analysis, as it is also crucial in RAN performance. Given that MILP's execution time can be significant, we propose a heuristic algorithm that adheres to RAN constraints. Our results reveal a trade-off between latency and energy consumption, highlighting the need for dynamic RAN reconfiguration. These insights provide a foundation to optimize existing and future RAN deployments."
2510.14348,"Mobile telecommunication networks are foundational to global infrastructure and increasingly support critical sectors such as manufacturing, transportation, and healthcare. The security and reliability of these networks are essential, yet depend heavily on accurate modeling of underlying protocols through state machines. While most prior work constructs such models manually from 3GPP specifications, this process is labor-intensive, error-prone, and difficult to maintain due to the complexity and frequent updates of the specifications. Recent efforts using natural language processing have shown promise, but remain limited in handling the scale and intricacy of cellular protocols. In this work, we propose SpecGPT, a novel framework that leverages large language models (LLMs) to automatically extract protocol state machines from 3GPP documents. SpecGPT segments technical specifications into meaningful paragraphs, applies domain-informed prompting with chain-of-thought reasoning, and employs ensemble methods to enhance output reliability. We evaluate SpecGPT on three representative 5G protocols (NAS, NGAP, and PFCP) using manually annotated ground truth, and show that it outperforms existing approaches, demonstrating the effectiveness of LLMs for protocol modeling at scale."
2510.15109,"In emerging networked systems, mobile edge devices such as ground vehicles and unmanned aerial system (UAS) swarms collectively aggregate vast amounts of data to make machine learning decisions such as threat detection in remote, dynamic, and infrastructure-constrained environments where power and bandwidth are scarce. Federated learning (FL) addresses these constraints and privacy concerns by enabling nodes to share local model weights for deep neural networks instead of raw data, facilitating more reliable decision-making than individual learning. However, conventional FL relies on a central server to coordinate model updates in each learning round, which imposes significant computational burdens on the central node and may not be feasible due to the connectivity constraints. By eliminating dependence on a central server, distributed federated learning (DFL) offers scalability, resilience to node failures, learning robustness, and more effective defense strategies. Despite these advantages, DFL remains vulnerable to increasingly advanced and stealthy cyberattacks. In this paper, we design sophisticated targeted training data poisoning and backdoor (Trojan) attacks, and characterize the emerging vulnerabilities in a vehicular network. We analyze how DFL provides resilience against such attacks compared to individual learning and present effective defense mechanisms to further strengthen DFL against the emerging cyber threats."
2510.1521,"This paper focuses on intelligent routing in microservice systems and proposes an end-to-end optimization framework based on graph neural networks. The goal is to improve routing decision efficiency and overall system performance under complex topologies. The method models invocation relationships among microservices as a graph. In this graph, service nodes and communication links are treated as graph nodes and edges. Multi-dimensional features such as node states, link latency, and call frequency are used as input. A multi-layer graph neural network is employed to perform high-order information aggregation and structural modeling. The model outputs a score for each candidate service path. These scores are then used to guide dynamic routing decisions. To improve the model's ability to assess path quality, an edge-aware attention mechanism is introduced. This mechanism helps the model capture instability and bottleneck risks in service communications more accurately. The paper also conducts a systematic analysis of the model's performance under different network depths, topology densities, and service scales. It evaluates the effectiveness of the method in terms of routing accuracy, prediction error, and system stability. Experimental results show that the proposed method outperforms existing mainstream strategies across multiple key metrics. It handles highly dynamic and concurrent microservice environments effectively and demonstrates strong performance, robustness, and structural generalization."
2510.15373,"The ubiquity of smartphones has fueled content consumption worldwide, leading to an ever-increasing demand for a better Internet experience. This has necessitated an upgrade of the capacity of the access network. The Internet service providers (ISPs) have been demanding that the content providers (CPs) share the cost of upgrading access network infrastructure. A \emph{public investment} in the infrastructure of a neutral ISP will boost the profit of the CPs, and hence, seems a rational strategy. A CP can also make a \emph{private investment} in its infrastructure and boost its profits. In this paper, we study the trade-off between public and private investments by a CP when the decision is made under different types of interaction between them. Specifically, we consider four interaction models between CPs -- centralized allocation, cooperative game, non-cooperative game, and a bargaining game -- and determine the public and private investment for each model. Via numerical results, we evaluate the impact of different incentive structures on the utility of the CPs. We see that the bargaining game can result in higher public investment than the non-cooperative and centralized models. However, this benefit gets reduced if the CPs are incentivized to invest in private infrastructure."
2510.15802,"Cloud computing and AI workloads are driving unprecedented demand for efficient communication within and across datacenters. However, the coexistence of intra- and inter-datacenter traffic within datacenters plus the disparity between the RTTs of intra- and inter-datacenter networks complicates congestion management and traffic routing. Particularly, faster congestion responses of intra-datacenter traffic causes rate unfairness when competing with slower inter-datacenter flows. Additionally, inter-datacenter messages suffer from slow loss recovery and, thus, require reliability. Existing solutions overlook these challenges and handle inter- and intra-datacenter congestion with separate control loops or at different granularities. We propose Uno, a unified system for both inter- and intra-DC environments that integrates a transport protocol for rapid congestion reaction and fair rate control with a load balancing scheme that combines erasure coding and adaptive routing. Our findings show that Uno significantly improves the completion times of both inter- and intra-DC flows compared to state-of-the-art methods such as Gemini."
2510.16144,"The increasing complexity of Beyond 5G and 6G networks necessitates new paradigms for autonomy and assur- ance. Traditional O-RAN control loops rely heavily on RIC- based orchestration, which centralizes intelligence and exposes the system to risks such as policy conflicts, data drift, and unsafe actions under unforeseen conditions. In this work, we argue that the future of autonomous networks lies in a multi-agentic architecture, where specialized agents collaborate to perform data collection, model training, prediction, policy generation, verification, deployment, and assurance. By replacing tightly- coupled centralized RIC-based workflows with distributed agents, the framework achieves autonomy, resilience, explainability, and system-wide safety. To substantiate this vision, we design and evaluate a traffic steering use case under surge and drift conditions. Results across four KPIs: RRC connected users, IP throughput, PRB utilization, and SINR, demonstrate that a naive predictor-driven deployment improves local KPIs but destabilizes neighbors, whereas the agentic system blocks unsafe policies, preserving global network health. This study highlights multi- agent architectures as a credible foundation for trustworthy AI- driven autonomy in next-generation RANs."
2510.17009,"Industrial Internet of Things (IIoT) promises to revolutionize industrial operations and productions through utilizing Machine-to-Machine (M2M) communications. Since each node in such environments generates various types of data with diverse service requirements, MAC protocol holds crucial importance to ensure efficient delivery. In this context, simple to complex MAC schemes are found in literature. This paper focuses on evaluating the performance of two major techniques ""slot stealing"" and ""packet fragmentation"" for the IIoT; representative protocols SS-MAC and FROG-MAC have been chosen from each category respectively. We conducted realistic simulations for the two protocols using Contiki. Delay and packet loss comparison for SS-MAC and FROG-MAC indicates the superiority of FROG-MAC due to reduction in the waiting time for urgent traffic. Thus, a simple fragmentation scheme could be deployed for efficient scheduling of heterogenous traffic in the industrial environments."
2510.17147,"Transformer-based large language models (LLMs) are increasingly being adopted in networking research to address domain-specific challenges. However, their quadratic time complexity and substantial model sizes often result in significant computational overhead and memory constraints, particularly in resource-constrained environments. Drawing inspiration from the efficiency and performance of the Deepseek-R1 model within the knowledge distillation paradigm, this paper introduces Mamba4Net, a novel cross-architecture distillation framework. Mamba4Net transfers networking-specific knowledge from transformer-based LLMs to student models built on the Mamba architecture, which features linear time complexity. This design substantially enhances computational efficiency compared to the quadratic complexity of transformer-based models, while the reduced model size further minimizes computational demands, improving overall performance and resource utilization. To evaluate its effectiveness, Mamba4Net was tested across three diverse networking tasks: viewport prediction, adaptive bitrate streaming, and cluster job scheduling. Compared to existing methods that do not leverage LLMs, Mamba4Net demonstrates superior task performance. Furthermore, relative to direct applications of transformer-based LLMs, it achieves significant efficiency gains, including a throughput 3.96 times higher and a storage footprint of only 5.48% of that required by previous LLM-based approaches. These results highlight Mamba4Net's potential to enable the cost-effective application of LLM-derived knowledge in networking contexts. The source code is openly available to support further research and development."
2510.17342,"Accurate positioning is a key enabler for emerging 5G applications. While the standardized Location Management Function (LMF) operates centrally within the core network, its scalability and latency limitations hinder low-latency and fine-grained localization. A practical alternative is to shift positioning intelligence toward the radio access network (RAN), where uplink sounding reference signal (SRS)-based angle-of-arrival (AoA) estimation offers a lightweight, network-native solution. In this work, we present the first fully open-source 5G testbed for AoA estimation, enabling systematic and repeatable experimentation under realistic yet controllable channel conditions. The framework integrates the NVIDIA Sionna RT with a Keysight PROPSIM channel emulator and includes a novel phase calibration procedure for USRP N310 devices. Experimental results show sub-degree to few-degree accuracy, validating the feasibility of lightweight, single-anchor, network-native localization within next-generation 5G systems."
2510.17395,"The emerging road safety and autonomous vehicle applications require timely and reliable data delivery between vehicles and between vehicles and infrastructure. To satisfy this demand, 3GPP develops a 5G Vehicle-to-Everything (V2X) technology. Depending on the served traffic type, 5G V2X specifications propose two channel access methods: (i) Mode 1, according to which a base station allocates resources to users, and (ii) Mode 2, according to which users autonomously select resources for their transmissions. In the paper, we consider a scenario with sporadic traffic, e.g., a vehicle generates a packet at a random time moment when it detects a dangerous situation, which imposes strict requirements on delay and reliability. To satisfy strict delay requirements, vehicles use Mode 2. We analyze the performance of Mode 2 for sporadic traffic and propose several approaches to improve it. Simulation results show that the proposed approaches can increase the system capacity by up to 40% with a low impact on complexity."
2510.1741,"5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to support inter-vehicle communication. In contrast to 4G V2X which allows only broadcast communication, 5G V2X enables groupcast and unicast communication. Such types of communication are needed for new V2X scenarios: platooning, extended sensors, remote driving, etc. To improve the data transmission reliability and assist in the selection of the transmission parameters in these scenarios, 5G V2X introduces a feedback channel that allows receivers to send acknowledgments in response to data packets. However, some part of the overall resource shall be allocated for the feedback channel, which reduces the amount of channel resources available for data transmission. In this paper, we consider a scenario with a platoon, which generates groupcast traffic, and surrounding vehicles, which generate legacy broadcast traffic. Using extensive simulations in NS-3, we analyze how the usage of the feedback channel influences the overall system capacity. Our results show that depending on the platoon size, groupcast, and broadcast traffic intensities, and their quality of service requirements, the usage of the feedback channel can in some cases significantly increase the system capacity (up to 2x), while in other cases it almost halves the system capacity. We explain the reasons for such effects and discuss how to adaptively select the feedback channel parameters."
2510.17445,"A major bottleneck in uplink distributed massive multiple-input multiple-output networks is the sub-optimal performance of local combining schemes, coupled with high fronthaul load and computational cost inherent in centralized large scale fading decoding (LSFD) architectures. This paper introduces a decentralized decoding architecture that fundamentally breaks from the conventional LSFD, by allowing each AP calculates interference-suppressing local weights independently and applies them to its data estimates before transmission. Furthermore, two generalized local zero-forcing (ZF) framework, generalized partial full-pilot ZF (G-PFZF) and generalized protected weak PFZF (G-PWPFZF), are introduced, where each access point (AP) adaptively and independently determines its combining strategy through a local sum spectral efficiency optimization that classifies user equipments (UEs) as strong or weak using only local information, eliminating the fixed thresholds used in PFZF and PWPFZF. To further enhance scalability, pilot-dependent combining vectors instead of user-dependent ones are introduced and are shared among users with the same pilot. The corresponding closed-form spectral efficiency expressions are derived. Numerical results show that the proposed generalized schemes consistently outperform fixed-threshold counterparts, while the introduction of local weights yields lower overhead and computation costs with minimal performance penalty compared to them."
2510.17647,"We analyze the open-loop mechanical tracking performance of a sub-Terahertz (sub-THz) and Terahertz (THz) uplink communication system. These high-frequency bands enable multi-gigabit links through large bandwidths and narrow beams, but require precise pointing to overcome spreading loss. A tracking system can be used to orient horn antennas toward mobile targets. We develop a mathematical model that captures the mechanical dynamics of a real tracking system, which includes motion latency and acceleration and velocity limits, to quantify pointing errors during satellite passes and integrate these effects into the link budget. We evaluate the trade-offs between beam directionality and pointing tolerance across different Low Earth Orbit (LEO) satellite trajectories and control strategies. The results link the hardware limitations to the communications performance, providing design guidelines for high-frequency Non-Terrestrial Network (NTN) uplink under practical mechanical constraints."
2510.18058,"We present Broadcast by Balanced Saturation (BBS), a general broadcast algorithm designed to optimize communication efficiency across diverse network topologies. BBS maximizes node utilization, addressing challenges in broadcast operations such as topology constraints, bandwidth limitations, and synchronization overhead, particularly in large-scale systems like supercomputers. The algorithm ensures sustained activity with nodes throughout the broadcast, thereby enhancing data propagation and significantly reducing latency. Through a precise communication cycle, BBS provides a repeatable, streamlined, stepwise broadcasting framework. Simulation results across various topologies demonstrate that the BBS algorithm consistently outperforms common general broadcast algorithms, often by a substantial margin. These findings suggest that BBS is a versatile and robust framework with the potential to redefine broadcast strategies across network topologies."
2510.18285,"We revisit the problem of missing tag identification in RFID networks by making three contributions. Firstly, we quantitatively compare and gauge the existing propositions spanning over a decade on missing tag identification. We show that the expected execution time of the best solution in the literature is $\Theta \left(N+\frac{(1-\alpha)^2(1-\delta)^2}{ \epsilon^2}\right)$, where $\delta$ and $\epsilon$ are parameters quantifying the required identification accuracy, $N$ denotes the number of tags in the system, among which $\alpha N$ tags are missing. Secondly, we analytically establish the expected execution time lower-bound for any missing tag identification algorithm as $\Theta\left(\frac{N}{\log N}+\frac{(1-\delta)^2(1-\alpha)^2}{\epsilon^2 \log \frac{(1-\delta)(1-\alpha)}{\epsilon}}\right)$, thus giving the theoretical performance limit. Thirdly, we develop a novel missing tag identification algorithm by leveraging a tree structure with the expected execution time of $\Theta \left(\frac{\log\log N}{\log N}N+\frac{(1-\alpha)^2(1-\delta)^2}{ \epsilon^2}\right)$, reducing the time overhead by a factor of up to $\log N$ over the best algorithm in the literature. The key technicality in our design is a novel data structure termed as collision-partition tree (CPT), built on a subset of bits in tag pseudo-IDs, leading to more balanced tree structure and reducing the time complexity in parsing the entire tree."
2510.18417,"Open RAN introduces a flexible, cloud-based architecture for the Radio Access Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning (ML)-driven automation across heterogeneous, multi-vendor deployments. While EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI models, explainability alone does not guarantee reliable network operations. In this article, we propose a lightweight verification approach based on interpretable models to validate the behavior of Deep Reinforcement Learning (DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use Decision Tree (DT)-based verifiers to perform near-real-time consistency checks at runtime, which would be otherwise unfeasible with computationally expensive state-of-the-art verifiers. We analyze the landscape of XAI and AI verification, propose a scalable architectural integration, and demonstrate feasibility with a DT-based slice-verifier. We also outline future challenges to ensure trustworthy AI adoption in Open RAN."
2510.1855,"Large Language Models (LLMs) increasingly rely on emerging protocols such as the Model Context Protocol (MCP) to invoke external tools and services. However, current tool routing mechanisms remain fragile because they only consider functional matching between users' queries and tools. In practice, user intent expressed through queries can be vague or underspecified, and the actual Quality of Experience (QoE) also depends on external factors such as link latency and server availability that are not captured by semantics alone. To address this challenge, we propose JAUNT, a framework for Joint Alignment of User intent and Network state in QoE-centric Tool routing. JAUNT introduces a dual-view alignment strategy that interprets user intent while employing LLM agents to construct network profiles, mapping numerical performance indicators into the semantic space to guide routing. We further design a benchmark that integrates diverse user request patterns with heterogeneous network states, enabling systematic evaluation of QoE outcomes. Experimental results show that JAUNT significantly improves QoE compared with several baselines, demonstrating the importance of aligning both intent and network state for scalable LLM service orchestration."
2511.0021,"We consider a hybrid LiFi/WiFi network consisting of commercially available equipment, for mobile scenarios, where WiFi backs up communications, through vertical handovers, in case of insufficient LiFi QoS. When QoS requirements in terms of goodput are defined, tools are needed to anticipate the vertical handover relative to what is possible with standard basic mechanisms, which are only based on a complete loss of connectivity. We introduce two such mechanisms, based on signal power level readings and CRC-based packet failure ratio, and evaluate their performance in terms of QoS-outage duration, considering as a benchmark an existing baseline solution based on the detection of a connectivity loss. In doing this, we provide insights into the interplay between such mechanisms and the LiFi protocol channel adaptation capabilities. Our experimental results are obtained using a lab-scale testbed equipped with a conveyor belt, which allows us to accurately replicate experiments with devices in motion. With the proposed methods, we achieve QoS outages below one second for a QoS level of 20 Mbps, compared to outage durations of a few seconds obtained with the baseline solution."
2511.0107,"While 5G is being deployed worldwide, 6G is receiving increasing attention from researchers to meet the growing demand for higher data rates, lower latency, higher density, and seamless communications worldwide. To meet the stringent requirements of 6G wireless communications networks, AI-integrated communications have become an indispensable part of supporting 6G systems with intelligence, automation, and big data training capabilities. However, traditional artificial intelligence (AI) systems are difficult to meet the stringent latency and high throughput requirements of 6G with limited resources. In this article, we summarize, analyze, discuss the potential, and benefits of Quantum Reinforcement Learning (QRL) in 6G. As an example, we show the superiority of QRL in dynamic spectrum access compared to the conventional Deep Reinforcement Learning (DRL) approach. In addition, we provide an overview of what DRL has accomplished in 6G and its challenges and limitations. From there, we introduce QRL and potential research directions that should continue to be of interest in 6G. To the best of our knowledge, this is the first review and vision article on QRL for 6G wireless communication networks."
2511.0116,"In this paper, we establish a multi-access edge computing (MEC)-enabled sea lane monitoring network (MSLMN) architecture with energy harvesting (EH) to support dynamic ship tracking, accident forensics, and anti-fouling through real-time maritime traffic scene monitoring. Under this architecture, the computation offloading and resource allocation are jointly optimized to maximize the long-term average throughput of MSLMN. Due to the dynamic environment and unavailable future network information, we employ the Lyapunov optimization technique to tackle the optimization problem with large state and action spaces and formulate a stochastic optimization program subject to queue stability and energy consumption constraints. We transform the formulated problem into a deterministic one and decouple the temporal and spatial variables to obtain asymptotically optimal solutions. Under the premise of queue stability, we develop a joint computation offloading and resource allocation (JCORA) algorithm to maximize the long-term average throughput by optimizing task offloading, subchannel allocation, computing resource allocation, and task migration decisions. Simulation results demonstrate the effectiveness of the proposed scheme over existing approaches."
2511.00196,"Next-generation services demand stringent Quality of Service (QoS) guarantees, such as per-flow bandwidth assurance, ultra-low latency, and traffic prioritization, posing significant challenges to 5G and beyond networks. As 5G network functions increasingly migrate to edge and central clouds, the transport layer becomes a critical enabler of end-to-end QoS compliance. However, traditional fixed-function infrastructure lacks the flexibility to support the diverse and dynamic QoS profiles standardized by 3GPP.This paper presents a QoS-aware data plane model for programmable transport networks, designed to provide predictable behavior and fine-grained service differentiation. The model supports all 3GPP QoS resource types and integrates per-flow metering, classification, strict priority scheduling, and delay-aware queuing. Implemented on off-the-shelf programmable hardware using P4 and evaluated on an Intel Tofino switch, our approach ensures per-flow bandwidth guarantees, sub-millisecond delay for delay-critical traffic, and resilience under congestion. Experimental results demonstrate that the model achieves microsecond-level latencies and near-zero packet loss for mission-critical flows, validating its suitability for future QoS-sensitive applications in 5G and beyond."
2511.00271,"The rapid growth of the Internet of Things (IoT) offers new opportunities but also expands the attack surface of distributed, resource-limited devices. Intrusion detection in such environments is difficult due to data heterogeneity from diverse sensing modalities and the non-IID distribution of samples across clients. Federated Learning (FL) provides a privacy-preserving alternative to centralized training, yet conventional frameworks struggle under these conditions. To address this, we propose a Mist-assisted hierarchical framework for IoT intrusion detection. The architecture spans four layers: (i) Mist, where raw data are abstracted into a unified feature space and lightweight models detect anomalies; (ii) Edge, which applies utility-based client selection; (iii) Fog, where multiple regional aggregators use FedProx to stabilize training; and (iv) Cloud, which consolidates and disseminates global models. Evaluations on the TON-IoT dataset show the framework achieves 98-99% accuracy, PR-AUC> 0.97, and stable convergence under heterogeneous and large-scale settings, while maintaining efficiency and preserving privacy."
2511.00276,"The exponential growth of Internet of Things (IoT) devices, smart vehicles, and latency-sensitive applications has created an urgent demand for efficient distributed computing paradigms. Multi-Fog Computing (MFC), as an extension of fog and edge computing, deploys multiple fog nodes near end users to reduce latency, enhance scalability, and ensure Quality of Service (QoS). However, resource allocation in MFC environments is highly challenging due to dynamic vehicular mobility, heterogeneous resources, and fluctuating workloads. Traditional optimization-based methods often fail to adapt to such dynamics. Reinforcement Learning (RL), as a model-free decision-making framework, enables adaptive task allocation by continuously interacting with the environment. This paper formulates the resource allocation problem in MFC as a Markov Decision Process (MDP) and investigates the application of RL algorithms such as Q-learning, Deep Q-Networks (DQN), and Actor-Critic. We present experimental results demonstrating improvements in latency, workload balance, and task success rate. The contributions and novelty of this study are also discussed, highlighting the role of RL in addressing emerging vehicular computing challenges."
2511.00439,"The evolution of wireless networks and radio access technologies (RATs) has transformed communication from user-driven traffic into a dynamic ecosystem of autonomous systems, including IoT devices, edge nodes, autonomous vehicles, AR/XR clients, and AI-powered agents. These systems exhibit diverse traffic patterns, latency requirements, and mobility behaviors, increasingly operating across overlapping heterogeneous RATs such as 5G, WiFi, satellite, NB-IoT, LoRaWAN, Zigbee, etc. This multi-RAT coexistence creates opportunities for intelligent access, mobility, and routing strategies. However, most mobility decisions still rely heavily on RSSI, which neglects RAT-specific features, congestion, queuing delays, and application needs, favoring high-power links over optimal ones. To address this gap, we propose chrome (Congestion-aware Offloading and Handover via Empirical RAT Evaluation), a multi criteria framework for dense multi-RAT networks. chrome enhances RSSI with multiple criteria and applies the Technique for Order of Preference by Similarity to the Ideal Solution (TOPSIS) to rank available RATs. Criteria weights are determined using both subjective (operator-driven) and objective (measurement-based) approaches. Based on this ranking, chrome performs intelligent cross-RAT offloading to reduce congestion on over-utilized links. We evaluate chrome in a dense SDN-controlled 5G/WiFi Multi-RAT environment using Mininet WiFi. Compared to RSSI-only handover, COHERE reduces the load on the congested RAT by up to 32%, reduces total handovers by 25%, lowers handovers to the congested RAT by 55%, and improves link delay by up to 166%, while maintaining comparable or up to 11% higher throughput. These results demonstrate that guarded, multi-criteria decision-making can exploit RAT coexistence to deliver robust, congestion-aware performance across heterogeneous deployments."
2511.00502,"The extremely short wavelength of terahertz (THz) communications leads to an extended radiative near-field region, in which some canonical far-field assumptions fail. Existing near-field boundary formulations (Fraunhofer distance) for uniform linear/planar array (ULA/UPA) configurations assume ideal alignment between transceivers, overlooking practical misalignments caused by mobility or mechanical imperfections. This paper addresses this critical gap by analyzing the impact of spatial misalignment on near-field distance calculations in THz systems. We derive exact analytical expressions and simplified approximations for the near-field boundary in both ULA--ULA and UPA--UPA configurations under arbitrary misalignment offsets. Through numerical simulations, we validate our theoretical models and quantify how misalignment reshapes the near-field region. These findings provide essential guidelines for optimizing THz system deployment in realistic scenarios."
2511.00569,"With the surging demand for ultra-reliable, low-latency, and ubiquitous connectivity in Sixth-Generation (6G) networks, Non-Terrestrial Networks (NTNs) emerge as a key complement to terrestrial networks by offering flexible access and global coverage. Despite the significant potential, NTNs still face critical challenges, including dynamic propagation environments, energy constraints, and dense interference. As a key 6G technology, Fluid Antennas (FAs) can reshape wireless channels by reconfiguring radiating elements within a limited space, such as their positions and rotations, to provide higher channel diversity and multiplexing gains. Compared to fixed-position antennas, FAs can present a promising integration path for NTNs to mitigate dynamic channel fading and optimize resource allocation. This paper provides a comprehensive review of FA-assisted NTNs. We begin with a brief overview of the classical structure and limitations of existing NTNs, the fundamentals and advantages of FAs, and the basic principles of FA-assisted NTNs. We then investigate the joint optimization solutions, detailing the adjustments of FA configurations, NTN platform motion modes, and resource allocations. We also discuss the combination with other emerging technologies and explore FA-assisted NTNs as a novel network architecture for intelligent function integrations. Furthermore, we delve into the physical layer security and covert communication in FA-assisted NTNs. Finally, we highlight the potential future directions to empower broader applications of FA-assisted NTNs."
2511.00767,"In device-to-device (D2D) communication under a cell with resource sharing mode the spectrum resource utilization of the system will be improved. However, if the interference generated by the D2D user is not controlled, the performance of the entire system and the quality of service (QOS) of the cellular user may be degraded. Power control is important because it helps to reduce interference in the system. In this paper, we propose a reinforcement learning algorithm for adaptive power control that helps reduce interference to increase system throughput. Simulation results show the proposed algorithm has better performance than traditional algorithm in LTE (Long Term Evolution)."
2511.00823,"Blockchain technology facilitates the development of decentralized systems that ensure trust and transparency without the need for expensive centralized intermediaries. However, existing blockchain architectures particularly consortium blockchains face critical challenges related to scalability and efficiency. State sharding has emerged as a promising approach to enhance blockchain scalability and performance. However, current shard-based solutions often struggle to guarantee fair participation and a balanced workload distribution among consortium members. To address these limitations, we propose Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture specifically designed for consortium blockchains. TINC incorporates intelligent mechanisms for adaptive node assignment and dynamic workload balancing, enabling the system to respond effectively to changing network conditions while maintaining equitable shard utilization. By decoupling the control and data planes, TINC allows control nodes to focus on consensus operations, while data nodes handle large-scale storage, thus improving overall resource efficiency. Extensive experimental evaluation and formal analysis demonstrate that TINC significantly outperforms existing shard-based blockchain frameworks. It achieves higher throughput, lower latency, balanced node and transaction distributions, and reduced transaction failure rates. Furthermore, TINC maintains essential blockchain security guarantees, exhibiting resilience against Byzantine faults and dynamic network environments. The integration of Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and security management within the consortium network."
2511.00906,"Passive monitoring is a network measurement technique which analyzes the traffic carried by an operational network. It has several applications for traffic engineering, Quality of Experience monitoring and cyber security. However, it entails the processing of personal information, thus, threatening users' privacy. In this work, we propose DPMon, a tool to run privacy-preserving queries to a dataset of passive network measurements. It exploits differential privacy to perturb the output of the query to preserve users' privacy. DPMon can exploit big data infrastructures running Apache Spark and operate on different data formats. We show that DPMon allows extracting meaningful insights from the data, while at the same time controlling the amount of disclosed information."
2511.00955,"The proliferation of IoT devices in smart cities challenges 6G networks with conflicting energy-latency requirements across heterogeneous slices. Existing approaches struggle with the energy-latency trade-off, particularly for massive scale deployments exceeding 50,000 devices km. This paper proposes an edge-aware CyberTwin framework integrating hybrid federated learning for energy-latency co-optimization in 6G network slicing. Our approach combines centralized Artificial Intelligence scheduling for latency-sensitive slices with distributed federated learning for non-critical slices, enhanced by compressive sensing-based digital twins and renewable energy-aware resource allocation. The hybrid scheduler leverages a three-tier architecture with Physical Unclonable Function (PUF) based security attestation achieving 99.7% attack detection accuracy. Comprehensive simulations demonstrate 52% energy reduction for non-real-time slices compared to Diffusion-Reinforcement Learning baselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA compliance. The framework scales to 50,000 devices km with CPU overhead below 25%, validated through NS-3 hybrid simulations across realistic smart city scenarios."
2511.00965,"Contour detection in Wireless Sensor Networks (WSNs) is crucial for tasks like energy saving and network optimization, especially in security and surveillance applications. Coverage holes, where data transmission is not achievable, are a significant issue caused by factors such as energy depletion and physical damage. Traditional methods for detecting these holes often suffer from inaccuracy, low processing speed, and high energy consumption, relying heavily on physical information like node coordinates and sensing range. To address these challenges, we propose a novel, coordinate-free coverage hole detection method using Connected Component Labeling (CCL) and Force-Directed (FD) algorithms, termed FD-CCL. This method does not require node coordinates or sensing range information. We also investigate Suzuki's Contour Tracing (CT) algorithm and compare its performance with CCL on various FD graphs. Our experiments demonstrate the effectiveness of FD-CCL in terms of processing time and accuracy. Simulation results confirm the superiority of FD-CCL in detecting and locating coverage holes in WSNs."
2511.01074,"The goal of quantum network tomography (QNT) is the characterization of internal quantum channels in a quantum network from external peripheral operations. Prior research has primarily focused on star networks featuring bit-flip and depolarizing channels, leaving the broader problem -- such as QNT for networks with arbitrary topologies and general Pauli channels -- largely unexplored. Moreover, establishing channel identifiability remains a significant challenge even in simplified quantum star networks.In the first part of this paper, we introduce a novel network tomography method, termed Mergecast, in quantum networks. We demonstrate that Mergecast, together with a progressive etching procedure, enables the unique identification of all internal quantum channels in networks characterized by arbitrary topologies and Pauli channels. As a side contribution, we introduce a subclass of Pauli channels, termed bypassable Pauli channels, and propose a more efficient unicast-based tomography method, called BypassUnicast, for networks exclusively comprising these channels. In the second part, we extend our investigation to a more realistic QNT scenario that incorporates state preparation and measurement (SPAM) errors. We rigorously formulate SPAM errors in QNT, propose estimation protocols for such errors within QNT, and subsequently adapt our Mergecast approaches to handle networks affected by SPAM errors. Lastly, we conduct NetSquid-based simulations to corroborate the effectiveness of our proposed protocols in identifying internal quantum channels and estimating SPAM errors in quantum networks. In particular, we demonstrate that Mergecast maintains good performance under realistic conditions, such as photon loss and quantum memory decoherence."
2511.01373,"The integration of reconfigurable intelligent surfaces (RIS) and fluid antenna systems (FAS) has attracted considerable attention due to its tremendous potential in enhancing wireless communication performance. However, under fast-fading channel conditions, rapidly and effectively performing joint optimization of the antenna positions in an FAS system and the RIS phase configuration remains a critical challenge. Traditional optimization methods typically rely on complex iterative computations, thus making it challenging to obtain optimal solutions in real time within dynamic channel environments. To address this issue, this paper introduces a field information-driven optimization method based on three-dimensional Gaussian radiation-field modeling for real-time optimization of integrated FAS-RIS systems. In the proposed approach, obstacles are treated as virtual transmitters and, by separately learning the amplitude and phase variations, the model can quickly generate high-precision channel information based on the transmitter's position. This design eliminates the need for extensive pilot overhead and cumbersome computations. On this framework, an alternating optimization scheme is presented to jointly optimize the FAS position and the RIS phase configuration. Simulation results demonstrate that the proposed method significantly outperforms existing approaches in terms of spectrum prediction accuracy, convergence speed, and minimum achievable rate, validating its effectiveness and practicality in fast-fading scenarios."
2511.01886,"This work studies two types of computer networking models. The primary focus is to understand the different dynamical phenomena observed in practice due to the presence of severe nonlinearities, delays and widely varying operating conditions. The first models considered are of senders running TCP (Transmission Control Protocol) and traffic passing through RED (Random Early Detection) gateways. Building on earlier work, a first order nonlinear discrete-time model is developed for the interaction scenario between transport protocols like TCP and UDP (User Datagram Protocol) and Active Queuing Management schemes like RED. It is shown that the dynamics resulting from the interaction with TCP is consistent with various dynamical behaviors and parameter sensitivities observed in practice. Using bifurcation-theoretic ideas it is shown that TCP-RED type networks may lose their stability through a period doubling bifurcation followed by border collision bifurcations. The nonlinear dependence of the throughput function of TCP-type flows on drop probability is found to be responsible for the period doubling bifurcation, whereas limited buffer space and lack of sufficient damping results in border collision bifurcations. A second class of models studied in this work deals with optimal rate control in networks and are based on the rate-control framework proposed by Kelly. Using the results on delay-differential equation stability, the stability and its lack thereof is studied through an underlying map which arises naturally in time delay systems. An invariance property of this map is used to prove delay-independent stability and to compute bounds on periodic oscillations."
2511.01989,"The sixth generation (6G) of wireless networks will require fundamentally new orchestration paradigms to meet stringent requirements for ultra-low latency, high reliability, and pervasive intelligence. Network slicing emerges as a key enabler to support diverse services with customized quality-of-service (QoS) guarantees. However, dynamic and fine-grained slice management poses significant challenges in terms of real-time provisioning, SLA assurance, and cross-layer observability. In this paper, we propose a novel Digital Twin as a Service (DTaaS) framework that embeds per-slice digital twins (SDTs) into the orchestration loop. Each SDT maintains a synchronized, real-time representation of its slice, leveraging multi-domain telemetry and deep sequential models to predict traffic evolution and SLA risks. The framework introduces modular intelligence layers, programmable interfaces, and edge-embedded decision-making to enable proactive provisioning, adaptive scaling, and closed-loop SLA assurance. Mathematical formulations for fidelity measurement, predictive control, and optimization objectives are provided to ensure rigor and transparency. Evaluation results demonstrate that DTaaS significantly improves SLA compliance ratio, reduces resource over-provisioning, and lowers average SLA violation probability, offering a scalable and reliable orchestration approach for 6G networks."
2511.02171,"The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation authorities to propose distributed Uncrewed Traffic Management (UTM) architectures. Several studies have advocated blockchain as a promising technology to meet these requirements. However, since UTM is a safety-critical and highly regulated domain, compliance with standards and regulatory frameworks is as crucial as performance and security. This work benchmarks two distributed architectures aligned with current regulatory frameworks: the Linux Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger. Our findings reveal that blockchain-based systems require architectures specifically designed for aeronautical performance constraints."
2511.02368,"In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative sensing system where UAVs are deployed to sense multiple targets in terrain-aware line of sight (LoS) conditions in uneven terrain equipped with directional antennas. To mitigate terrain-induced LoS blockages that degrade detection performance, we incorporate a binary LoS indicator and propose a bounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS evaluation. We formulate a bi-objective problem that maximizes the probability of cooperative detection with minimal hover energy constraints governing spatial, orientational, and safety constraints. To address the problem, which is inherently non-convex, we propose a hierarchical heuristic framework that combines exploration through a genetic algorithm (GA) with per-UAV refinement via particle swarm optimization (PSO), where a penalty-based fitness evaluation guides solutions toward feasibility, bounded within constraints. The proposed methodology is an effective trade-off method of traversing through a complex search space and maintaining terrain-aware LoS connectivity and energy aware deployment. Monte Carlo simulations on real-world terrain data show that the proposed GA+PSO framework improves detection probability by 37.02% and 36.5% for 2 and 3 UAVs, respectively, while reducing average excess hover energy by 45.0% and 48.9% compared to the PSO-only baseline. Relative to the non-optimized scheme, it further achieves 59.5% and 54.2% higher detection probability with 59.8% and 65.9% lower excess hover energy, thereby showing its effectiveness with a small number of UAVs over uneven terrain."
2511.02501,"Accurately predicting end-to-end network latency is essential for enabling reliable task offloading in real-time edge computing applications. This paper introduces a lightweight latency prediction scheme based on rational modelling that uses features such as frame size, arrival rate, and link utilization, eliminating the need for intrusive active probing. The model achieves state-of-the-art prediction accuracy through extensive experiments and 5-fold cross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference time, offering a substantial trade-off between precision and efficiency compared to traditional regressors and neural networks."
2511.02559,"Existing DNS configuration verification tools face significant issues (e.g., inefficient and lacking support for incremental verification). Inspired by the advancements in recent work of distributed data plane verification and the resemblance be- tween the data plane and DNS configuration, we tackle the challenge of DNS misconfiguration by introducing Janus, a DNS verification tool. Our key insight is that the process of a nameserver handling queries can be transformed into a matching process on a match-action table. With this insight, Janus consists of (1) an efficient data structure for partition query space based on the behaviors, (2) a symbolic execution algorithm that specifies how a single nameserver can efficiently cover all possible queries and ensure the accuracy of verification, (3) a mechanism to support incremental verification with less computational effort. Extensive experiments on real-world datasets (with over 6 million resource records) show that Janus achieves significant speedups, with peak improvements of up to 255.7x and a maximum 6046x reduction in the number of LECs."
2511.02638,"The rapid development and usage of large-scale AI models by mobile users will dominate the traffic load in future communication networks. The advent of AI technology also facilitates a decentralized AI ecosystem where small organizations or even individuals can host AI services. In such scenarios, AI service (models) placement, selection, and request routing decisions are tightly coupled, posing a challenging yet fundamental trade-off between service quality and service latency, especially when considering user mobility. Existing solutions for related problems in mobile edge computing (MEC) and data-intensive networks fall short due to restrictive assumptions about network structure or user mobility. To bridge this gap, we propose a decentralized framework that jointly optimizes AI service placement, selection, and request routing. In the proposed framework, we use traffic tunneling to support user mobility without costly AI service migrations. To account for nonlinear queuing delays, we formulate a nonconvex problem to optimize the trade-off between service quality and end-to-end latency. We derive the node-level KKT conditions and develop a decentralized Frank--Wolfe algorithm with a novel messaging protocol. Numerical evaluations validate the proposed approach and show substantial performance improvements over existing methods."
2511.02692,"System-level simulation is indispensable for developing and testing novel algorithms for 5G and future wireless networks, yet a gap persists between the needs of the machine learning re- search community and the available tooling. To address this, we introduce the Cellular Radio Reference Model (CRRM), an open-source, pure Python simulator we designed specifically for speed, usability, and direct integration with modern AI frameworks. The core scientific contribution of CRRM lies in its architecture, which departs from traditional discrete-event simulation. We model the system as a set of inter-dependent computational blocks which form nodes in a directed graph. This enables a compute-on-demand mechanism we term smart update."
2511.02703,"The rapid increase in connected devices has signifi- cantly intensified the computational and communication demands on modern telecommunication networks. To address these chal- lenges, integrating advanced Machine Learning (ML) techniques like Federated Learning (FL) with emerging paradigms such as Multi-access Edge Computing (MEC) and Software-Defined Wide Area Networks (SD-WANs) is crucial. This paper intro- duces online resource management strategies specifically designed for FL model aggregation, utilizing intermediate aggregation at edge nodes. Our analysis highlights the benefits of incorporating edge aggregators to reduce network link congestion and maximize the potential of edge computing nodes. However, the risk of network congestion persists. To mitigate this, we propose a novel aggregation approach that deploys an aggregator overlay network. We present an Integer Linear Programming (ILP) model and a heuristic algorithm to optimize the routing within this overlay network. Our solution demonstrates improved adapt- ability to network resource utilization, significantly reducing FL training round failure rates by up to 15% while also alleviating cloud link congestion."
2511.02748,"We argue that sixth-generation (6G) intelligence is not fluent token prediction but the capacity to imagine and choose -- to simulate future scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe open radio access network (O-RAN) near-real-time (Near-RT) control via counterfactual dynamics and a world modeling (WM) paradigm that learns an action-conditioned generative state space. This enables quantitative ""what-if"" forecasting beyond large language models (LLMs) as the primary modeling primitive. Actions such as physical resource blocks (PRBs) are treated as first-class control inputs in a causal world model, and both aleatoric and epistemic uncertainty are modeled for prediction and what-if analysis. An agentic, model predictive control (MPC)-based cross-entropy method (CEM) planner operates over short horizons, using prior-mean rollouts within data-driven PRB bounds to maximize a deterministic reward. The model couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories and predicting next-step KPIs under hypothetical PRB sequences. On realistic O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with 32% fewer parameters and similar latency, and achieves 35-80% lower root mean squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster inference, enabling rare-event simulation and offline policy screening."
2511.02952,"Emerging virtualized radio access networks (vRANs) demand flexible and efficient baseband processing across heterogeneous compute substrates. In this paper, we present DecodeX, a unified benchmarking framework for evaluating low-density parity-check (LDPC) decoding acceleration across different hardware platforms. DecodeX integrates a comprehensive suite of LDPC decoder implementations, including kernels, APIs, and test vectors for CPUs (FlexRAN), GPUs (Aerial and Sionna-RK), and ASIC (ACC100), and can be readily extended to additional architectures and configurations. Using DecodeX, we systematically characterize how different platforms orchestrate computation-from threading and memory management to data movement and accelerator offload-and quantify the resulting decoding latency under varying Physical layer parameters. Our observations reveal distinct trade-offs in parallel efficiency and offload overhead, showing that accelerator gains strongly depend on data-movement and workload granularity. Building on these insights, we discuss how cross-platform benchmarking can inform adaptive scheduling and co-design for future heterogeneous vRANs, enabling scalable and energy-efficient baseband processing for NextG wireless systems."
2511.03039,"Incast traffic in data centers can lead to severe performance degradation, such as packet loss and increased latency. Effectively addressing incast requires prompt and accurate detection. Existing solutions, including MA-ECN, BurstRadar and Pulser, typically rely on fixed thresholds of switch port egress queue lengths or their gradients to identify microburst caused by incast flows. However, these queue length related methods often suffer from delayed detection and high error rates. In this study, we propose a distributed incast detection method for data center networks at the switch-level, leveraging a probabilistic hypothesis test with an optimal detection threshold. By analyzing the arrival intervals of new flows, our algorithm can immediately determine if a flow is part of an incast traffic from its initial packet. The experimental results demonstrate that our method offers significant improvements over existing approaches in both detection speed and inference accuracy."
2511.03081,"Sixth-generation (6G) networks are envisioned to support interconnected local subnetworks that can share specialized, beyond-connectivity services. However, a standardized architecture for discovering and selecting these services across network boundaries has not existed yet. To address this gap, this paper introduces the Central Repository and Selection Function (CRSF), a novel network function for the 6G core that facilitates efficient inter-subnetwork service discovery and selection. We formulate the selection process as a QoS-aware optimization problem designed to balance service quality metrics with user-defined priorities. We evaluate our system model through simulations for a sensing service scenario and observe a consistently higher aggregate Quality of Service (QoS) compared to the baseline selection strategy. The proposed CRSF provides a foundational and extensible mechanism for building standardized, collaborative, and service-centric interconnected networks essential for the 6G era."
2511.03116,"Mobility management in cellular networks, especially the handover (HO) process, plays a key role in providing seamless and ubiquitous Internet access. The wide-scale deployment of 5G and the resulting co-existence of 4G/5G in the past six years have significantly changed the landscape of all mobile network operators and made the HO process much more complex than before. While several recent works have studied the impact of HOs on user experience, why and how HOs occur and how HO configurations affect performance in 5G operational networks remains largely unknown. Through four cross-country driving trips across the US spread out over a 27-month period, we conduct an in-depth measurement study of HO configurations across all three major US operators. Our study reveals (a) new types of HOs and new HO events used by operators to handle these new types of HOs, (b) overly aggressive HO configurations that result in unnecessarily high signaling overhead, (c) large diversity in HO configuration parameter values, which also differ across operators, but significantly lower diversity in 5G compared to LTE, and (d) sub-optimal HO configurations/decisions leading to poor pre- or post-HO performance. Our findings have many implications for mobile operators, as they keep fine-tuning their 5G HO configurations."
2511.03159,"Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE remains underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. This raises the pressing issue of jointly deciding request routing and submodel caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an offline algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Furthermore, we develop an online variant of CoCaR, named CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online request patterns. The simulation results demonstrate that the proposed CoCaR improves the average inference precision of user requests by 46\% compared to state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves an improvement of no less than 32.3\% in user QoE over competitive baselines."
2511.03312,"The advent of 5G networks, with network slicing as a cornerstone technology, promises customized, high-performance services, but also introduces novel attack surfaces beyond traditional threats. This article investigates a critical and underexplored integrity vulnerability: the manipulation of network slice allocation to compromise Quality of Service (QoS) and resource integrity. We introduce a threat model, grounded in a risk analysis of permissible yet insecure configurations like null-ciphering (5G-EA0), demonstrating how a rogue gNodeB acting as a Man-in-the-Middle can exploit protocol weaknesses to forge slice requests and hijack a User Equipment's (UE) connection. Through a comprehensive experimental evaluation on a 5G testbed, we demonstrate the attack's versatile and severe impacts. Our findings show this integrity breach can manifest as obvious QoS degradation, such as a 95% bandwidth reduction and 150% latency increase when forcing UE to a suboptimal slice, or as stealthy slice manipulation that is indistinguishable from benign network operation and generates no core network errors. Furthermore, we validate a systemic resource contamination attack where redirecting a crowd of UE orchestrates a Denial-of-Service, causing packet loss to exceed 60% and inducing measurable CPU saturation (~80%) on core network User Plane Functions (UPFs). Based on these results, we discuss the profound implications for Service Level Agreements (SLAs) and critical infrastructure. We propose concrete, cross-layer mitigation strategies for network operators as future work, underscoring the urgent need to secure the integrity of dynamic resource management in 5G networks."
2511.04173,"Wideband and low-latency requirements in sixth-generation (6G) networks demand detectors that approach maximum-likelihood (ML) performance without incurring exponential complexity. This work develops a hybrid quantum-classical detection framework for reconfigurable intelligent surface (RIS)-assisted single-carrier (SC) frequency-domain equalization (FDE) over frequency-selective channels. The ML detection objective is reformulated as a quadratic unconstrained binary optimization (QUBO) problem and solved via Grover adaptive search (GAS). To accelerate convergence, we introduce a frequency-domain MMSE threshold that exploits the circulant structure of SC-FDE channels, yielding low-complexity initialization. The framework is evaluated across varying channel lengths and RIS sizes, confirming robustness and scalability. In addition, GAS requirements are quantified through register widths and gate counts, and its query complexity is analyzed to characterize the algorithm's cost for block transmission in frequency-selective channels. Quantum circuit simulations are conducted in Qiskit under both ideal and noisy conditions. In the ideal case, the detector achieves near-optimal performance while benefiting from Grover's quadratic speedup, reducing the search cost from from O(M^N) exhaustive evaluations to O(SQRT(M^N)) oracle queries. Under noise, the shallow depth of the GAS circuits, aided by MMSE initialization, makes depolarizing errors negligible, while readout errors introduce moderate degradation yet still preserve performance close to the MMSE baseline. These results establish the feasibility of quantum-enhanced detection for RIS-assisted broadband communications, highlighting both algorithmic scalability and practical robustness for 6G networks."
2511.04639,"The rise of distributed AI and large-scale applications has impacted the communication operations of data-center and Supercomputer interconnection networks, leading to dramatic incast or in-network congestion scenarios and challenging existing congestion control mechanisms, such as injection throttling (e.g., DCQCN) or congestion isolation (CI). While DCQCN provides a scalable traffic rate adjustment for congesting flows at end nodes (which is slow) and CI effectively isolates these flows in special network resources (which requires extra logic in the switches), their combined use, although it diminishes their particular drawbacks, leads to false congestion scenarios identification and signaling, excessive throttling, and inefficient network resource utilization. In this paper, we propose a new CI mechanism, called Improved Congestion Isolation (ICI), which efficiently combines CI and DCQCN so that the information of the isolated congesting flows is used to guide the ECN marking performed by DCQCN in a way that victim flows do not end up being marked. This coordination reduces false-positive congestion detection, suppresses unnecessary closed-loop feedback (i.e., wrong congestion notifications), and improves responsiveness to communication microbursts. Evaluated under diverse traffic patterns, including incast and Data-center workloads, ICI reduces the number of generated BECNs by up to 32x and improves tail latency by up to 31%, while maintaining high throughput and scalability."
2511.05022,"PriorityFresh is a semantic, actionability-first caching policy designed for offline emergency warning systems. Within the AWARE system's simulation environment, PriorityFresh optimizes which alerts to retain and surface under constrained connectivity. Experiments indicate improved actionability-first performance without harming efficiency. A separate Priority Forecasting model is used only to synthesize realistic alert sequences for controlled experiments and does not influence caching or push decisions."
2511.05027,"The directional RTS/CTS mechanism of mm-wave Wi-Fi hardly resolves the hidden terminal problem perfectly. This paper proposes cross-link RTS/CTS under multi-link operation (MLO) to address this problem and introduces a novel point process, named the generalized RTS/CTS hard-core process (G-HCP), to model the spatial transceiver relationships under the RTS/CTS mechanism, including the directional case and the omnidirectional case. Analytical expressions are derived for the intensity, the mean interference, an approximation of the success probability, and the expected number of hidden nodes for the directional RTS/CTS mechanism. Theoretical and numerical results demonstrate the performance difference between two RTS/CTS mechanisms. The cross-link RTS/CTS mechanism ensures higher link quality at the cost of reduced network throughput. In contrast, the directional RTS/CTS sacrifices the link quality for higher throughput. Our study reveals a fundamental trade-off between link reliability and network throughput, providing critical insights into the selection and optimization of RTS/CTS mechanisms in next-generation WLAN standards."
2511.05149,"Over the past decade, Supercomputers and Data centers have evolved dramatically to cope with the increasing performance requirements of applications and services, such as scientific computing, generative AI, social networks or cloud services. This evolution have led these systems to incorporate high-speed networks using faster links, end nodes using multiple and dedicated accelerators, or a advancements in memory technologies to bridge the memory bottleneck. The interconnection network is a key element in these systems and it must be thoroughly designed so it is not the bottleneck of the entire system, bearing in mind the countless communication operations that generate current applications and services. Congestion is serious threat that spoils the interconnection network performance, and its effects are even more dramatic when looking at the traffic dynamics and bottlenecks generated by the communication operations mentioned above. In this vein, numerous congestion control (CC) techniques have been developed to address congestion negative effects. One popular example is Data Center Quantized Congestion Notification (DCQCN), which allows congestion detection at network switch buffers, then marking congesting packets and notifying about congestion to the sources, which finally apply injection throttling of those packets contributing to congestion. While DCQCN has been widely studied and improved, its main principles for congestion detection, notification and reaction remain largely unchanged, which is an important shortcoming considering congestion dynamics in current high-performance interconnection networks. In this paper, we revisit the DCQCN closed-loop mechanism and refine its design to leverage a more accurate congestion detection, signaling, and injection throttling, reducing control traffic overhead and avoiding unnecessary throttling of non-congesting flows."
2511.05238,"Radio Environment Map (REM) is transitioning from 5G homogeneous environments to B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL), a natural fit for this distributed task, struggles with performance degradation in accuracy and communication efficiency under the non-independent and identically distributed (Non-IID) data conditions inherent to these new environments. This paper proposes EPFL-REMNet, an efficient personalized federated framework for constructing a high-fidelity digital twin of the 6G heterogeneous radio environment. The proposed EPFL-REMNet employs a""shared backbone + lightweight personalized head"" model, where only the compressed shared backbone is transmitted between the server and clients, while each client's personalized head is maintained locally. We tested EPFL-REMNet by constructing three distinct Non-IID scenarios (light, medium, and heavy) based on radio environment complexity, with data geographically partitioned across 90 clients. Experimental results demonstrate that EPFL-REMNet simultaneously achieves higher digital twin fidelity (accuracy) and lower uplink overhead across all Non-IID settings compared to standard FedAvg and recent state-of-the-art methods. Particularly, it significantly reduces performance disparities across datasets and improves local map accuracy for long-tail clients, enhancing the overall integrity of digital twin."
2511.05334,"In graph theory and its practical networking applications, e.g., telecommunications and transportation, the problem of finding paths has particular importance. Selecting paths requires giving scores to the alternative solutions to drive a choice. While previous studies have provided comprehensive evaluation of single-path solutions, the same level of detail is lacking when considering sets of paths. This paper emphasizes that the path characterization strongly depends on the properties under consideration. While property-based characterization is also valid for single paths, it becomes crucial to analyse multiple path sets. From the above consideration, this paper proposes a mathematical approach, defining a functional model that lends itself well to characterizing the path set in its general formulation. The paper shows how the functional model contextualizes specific attributes."
2511.05362,"With the large increase in the adoption of blockchain technologies, their underlying peer-to-peer networks must also scale with the demand. In this context, previous works highlighted the importance of ensuring efficient and resilient communication for the underlying consensus and replication mechanisms. However, they were mainly focused on mainstream, Proof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.In this paper, the problem is investigated in the context of consensus-validation based blockchains, like the XRP Ledger. The latter relies on a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to have a good scalability in regards to transaction throughput. However, it is known that significant increases in the size of the XRP Ledger network would be challenging to achieve. The main reason is the flooding mechanism used to disseminate the messages related to the consensus protocol, which creates many duplicates in the network. Squelching is a recent solution proposed for limiting this duplication, however, it was never evaluated quantitatively in real-life scenarios involving the XRPL production network. In this paper, our aim is to assess this mechanism using a real-life controllable testbed and the XRPL production network, to assess its benefit and compare it to alternative solutions relying on Named Data Networking and on a gossip-based approach."
2511.05423,"Identifying active IPv6 addresses is challenging. Various methods emerged to master the measurement challenge in this huge address space, including hitlists, new probing techniques, and AI-generated target lists. In this paper, we apply active Subnet-Router anycast (SRA) probing, a commonly unused method to explore the IPv6 address space. We compare our results with lists of active IPv6 nodes obtained from prior methods and with random probing. Our findings indicate that probing an SRA address reveals on average 10% more router IP addresses than random probing and is far less affected by ICMP rate limiting. Compared to targeting router addresses directly, SRA probing discovers 80% more addresses. We conclude that SRA probing is an important addition to the IPv6 measurement toolbox and may improve the stability of results significantly. We also find evidence that some active scans can cause harmful conditions in current IPv6 deployments, which we started to fix in collaboration with network operators."
2511.05789,"The convergence of the Internet of vehicles (IoV) and 6G networks is driving the evolution of next-generation intelligent transportation systems. However, IoV networks face persistent challenges, including low spectral efficiency in vehicular communications, difficulty in achieving dynamic and adaptive resource optimization, and the need for long-term stability under highly dynamic environments. In this paper, we study the problem of digital twin (DT)-assisted task offloading and resource allocation in integrated sensing and communication (ISAC)-enabled IoV networks. The objective is to minimize the long-term average system cost, defined as a weighted combination of delay and energy consumption, while ensuring queue stability over time. To address this, we employ an ISAC-enabled design and introduce two transmission modes (i.e., raw data transmission (DataT) and instruction transmission (InstrT)). The InstrT mode enables instruction-level transmission, thereby reducing data volume and improving spectral efficiency. We then employ Lyapunov optimization to decompose the long-term stochastic problem into per-slot deterministic problems, ensuring long-term queue stability. Building upon this, we propose a Lyapunov-driven DT-enhanced multi-agent proximal policy optimization (Ly-DTMPPO) algorithm, which leverages DT for global state awareness and intelligent decision-making within a centralized training and decentralized execution (CTDE) architecture. Extensive simulations verify that Ly-DTMPPO achieves superior performance compared with existing benchmarks."
2511.05838,"Poor broadband access undermines civic and economic life, a challenge exacerbated by the fact that millions of Americans still lack reliable high-speed connectivity. Federal broadband funding initiatives aim to address these gaps, but their success depends on accurate availability and affordability data. Existing data, often based on self-reported ISP information, can overstate coverage and speeds, risking misallocation of funds and leaving unserved communities behind. We present BQT+, an AI-agent data collection platform that queries ISP web interfaces by inputting residential street addresses and extracting data on service availability, quality, and pricing. BQT+ has been used in policy evaluation studies, including an independent assessment of broadband availability, speed tiers, and affordability in areas targeted by the $42.45 billion BEAD program."
2511.06001,"In large-scale Internet of things networks, efficient medium access control (MAC) is critical due to the growing number of devices competing for limited communication resources. In this work, we consider a new challenge in which a set of nodes must transmit a set of shared messages to a central controller, without inter-node communication or retransmissions. Messages are distributed among random subsets of nodes, which must implicitly coordinate their transmissions over shared communication opportunities. The objective is to guarantee the delivery of all shared messages, regardless of which nodes transmit them. We first prove the optimality of deterministic strategies, and characterize the success rate degradation of a deterministic strategy under dynamic message-transmission patterns. To solve this problem, we propose a decentralized learning-based framework that enables nodes to autonomously synthesize deterministic transmission strategies aiming to maximize message delivery success, together with an online adaptation mechanism that maintains stable performance in dynamic scenarios. Extensive simulations validate the framework's effectiveness, scalability, and adaptability, demonstrating its robustness to varying network sizes and fast adaptation to dynamic changes in transmission patterns, outperforming existing multi-armed bandit approaches."
2511.07176,"Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm."
2511.07189,"Due to the recent shortage of resources in the healthcare industry, Remote Patient Monitoring (RPM) systems arose to establish a convenient alternative for accessing healthcare services remotely. However, as the usage of this system grows with the increase of patients and sensing devices, data and network management becomes an issue. As a result, wireless architecture challenges in patient privacy, data flow, and service interactability surface that need addressing. We propose a fog-based Internet of Things (IoT) platform to address these issues and reinforce the existing RPM system. The introduced platform can allocate resources to alleviate server overloading and provide an interactive means of monitoring patients through speech recognition. We designed a testbed to simulate and test the platform in terms of accuracy, latency, and throughput. The results show the platform's potential as a viable RPM system for sound-based healthcare services."
2511.07265,"The exponential growth of AI agents and connected devices fundamentally transforms the structure and capacity demands of global digital infrastructure. This paper introduces a unified forecasting model that projects AI agent populations to increase by more than 100 times between 2026 and 2036+, reaching trillions of instances globally. In parallel, bandwidth demand is expected to surge from 1 EB/day in 2026 to over 8,000 EB/day by 2036, which is an increase of 8000 times in a single decade. Through this growth model, we identify critical bottleneck domains across access networks, edge gateways, interconnection exchanges, and cloud infrastructures. Simulations reveal that edge and peering systems will experience saturation as early as 2030, with more than 70% utilization of projected maximum capacity by 2033. To address these constraints, we propose a coevolutionary shift in compute-network design, emphasizing distributed inference, AI-native traffic engineering, and intent-aware orchestration. Security, scalability, and coordination challenges are examined with a focus on sustaining intelligent connectivity throughout the next digital decade."
