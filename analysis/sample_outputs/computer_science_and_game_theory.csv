paper_id,abstract
2501.00191,"We study a networked economic system composed of $n$ producers supplying a single homogeneous good to a number of geographically separated markets and of a centralized authority, called the market maker. Producers compete à la Cournot, by choosing the quantities of good to supply to each market they have access to in order to maximize their profit. Every market is characterized by its inverse demand functions returning the unit price of the considered good as a function of the total available quantity. Markets are interconnected by a dispatch network through which quantities of the considered good can flow within finite capacity constraints. Such flows are determined by the market maker, who aims at maximizing a designated welfare function. We model such competition as a strategic game with $n+1$ players: the producers and the market game. For this game, we first establish the existence of Nash equilibria under standard concavity assumptions. We then identify sufficient conditions for the game to be potential with an essentially unique Nash equilibrium. Next, we present a general result that connects the optimal action of the market maker with the capacity constraints imposed on the network. For the commonly used Walrasian welfare, our finding proves a connection between capacity bottlenecks in the market network and the emergence of price differences between markets separated by saturated lines. This phenomenon is frequently observed in real-world scenarios, for instance in power networks. Finally, we validate the model with data from the Italian day-ahead electricity market."
2501.0086,"The successive and the amendment procedures have been widely employed in parliamentary and legislative decision making and have undergone extensive study in the literature from various perspectives. However, investigating them through the lens of computational complexity theory has not been as thoroughly conducted as for many other prevalent voting procedures heretofore. To the best of our knowledge, there is only one paper which explores the complexity of several strategic voting problems under these two procedures, prior to our current work. To provide a better understanding of to what extent the two procedures resist strategic behavior, we study the parameterized complexity of constructive/destructive control by adding/deleting voters/candidates for both procedures. To enhance the generalizability of our results, we also examine a more generalized form of the amendment procedure. Our exploration yields a comprehensive (parameterized) complexity landscape of these problems with respect to numerous parameters."
2501.00976,"In this paper, we study non-obvious manipulability (NOM), a relaxed form of strategyproofness, in the context of Hedonic Games (HGs) with Friends Appreciation (FA) preferences. In HGs, the aim is to partition agents into coalitions according to their preferences which solely depend on the coalition they are assigned to. Under FA preferences, agents consider any other agent either a friend or an enemy, preferring coalitions with more friends and, in case of ties, the ones with fewer enemies. Our goal is to design mechanisms that prevent manipulations while optimizing social welfare.Prior research established that computing a welfare maximizing (optimum) partition for FA preferences is not strategyproof, and the best-known approximation to the optimum subject to strategyproofness is linear in the number of agents. In this work, we explore NOM to improve approximation results. We first prove the existence of a NOM mechanism that always outputs the optimum; however, we also demonstrate that the computation of an optimal partition is NP-hard. To address this complexity, we focus on approximation mechanisms and propose a NOM mechanism guaranteeing a $(4+o(1))$-approximation in polynomial time.Finally, we briefly discuss NOM in the case of Enemies Aversion (EA) preferences, the counterpart of FA, where agents give priority to coalitions with fewer enemies and show that no mechanism computing the optimum can be NOM."
2501.01111,"Mechanism design in resource allocation studies dividing limited resources among self-interested agents whose satisfaction with the allocation depends on privately held utilities. We consider the problem in a payment-free setting, with the aim of maximizing social welfare while enforcing incentive compatibility (IC), i.e., agents cannot inflate allocations by misreporting their utilities. The well-known proportional fairness (PF) mechanism achieves the maximum possible social welfare but incurs an undesirably high exploitability (the maximum unilateral inflation in utility from misreport and a measure of deviation from IC). In fact, it is known that no mechanism can achieve the maximum social welfare and exact incentive compatibility (IC) simultaneously without the use of monetary incentives (Cole et al., 2013). Motivated by this fact, we propose learning an approximate mechanism that desirably trades off the competing objectives. Our main contribution is to design an innovative neural network architecture tailored to the resource allocation problem, which we name Regularized Proportional Fairness Network (RPF-Net). RPF-Net regularizes the output of the PF mechanism by a learned function approximator of the most exploitable allocation, with the aim of reducing the incentive for any agent to misreport. We derive generalization bounds that guarantee the mechanism performance when trained under finite and out-of-distribution samples and experimentally demonstrate the merits of the proposed mechanism compared to the state-of-the-art."
2501.0166,"We adopt a parametric approach to analyze the worst-case degradation in social welfare when the allocation of indivisible goods is constrained to be fair. Specifically, we are concerned with cardinality-constrained allocations, which require that each agent has at most $k$ items in their allocated bundle. We propose the notion of the price of cardinality, which captures the worst-case multiplicative loss of utilitarian or egalitarian social welfare resulting from imposing the cardinality constraint. We then characterize tight or almost-tight bounds on the price of cardinality as exact functions of the instance parameters, demonstrating how the social welfare improves as $k$ is increased. In particular, one of our main results refines and generalizes the existing asymptotic bound on the price of balancedness, as studied by Bei et al. [BLMS21]. We also further extend our analysis to the problem where the items are partitioned into disjoint categories, and each category has its own cardinality constraint. Through a parametric study of the price of cardinality, we provide a framework which aids decision makers in choosing an ideal level of cardinality-based fairness, using their knowledge of the potential loss of utilitarian and egalitarian social welfare."
2501.01969,"In perpetual voting, multiple decisions are made at different moments in time. Taking the history of previous decisions into account allows us to satisfy properties such as proportionality over periods of time. In this paper, we consider the following question: is there a perpetual approval voting method that guarantees that no voter is dissatisfied too many times? We identify a sufficient condition on voter behavior -- which we call 'bounded conflicts' condition -- under which a sublinear growth of dissatisfaction is possible. We provide a tight upper bound on the growth of dissatisfaction under bounded conflicts, using techniques from Kolmogorov complexity. We also observe that the approval voting with binary choices mimics the machine learning setting of prediction with expert advice. This allows us to present a voting method with sublinear guarantees on dissatisfaction under bounded conflicts, based on the standard techniques from prediction with expert advice."
2501.03001,"Decoding how rational agents should behave in shared systems remains a critical challenge within theoretical computer science, artificial intelligence and economics studies. Central to this challenge is the task of computing the solution concept of games, which is Nash equilibrium (NE). Although computing NE in even two-player cases are known to be PPAD-hard, approximation solutions are of intensive interest in the machine learning domain. In this paper, we present a gradient-based approach to obtain approximate NE in N-player general-sum games. Specifically, we define a distance measure to an NE based on pure strategy best response, thereby computing an NE can be effectively transformed into finding the global minimum of this distance function through gradient descent. We prove that the proposed procedure converges to NE with rate $O(1/T)$ ($T$ is the number of iterations) when the utility function is convex. Experimental results suggest our method outperforms Tsaknakis-Spirakis algorithm, fictitious play and regret matching on various types of N-player normal-form games in GAMUT. In addition, our method demonstrates robust performance with increasing number of players and number of actions."
2501.03055,"In congestion games, selfish users behave myopically to crowd to the shortest paths, and the social planner designs mechanisms to regulate such selfish routing through information or payment incentives. However, such mechanism design requires the knowledge of time-varying traffic conditions and it is the users themselves to learn and report past road experiences to the social planner (e.g., Waze or Google Maps). When congestion games meet mobile crowdsourcing, it is critical to incentivize selfish users to explore non-shortest paths in the best exploitation-exploration trade-off. First, we consider a simple but fundamental parallel routing network with one deterministic path and multiple stochastic paths for users with an average arrival probability $\lambda$. We prove that the current myopic routing policy (widely used in Waze and Google Maps) misses both exploration (when strong hazard belief) and exploitation (when weak hazard belief) as compared to the social optimum. Due to the myopic policy's under-exploration, we prove that the caused price of anarchy (PoA) is larger than \(\frac{1}{1-\rho^{\frac{1}{\lambda}}}\), which can be arbitrarily large as discount factor \(\rho\rightarrow1\). To mitigate such huge efficiency loss, we propose a novel selective information disclosure (SID) mechanism: we only reveal the latest traffic information to users when they intend to over-explore stochastic paths upon arrival, while hiding such information when they want to under-explore. We prove that our mechanism successfully reduces PoA to be less than~\(2\). Besides the parallel routing network, we further extend our mechanism and PoA results to any linear path graphs with multiple intermediate nodes."
2501.03141,"Today, many auctions are carried out with the help of intermediary platforms like Google and eBay. We refer to such auctions as platform-assistedthis http URL, the auction theory literature mainly focuses on designing auctions that incentivize the buyers to bid truthfully,assuming that the platform always faithfully implements the auction. In practice, however, the platforms have been found to manipulate the auctions to earn more profit, resulting in high-profile anti-trust lawsuits. We propose a new model for studying platform-assisted auctions in the permissionless setting. We explore whether it is possible to design a dream auction in thisnew model, such that honest behavior is the utility-maximizing strategy for each individual buyer, the platform, the seller, as well as platform-seller or platform-buyerthis http URLa collection of feasibility and infeasibility results,we carefully characterize the mathematical landscape of platform-assisted auctions. We show how cryptography can lend to the design of an efficient platform-assisted auction with dream properties. Although a line of works have also used MPC or the blockchain to remove the reliance on a trusted auctioneer, our work is distinct in nature in severalthis http URL, we initiate a systematic exploration of the game theoretic implications when the service providers are strategic and can collude with sellers or buyers. Second, we observe that the full simulation paradigm is too stringent and leads to high asymptotical costs. Specifically, because every player has a different private outcomein an auction protocol, running any generic MPC protocol among the players would incur at least $n^2$ total cost. We propose a new notion of simulation calledutility-dominatedthis http URLthis new notion, we showhow to design efficient auction protocols with quasilinear efficiency."
2501.04507,"To facilitate responsive and cost-effective computing service delivery over edge networks, this paper investigates a novel two-stage double auction methodology via discovering an interesting idea of resource overbooking to overcome dynamic and uncertain nature of supply of edge servers (sellers) and demand generated from mobile devices (as buyers). The proposed auction integrates multiple essential goals such as maximizing social welfare as well as accelerating the decision-making process from both short-term and long-term views, (e.g., the time for determining winning seller-buyer pairs), by introducing a stagewise strategy: an overbooking-driven pre-double auction (OPDAuction) for determining long-term cooperations between sellers and buyers before practical resource transactions as Stage I, and a real-time backup double auction (RBDAuction) for quickly coping with residual resource demands during actual transactions. In particular, by embedding a proper overbooking rate, OPDAuction helps with facilitating trading contracts between appropriate sellers and buyers as guidance for future transactions, by allowing the booked resources to exceed theoretical supply. Then, since pre-auctions may cause risks, our RBDAuction adjusts to real-time market changes, further enhancing the overall social welfare. More importantly, we offer an interesting view to show that our proposed two-stage auction can support significant design properties such as truthfulness, individual rationality, and budget balance. Through extensive experiments, we demonstrate good performance in social welfare, time efficiency, and computational scalability, outstripping conventional methods in dynamic edge computing settings."
2501.0455,"We consider the computation for allocations of indivisible chores that are approximately EFX and Pareto optimal (PO). Recently, Garg et al. (2024) show the existence of $3$-EFX and PO allocations for bi-valued instances, where the cost of an item to an agent is either $1$ or $k$ (where $k > 1$) by rounding the (fractional) earning restricted equilibrium. In this work, we improve the approximation ratio to $(2-1/k)$, while preserving the Pareto optimality. Instead of rounding fractional equilibrium, our algorithm starts with the integral EF1 equilibrium for bi-valued chores, introduced by Garg et al. (AAAI 2022) and Wu et al. (EC 2023), and reallocates items until approximate EFX is achieved. We further improve our result for the case when $k=2$ and devise an algorithm that computes EFX and PO allocations."
2501.04882,"The growth in the use of online advertising to foster brand awareness over recent years is largely attributable to the ubiquity of social media. One pivotal technology contributing to the success of online brand advertising is frequency capping, a mechanism that enables marketers to control the number of times an ad is shown to a specific user. However, the very foundation of this technology is being scrutinized as the industry gravitates towards advertising solutions that prioritize user privacy. This paper delves into the issue of reach measurement and optimization within the context of $k$-anonymity, a privacy-preserving model gaining traction across major online advertising platforms. We outline how to report reach within this new privacy landscape and demonstrate how probabilistic discounting, a probabilistic adaptation of traditional frequency capping, can be employed to optimize campaign performance. Experiments are performed to assess the trade-off between user privacy and the efficacy of online brand advertising. Notably, we discern a significant dip in performance as long as privacy is introduced, yet this comes with a limited additional cost for advertising platforms to offer their users more privacy."
2501.05169,"We introduce a new allocation rule, the uniform-dividend value (UD-value), for cooperative games whose characteristic function is incomplete. The UD-value assigns payoffs by distributing the total surplus of each family of indistinguishable coalitions uniformly among them. Our primary focus is on set systems that are intersection-closed, for which we show the UD-value is uniquely determined and can be interpreted as the expected Shapley value over all positive (i.e., nonnegative-surplus) extensions of the incomplete game.We compare the UD-value to two existing allocation rules for intersection-closed games: the R-value, defined as the Shapley value of a game that sets surplus of absent coalition values to zero, and the IC-value, tailored specifically for intersection-closed systems. We provide axiomatic characterizations of the UD-value motivated by characterizations of the IC-value and discuss further properties such as fairness and balanced contributions. Further, our experiments suggest that the UD-value and the R-value typically lie closer to each other than either does to the IC-value.Beyond intersection-closed systems, we find that while the UD-value is not always unique, a surprisingly large fraction of non-intersection-closed set systems still yield a unique UD-value, making it a practical choice in broader scenarios of incomplete cooperative games."
2501.05334,"We study strategic location choice by customers and sellers, termed the Bakers and Millers Game in the literature. In our generalized setting, each miller can freely choose any location for setting up a mill, while each baker is restricted in the choice of location for setting up a bakery. For optimal bargaining power, a baker would like to select a location with many millers to buy flour from and with little competition from other bakers. Likewise, a miller aims for a location with many bakers and few competing millers. Thus, both types of agents choose locations to optimize the ratio of agents of opposite type divided by agents of the same type at their chosen location. Originally raised in the context of Fractional Hedonic Games, the Bakers and Millers Game has applications that range from commerce to product design.We study the impact of location restrictions on the properties of the game. While pure Nash equilibria trivially exist in the setting without location restrictions, we show via a sophisticated, efficient algorithm that even the more challenging restricted setting admits equilibria. Moreover, the computed equilibrium approximates the optimal social welfare by a factor of at most $2\left(\frac{e}{e-1}\right)$. Furthermore, we give tight bounds on the price of anarchy/stability.On the conceptual side, the location choice feature adds a new layer to the standard setting of Hedonic Games, in the sense that agents that select the same location form a coalition. This allows to naturally restrict the possible coalitions that can be formed. With this, our model generalizes simple symmetric Fractional Hedonic Games on complete bipartite valuation graphs and also Hedonic Diversity Games with utilities single-peaked at 0. We believe that this generalization is also a very interesting direction for other types of Hedonic Games."
2501.0536,"Corrigibility of autonomous agents is an under explored part of system design, with previous work focusing on single agent systems. It has been suggested that uncertainty over the human preferences acts to keep the agents corrigible, even in the face of human irrationality. We present a general framework for modelling corrigibility in a multi-agent setting as a 2 player game in which the agents always have a move in which they can ask the human for supervision. This is formulated as a Bayesian game for the purpose of introducing uncertainty over the human beliefs. We further analyse two specific cases. First, a two player corrigibility game, in which we want corrigibility displayed in both agents for both common payoff (monotone) games and harmonic games. Then we investigate an adversary setting, in which one agent is considered to be a `defending' agent and the other an `adversary'. A general result is provided for what belief over the games and human rationality the defending agent is required to have to induce corrigibility."
2501.05384,"The window mean-payoff objective strengthens the classical mean-payoff objective by computing the mean-payoff over a finite window that slides along an infinite path. Two variants have been considered: in one variant, the maximum window length is fixed and given, while in the other, it is not fixed but is required to be bounded. In this paper, we look at the problem of synthesising strategies in Markov decision processes that maximise the window mean-payoff value in expectation, while also simultaneously guaranteeing that the value is above a certain threshold. We solve the synthesis problem for three different kinds of guarantees: sure (that needs to be satisfied in the worst-case, that is, for an adversarial environment), almost-sure (that needs to be satisfied with probability one), and probabilistic (that needs to be satisfied with at least some given probability $p$).We show that for fixed window mean-payoff objective, all the three problems are in $\mathsf{PTIME}$, while for bounded window mean-payoff objective, they are in $\mathsf{NP} \cap \mathsf{coNP}$, and thus have the same complexity as for maximising the expected performance without any guarantee. Moreover, we show that pure finite-memory strategies suffice for maximising the expectation with sure and almost-sure guarantees, whereas, for maximising expectation with a probabilistic guarantee, randomised strategies are necessary in general."
2501.05574,"We study the stable matching problem under the random matching model where the preferences of the doctors and hospitals are sampled uniformly and independently at random. In a balanced market with $n$ doctors and $n$ hospitals, the doctor-proposal deferred-acceptance algorithm gives doctors an expected rank of order $\log n$ for their partners and hospitals an expected rank of order $\frac{n}{\log n}$ for their partners. This situation is reversed in an unbalanced market with $n+1$ doctors and $n$ hospitals, a phenomenon known as the short-side advantage. The current proofs of this fact are indirect, counter-intuitively being based upon analyzing the hospital-proposal deferred-acceptance algorithm. In this paper we provide a direct proof of the short-side advantage, explicitly analyzing the doctor-proposal deferred-acceptance algorithm. Our proof sheds light on how and why the phenomenon arises."
2501.06506,"A Latin square is an $n \times n$ matrix filled with $n$ distinct symbols, each of which appears exactly once in each row and exactly once in each column. We introduce a problem of allocating $n$ indivisible items among $n$ agents over $n$ rounds while satisfying the Latin square constraint. This constraint ensures that each agent receives no more than one item per round and receives each item at most once. Each agent has an additive valuation on the item--round pairs. Real-world applications like scheduling, resource management, and experimental design require the Latin square constraint to satisfy fairness or balancedness in allocation. Our goal is to find a partial or complete allocation that maximizes the sum of the agents' valuations (utilitarian social welfare) or the minimum of the agents' valuations (egalitarian social welfare). For the problem of maximizing utilitarian social welfare, we prove NP-hardness even when the valuations are binary additive. We then provide $(1-1/e)$ and $(1-1/e)/4$-approximation algorithms for partial and complete settings, respectively. Additionally, we present fixed-parameter tractable (FPT) algorithms with respect to the order of Latin square and the optimum value for both partial and complete settings. For the problem of maximizing egalitarian social welfare, we establish that deciding whether the optimum value is at most $1$ or at least $2$ is NP-hard for both the partial and complete settings, even when the valuations are binary. Furthermore, we demonstrate that checking the existence of a complete allocation that satisfies each of envy-free, proportional, equitable, envy-free up to any good, proportional up to any good, or equitable up to any good is NP-hard, even when the valuations are identical."
2501.06799,"Equitable allocation of indivisible items involves partitioning the items among agents such that everyone derives (almost) equal utility. We consider the approximate notion of \textit{equitability up to one item} (EQ1) and focus on the settings containing mixtures of items (goods and chores), where an agent may derive positive, negative, or zero utility from an item. We first show that -- in stark contrast to the goods-only and chores-only settings -- an EQ1 allocation may not exist even for additive $\{-1,1\}$ bivalued instances, and its corresponding decision problem is computationally intractable. We focus on a natural domain of normalized valuations where the value of the entire set of items is constant for all agents. On the algorithmic side, we show that an EQ1 allocation can be computed efficiently for (i) $\{-1, 0, 1\}$ normalized valuations, (ii) objective but non-normalized valuations, (iii) two agents with type-normalized valuations. Previously, EQX allocations were known to exist only for 2 agents and objective valuations, while the case of subjective valuations remained computationally intractable even with two agents. We make progress by presenting an efficient algorithm that outputs an EQX allocation for $\{-1,1\}$ normalized subjective valuations for any number of agents. We complement our study by providing a comprehensive picture of achieving EQ1 allocations in conjunction with economic efficiency notions such as Pareto optimality and social welfare."
2501.06871,"The Banzhaf Power Index (BPI) is a method of measuring the power of voters in determining the outcome of a voting game. Some voting games exhibit a hierarchical structure, including the US electoral college and ensemble learning methods; we call such games hierarchical voting games. It is generally understood that BPI in hierarchical voting games can be computed via a recursive decomposition of the hierarchy, which can substantially reduce the calculation's complexity. We identify a key (previously undocumented) assumption on which this decomposition is based, namely balance, meaning one group of voters has enough votes to win whenever the complementary group of voters does not, and vice versa. We then introduce a generalization of BPI that we call Extended BPI (EBPI) for all voting games, including those that are not balanced, which simplifies to BPI in balanced games. We show that BPI in unbalanced hierarchical voting games decomposes in terms of EBPI at each level in the hierarchy, which yields computational savings analogous to those achieved in the balanced case. As a sample application, we take advantage of the compositionality of language, and model the impact of individual words on a sentence's sentiment as a voting game. As the complement of a phrase in a sentence does not necessarily have the opposite sentiment, this voting game is unbalanced and requires our decomposition of BPI in terms of EBPI. Our results suggest that EBPI is an effective proxy for BPI (because the meaning of a sentence is not always 100\% compositional), and demonstrate a dramatic improvement in run time."
2501.06923,"In online betting, the bookmaker can update the payoffs it offers on a particular event many times before the event takes place, and the updated payoffs may depend on the bets accumulated thus far. We study the problem of bookmaking with the goal of maximizing the return in the worst-case, with respect to the gamblers' behavior and the event's outcome. We formalize this problem as the \emph{Optimal Online Bookmaking game}, and provide the exact solution for the binary case. To this end, we develop the optimal bookmaking strategy, which relies on a new technique called bi-balancing trees, that assures that the house loss is the same for all \emph{decisive} betting sequences, where the gambler bets all its money on a single outcome in each round."
2501.07022,"We study online fair division when there are a finite number of item types and the player values for the items are drawn randomly from distributions with unknown means. In this setting, a sequence of indivisible items arrives according to a random online process, and each item must be allocated to a single player. The goal is to maximize expected social welfare while maintaining that the allocation satisfies proportionality in expectation. When player values are normalized, we show that it is possible to with high probability guarantee proportionality constraint satisfaction and achieve $\tilde{O}(\sqrt{T})$ regret. To achieve this result, we present an upper confidence bound (UCB) algorithm that uses two rounds of linear optimization. This algorithm highlights fundamental aspects of proportionality constraints that allow for a UCB algorithm despite the presence of many (potentially tight) constraints. This result improves upon the previous best regret rate of $\tilde{O}(T^{2/3})$."
2501.07233,"We examine a type of modified Monte Carlo Tree Search (MCTS) for strategising in combinatorial games. The modifications are derived by analysing simplified strategies and simplified versions of the underlying game and then using the results to construct an ensemble-type strategy. We present some instances where relative algorithm performance can be predicted from the results in the simplifications, making the approach useful as a heuristic for developing strategies in highly complex games, especially when simulation-type strategies and comparative analyses are largely intractable."
2501.07265,"In this work, we study a generalized Fisher market model that incorporates social influence. In this extended model, a buyer's utility depends not only on their own resource allocation but also on the allocations received by their competitors. We propose a novel competitive equilibrium formulation for this generalized Fisher market using a variational inequality approach. This framework effectively captures competitive equilibrium in markets that extend beyond the traditional assumption of homogeneous utility functions. We analyze key structural properties of the proposed variational inequality problem, including monotonicity, stability, and uniqueness. Additionally, we present two decentralized learning algorithms for buyers to achieve competitive equilibrium: a two-timescale stochastic approximation-based t{â}tonnement method and a trading-post mechanism-based learning method. Finally, we validate the proposed algorithms through numerical simulations."
2501.08626,"When humans interact with learning-based control systems, a common goal is to minimize a cost function known only to the human. For instance, an exoskeleton may adapt its assistance in an effort to minimize the human's metabolic cost-of-transport. Conventional approaches to synthesizing the learning algorithm solve an inverse problem to infer the human's cost. However, these problems can be ill-posed, hard to solve, or sensitive to problem data. Here we show a game-theoretic learning algorithm that works solely by observing human actions to find the cost minimum, avoiding the need to solve an inverse problem. We evaluate the performance of our algorithm in an extensive set of human subjects experiments, demonstrating consistent convergence to the minimum of a prescribed human cost function in scalar and multidimensional instantiations of the game. We conclude by outlining future directions for theoretical and empirical extensions of our results."
2501.08905,"Strategic interactions can be represented more concisely, and analyzed and solved more efficiently, if we are aware of the symmetries within the multiagent system. Symmetries also have conceptual implications, for example for equilibrium selection. We study the computational complexity of identifying and using symmetries. Using the classical framework of normal-form games, we consider game symmetries that can be across some or all players and/or actions. We find a strong connection between game symmetries and graph automorphisms, yielding graph automorphism and graph isomorphism completeness results for characterizing the symmetries present in a game. On the other hand, we also show that the problem becomes polynomial-time solvable when we restrict the consideration of actions in one of two ways.Next, we investigate when exactly game symmetries can be successfully leveraged for Nash equilibrium computation. We show that finding a Nash equilibrium that respects a given set of symmetries is PPAD- and CLS-complete in general-sum and team games respectively -- that is, exactly as hard as Brouwer fixed point and gradient descent problems. Finally, we present polynomial-time methods for the special cases where we are aware of a vast number of symmetries, or where the game is two-player zero-sum and we do not even know the symmetries."
2501.0933,"We present a new approach to solving games with a countably or uncountably infinite number of players. Such games are often used to model multiagent systems with a large number of agents. The latter are frequently encountered in economics, financial markets, crowd dynamics, congestion analysis, epidemiology, and population ecology, among other fields. Our two primary contributions are as follows. First, we present a way to represent strategy profiles for an infinite number of players, which we name a Player-to-Strategy Network (P2SN). Such a network maps players to strategies, and exploits the generalization capabilities of neural networks to learn across an infinite number of inputs (players) simultaneously. Second, we present an algorithm, which we name Shared-Parameter Simultaneous Gradient (SPSG), for training such a network, with the goal of finding an approximate Nash equilibrium. This algorithm generalizes simultaneous gradient ascent and its variants, which are classical equilibrium-seeking dynamics used for multiagent reinforcement learning. We test our approach on infinite-player games and observe its convergence to approximate Nash equilibria. Our method can handle games with infinitely many states, infinitely many players, infinitely many actions (and mixed strategies on them), and discontinuous utility functions."
2501.0974,"We study the regulation of algorithmic (non-)collusion amongst sellers in dynamic imperfect price competition by auditing their data as introduced by Hartline et al. [2024].We develop an auditing method that tests whether a seller's pessimistic calibrated regret is low. The pessimistic calibrated regret is the highest calibrated regret of outcomes compatible with the observed data. This method relaxes the previous requirement that a pricing algorithm must use fully-supported price distributions to be auditable. This method is at least as permissive as any auditing method that has a high probability of failing algorithmic outcomes with non-vanishing calibrated regret. Additionally, we strengthen the justification for using vanishing calibrated regret, versus vanishing best-in-hindsight regret, as the non-collusion definition, by showing that even without any side information, the pricing algorithms that only satisfy weaker vanishing best-in-hindsight regret allow an opponent to manipulate them into posting supra-competitive prices. This manipulation cannot be excluded with a non-collusion definition of vanishing best-in-hindsight regret.We motivate and interpret the approach of auditing algorithms from their data as suggesting a per se rule. However, we demonstrate that it is possible for algorithms to pass the audit by pretending to have higher costs than they actually do. For such scenarios, the rule of reason can be applied to bound the range of costs to those that are reasonable for the domain."
2501.10181,"Motivated by the strategic participation of electricity producers in electricity day-ahead market, we study the problem of online learning in repeated multi-unit uniform price auctions focusing on the adversarial opposing bid setting. The main contribution of this paper is the introduction of a new modeling of the bid space. Indeed, we prove that a learning algorithm leveraging the structure of this problem achieves a regret of $\tilde{O}(K^{4/3}T^{2/3})$ under bandit feedback, improving over the bound of $\tilde{O}(K^{7/4}T^{3/4})$ previously obtained in the literature. This improved regret rate is tight up to logarithmic terms. Inspired by electricity reserve markets, we further introduce a different feedback model under which all winning bids are revealed. This feedback interpolates between the full-information and bandit scenarios depending on the auctions' results. We prove that, under this feedback, the algorithm that we propose achieves regret $\tilde{O}(K^{5/2}\sqrt{T})$."
2501.10464,"We study the problem of adapting to a known sub-rational opponent during online play while remaining robust to rational opponents. We focus on large imperfect-information (zero-sum) games, which makes it impossible to inspect the whole game tree at once and necessitates the use of depth-limited search. However, all existing methods assume rational play beyond the depth-limit, which only allows them to adapt a very limited portion of the opponent's behaviour. We propose an algorithm Adapting Beyond Depth-limit (ABD) that uses a strategy-portfolio approach - which we refer to as matrix-valued states - for depth-limited search. This allows the algorithm to fully utilise all information about the opponent model, making it the first robust-adaptation method to be able to do so in large imperfect-information games. As an additional benefit, the use of matrix-valued states makes the algorithm simpler than traditional methods based on optimal value functions. Our experimental results in poker and battleship show that ABD yields more than a twofold increase in utility when facing opponents who make mistakes beyond the depth limit and also delivers significant improvements in utility and safety against randomly generated opponents."
2501.10884,"We propose a new algorithm that finds an $\varepsilon$-approximate fixed point of a smooth function from the $n$-dimensional $\ell_2$ unit ball to itself. We use the general framework of finding approximate solutions to a variational inequality, a problem that subsumes fixed point computation and the computation of a Nash Equilibrium. The algorithm's runtime is bounded by $e^{O(n)}/\varepsilon$, under the smoothed-analysis framework. This is the first known algorithm in such a generality whose runtime is faster than $(1/\varepsilon)^{O(n)}$, which is a time that suffices for an exhaustive search. We complement this result with a lower bound of $e^{\Omega(n)}$ on the query complexity for finding an $O(1)$-approximate fixed point on the unit ball, which holds even in the smoothed-analysis model, yet without the assumption that the function is smooth. Existing lower bounds are only known for the hypercube, and adapting them to the ball does not give non-trivial results even for finding $O(1/\sqrt{n})$-approximate fixed points."
2501.11897,"We formulate and study a general time-varying multi-agent system where players repeatedly compete under incomplete information. Our work is motivated by scenarios commonly observed in online advertising and retail marketplaces, where agents and platform designers optimize algorithmic decision-making in dynamic competitive settings. In these systems, no-regret algorithms that provide guarantees relative to \emph{static} benchmarks can perform poorly and the distributions of play that emerge from their interaction do not correspond anymore to static solution concepts such as coarse correlated equilibria. Instead, we analyze the interaction of \textit{dynamic benchmark} consistent policies that have performance guarantees relative to \emph{dynamic} sequences of actions, and through a novel \textit{tracking error} notion we delineate when their empirical joint distribution of play can approximate an evolving sequence of static equilibria. In systems that change sufficiently slowly (sub-linearly in the horizon length), we show that the resulting distributions of play approximate the sequence of coarse correlated equilibria, and apply this result to establish improved welfare bounds for smooth games. On a similar vein, we formulate internal dynamic benchmark consistent policies and establish that they approximate sequences of correlated equilibria. Our findings therefore suggest that in a broad range of multi-agent systems where non-stationarity is prevalent, algorithms designed to compete with dynamic benchmarks can improve both individual and welfare guarantees, and their emerging dynamics approximate a sequence of static equilibrium outcomes."
2501.12015,"In multiwinner approval voting, forming a committee that proportionally represents voters' approval ballots is an essential task. The notion of justified representation (JR) demands that any large ""cohesive"" group of voters should be proportionally ""represented"". The ""cohesiveness"" is defined in different ways; two common ways are the following: (C1) demands that the group unanimously approves a set of candidates proportional to its size, while (C2) requires each member to approve at least a fixed fraction of such a set. Similarly, ""representation"" have been considered in different ways: (R1) the coalition's collective utility from the winning set exceeds that of any proportionally sized alternative, and (R2) for any proportionally sized alternative, at least one member of the coalition derives less utility from it than from the winning set.Three of the four possible combinations have been extensively studied: (C1)-(R1) defines Proportional Justified Representation (PJR), (C1)-(R2) defines Extended Justified Representation (EJR), (C2)-(R2) defines Full Justified Representation (FJR). All three have merits, but also drawbacks. PJR is the weakest notion, and perhaps not sufficiently demanding; EJR may not be compatible with perfect representation; and it is open whether a committee satisfying FJR can be found efficiently.We study the combination (C2)-(R1), which we call Full Proportional Justified Representation (FPJR). We investigate FPJR's properties and find that it shares PJR's advantages over EJR: several proportionality axioms (e.g. priceability, perfect representation) imply FPJR and PJR but not EJR. We also find that efficient rules like the greedy Monroe rule and the method of equal shares satisfy FPJR, matching a key advantage of EJR over FJR. However, the Proportional Approval Voting (PAV) rule may violate FPJR, so neither of EJR and FPJR implies the other."
2501.12576,"In blockchain-based order book systems, buyers and sellers trade assets, while it is miners to match them and include their transactions in the blockchain. It is found that many miners behave selfishly and myopically, prioritizing transactions with high fees and ignoring many desirable matches that could enhance social welfare. Existing blockchain mechanisms fail to address this issue by overlooking miners' selfish behaviors. To our best knowledge, this work presents the first analytical study to quantify and understand buyer and seller transaction fee choices and selfish miners' transaction matching strategies, proving an infinitely large price of anarchy (PoA) for social welfare loss. To mitigate this, we propose an adjustable block size mechanism that is easy to implement without altering the existing decentralized protocols and still allows buyers and sellers to freely decide transaction fees and miners to selfishly match. The analysis is challenging, as pure strategy Nash equilibria do not always exist, requiring the analysis of many buyers' or sellers' interactive mixed-strategy distributions. Moreover, the system designer may even lack information about each buyer's or seller's bid/ask prices and trading quantities. Nevertheless, our mechanism achieves a well-bounded PoA, and under the homogeneous-quantity trading for non-fungible tokens (NFT), it attains a PoA of 1 with no social welfare loss. We implement our mechanism on a local instance of Ethereum to demonstrate the feasibility of our approach. Experiments based on the realistic dataset demonstrate that our mechanism achieves social optimum for homogeneous-quantity trading like NFT. It can enhance social welfare up to 3.7 times compared to the existing order book benchmarks for heterogeneous-quantity trading of Bitcoin tokens. It exhibits robustness against random variations in buyers and sellers."
2501.12827,"This study analyzes proposer-builder data and MEV-Boost payment data following the Ethereum merge in September 2022 to identify patterns of multi-block MEV. Our findings reveal fewer multi-slot sequences of builders than predicted by a random Monte Carlo simulation, with the longest observed sequence spanning 25 slots. Additionally, we observe that average MEV-Boost payments increase with the length of consecutive sequences, from approximately 0.05 ETH for single slots to 0.08 ETH for nine consecutive slots. Within longer sequences, payments per slot show a slight increase, suggesting that builders bid higher for longer sequences or the first slot after a longer sequence. A weak positive autocorrelation is found between subsequent MEV-Boost payments, challenging the hypothesis of alternating periods of low and high MEV. Finally, our comparison of builders during periods of low and high base fee volatility reveals minimal correlation, indicating the absence of builder specialization based on base fee volatility."
2501.13043,"We study a practical centralized matching problem which assigns children to daycare centers. The collective preferences of siblings from the same family introduce complementarities, which can lead to the absence of stable matchings, as observed in the hospital-doctor matching problems involving couples. Intriguingly, stable matchings are consistently observed in real-world daycare markets, despite the prevalence of sibling applicants.We conduct a probabilistic analysis of large random markets to examine the existence of stable matchings in such markets. Specifically, we examine scenarios where daycare centers have similar priorities over children, a common characteristic in real-world markets. Our analysis reveals that as the market size approaches infinity, the likelihood of stable matchings existing converges to 1.To facilitate our exploration, we refine an existing heuristic algorithm to address a more rigorous stability concept, as the original one may fail to meet this criterion. Through extensive experiments on both real-world and synthetic datasets, we demonstrate the effectiveness of our revised algorithm in identifying stable matchings, particularly when daycare priorities exhibit high similarity."
2501.13364,"Multi-agent systems (MAS) are increasingly applied to complex task allocation in two-sided markets, where agents such as companies and customers interact dynamically. Traditional company-led Stackelberg game models, where companies set service prices, and customers respond, struggle to accommodate diverse and personalised customer demands in emerging markets like crowdsourcing. This paper proposes a customer-led Stackelberg game model for cost-efficient task allocation, where customers initiate tasks as leaders, and companies create their strategies as followers to meet these demands. We prove the existence of Nash Equilibrium for the follower game and Stackelberg Equilibrium for the leader game while discussing their uniqueness under specific conditions, ensuring cost-efficient task allocation and improved market performance. Using the satellite constellation services market as a real-world case, experimental results show a 23% reduction in customer payments and a 6.7-fold increase in company revenues, demonstrating the model's effectiveness in emerging markets."
2501.13481,"This paper addresses the problem of finding fair orientations of graphs of chores, in which each vertex corresponds to an agent, each edge corresponds to a chore, and a chore has zero marginal utility to an agent if its corresponding edge is not incident to the vertex corresponding to the agent. Recently, Zhou et al. (IJCAI, 2024) analyzed the complexity of deciding whether graphs containing a mixture of goods and chores have EFX orientations, and conjectured that deciding whether graphs containing only chores have EFX orientations is NP-complete. We resolve this conjecture by giving polynomial-time algorithms that find EF1 and EFX orientations of graphs containing only chores if they exist, even if there are self-loops. Remarkably, our result demonstrates a surprising separation between the case of goods and the case of chores, because deciding whether graphs containing only goods have EFX orientations was shown to be NP-complete by Christodoulou et al. (EC, 2023). In addition, we show the EF1 and EFX orientation problems for multigraphs to be NP-complete."
2501.13686,"We extend the formalism of Conjectural Variations games to Stackelberg games involving multiple leaders and a single follower. To solve these nonconvex games, a common assumption is that the leaders compute their strategies having perfect knowledge of the follower's best response. However, in practice, the leaders may have little to no knowledge about the other players' reactions. To deal with this lack of knowledge, we assume that each leader can form conjectures about the other players' best responses, and update its strategy relying on these conjectures. Our contributions are twofold: (i) On the theoretical side, we introduce the concept of Conjectural Stackelberg Equilibrium -- keeping our formalism conjecture agnostic -- with Stackelberg Equilibrium being a refinement of it. (ii) On the algorithmic side, we introduce a two-stage algorithm with guarantees of convergence, which allows the leaders to first learn conjectures on a training data set, and then update their strategies. Theoretical results are illustrated numerically."
2501.14159,"In many two-sided labor markets, interviews are conducted before matches are formed. The growing number of interviews in medical residency markets has increased demand for signaling mechanisms, where applicants send a limited number of signals to communicate interest. We study the role of signaling mechanisms to reduce interviews in centralized random matching markets where initial preferences are refined through interviews. Agents can only match with those they interview. For the market to clear, we focus on perfect interim stability: no pair of agents-even if they never interviewed each other-prefers each other to their assigned partners under their interim preferences. A matching is almost interim stable if it is perfect interim stable after removing a vanishingly small fraction of agents.We analyze signaling mechanisms in random matching markets with $n$ agents where agents on the short side, long side, or both sides signal their top $d$ preferred partners. The interview graph connects pairs where at least one party signaled the other. We reveal a fundamental trade-off between almost and perfect interim stability. For almost interim stability, $d=\omega(1)$ signals suffice: short-side signaling is always effective, whereas long-side signaling is effective only when the market is weakly imbalanced, i.e., when any size difference between the two sides becomes negligible as the market grows. For perfect interim stability, at least $d=\Omega(\log^2 n)$ signals are necessary, and short-side signaling becomes crucial in any imbalanced market. We establish that truthful signaling is a Bayes-Nash equilibrium and extend our analysis to markets with hierarchical structure. As a technical contribution, we develop a message-passing algorithm that efficiently determines interim stability by leveraging local neighborhood structures."
2501.14474,"Algorithmic contract design studies scenarios where a principal incentivizes an agent to exert effort on her behalf. In this work, we focus on settings where the agent's type is drawn from an unknown distribution, and formalize an offline learning framework for learning near-optimal contracts from sample agent types. A central tool in our analysis is the notion of pseudo-dimension from statistical learning theory. Beyond its role in establishing upper bounds on the sample complexity, pseudo-dimension measures the intrinsic complexity of a class of contracts, offering a new perspective on the tradeoffs between simplicity and optimality in contract design. Our main results provide essentially optimal tradeoffs between pseudo-dimension and representation error (defined as the loss in principal's utility) with respect to linear and bounded contracts. Using these tradeoffs, we derive sample- and time-efficient learning algorithms, and demonstrate their near-optimality by providing almost matching lower bounds on the sample complexity. Conversely, for unbounded contracts, we prove an impossibility result showing that no learning algorithm exists.Finally, we extend our techniques in three important ways. First, we provide refined pseudo-dimension and sample complexity guarantees for the combinatorial actions model, revealing a novel connection between the number of critical values and sample complexity. Second, we extend our results to menus of contracts, showing that their pseudo-dimension scales linearly with the menu size. Third, we adapt our algorithms to the online learning setting, where we show that, a polynomial number of type samples suffice to learn near-optimal bounded contracts. Combined with prior work, this establishes a formal separation between expert advice and bandit feedback for this setting."
2501.14609,"This paper studies fair division of divisible and indivisible items among agents whose cardinal preferences are not necessarily monotone. We establish the existence of fair divisions and develop approximation algorithms to compute them.We address two complementary valuation classes, subadditive and nonnegative, which go beyond monotone functions. Considering both the division of cake (divisible resources) and allocation of indivisible items, we obtain fairness guarantees in terms of (approximate) envy-freeness (EF) and equability (EQ)In the context of envy-freeness, we prove that an EF division of a cake always exists under cake valuations that are subadditive and globally nonnegative. This result complements the nonexistence of EF allocations for burnt cakes known for more general valuations. In the indivisible-items setting, we establish the existence of EF3 allocations for subadditive and globally nonnegative valuations. In addition, we obtain universal existence of EF3 allocations under nonnegative valuations.We study equitability under nonnegative valuations. Here, we prove that EQ3 allocations always exist when the agents' valuations are nonnegative. Also, in the indivisible-items setting, we develop an approximation algorithm that, for given nonnegative valuations, finds allocations that are equitable within additive margins.Our results have combinatorial implications. For instance, the developed results imply the universal existence of proximately dense subgraphs: Given any graph $G=(V, E)$ and integer $k$ (at most $|V|$), there always exists a partition $V_1, V_2, \ldots, V_k$ of the vertex set such that the edge densities within the parts, $V_i$, are additively within four of each other. Further, such a partition can be computed efficiently."
2501.14625,"Bidders in combinatorial auctions face significant challenges when describing their preferences to an auctioneer. Classical work on preference elicitation focuses on query-based techniques inspired from proper learning--often via proxies that interface between bidders and an auction mechanism--to incrementally learn bidder preferences as needed to compute efficient allocations. Although such elicitation mechanisms enjoy theoretical query efficiency, the amount of communication required may still be too cognitively taxing in practice.We propose a family of efficient LLM-based proxy designs for eliciting preferences from bidders using natural language. Our proposed mechanism combines LLM pipelines and DNF-proper-learning techniques to quickly approximate preferences when communication is limited. To validate our approach, we create a testing sandbox for elicitation mechanisms that communicate in natural language. In our experiments, our most promising LLM proxy design reaches approximately efficient outcomes with five times fewer queries than classical proper learning based elicitation mechanisms."
2501.14847,"The single transferable vote (STV) is a system of preferential proportional voting employed in multi-seat elections. Each ballot cast by a voter is a (potentially partial) ranking over a set of candidates. The margin of victory, or simply 'margin', is the smallest number of ballots that need to be manipulated to alter the set of winners. Knowledge of the margin of an election gives greater insight into both how much time and money should be spent on auditing the election, and whether uncovered mistakes throw the election result into doubt -- requiring a costly repeat election -- or can be safely ignored without compromising the integrity of the result. Lower bounds on the margin can also be used for this purpose, in cases where exact margins are difficult to compute. There is one existing approach to computing lower bounds on the margin of STV elections, while there are multiple approaches to finding upper bounds. In this paper, we present improvements to this existing lower bound computation method for STV margins. The improvements lead to increased computational efficiency and, in many cases, to the algorithm computing tighter (higher) lower bounds."
2501.14916,"Dynamic max-min fair allocation (DMMF) is a simple and popular mechanism for the repeated allocation of a shared resource among competing agents: in each round, each agent can choose to request or not for the resource, which is then allocated to the requesting agent with the least number of allocations received till then. Recent work has shown that under DMMF, a simple threshold-based request policy enjoys surprisingly strong robustness properties, wherein each agent can realize a significant fraction of her optimal utility irrespective of how other agents' behave. While this goes some way in mitigating the possibility of a 'tragedy of the commons' outcome, the robust policies require that an agent defend against arbitrary (possibly adversarial) behavior by other agents. This however may be far from optimal compared to real world settings, where other agents are selfish optimizers rather than adversaries. Therefore, robust guarantees give no insight on how agents behave in an equilibrium, and whether outcomes are improved under one.Our work aims to bridge this gap by studying the existence and properties of equilibria under DMMF. To this end, we first show that despite the strong robustness guarantees of the threshold based strategies, no Nash equilibrium exists when agents participate in DMMF, each using some fixed threshold-based policy. On the positive side, however, we show that for the symmetric case, a simple data-driven request policy guarantees that no agent benefits from deviating to a different fixed threshold policy. In our proposed policy agents aim to match the historical allocation rate with a vanishing drift towards the rate optimizing overall welfare for all users. Furthermore, the resulting equilibrium outcome can be significantly better compared to what follows from the robustness guarantees."
2501.15006,"Approval-Based Committee (ABC) rules are an important tool for choosing a fair set of candidates when given the preferences of a collection of voters. Though finding a winning committee for many ABC rules is NP-hard, natural variations for these rules with polynomial-time algorithms exist. The recently introduced Method of Equal Shares, an important ABC rule with desirable properties, is also computable in polynomial time. However, when working with very large elections, polynomial time is not enough and parallelization may be necessary. We show that computing a winning committee using these polynomial-time ABC rules (including the Method of Equal Shares) is P-hard, thus showing they cannot be parallelized. In contrast, we show that finding a winning committee can be parallelized when the votes are single-peaked or single-crossing for the important ABC rule Chamberlin-Courant."
2501.15295,"In this paper, we revisit the problem of approximating a pacing equilibrium in second-price auctions, introduced by Conitzer, Kroer, Sodomka, and Moses [Oper. Res. 2022]. We show that finding a constant-factor approximation of a pacing equilibrium is PPAD-hard, thereby strengthening previous results of Chen, Kroer, and Kumar [Math. Oper. Res. 2024], which established PPAD-hardness only for inverse-polynomial approximations."
2501.15324,"Motivated by applications in job scheduling, queuing networks, and load balancing in cyber-physical systems, we develop and analyze a game-theoretic framework to balance the load among servers in static and dynamic settings. In these applications, jobs/tasks are held by selfish entities that do not want to coordinate with each other, yet the goal is to balance the load among servers in a distributed manner. First, we provide a static game formulation in which each player holds a job with a specific processing requirement and wants to schedule it fractionally among a set of heterogeneous servers to minimize its average processing time. We show that this static game is a potential game with a pure Nash equilibrium (NE). In particular, the best-response dynamics converge to such an NE after $n$ iterations, where $n$ is the number of players. Additionally, we bound the price of anarchy (PoA) of the static game in terms of game parameters. We then extend our results to a dynamic game setting, where jobs arrive and get processed, and players observe the load on the servers to decide how to schedule their jobs. In this setting, we show that if the players update their strategies using dynamic best-response, the system eventually becomes fully load-balanced and the players' strategies converge to the pure NE of the static game. In particular, we show that the convergence time scales only polynomially with respect to the game parameters. Finally, we provide numerical results to evaluate the performance of our proposed algorithms."
2501.15338,"Contextual pricing strategies are prevalent in online retailing, where the seller adjusts prices based on products' attributes and buyers' characteristics. Although such strategies can enhance seller's profits, they raise concerns about fairness when significant price disparities emerge among specific groups, such as gender or race. These disparities can lead to adverse perceptions of fairness among buyers and may even violate the law and regulation. In contrast, price differences can incentivize disadvantaged buyers to strategically manipulate their group identity to obtain a lower price. In this paper, we investigate contextual dynamic pricing with fairness constraints, taking into account buyers' strategic behaviors when their group status is private and unobservable from the seller. We propose a dynamic pricing policy that simultaneously achieves price fairness and discourages strategic behaviors. Our policy achieves an upper bound of $O(\sqrt{T}+H(T))$ regret over $T$ time horizons, where the term $H(T)$ arises from buyers' assessment of the fairness of the pricing policy based on their learned price difference. When buyers are able to learn the fairness of the price policy, this upper bound reduces to $O(\sqrt{T})$. We also prove an $\Omega(\sqrt{T})$ regret lower bound of any pricing policy under our problem setting. We support our findings with extensive experimental evidence, showcasing our policy's effectiveness. In our real data analysis, we observe the existence of price discrimination against race in the loan application even after accounting for other contextual information. Our proposed pricing policy demonstrates a significant improvement, achieving 35.06% reduction in regret compared to the benchmark policy."
2501.15782,"We introduce and study a multi-class online resource allocation problem with group fairness guarantees. The problem involves allocating a fixed amount of resources to a sequence of agents, each belonging to a specific group. The primary objective is to ensure fairness across different groups in an online setting. We focus on three fairness notions: one based on quantity and two based on utility. To achieve fair allocations, we develop two threshold-based online algorithms, proving their optimality under two fairness notions and near-optimality for the more challenging one. Additionally, we demonstrate a fundamental trade-off between group fairness and individual welfare using a novel representative function-based approach. To address this trade-off, we propose a set-aside multi-threshold algorithm that reserves a portion of the resource to ensure fairness across groups while utilizing the remaining resource to optimize efficiency under utility-based fairness notions. This algorithm is proven to achieve the Pareto-optimal trade-off. We also demonstrate that our problem can model a wide range of real-world applications, including network caching and cloud computing, and empirically evaluate our proposed algorithms in the network caching problem using real datasets."
2501.15834,"We study the strong core of housing markets when agents' preferences over houses are expressed as partial orders. We provide a structural characterization of the strong core, and propose an efficient algorithm that finds an allocation in the strong core or decides that it is empty, even in the presence of forced and forbidden arcs. The algorithm satisfies the property of group-strategyproofness. Additionally, we show that certain results known for the strong core in the case when agents' preferences are weak orders can be extended to the setting with partial order preferences; among others, we show that the strong core in such housing markets satisfies the property of respecting improvements."
2501.15916,"This paper studies an online variant of the celebrated housing market problem, where each agent has a single house and seeks to exchange it for another based on her preferences. In this online setting, agents may arrive and depart at any time, meaning that not all agents are present on the housing market simultaneously. I extend the well known serial dictatorship and Gale s top trading cycle mechanisms to this online scenario, aiming to retain their desirable properties such as Pareto efficiency, individual rationality, and strategy proofness. These extensions also seek to prevent agents from strategically delaying their arrival or advancing their departure. I demonstrate that achieving all of these properties simultaneously is impossible in the online context, and I present several variants that achieve different subsets of these properties."
2501.1609,"Execution Tickets are currently discussed as a next evolutionary step in Ethereum's block space allocation mechanism, separating consensus rewards from execution rewards and selling execution rights through a dedicated market. We present a theoretical framework identifying three core objectives for this mechanism - Decentralization, MEV capture, and Block Producer Incentive Compatibility - alongside practical metrics for evaluating each objective. To meet these goals, we explore seven key design parameters: ticket quantity, expiry, refundability, resalability, enhanced lookahead, pricing mechanism, and target ticket amount.We then evaluate four pricing mechanisms and construct six concrete mechanism designs from these parameters. To assess trade-offs in real-world conditions, we perform an agent-based simulation with over 300 runs. Our findings suggest that auction-driven formats, particularly second-price, excel in capturing significant MEV. The simulation also indicates that offering a secondary market can help alleviate centralization, since specialized ticket holders can enter and exit the market as needed.Non-expiring tickets show promise in reducing valuation risks, as ticket holders are not influenced by expiry-related discounting. Likewise, removing refundability simplifies the mechanism without notably impairing performance. Extended lookahead periods benefit price predictability and smoothness at a slight cost to price accuracy.Overall, this study provides a theoretical framework on the mechanism design space for Execution Tickets as well as a practical implementation of an agent-based simulation to test mechanism design choices. Further, it provides an exploratory evaluation of Execution Ticket mechanism designs, offering insights into optimal configurations that balance MEV capture, decentralization, and operational efficiency in Ethereum's block space allocation."
2501.16138,"This paper introduces a novel method for estimating the self-interest level of Markov social dilemmas. We extend the concept of self-interest level from normal-form games to Markov games, providing a quantitative measure of the minimum reward exchange required to align individual and collective interests. We demonstrate our method on three environments from the Melting Pot suite, representing either common-pool resources or public goods. Our results illustrate how reward exchange can enable agents to transition from selfish to collective equilibria in a Markov social dilemma. This work contributes to multi-agent reinforcement learning by providing a practical tool for analysing complex, multistep social dilemmas. Our findings offer insights into how reward structures can promote or hinder cooperation, with potential applications in areas such as mechanism design."
2501.16291,"Ensuring that AI systems make strategic decisions aligned with the specified preferences in adversarial sequential interactions is a critical challenge for developing trustworthy AI systems, especially when the environment is stochastic and players' incomplete preferences leave some outcomes unranked. We study the problem of synthesizing preference-satisfying strategies in two-player stochastic games on graphs where players have opposite (possibly incomplete) preferences over a set of temporal goals. We represent these goals using linear temporal logic over finite traces (LTLf), which enables modeling the nuances of human preferences where temporal goals need not be mutually exclusive and comparison between some goals may be unspecified. We introduce a solution concept of non-dominated almost-sure winning, which guarantees to achieve a most preferred outcome aligned with specified preferences while maintaining robustness against the adversarial behaviors of the opponent. Our results show that strategy profiles based on this concept are Nash equilibria in the game where players are risk-averse, thus providing a practical framework for evaluating and ensuring stable, preference-aligned outcomes in the game. Using a drone delivery example, we demonstrate that our contributions offer valuable insights not only for synthesizing rational behavior under incomplete preferences but also for designing games that motivate the desired behavior from the players in adversarial conditions."
2501.16307,"Nash equilibrium is a central solution concept for reasoning about self-interested agents. We address the problem of synthesizing Nash equilibria in two-player deterministic games on graphs, where players have private, partially-ordered preferences over temporal goals. Unlike prior work, which assumes preferences are common knowledge, we develop a communication protocol for equilibrium synthesis in settings where players' preferences are private information. In the protocol, players communicate to synthesize equilibria by exchanging information about when they can force desirable outcomes. We incorporate privacy by ensuring the protocol stops before enough information is revealed to expose a player's preferences. We prove completeness by showing that, when no player halts communication, the protocol either returns an equilibrium or certifies that none exists. We then prove privacy by showing that, with stopping, the messages a player sends are always consistent with multiple possible preferences and thus do not reveal some given secret regarding a player's true preference ordering. Experiments demonstrate that we can synthesize non-trivial equilibria while preserving privacy of preferences, highlighting the protocol's potential for applications in strategy synthesis with constrained information sharing."
2501.166,"We investigate how perturbation does and does not improve the Follow-the-Regularized-Leader (FTRL) algorithm in solving imperfect-information extensive-form games under sampling, where payoffs are estimated from sampled trajectories. While optimistic algorithms are effective under full feedback, they often become unstable in the presence of sampling noise. Payoff perturbation offers a promising alternative for stabilizing learning and achieving \textit{last-iterate convergence}. We present a unified framework for \textit{Perturbed FTRL} algorithms and study two variants: PFTRL-KL (standard KL divergence) and PFTRL-RKL (Reverse KL divergence), the latter featuring an estimator with both unbiasedness and conditional zero variance. While PFTRL-KL generally achieves equivalent or better performance across benchmark games, PFTRL-RKL consistently outperforms it in Leduc poker, whose structure is more asymmetric than the other games in a sense. Given the modest advantage of PFTRL-RKL, we design the second experiment to isolate the effect of conditional zero variance, showing that the variance-reduction property of RKL improve last-iterate performance."
2501.17255,"We examine two-player games over finite weighted graphs with quantitative (mean-payoff or energy) objective, where one of the players additionally needs to satisfy a fairness objective. The specific fairness we consider is called 'strong transition fairness', given by a subset of edges of one of the players, which asks the player to take fair edges infinitely often if their source nodes are visited infinitely often.We show that when fairness is imposed on player 1, these games fall within the class of previously studied omega-regular mean-payoff and energy games. On the other hand, when the fairness is on player 2, to the best of our knowledge, these games have not been previously studied.We provide gadget-based algorithms for fair mean-payoff games where fairness is imposed on either player, and for fair energy games where the fairness is imposed on player 1. For all variants of fair mean-payoff and fair energy (under unknown initial credit) games, we give pseudo-polynomial algorithms to compute the winning regions of both players. Additionally, we analyze the strategy complexities required for these games. Our work is the first to extend the study of strong transition fairness, as well as gadget-based approaches, to the quantitative setting. We thereby demonstrate that the simplicity of strong transition fairness, as well as the applicability of gadget-based techniques, can be leveraged beyond the omega-regular domain."
2501.17385,"We consider multi-agent systems with general information networks where an agent may only observe a subset of other agents. A system designer assigns local utility functions to the agents guiding their actions towards an outcome which determines the value of a given system objective. The aim is to design these local utility functions such that the Price of Anarchy (PoA), which equals the ratio of system objective at worst possible outcome to that at the optimal, is maximized. Towards this, we first develop a linear program (LP) that characterizes the PoA for any utility design and any information network. This leads to another LP that optimizes the PoA and derives the optimal utility design. Our work substantially generalizes existing approaches to the utility design problem. We also numerically show the robustness of proposed framework against unanticipated communication failures."
2501.18022,"In multiplayer games with sequential decision-making, self-interested players form dynamic coalitions to achieve most-preferred temporal goals beyond their individual capabilities. We introduce a novel procedure to synthesize strategies that jointly determine which coalitions should form and the actions coalition members should choose to satisfy their preferences in a subclass of deterministic multiplayer games on graphs. In these games, a leader decides the coalition during each round and the players not in the coalition follow their admissible strategies. Our contributions are threefold. First, we extend the concept of admissibility to games on graphs with preferences and characterize it using maximal sure winning, a concept originally defined for adversarial two-player games with preferences. Second, we define a value function that assigns a vector to each state, identifying which player has a maximal sure winning strategy for certain subset of objectives. Finally, we present a polynomial-time algorithm to synthesize admissible strategies for all players based on this value function and prove their existence in all games within the chosen subclass. We illustrate the benefits of dynamic coalitions over fixed ones in a blocks-world domain. Interestingly, our experiment reveals that aligned preferences do not always encourage cooperation, while conflicting preferences do not always lead to adversarial behavior."
2501.18304,"In an approval-based committee election, the goal is to select a committee consisting of $k$ out of $m$ candidates, based on $n$ voters who each approve an arbitrary number of the candidates. The core of such an election consists of all committees that satisfy a certain stability property which implies proportional representation. In particular, committees in the core cannot be ""objected to"" by a coalition of voters who is underrepresented. The notion of the core was proposed in 2016, but it has remained an open problem whether it is always non-empty. We prove that core committees always exist when $k \le 8$, for any number of candidates $m$ and any number of voters $n$, by showing that the Proportional Approval Voting (PAV) rule due to Thiele [1895] always satisfies the core when $k \le 7$ and always selects at least one committee in the core when $k = 8$. We also develop an artificial rule based on recursive application of PAV, and use it to show that the core is non-empty whenever there are $m \le 15$ candidates, for any committee size $k \le m$ and any number of voters $n$. These results are obtained with the help of computer search using linear programs."
2501.18343,"We study the Student Project Allocation problem with lecturer preferences over Students (SPA-S), which involves the assignment of students to projects based on student preferences over projects, lecturer preferences over students, and capacity constraints on both projects and lecturers. The goal is to find a stable matching that ensures no student and lecturer can mutually benefit by deviating from a given assignment to form an alternative arrangement involving some project. We explore the structural properties of SPA-S and characterise the set of stable matchings for an arbitrary SPA-S instance. We prove that, similar to the classical Stable Marriage problem (SM) and the Hospital Residents problem (HR), the set of all stable matchings in SPA-S forms a distributive lattice. In this lattice, the student-optimal and lecturer-optimal stable matchings represent the minimum and maximum elements, respectively. Finally, we introduce meta-rotations in the SPA-S setting using illustrations, demonstrating how they capture the relationships between stable matchings. These novel structural insights paves the way for efficient algorithms that address several open problems related to stable matchings in SPA-S."
2501.18442,"We consider the stable matching problem (e.g. between doctors and hospitals) in a one-to-one matching setting, where preferences are drawn uniformly at random. It is known that when doctors propose and the number of doctors equals the number of hospitals, then the expected rank of doctors for their match is $\Theta(\log n)$, while the expected rank of the hospitals for their match is $\Theta(n/\log n)$, where $n$ is the size of each side of the market. However, when adding even a single doctor, [Ashlagi, Kanoria and Leshno, 2017] show that the tables have turned: doctors have expected rank of $\Theta(n/\log n)$ while hospitals have expected rank of $\Theta(\log n)$. That is, (slight) competition has a much more dramatically harmful effect than the benefit of being on the proposing side. Motivated by settings where agents inflate their value for an item if it is already allocated to them (termed endowment effect), we study the case where hospitals exhibit ``loyalty"".We model loyalty as a parameter $k$, where a hospital currently matched to their $\ell$th most preferred doctor accepts proposals from their $\ell-k-1$th most preferred doctors. Hospital loyalty should help doctors mitigate the harmful effect of competition, as many more outcomes are now stable. However, we show that the effect of competition is so dramatic that, even in settings with extremely high loyalty, in unbalanced markets, the expected rank of doctors already becomes $\tilde{\Theta}(\sqrt{n})$ for loyalty $k=n-\sqrt{n}\log n=n(1-o(1))$."
2501.19138,"We focus on the design of algorithms for finding equilibria in 2-player zero-sum games. Although it is well known that such problems can be solved by a single linear program, there has been a surge of interest in recent years for simpler algorithms, motivated in part by applications in machine learning. Our work proposes such a method, inspired by the observation that the duality gap (a standard metric for evaluating convergence in min-max optimization problems) is a convex function for bilinear zero-sum games. To this end, we analyze a descent-based approach, variants of which have also been used as a subroutine in a series of algorithms for approximating Nash equilibria in general non-zero-sum games. In particular, we study a steepest descent approach, by finding the direction that minimises the directional derivative of the duality gap function. Our main theoretical result is that the derived algorithms achieve a geometric decrease in the duality gap and improved complexity bounds until we reach an approximate equilibrium. Finally, we complement this with an experimental evaluation, which provides promising findings. Our algorithm is comparable with (and in some cases outperforms) some of the standard approaches for solving 0-sum games, such as OGDA (Optimistic Gradient Descent/Ascent), even with thousands of available strategies per player."
2501.19144,"The framework of uncoupled online learning in multiplayer games has made significant progress in recent years. In particular, the development of time-varying games has considerably expanded its modeling capabilities. However, current regret bounds quickly become vacuous when the game undergoes significant variations over time, even when these variations are easy to predict. Intuitively, the ability of players to forecast future payoffs should lead to tighter guarantees, yet existing approaches fail to incorporate this aspect. This work aims to fill this gap by introducing a novel prediction-aware framework for time-varying games, where agents can forecast future payoffs and adapt their strategies accordingly. In this framework, payoffs depend on an underlying state of nature that agents predict in an online manner. To leverage these predictions, we propose the POWMU algorithm, a contextual extension of the optimistic Multiplicative Weight Update algorithm, for which we establish theoretical guarantees on social welfare and convergence to equilibrium. Our results demonstrate that, under bounded prediction errors, the proposed framework achieves performance comparable to the static setting. Finally, we empirically demonstrate the effectiveness of POWMU in a traffic routing experiment."
2501.19148,"In the $k$-committee election problem, we wish to aggregate the preferences of $n$ agents over a set of alternatives and select a committee of $k$ alternatives that minimizes the cost incurred by the agents. While we typically assume that agent preferences are captured by a cardinal utility function, in many contexts we only have access to ordinal information, namely the agents' rankings over the outcomes. As preference rankings are not as expressive as cardinal utilities, a loss of efficiency is inevitable, and is quantified by the notion of \emph{distortion}.We study the problem of electing a $k$-committee that minimizes the sum of the $\ell$-largest costs incurred by the agents, when agents and candidates are embedded in a metric space. This problem is called the $\ell$-centrum problem and captures both the utilitarian and egalitarian objectives. When $k \geq 2$, it is not possible to compute a bounded-distortion committee using purely ordinal information. We develop the first algorithms (that we call mechanisms) for the $\ell$-centrum problem (when $k \geq 2$), which achieve $O(1)$-distortion while eliciting only a very limited amount of cardinal information via value queries. We obtain two types of query-complexity guarantees: $O(\log k \log n)$ queries \emph{per agent}, and $O(k^2 \log^2 n)$ queries \emph{in total} (while achieving $O(1)$-distortion in both cases). En route, we give a simple adaptive-sampling algorithm for the $\ell$-centrum $k$-clustering problem."
2501.19219,"Differentiable economics, which uses neural networks as function approximators and gradient-based optimization in automated mechanism design (AMD), marked a significant breakthrough with the introduction of RegretNet \citep{regretnet_paper}. It combines the flexibility of deep learning with a regret-based approach to relax incentive compatibility, allowing for approximations of revenue-maximizing auctions. However, applying these techniques to combinatorial auctions (CAs) - where bidders value bundles rather than individual items, capturing item interdependencies - remains a challenge, primarily due to the lack of methodologies that can effectively deal with combinatorial constraints. To tackle this, we propose two architectures: CANet, a fully connected neural network, and CAFormer, a transformer-based model designed to learn optimal randomized mechanisms. Unlike existing methods in traditional AMD, our approach is more scalable and free of assumptions about the structures of allowable bundles or bidder valuations. We demonstrate that our models match current methods in non-combinatorial settings and set new benchmarks for CAs. Specifically, our models consistently outperform benchmark mechanisms derived from heuristic approaches and provide empirical solutions where analytical results are unavailable. This work bridges the gap in applying differentiable economics to combinatorial auctions, offering a scalable and flexible framework for designing revenue-maximizing mechanisms."
2501.19294,"Many ethical issues in machine learning are connected to the training data. Online data markets are an important source of training data, facilitating both production and distribution. Recently, a trend has emerged of for-profit ""ethical"" participants in online data markets. This trend raises a fascinating question: Can online data markets sustainably and efficiently address ethical issues in the broader machine-learning economy?In this work, we study this question in a stylized model of an online data market. We investigate the effects of intervening in the data market to achieve balanced training-data production. The model reveals the crucial role of market conditions. In small and emerging markets, an intervention can drive the data producers out of the market, so that the cost of fairness is maximal. Yet, in large and established markets, the cost of fairness can vanish (as a fraction of overall welfare) as the market grows.Our results suggest that ""ethical"" online data markets can be economically feasible under favorable market conditions, and motivate more models to consider the role of data production and distribution in mediating the impacts of ethical interventions."
2501.19388,"The widespread deployment of Machine Learning systems everywhere raises challenges, such as dealing with interactions or competition between multiple learners. In that goal, we study multi-agent sequential decision-making by considering principal-agent interactions in a tree structure. In this problem, the reward of a player is influenced by the actions of her children, who are all self-interested and non-cooperative, hence the complexity of making good decisions. Our main finding is that it is possible to steer all the players towards the globally optimal set of actions by simply allowing single-step transfers between them. A transfer is established between a principal and one of her agents: the principal actually offers the proposed payment if the agent picks the recommended action. The analysis poses specific challenges due to the intricate interactions between the nodes of the tree and the propagation of the regret within this tree. Considering a bandit setup, we propose algorithmic solutions for the players to end up being no-regret with respect to the optimal pair of actions and incentives. In the long run, allowing transfers between players makes them act as if they were collaborating together, although they remain self-interested non-cooperative: transfers restore efficiency."
2502.00149,"We study the distortion of one-sided and two-sided matching problems on the line. In the one-sided case, $n$ agents need to be matched to $n$ items, and each agent's cost in a matching is their distance from the item they were matched to. We propose an algorithm that is provided only with ordinal information regarding the agents' preferences (each agent's ranking of the items from most- to least-preferred) and returns a matching aiming to minimize the social cost with respect to the agents' true (cardinal) costs. We prove that our algorithm simultaneously achieves the best-possible approximation of $3$ (known as distortion) with respect to a variety of social cost measures which include the utilitarian and egalitarian social cost. In the two-sided case, where the agents need be matched to $n$ other agents and both sides report their ordinal preferences over each other, we show that it is always possible to compute an optimal matching. In fact, we show that this optimal matching can be achieved using even less information, and we provide bounds regarding the sufficient number of queries."
2502.00198,"Training data is the backbone of large language models (LLMs), yet today's data markets often operate under exploitative pricing -- sourcing data from marginalized groups with little pay or recognition. This paper introduces a theoretical framework for LLM data markets, modeling the strategic interactions between buyers (LLM builders) and sellers (human annotators). We begin with theoretical and empirical analysis showing how exploitative pricing drives high-quality sellers out of the market, degrading data quality and long-term model performance. Then we introduce fairshare, a pricing mechanism grounded in data valuation that quantifies each data's contribution. It aligns incentives by sustaining seller participation and optimizing utility for both buyers and sellers. Theoretically, we show that fairshare yields mutually optimal outcomes: maximizing long-term buyer utility and seller profit while sustaining market participation. Empirically when training open-source LLMs on complex NLP tasks, including math problems, medical diagnosis, and physical reasoning, fairshare boosts seller earnings and ensures a stable supply of high-quality data, while improving buyers' performance-per-dollar and long-term welfare. Our findings offer a concrete path toward fair, transparent, and economically sustainable data markets for LLM."
2502.00228,"In this paper, we study the Markovian Pandora's Box Problem, where decisions are governed by both order constraints and Markovianly correlated rewards, structured within a shared directed acyclic graph. To the best of our knowledge, previous work has not incorporated Markovian dependencies in this setting. This framework is particularly relevant to applications such as data or computation driven algorithm design, where exploration of future models incurs cost.We present optimal fully adaptive strategies where the associated graph forms a forest. Under static transition, we introduce a strategy that achieves a near optimal expected payoff in multi line graphs and a 1/2 approximation in forest-structured graphs. Notably, this algorithm provides a significant speedup over the exact solution, with the improvement becoming more pronounced as the graph size increases. Our findings deepen the understanding of sequential exploration under Markovian correlations in graph-based decision-making."
2502.0026,"We study the group strategic behaviors in Bayesian games. Equilibria in previous work do not consider group strategic behaviors with bounded sizes and are too ``strong'' to exist in many scenarios. We propose the ex-ante Bayesian $k$-strong equilibrium and the Bayesian $k$-strong equilibrium, where no group of at most $k$ agents can benefit from deviation. The two solution concepts differ in how agents calculate their utilities when contemplating whether a deviation is beneficial. Intuitively, agents are more conservative in the Bayesian $k$-strong equilibrium than in the ex-ante Bayesian $k$-strong equilibrium. With our solution concepts, we study collusion in the peer prediction mechanisms, as a representative of the Bayesian games with group strategic behaviors. We characterize the thresholds of the group size $k$ so that truthful reporting in the peer prediction mechanism is an equilibrium for each solution concept, respectively. Our solution concepts can serve as criteria to evaluate the robustness of a peer prediction mechanism against collusion. Besides the peer prediction problem, we also discuss two other potential applications of our new solution concepts, voting and Blotto games, where introducing bounded group sizes provides more fine-grained insights into the behavior of strategic agents."
2502.00313,"The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g. intentions or personas) or non-semantic prompting changes (e.g. templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts."
2502.00484,"A divisible budget must be allocated to several projects, and agents are asked for their opinion on how much they would give to each project. We consider that an agent is satisfied by a division of the budget if, for at least a certain predefined number $\tau$ of projects, the part of the budget actually allocated to each project is at least as large as the amount the agent requested. The objective is to find a budget division that ``best satisfies'' the agents. In this context, different problems can be stated and we address the following ones. We study $(i)$ the largest proportion of agents that can be satisfied for any instance, $(ii)$ classes of instances admitting a budget division that satisfies all agents, $(iii)$ the complexity of deciding if, for a given instance, every agent can be satisfied, and finally $(iv)$ the question of finding, for a given instance, the smallest total budget to satisfy all agents. We provide answers to these complementary questions for several natural values of the parameter $\tau$, capturing scenarios where we seek to satisfy for each agent all; almost all; half; or at least one of her requests."
2502.0056,"For a two-player imperfect-information extensive-form game (IIEFG) with $K$ time steps and a player action space of size $U$, the game tree complexity is $U^{2K}$, causing existing IIEFG solvers to struggle with large or infinite $(U,K)$, e.g., differential games with continuous action spaces. To partially address this scalability challenge, we focus on an important class of 2p0s games where the informed player (P1) knows the payoff while the uninformed player (P2) only has a belief over the set of $I$ possible payoffs. Such games encompass a wide range of scenarios in sports, defense, cybersecurity, and finance. We prove that under mild conditions, P1's (resp. P2's) equilibrium strategy at any infostate concentrates on at most $I$ (resp. $I+1$) action prototypes. When $I\ll U$, this equilibrium structure causes the game tree complexity to collapse to $I^K$ for P1 when P2 plays pure best responses, and $(I+1)^K$ for P2 in a dual game where P1 plays pure best responses. We then show that exploiting this structure in standard learning modes, i.e., model-free multiagent reinforcement learning and model predictive control, is straightforward, leading to significant improvements in learning accuracy and efficiency from SOTA IIEFG solvers. Our demonstration solves a 22-player football game ($K=10$, $U=\infty$) where the attacking team has to strategically conceal their intention until a critical moment in order to exploit information advantage. Code is available atthis https URL"
2502.01001,"In the digital age, resources such as open-source software and publicly accessible databases form a crucial category of digital public goods, providing extensive benefits for Internet. This paper investigates networked public goods games involving heterogeneous players and convex costs, focusing on the characterization of Nash Equilibrium (NE). In these games, each player can choose her effort level, representing her contributions to public goods. Network structures are employed to model the interactions among participants. Each player's utility consists of a concave value component, influenced by the collective efforts of all players, and a convex cost component, determined solely by the individual's own effort. To the best of our knowledge, this study is the first to explore the networked public goods game with convex costs.Our research begins by examining welfare solutions aimed at maximizing social welfare and ensuring the convergence of pseudo-gradient ascent dynamics. We establish the presence of NE in this model and provide an in-depth analysis of the conditions under which NE is unique. We also delve into comparative statics, an essential tool in economics, to evaluate how slight modifications in the model--interpreted as monetary redistribution--affect player utilities. In addition, we analyze a particular scenario with a predefined game structure, illustrating the practical relevance of our theoretical insights. Overall, our research enhances the broader understanding of strategic interactions and structural dynamics in networked public goods games, with significant implications for policy design in internet economic and social networks."
2502.01127,"When multiple influencers attempt to compete for a receiver's attention, their influencing strategies must account for the presence of one another. We introduce the Battling Influencers Game (BIG), a multi-player simultaneous-move general-sum game, to provide a game-theoretic characterization of this social phenomenon. We prove that BIG is a potential game, that it has either one or an infinite number of pure Nash equilibria (NEs), and these pure NEs can be found by convex optimization. Interestingly, we also prove that at any pure NE, all (except at most one) influencers must exaggerate their actions to the maximum extent. In other words, it is rational for the influencers to be non-truthful and extreme because they anticipate other influencers to cancel out part of their influence. We discuss the implications of BIG to value alignment."
2502.01215,"We study control problems in the context of matching under preferences: We examine how a central authority, called the controller, can manipulate an instance of the Stable Marriage or Stable Roommates problems in order to achieve certain goals. We investigate the computational complexity of the emerging problems, and provide both efficient algorithms and intractability results."
2502.01252,"There have been extensive studies on learning in zero-sum games, focusing on the analysis of the existence and algorithmic convergence of Nash equilibrium (NE). Existing studies mainly focus on symmetric games where the strategy spaces of the players are of the same type and size. For the few studies that do consider asymmetric games, they are mostly restricted to matrix games. In this paper, we define and study a new practical class of asymmetric games called two-player Asymmetric Combinatorial-Continuous zEro-Sum (ACCES) games, featuring a combinatorial action space for one player and an infinite compact space for the other. Such ACCES games have broad implications in the real world, particularly in combinatorial optimization problems (COPs) where one player optimizes a solution in a combinatorial space, and the opponent plays against it in an infinite (continuous) compact space (e.g., a nature player deciding epistemic parameters of the environmental model). Our first key contribution is to prove the existence of NE for two-player ACCES games, using the idea of essentially finite game approximation. Building on the theoretical insights and double oracle (DO)-based solutions to complex zero-sum games, our second contribution is to design the novel algorithm, Combinatorial Continuous DO (CCDO), to solve ACCES games, and prove the convergence of the proposed algorithm. Considering the NP-hardness of most COPs and recent advancements in reinforcement learning (RL)-based solutions to COPs, our third contribution is to propose a practical algorithm to solve NE in the real world, CCDORL (based on CCDO), and provide the novel convergence analysis in the ACCES game. Experimental results across diverse instances of COPs demonstrate the empirical effectiveness of our algorithms. The code of this work is available atthis https URL."
2502.0138,"We consider models for social choice where voters rank a set of choices (or alternatives) by deliberating in small groups of size at most $k$, and these outcomes are aggregated by a social choice rule to find the winning alternative. We ground these models in the metric distortion framework, where the voters and alternatives are embedded in a latent metric space, with closer alternative being more desirable for a voter. We posit that the outcome of a small-group interaction optimally uses the voters' collective knowledge of the metric, either deterministically or probabilistically.We characterize the distortion of our deliberation models for small $k$, showing that groups of size $k=3$ suffice to drive the distortion bound below the deterministic metric distortion lower bound of $3$, and groups of size $4$ suffice to break the randomized lower bound of $2.11$. We also show nearly tight asymptotic distortion bounds in the group size, showing that for any constant $\epsilon > 0$, achieving a distortion of $1+\epsilon$ needs group size that only depends on $1/\epsilon$, and not the number of alternatives. We obtain these results via formulating a basic optimization problem in small deviations of the sum of $i.i.d.$ random variables, which we solve to global optimality via non-convex optimization. The resulting bounds may be of independent interest in probability theory."
2502.01587,"Information design (ID) explores how a sender influence the optimal behavior of receivers to achieve specific objectives. While ID originates from everyday human communication, existing game-theoretic and machine learning methods often model information structures as numbers, which limits many applications to toy games. This work leverages LLMs and proposes a verbalized framework in Bayesian persuasion (BP), which extends classic BP to real-world games involving human dialogues for the first time. Specifically, we map the BP to a verbalized mediator-augmented extensive-form game, where LLMs instantiate the sender and receiver. To efficiently solve the verbalized game, we propose a generalized equilibrium-finding algorithm combining LLM and game solver. The algorithm is reinforced with techniques including verbalized commitment assumptions, verbalized obedience constraints, and information obfuscation. Numerical experiments in dialogue scenarios, such as recommendation letters, courtroom interactions, and law enforcement, validate that our framework can both reproduce theoretical results in classic BP and discover effective persuasion strategies in more complex natural language and multi-stage scenarios."
2502.01792,"In two-sided platforms (e.g., video streaming or e-commerce), viewers and providers engage in interactive dynamics: viewers benefit from increases in provider populations, while providers benefit from increases in viewer population. Despite the importance of such ""population effects"" on long-term platform health, recommendation policies do not generally take the participation dynamics into account. This paper thus studies the dynamics and recommender policy design on two-sided platforms under the population effects for the first time. Our control- and game-theoretic findings warn against the use of the standard ""myopic-greedy"" policy and shed light on the importance of provider-side considerations (i.e., effectively distributing exposure among provider groups) to improve social welfare via population growth. We also present a simple algorithm to optimize long-term social welfare by taking the population effects into account, and demonstrate its effectiveness in synthetic and real-data experiments. Our experiment code is available atthis https URL."
2502.01862,"The majority of online marketplaces offer promotion programs to sellers to acquire additional customers for their products. These programs typically allow sellers to allocate advertising budgets to promote their products, with higher budgets generally correlating to improve ad performance. Auction mechanisms with budget pacing are commonly employed to implement such ad systems. While auctions deliver satisfactory average effectiveness, ad performance under allocated budgets can be unfair in practice.To address this issue, we propose a novel ad allocation model that departs from traditional auction mechanics. Our approach focuses on solving a global optimization problem that balances traffic allocation while considering platform efficiency and fairness constraints.This study presents the following contributions. First, we introduce a fairness metric based on the Gini index. Second, we formulate the optimization problem incorporating efficiency and fairness objectives. Third, we offer an online algorithm to solve this optimization problem. Finally, we demonstrate that our approach achieves superior fairness compared to baseline auction-based algorithms without sacrificing efficiency. We contend that our proposed method can be effectively applied in real-time ad allocation scenarios and as an offline benchmark for evaluating the fairness-efficiency trade-off of existing auction-based systems."
2502.01914,"The core is a quintessential solution concept for profit sharing in cooperative game theory. An imputation allocates the worth of the given game among its agents. The imputation lies in the core of the game if, for each sub-coalition, the amount allocated to its agents is at least the worth of this sub-coalition. Hence, under a core imputation, each of exponentially many sub-coalitions gets satisfied. The following computational question has received much attention: Given an imputation, does it lie in the core? Clearly, this question lies in co-NP, since a co-NP certificate for this problem would be a sub-coalition which is not satisfied under the imputation. This question is in P for the assignment game [SS71] and has been shown to be co-NP-hard for several natural games, including max-flow [FZCD02] and MST [FKFH97]. The one natural game for which this question has remained open is the b-matching game when the number of times an edge can be matched is unconstrained; in case each edge can be matched at most once, it is co-NP-hard [BKPW18]. At the outset, it was not clear which way this open question would resolve: on the one hand, for all but one game, this problem was shown co-NP-hard and on the other hand, proximity to the assignment problem and the deep structural properties of matching could lead to a positive result. In this paper, we show that the problem is indeed co-NP-hard."
2502.02059,"We consider the problem of optimally executing a user trade over networks of constant function market makers (CFMMs) in the presence of hooks. Hooks, introduced in an upcoming version of Uniswap, are auxiliary smart contracts that allow for extra information to be added to liquidity pools. This allows liquidity providers to enable constraints on trades, allowing CFMMs to read external data, such as volatility information, and implement additional features, such as onchain limit orders. We consider three important case studies for how to optimally route trades in the presence of hooks: 1) routing through limit orders, 2) optimal liquidations and time-weighted average market makers (TWAMMs), and 3) noncomposable hooks, which provide additional output in exchange for fill risk. Leveraging tools from convex optimization and dynamic programming, we propose simple methods for formulating and solving these problems that can be useful for practitioners."
2502.0238,"We examine an approval-based model of Liquid Democracy with a budget constraint on voting and delegating costs, aiming to centrally select casting voters ensuring complete representation of the electorate. From a computational complexity perspective, we focus on minimizing overall costs, maintaining short delegation paths, and preventing excessive concentration of voting power. Furthermore, we explore computational aspects of strategic control, specifically, whether external agents can change election components to influence the voting power of certain voters."
2502.02503,"In this paper, we demonstrate that in many NP-complete variants of the stable matching problem, such as the Stable Hypergraph Matching problem, the Stable Multicommodity Flow problem, and the College Admission problem with common quotas, a near-feasible stable solution - that is, a solution which is stable, but may slightly violate some capacities - always exists. Our results provide strong theoretical guarantees that even under complex constraints, stability can be restored with minimal capacity modifications.To achieve this, we present an iterative rounding algorithm that starts from a stable fractional solution and systematically adjusts capacities to ensure the existence of an integral stable solution. This approach leverages Scarf's algorithm to compute an initial fractional stable solution, which serves as the foundation for our rounding process. Notably, in the case of the Stable Fixtures problem, where a stable fractional matching can be computed efficiently, our method runs in polynomial time.These findings have significant practical implications for market design, college admissions, and other real-world allocation problems, where small adjustments to institutional constraints can guarantee stable and implementable outcomes."
2502.02541,"Cybersecurity is one of the most pressing technological challenges of our time and requires measures from all sectors of society. A key measure is automated security response, which enables automated mitigation and recovery from cyber attacks. Significant strides toward such automation have been made due to the development of rule-based response systems. However, these systems have a critical drawback: they depend on domain experts to configure the rules, a process that is both error-prone and inefficient. Framing security response as an optimal control problem shows promise in addressing this limitation but introduces new challenges. Chief among them is bridging the gap between theoretical optimality and operational performance. Current response systems with theoretical optimality guarantees have only been validated analytically or in simulation, leaving their practical utility unproven.This thesis tackles the aforementioned challenges by developing a practical methodology for optimal security response in IT infrastructures. It encompasses two systems. First, it includes an emulation system that replicates key components of the target infrastructure. We use this system to gather measurements and logs, based on which we identify a game-theoretic model. Second, it includes a simulation system where game-theoretic response strategies are optimized through stochastic approximation to meet a given objective, such as mitigating potential attacks while maintaining operational services. These strategies are then evaluated and refined in the emulation system to close the gap between theoretical and operational performance. We prove structural properties of optimal response strategies and derive efficient algorithms for computing them. This enables us to solve a previously unsolved problem: demonstrating optimal security response against network intrusions on an IT infrastructure."
2502.02543,"This paper addresses the online $k$-selection problem with diseconomies of scale (OSDoS), where a seller seeks to maximize social welfare by optimally pricing items for sequentially arriving buyers, accounting for increasing marginal production costs. Previous studies have investigated deterministic dynamic pricing mechanisms for such settings. However, significant challenges remain, particularly in achieving optimality with small or finite inventories and developing effective randomized posted price mechanisms. To bridge this gap, we propose a novel randomized dynamic pricing mechanism for OSDoS, providing a tighter lower bound on the competitive ratio compared to prior work. Our approach ensures optimal performance in small inventory settings (i.e., when $k$ is small) and surpasses existing online mechanisms in large inventory settings (i.e., when $k$ is large), leading to the best-known posted price mechanism for optimizing online selection and allocation with diseconomies of scale across varying inventory sizes."
2502.02815,"Fair allocation of indivisible items among agents is a fundamental and extensively studied problem. However, fairness does not have a single universally accepted definition, leading to a variety of competing fairness notions. Some of these notions are considered stronger or more desirable, but they are also more difficult to guarantee. In this work, we examine 22 different notions of fairness and organize them into a hierarchy. Formally, we say that a fairness notion $F_1$ implies another notion $F_2$ if every $F_1$-fair allocation is also $F_2$-fair. We give a near-complete picture of implications among fairness notions: for almost every pair of notions, we either prove an implication or give a counterexample demonstrating that the implication does not hold. Although some of these results are already known, many are new. We examine multiple settings, including the allocation of goods, chores, and mixed manna. We believe this work clarifies the relative strengths and applicability of these notions, providing a foundation for future research in fair division. Moreover, we developed an inference engine to automate part of our work. It is available as a user-friendly web application and may have broader applications beyond fair division."
2502.02901,"Policy Space Response Oracles (PSRO) interleaves empirical game-theoretic analysis with deep reinforcement learning (DRL) to solve games too complex for traditional analytic methods. Tree-exploiting PSRO (TE-PSRO) is a variant of this approach that iteratively builds a coarsened empirical game model in extensive form using data obtained from querying a simulator that represents a detailed description of the game. We make two main methodological advances to TE-PSRO that enhance its applicability to complex games of imperfect information. First, we introduce a scalable representation for the empirical game tree where edges correspond to implicit policies learned through DRL. These policies cover conditions in the underlying game abstracted in the game model, supporting sustainable growth of the tree over epochs. Second, we leverage extensive form in the empirical model by employing refined Nash equilibria to direct strategy exploration. To enable this, we give a modular and scalable algorithm based on generalized backward induction for computing a subgame perfect equilibrium (SPE) in an imperfect-information game. We experimentally evaluate our approach on a suite of games including an alternating-offer bargaining game with outside offers; our results demonstrate that TE-PSRO converges toward equilibrium faster when new strategies are generated based on SPE rather than Nash equilibrium, and with reasonable time/memory requirements for the growing empirical model."
2502.03113,"In classical job-scheduling games, each job behaves as a selfish player, choosing a machine to minimize its own completion time. To reduce the equilibria inefficiency, coordination mechanisms are employed, allowing each machine to follow its own scheduling policy. In this paper we study the effects of incorporating rank-based utilities within coordination mechanisms across environments with either identical or unrelated machines.With rank-based utilities, players aim to perform well relative to their competitors, rather than solely minimizing their completion time. We first demonstrate that even in basic setups, such as two identical machines with unit-length jobs, a pure Nash equilibrium (NE) assignment may not exist. This observation motivates our inquiry into the complexity of determining whether a given game instance admits a NE. We prove that this problem is NP-complete, even in highly restricted cases. In contrast, we identify specific classes of games where a NE is guaranteed to exist, or where the decision problem can be resolved in polynomial time.Additionally, we examine how competition impacts the efficiency of Nash equilibria, or sink equilibria if a NE does not exist. We derive tight bounds on the price of anarchy, and show that competition may either enhance or degrade overall performance."
2502.03197,"Consider elections where the set of candidates is partitioned into parties, and each party must nominate exactly one candidate. The Possible President problem asks whether some candidate of a given party can become the winner of the election for some nominations from other parties. We perform a multivariate computational complexity analysis of Possible President for a range of Condorcet-consistent voting rules, namely for Copeland$^\alpha$ for $\alpha \in [0,1]$ and Maximin. The parameters we study are the number of voters, the number of parties, and the maximum size of a party. For all voting rules under consideration, we obtain dichotomies based on the number of voters, classifying $\mathsf{NP}$-complete and polynomial-time solvable cases. Moreover, for each $\mathsf{NP}$-complete variant, we determine the parameterized complexity of every possible parameterization with the studied parameters as either (a) fixed-parameter tractable, (b) $\mathsf{W}[1]$-hard but in $\mathsf{XP}$, or (c) $\mathsf{paraNP}$-hard, outlining the limits of tractability for these problems."
2502.03545,"We address the problem of selecting $k$ representative nodes from a network, aiming to achieve two objectives: identifying the most influential nodes and ensuring the selection proportionally reflects the network's diversity. We propose two approaches to accomplish this, analyze them theoretically, and demonstrate their effectiveness through a series of experiments."
2502.03546,"The preference graph is a combinatorial representation of the structure of a normal-form game. Its nodes are the strategy profiles, with an arc between profiles if they differ in the strategy of a single player, where the orientation indicates the preferred choice for that player. We show that the preference graph is a surprisingly fundamental tool for studying normal-form games, which arises from natural axioms and which underlies many key game-theoretic concepts, including dominated strategies and strict Nash equilibria, as well as classes of games like potential games, supermodular games and weakly acyclic games. The preference graph is especially related to game dynamics, playing a significant role in the behaviour of fictitious play and the replicator dynamic. Overall, we aim to equip game theorists with the tools and understanding to apply the preference graph to new problems in game theory."
2502.03616,"Noncooperative multi-agent systems often face coordination challenges due to conflicting preferences among agents. In particular, agents acting in their own self-interest can settle on different equilibria, leading to suboptimal outcomes or even safety concerns. We propose an algorithm named trading auction for consensus (TACo), a decentralized approach that enables noncooperative agents to reach consensus without communicating directly or disclosing private valuations. TACo facilitates coordination through a structured trading-based auction, where agents iteratively select choices of interest and provably reach an agreement within an a priori bounded number of steps. A series of numerical experiments validate that the termination guarantees of TACo hold in practice, and show that TACo achieves a median performance that minimizes the total cost across all agents, while allocating resources significantly more fairly than baseline approaches."
2502.03789,"This work addresses fair allocation of indivisible items in settings wherein it is feasible to create copies of resources or dispose of tasks. We establish that exact maximin share (MMS) fairness can be achieved via limited duplication of goods even under monotone valuations. We also show that, when allocating chores under monotone costs, MMS fairness is always feasible with limited disposal of chores. Since monotone valuations do not admit any nontrivial approximation guarantees for MMS, our results highlight that such barriers can be circumvented by post facto adjustments in the supply of the items.We prove that, for division of $m$ goods among $n$ agents with monotone valuations, there always exists an assignment of subsets of goods to the agents such that they receive at least their maximin shares and no single good is allocated to more than $3 \log m$ agents. In addition, the sum of the sizes of the assigned subsets does not exceed $m$. For identically ordered valuations, we obtain an upper bound of $O(\sqrt{\log m})$ on the maximum assignment multiplicity across goods and an $m + \widetilde{O}\left(\frac{m}{\sqrt{n}} \right)$ bound for the total number of goods assigned. Further, for additive valuations, we prove that there always exists an MMS assignment in which no single good is allocated to more than $2$ agents and the total number of goods assigned is at most $2m$.For chores, we upper bound the number of chores that need to be discarded for ensuring MMS fairness. We prove that, under monotone costs, there exists an MMS assignment in which at most $\frac{m}{e}$ remain unassigned. For identically ordered costs, we establish that MMS fairness can be achieved while keeping at most $\widetilde{O} \left(\frac{m}{n^{1/4}} \right)$ chores unassigned. We also prove that the obtained bounds for monotone valuations and monotone costs are essentially tight."
2502.04343,"Traffic assignment analyzes traffic flows in road networks that emerge due to traveler interaction. Traditionally, travelers are assumed to use private cars, so road costs grow with the number of users due to congestion. However, in sustainable transit systems, travelers share vehicles s.t. more users on a road lead to higher sharing potential and reduced cost per user. Thus, we invert the usual avoidant traffic assignment (ATA) and instead consider synergistic traffic assignment (STA) where road costs decrease with use.We find that STA is significantly different from ATA from a game-theoretical point of view. We show that a simple iterative best-response method with simultaneous updates converges to an equilibrium state. This enables efficient computation of equilibria using optimized speedup techniques for shortest-path queries. In contrast, ATA requires slower sequential updates or more complicated iteration schemes that only approximate an equilibrium. Experiments with a realistic scenario for the city of Stuttgart indicate that STA indeed quickly converges to an equilibrium. We envision STA as a part of software-defined transportation systems that dynamically adapt to current travel demand. As a first demonstration, we show that an STA equilibrium can be used to incorporate traveler synergism in a simple bus line planning algorithm to potentially greatly reduce the required vehicle resources."
2502.04763,"The Shapley value is the prevalent solution for fair division problems in which a payout is to be divided among multiple agents. By adopting a game-theoretic view, the idea of fair division and the Shapley value can also be used in machine learning to quantify the individual contribution of features or data points to the performance of a predictive model. Despite its popularity and axiomatic justification, the Shapley value suffers from a computational complexity that scales exponentially with the number of entities involved, and hence requires approximation methods for its reliable estimation. We propose SVA$k_{\text{ADD}}$, a novel approximation method that fits a $k$-additive surrogate game. By taking advantage of $k$-additivity, we are able to elicit the exact Shapley values of the surrogate game and then use these values as estimates for the original fair division problem. The efficacy of our method is evaluated empirically and compared to competing methods."
2502.04811,"We investigate packet routing games in which network users selfishly route themselves through a network over discrete time, aiming to reach the destination as quickly as possible. Conflicts due to limited capacities are resolved by the first-in, first-out (FIFO) principle. Building upon the line of research on packet routing games initiated by Werth et al., we derive the first non-trivial bounds for packet routing games with FIFO. Specifically, we show that the price of anarchy is at most 2 for the important and well-motivated class of uniformly fastest route equilibria introduced by Scarsini et al. on any linear multigraph. We complement our results with a series of instances on linear multigraphs, where the price of stability converges to at least $\frac{e}{e-1}$. Furthermore, our instances provide a lower bound for the price of anarchy of continuous Nash flows over time on linear multigraphs which establishes the first lower bound of $\frac{e}{e-1}$ on a graph class where the monotonicity conjecture is proven by Correa et al."
2502.05141,"We study the problem of fairly allocating a set of indivisible items among a set of agents. We consider the notion of (approximate) maximin share (MMS) and we provide an improved lower bound of $1/2$ (which is tight) for the case of subadditive valuations when the number of agents is at most four. We also provide a tight lower bound for the case of multiple agents, when they are equipped with one of two possible types of valuations. Moreover, we propose a new model that extends previously studied models in the area of fair division, which will hopefully give rise to further research. We demonstrate the usefulness of this model by employing it as a technical tool to derive our main result, and we provide a thorough analysis for this model for the case of three agents. Finally, we provide an improved impossibility result for the case of three submodular agents."
2502.05187,"In online advertising, advertisers commonly utilize auto-bidding services to bid for impression opportunities. A typical objective of the auto-bidder is to optimize the advertiser's cumulative value of winning impressions within specified budget constraints. However, such a problem is challenging due to the complex bidding environment faced by diverse advertisers. To address this challenge, we introduce ABPlanner, a few-shot adaptable budget planner designed to improve budget-constrained auto-bidding. ABPlanner is based on a hierarchical bidding framework that decomposes the bidding process into shorter, manageable stages. Within this framework, ABPlanner allocates the budget across all stages, allowing a low-level auto-bidder to bids based on the budget allocation plan. The adaptability of ABPlanner is achieved through a sequential decision-making approach, inspired by in-context reinforcement learning. For each advertiser, ABPlanner adjusts the budget allocation plan episode by episode, using data from previous episodes as prompt for current decisions. This enables ABPlanner to quickly adapt to different advertisers with few-shot data, providing a sample-efficient solution. Extensive simulation experiments and real-world A/B testing validate the effectiveness of ABPlanner, demonstrating its capability to enhance the cumulative value achieved by auto-bidders."
2502.05246,"The objective is to find a Cellular Automata (CA) rule that can evolve 2D patterns that are optimal with respect to a global fitness function. The global fitness is defined as the sum of local computed utilities. A utility or value function computes a score depending on the states in the local neighborhood. First the method is explained that was followed to find such a CA rule. Then this method is applied to find a rule that maximizes social wealth. Here wealth is defined as the sum of the payoffs that all players (agents, cells) receive in a prisoner's dilemma game, and then shared equally among them. The problem is solved in four steps: (0) Defining the utility function, (1) Finding optimal master patterns with a Genetic Algorithm, (2) Extracting templates (local neighborhood configurations), (3) Inserting the templates in a general CA rule. The constructed CA rule finds optimal and near-optimal patterns for even and odd grid sizes. Optimal patterns of odd size contain exactly one singularity, a 2 x 2 block of cooperators."
2502.05314,"Unlike Poker where the action space $\mathcal{A}$ is discrete, differential games in the physical world often have continuous action spaces not amenable to discrete abstraction, rendering no-regret algorithms with $\mathcal{O}(|\mathcal{A}|)$ complexity not scalable. To address this challenge within the scope of two-player zero-sum (2p0s) games with one-sided information, we show that (1) a computational complexity independent of $|\mathcal{A}|$ can be achieved by exploiting the convexification property of incomplete-information games and the Isaacs' condition that commonly holds for dynamical systems, and that (2) the computation of the two equilibrium strategies can be decoupled under one-sidedness of information. Leveraging these insights, we develop an algorithm that successfully approximates the optimal strategy in a homing game. Code available inthis https URL"
2502.05316,"We consider simple stochastic games with terminal-node rewards and multiple players, who have differing perceptions of risk. Specifically, we study risk-sensitive equilibria (RSEs), where no player can improve their perceived reward -- based on their risk parameter -- by deviating from their strategy. We start with the entropic risk (ER) measure, which is widely studied in finance. ER characterises the players on a quantitative spectrum, with positive risk parameters representing optimists and negative parameters representing pessimists. Building on known results for Nash equilibira, we show that RSEs exist under ER for all games with non-negative terminal rewards. However, using similar techniques, we also show that the corresponding constrained existence problem -- to determine whether an RSE exists under ER with the payoffs in given intervals -- is undecidable.To address this, we introduce a new, qualitative risk measure -- called extreme risk (XR) -- which coincides with the limit cases of positively infinite and negatively infinite ER parameters. Under XR, every player is an extremist: an extreme optimist perceives their reward as the maximum payoff that can be achieved with positive probability, while an extreme pessimist expects the minimum payoff achievable with positive probability. Our first main result proves the existence of RSEs also under XR for non-negative terminal rewards. Our second main result shows that under XR the constrained existence problem is not only decidable, but NP-complete. Moreover, when all players are extreme optimists, the problem becomes PTIME-complete. Our algorithmic results apply to all rewards, positive or negative, establishing the first decidable fragment for equilibria in simple stochastic games with terminal objectives without restrictions on strategy types or number of players."
2502.05599,"Auto-bidding problem under a strict return-on-spend constraint (ROSC) is considered, where an algorithm has to make decisions about how much to bid for an ad slot depending on the revealed value, and the hidden allocation and payment function that describes the probability of winning the ad-slot depending on its bid. The objective of an algorithm is to maximize the expected utility (product of ad value and probability of winning the ad slot) summed across all time slots subject to the total expected payment being less than the total expected utility, called the ROSC. A (surprising) impossibility result is derived that shows that no online algorithm can achieve a sub-linear regret even when the value, allocation and payment function are drawn i.i.d. from an unknown distribution. The problem is non-trivial even when the revealed value remains constant across time slots, and an algorithm with regret guarantee that is optimal up to logarithmic factor is derived."
2502.05604,"Open data, as an essential element in the sustainable development of the digital economy, is highly valued by many relevant sectors in the implementation process. However, most studies suppose that there are only data providers and users in the open data process and ignore the existence of data regulators. In order to establish long-term green supply relationships between multi-stakeholders, we hereby introduce data regulators and propose an evolutionary game model to observe the cooperation tendency of multi-stakeholders (data providers, users, and regulators). The newly proposed game model enables us to intensively study the trading behavior which can be realized as strategies and payoff functions of the data providers, users, and regulators. Besides, a replicator dynamic system is built to study evolutionary stable strategies of multi-stakeholders. In simulations, we investigate the evolution of the cooperation ratio as time progresses under different parameters, which is proved to be in agreement with our theoretical analysis. Furthermore, we explore the influence of the cost of data users to acquire data, the value of open data, the reward (penalty) from the regulators, and the data mining capability of data users to group strategies and uncover some regular patterns. Some meaningful results are also obtained through simulations, which can guide stakeholders to make better decisions in the future."
2502.05851,"In billboard advertisement, a number of digital billboards are owned by an influence provider, and several commercial houses (which we call advertisers) approach the influence provider for a specific number of views of their advertisement content on a payment basis. Though the billboard slot allocation problem has been studied in the literature, this problem still needs to be addressed from a fairness point of view. In this paper, we introduce the Fair Billboard Slot Allocation Problem, where the objective is to allocate a given set of billboard slots among a group of advertisers based on their demands fairly and efficiently. As fairness criteria, we consider the maximin fair share, which ensures that each advertiser will receive a subset of slots that maximizes the minimum share for all the advertisers. We have proposed a solution approach that generates an allocation and provides an approximate maximum fair share. The proposed methodology has been analyzed to understand its time and space requirements and a performance guarantee. It has been implemented with real-world trajectory and billboard datasets, and the results have been reported. The results show that the proposed approach leads to a balanced allocation by satisfying the maximin fairness criteria. At the same time, it maximizes the utility of advertisers."
2502.05906,"We consider a strategic M/M/1 queueing model under a first-come-first-served regime, where customers are split into two classes and class $A$ has priority over class $B$. Customers can decide whether to join the queue or balk, and, in case they have joined the queue, whether and when to renege. We study the equilibrium strategies and compare the equilibrium outcome and the social optimum in the two cases where the social optimum is or is not constrained by priority."
2502.05949,"We study a model of temporal voting where there is a fixed time horizon, and at each round the voters report their preferences over the available candidates and a single candidate is selected. Prior work has adapted popular notions of justified representation as well as voting rules that provide strong representation guarantees from the multiwinner election setting to this model. In our work, we focus on the complexity of verifying whether a given outcome offers proportional representation. We show that in the temporal setting verification is strictly harder than in multiwinner voting, but identify natural special cases that enable efficient algorithms."
2502.06028,"Decentralized perpetuals protocols have collectively reached billions of dollars of daily trading volume, yet are still not serious competitors on the basis of trading volume with centralized venues such as Binance. One of the main reasons for this is the high cost of capital for market makers and sophisticated traders in decentralized settings. Recently, numerous decentralized finance protocols have been used to improve borrowing costs for perpetual futures traders. We formalize this class of mechanisms utilized by protocols such as Jupiter, Hyperliquid, and GMX, which we term~\emph{Perpetual Demand Lending Pools} (PDLPs). We then formalize a general target weight mechanism that generalizes what GMX and Jupiter are using in practice. We explicitly describe pool arbitrage and expected payoffs for arbitrageurs and liquidity providers within these mechanisms. Using this framework, we show that under general conditions, PDLPs are easy to delta hedge, partially explaining the proliferation of live hedged PDLP strategies. Our results suggest directions to improve capital efficiency in PDLPs via dynamic parametrization."
2502.06082,"The COVID-19 pandemic underscored the urgent need for fair and effective allocation of scarce resources, from hospital beds to vaccine distribution. In this paper, we study a healthcare rationing problem where identical units of a resource are divided into different categories, and agents are assigned based on priority rankings. % We first introduce a simple and efficient algorithm that satisfies four fundamental axioms critical to practical applications: eligible compliance, non-wastefulness, respect for priorities, and maximum cardinality. This new algorithm is not only conceptually simpler but also computationally faster than the Reverse Rejecting rules proposed in recent work. % We then extend our analysis to a more general sequential setting, where categories can be processed both sequentially and simultaneously. For this broader framework, we introduce a novel algorithm that preserves the four fundamental axioms while achieving additional desirable properties that existing rules fail to satisfy. Furthermore, we prove that when a strict precedence order over categories is imposed, this rule is the unique mechanism that satisfies these properties."
2502.06278,"This study explores the monotonicity of adaptive clinching auctions -- a key mechanism in budget-constrained auctions -- with respect to fluctuations in the number of bidders. Specifically, we investigate how the addition of new bidders affect efficiency and revenue. In a symmetric setting, where all bidders have equal budgets, we show that while the allocated goods and payments for many bidders decrease, overall both liquid welfare and revenue weakly increase. Our analysis also extends to scenarios where bidders arrive online during the auction. In contrast, for asymmetric budgets, we provide counterexamples showing that these monotonicity properties no longer hold. These findings contribute to a better theoretical understanding of budget-constrained auctions and offer insights into the behavior of adaptive clinching auctions in social networks, where new bidders emerge through information diffusion."
2502.06489,"We study the utilitarian distortion of social choice mechanisms under the recently proposed learning-augmented framework where some (possibly unreliable) predicted information about the preferences of the agents is given as input. In particular, we consider two fundamental social choice problems: single-winner voting and one-sided matching. In these settings, the ordinal preferences of the agents over the alternatives (either candidates or items) is known, and some prediction about their underlying cardinal values is also provided. The goal is to leverage the prediction to achieve improved distortion guarantees when it is accurate, while simultaneously still achieving reasonable worst-case bounds when it is not. This leads to the notions of consistency and robustness, and the quest to achieve the best possible tradeoffs between the two. We show tight tradeoffs between the consistency and robustness of ordinal mechanisms for single-winner voting and one-sided matching, for different levels of information provided by as prediction."
2502.06515,"Exploring measures to improve financial networks and mitigate systemic risks is an ongoing challenge. We study claims trading, a notion defined in Chapter 11 of the U.S. Bankruptcy Code. For a bank $v$ in distress and a trading partner $w$, the latter is taking over some claims of $v$ and in return giving liquidity to $v$. The idea is to rescue $v$ (or mitigate contagion effects from $v$'s insolvency). We focus on the impact of trading claims fractionally, when $v$ and $w$ can agree to trade only part of a claim. In addition, we study donations, in which $w$ only provides liquidity to $v$. They can be seen as special claims trades.When trading a single claim or making a single donation in networks without default cost, we show that it is impossible to strictly improve the assets of both banks $v$ and $w$. Since the goal is to rescue $v$ in distress, we study creditor-positive trades, in which $v$ improves and $w$ remains indifferent. We show that an optimal creditor-positive trade that maximizes the assets of $v$ can be computed in polynomial time. It also yields a (weak) Pareto-improvement for all banks in the entire network. In networks with default cost, we obtain a trade in polynomial time that weakly Pareto-improves all assets over the ones resulting from the optimal creditor-positive trade. We generalize these results to trading multiple claims for which $v$ is the creditor.Instead, when trading claims with a common debtor $u$, we obtain NP-hardness results for computing trades in networks with default cost that maximize the assets of the creditors and Pareto-improve the assets in the network. Similar results apply when $w$ donates to multiple banks in networks with default costs. For networks without default cost, we give an efficient algorithm to compute optimal donations to multiple banks."
2502.06561,"Network creation games are well-established for investigating the decentralized formation of communication networks, like the Internet or social networks. In these games, selfish agents that correspond to network nodes strategically create costly edges to maximize their centrality in the formed network. We depart from this by focusing on the simpler objective of maximizing the 2-neighborhood. This seems natural for social networks, as an agent's connection benefit is typically provided by her neighbors and their neighbors but not by strangers further away.For this natural model, we study the existence, the structure and the quality both of Nash equilibria (NE) and greedy equilibria (GE). We give structural results on the existence of degree-2 paths and cycles, and we provide tight constant bounds on the diameter. In contrast to most previous network creation game research, our bounds on the diameter are independent of edge cost $\alpha$ and the number of agents $n$. Also, bounding the diameter does not imply bounding the price of anarchy, which calls for other methods. Using them, we obtain non-trivial bounds on the price of anarchy, including a $\Omega(\log(\frac{n}{\alpha}))$ lower bound for NE, and a tight linear bound for GE for low $\alpha$."
2502.06749,"We study strategic classification in binary decision-making settings where agents can modify their features in order to improve their classification outcomes. Importantly, our work considers the causal structure across different features, acknowledging that effort in a given feature may affect other features. The main goal of our work is to understand \emph{when and how much agent effort is invested towards desirable features}, and how this is influenced by the deployed classifier, the causal structure of the agent's features, their ability to modify them, and the information available to the agent about the classifier and the feature causal graph.In the complete information case, when agents know the classifier and the causal structure of the problem, we derive conditions ensuring that rational agents focus on features favored by the principal. We show that designing classifiers to induce desirable behavior is generally non-convex, though tractable in special cases. We also extend our analysis to settings where agents have incomplete information about the classifier or the causal graph. While optimal effort selection is again a non-convex problem under general uncertainty, we highlight special cases of partial uncertainty where this selection problem becomes tractable. Our results indicate that uncertainty drives agents to favor features with higher expected importance and lower variance, potentially misaligning with principal preferences. Finally, numerical experiments based on a cardiovascular disease risk study illustrate how to incentivize desirable modifications under uncertainty."
2502.07454,"An election is a pair $(C,V)$ of candidates and voters. Each vote is a ranking (permutation) of the candidates. An election is $d$-Euclidean if there is an embedding of both candidates and voters into $\mathbb{R}^d$ such that voter $v$ prefers candidate $a$ over $b$ if and only if $a$ is closer to $v$ than $b$ is to $v$ in the embedding. For $d\geq 2$ the problem of deciding whether $(C,V)$ is $d$-Euclidean is $\exists \mathbb{R}$-complete.In this paper, we propose practical approach to recognizing and refuting $2$-Euclidean preferences. We design a new class of forbidden substructures that works very well on practical instances. We utilize the framework of integer linear programming (ILP) and quadratically constrained programming (QCP). We also introduce reduction rules that simplify many real-world instances significantly. Our approach beats the previous algorithm of Escoffier, Spanjaard and Tydrichová~[Algorithmic Recognition of 2-Euclidean Preferences, ECAI 2023] both in number of resolved instances and the running time. In particular, we were able to lower the number of unresolved PrefLib instances from $343$ to $60$. Moreover, $98.7\%$ of PrefLib instances are resolved in under $1$ second using our approach."
2502.07593,"In this paper, we propose a probabilistic game-theoretic model to study the properties of the worst-case regret of the greedy strategy under complete (Knightian) uncertainty. In a game between a decision-maker (DM) and an adversarial agent (Nature), the DM observes a realization of product ratings for each product. Upon observation, the DM chooses a strategy, which is a function from the set of observations to the set of products. We study the theoretical properties, including the worst-case regret of the greedy strategy that chooses the product with the highest observed average rating. We prove that, with respect to the worst-case regret, the greedy strategy is optimal and that, in the limit, the regret of the greedy strategy converges to zero. We validate the model on data collected from Google reviews for restaurants, showing that the greedy strategy not only performs according to the theoretical findings but also outperforms the uniform strategy and the Thompson Sampling algorithm."
2502.07606,"Algorithmic trading in modern financial markets is widely acknowledged to exhibit strategic, game-theoretic behaviors whose complexity can be difficult to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress in the setting of trading for position building. Here parties wish to buy or sell a fixed number of shares in a fixed time period in the presence of both temporary and permanent market impact, resulting in exponentially large strategy spaces. While these papers primarily consider the existence and structural properties of equilibrium strategies, in this work we focus on the algorithmic aspects of the proposed model. We give an efficient algorithm for computing best responses, and show that while the temporary impact only setting yields a potential game, best response dynamics do not generally converge for the general setting, for which no fast algorithm for (Nash) equilibrium computation is known. This leads us to consider the broader notion of Coarse Correlated Equilibria (CCE), which we show can be computed efficiently via an implementation of Follow the Perturbed Leader (FTPL). We illustrate the model and our results with an experimental investigation, where FTPL exhibits interesting behavior in different regimes of the relative weighting between temporary and permanent market impact."
2502.07652,"Real populations are seldom found at the Nash equilibrium strategy. The present work focuses on how population size can be a relevant evolutionary force diverting the population from its expected Nash equilibrium. We introduce the concept of insuperable strategy, a strategy that guarantees that no other player can have a larger payoff than the player that adopts it. We show that this concept is different from the rationality assumption frequently used in game theory and that for small populations the insuperable strategy is the most probable evolutionary outcome for any dynamics that equal game payoff and reproductive fitness. We support our ideas with several examples and numerical simulations. We finally discuss how to extend the concept to multiplayer games, introducing, in a limited way, the concept of game reduction."
2502.07975,"Characterizing the limit behavior -- that is, the attractors -- of learning dynamics is one of the most fundamental open questions in game theory. In recent work on this front, it was conjectured that the attractors of the replicator dynamic are in one-to-one correspondence with the sink equilibria of the game -- the sink strongly connected components of a game's preference graph -- , and it was established that they do stand in at least one-to-many correspondence with them. Here, we show that the one-to-one conjecture is false. We disprove this conjecture over the course of three theorems: the first disproves a stronger form of the conjecture, while the weaker form is disproved separately in the two-player and $N$-player ($N>2$) cases. By showing how the conjecture fails, we lay out the obstacles that lie ahead for characterizing attractors of the replicator, and introduce new ideas with which to tackle them. All three counterexamples derive from an object called a local source -- a point lying within the sink equilibrium, and yet which is `locally repelling'; we prove that the absence of local sources is necessary, but not sufficient, for the one-to-one property to be true. We complement this with a sufficient condition: we introduce a local property of a sink equilibrium called pseudoconvexity, and establish that when the sink equilibria of a two-player game are pseudoconvex then they precisely define the attractors. Pseudoconvexity generalizes the previous cases -- such as zero-sum games and potential games -- where this conjecture was known to hold, and reformulates these cases in terms of a simple graph property."
2502.08063,"We conduct a comprehensive analysis of the discrete-time exponential-weights dynamic with a constant step size on all \emph{general-sum and symmetric} $2 \times 2$ normal-form games, i.e. games with $2$ pure strategies per player, and where the ensuing payoff tuple is of the form $(A,A^\top)$ (where $A$ is the $2 \times 2$ payoff matrix corresponding to the first player). Such symmetric games commonly arise in real-world interactions between ""symmetric"" agents who have identically defined utility functions -- such as Bertrand competition, multi-agent performative prediction, and certain congestion games -- and display a rich multiplicity of equilibria despite the seemingly simple setting. Somewhat surprisingly, we show through a first-principles analysis that the exponential weights dynamic, which is popular in online learning, converges in the last iterate for such games regardless of initialization with an appropriately chosen step size. For certain games and/or initializations, we further show that the convergence rate is in fact exponential and holds for any step size.We illustrate our theory with extensive simulations and applications to the aforementioned game-theoretic interactions. In the case of multi-agent performative prediction, we formulate a new ""mortgage competition"" game between lenders (i.e. banks) who interact with a population of customers, and show that it fits into our framework."
2502.08248,"This paper studies allocation mechanisms in max-flow games with players' capacities as private information. We first show that no core-selection mechanism is truthful: there may exist a player whose payoff increases if she under-reports her capacity when a core-section mechanism is adopted. We then introduce five desirable properties for mechanisms in max-flow games: DSIC (truthful reporting is a dominant strategy), SIR (individual rationality and positive payoff for each player contributing positively to at least one coalition), SP (no edge has an incentive to split into parallel edges), MP (no parallel edges have incentives to merge), and CM (a player's payoff does not decrease as another player's capacity and max-flow increase). While the Shapley value mechanism satisfies DSIC and SIR, it fails to meet SP, MP and CM. We propose a new mechanism based on minimal cuts that satisfies all five properties."
2502.08284,"Machine learning (ML) models have become essential tools in various scenarios. Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance. Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data. These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers. However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world. Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data. Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers. Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible. The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to $40\%$ in both MacroF1 and MicroF1."
2502.08412,"We study a repeated resource allocation problem with strategic agents where monetary transfers are disallowed and the central planner has no prior information on agents' utility distributions. In light of Arrow's impossibility theorem, acquiring information about agent preferences through some form of feedback is necessary. We assume that the central planner can request powerful but expensive audits on the winner in any round, revealing the true utility of the winner in that round. We design a mechanism achieving $T$-independent $O(K^2)$ social welfare regret while only requesting $O(K^3 \log T)$ audits in expectation, where $K$ is the number of agents and $T$ is the number of rounds. We also show an $\Omega(K)$ lower bound on the regret and an $\Omega(1)$ lower bound on the number of audits when having low regret. Algorithmically, we show that incentive-compatibility can be mostly enforced via the imposition of adaptive future punishments, where the audit probability is inversely proportional to the winner's future winning probability. To accurately estimate such probabilities in presence of strategic agents, who may adversely react to any potential misestimate, we introduce a flagging component that allows agents to flag any biased estimate (we show that doing so aligns with individual incentives). On the technical side, without a unique and known distribution, one cannot apply the revelation principle and conclude that truthful reporting is exactly an equilibrium. Instead, we characterize the equilibrium via a reduction to a simpler auxiliary game, in which agents cannot strategize until close to the end of the game; we show equilibria in this game can induce equilibria in the actual, fully strategic game. The tools developed therein may be of independent interest for other mechanism design problems in which the revelation principle cannot be readily applied."
2502.08519,"We consider the problem of computing stationary points in min-max optimization, with a particular focus on the special case of computing Nash equilibria in (two-)team zero-sum games.We first show that computing $\epsilon$-Nash equilibria in $3$-player \emph{adversarial} team games -- wherein a team of $2$ players competes against a \emph{single} adversary -- is \textsf{CLS}-complete, resolving the complexity of Nash equilibria in such settings. Our proof proceeds by reducing from \emph{symmetric} $\epsilon$-Nash equilibria in \emph{symmetric}, identical-payoff, two-player games, by suitably leveraging the adversarial player so as to enforce symmetry -- without disturbing the structure of the game. In particular, the class of instances we construct comprises solely polymatrix games, thereby also settling a question left open by Hollender, Maystre, and Nagarajan (2024). We also provide some further results concerning equilibrium computation in adversarial team games.Moreover, we establish that computing \emph{symmetric} (first-order) equilibria in \emph{symmetric} min-max optimization is \textsf{PPAD}-complete, even for quadratic functions. Building on this reduction, we further show that computing symmetric $\epsilon$-Nash equilibria in symmetric, $6$-player ($3$ vs. $3$) team zero-sum games is also \textsf{PPAD}-complete, even for $\epsilon = \text{poly}(1/n)$. As an immediate corollary, this precludes the existence of symmetric dynamics -- which includes many of the algorithms considered in the literature -- converging to stationary points. Finally, we prove that computing a \emph{non-symmetric} $\text{poly}(1/n)$-equilibrium in symmetric min-max optimization is \textsf{FNP}-hard."
2502.08578,"The coordinate-wise median is a classic and most well-studied strategy-proof mechanism in social choice and facility location scenarios. Surprisingly, there is no systematic study of its approximation ratio in $d$-dimensional spaces. The best known approximation guarantee in $d$-dimensional Euclidean space $\mathbb{L}_2(\mathbb{R}^d)$ is $\sqrt{d}$ via embedding $\mathbb{L}_1(\mathbb{R}^d)$ into $\mathbb{L}_2(\mathbb{R}^d)$ metric space, that only appeared in appendix of [Meir 2019].This upper bound is known to be tight in dimension $d=2$, but there are no known super constant lower bounds. Still, it seems that the community's belief about coordinate-wise median is on the side of $\Theta(\sqrt{d})$. E.g., a few recent papers on mechanism design with predictions [Agrawal, Balkanski, Gkatzelis, Ou, Tan 2022], [Christodoulou, Sgouritsa, Vlachos 2024], and [Barak, Gupta, Talgam-Cohen 2024] directly rely on the $\sqrt{d}$-approximation result.In this paper, we systematically study approximate efficiency of the coordinate-median in $\mathbb{L}_{q}(\mathbb{R}^d)$ spaces for any $\mathbb{L}_q$ norm with $q\in[1,\infty]$ and any dimension $d$. We derive a series of constant upper bounds $UB(q)$ independent of the dimension $d$. This series $UB(q)$ is growing with parameter $q$, but never exceeds the constant $UB(\infty)= 3$. Our bound $UB(2)=\sqrt{6\sqrt{3}-8}<1.55$ for $\mathbb{L}_2$ norm is only slightly worse than the tight approximation guarantee of $\sqrt{2}>1.41$ in dimension $d=2$. Furthermore, we show that our upper bounds are essentially tight by giving almost matching lower bounds $LB(q,d)=UB(q)\cdot(1-O(1/d))$ for any dimension $d$ with $LB(q,d)=UB(q)$ when $d\to\infty$. We also extend our analysis to the generalized median mechanism in [Agrawal, Balkanski, Gkatzelis, Ou, Tan 2022] for $\mathbb{L}_2(\mathbb{R}^2)$ space to arbitrary dimensions $d$ with similar results."
2502.08597,"We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our main focus is on comparing Bayesian learners and no-regret learners who compete in markets and identifying the conditions under which each approach is more effective. Surprisingly, we find that low regret is not sufficient for survival: an agent can have regret as low as $O(\log T)$ but still vanish when competing against a Bayesian with a finite prior and any positive prior probability on the correct model. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Motivated by the strengths and weaknesses of both approaches, we propose a balanced strategy for utilizing Bayesian updates that improves robustness and adaptability to distribution shifts, providing a step toward a best-of-both-worlds learning approach. The method is general, efficient, and easy to implement. Finally, we formally establish the relationship between the notions of survival and market dominance studied in economics and the framework of regret minimization, thus bridging these theories. More broadly, our work contributes to the understanding of dynamics with heterogeneous types of learning agents and their impact on markets."
2502.08792,"We investigate a Bayesian mechanism design problem where a seller seeks to maximize revenue by selling an indivisible good to one of n buyers, incorporating potentially unreliable predictions (signals) of buyers' private values derived from a machine learning model. We propose a framework where these signals are sometimes reflective of buyers' true valuations but other times are hallucinations, which are uncorrelated with the buyers' true valuations. Our main contribution is a characterization of the optimal auction under this framework. Our characterization establishes a near-decomposition of how to treat types above and below the signal. For the one buyer case, the seller's optimal strategy is to post one of three fairly intuitive prices depending on the signal, which we call the ""ignore"", ""follow"" and ""cap"" actions."
2502.08827,"We study the NP-hard Stable Hypergraph Matching (SHM) problem and its generalization allowing capacities, the Stable Hypergraph $b$-Matching (SH$b$M) problem, and investigate their computational properties under various structural constraints. Our study is motivated by the fact that Scarf's Lemma (Scarf, 1967) together with a result of Lovász (1972) guarantees the existence of a stable matching whenever the underlying hypergraph is normal. Furthermore, if the hypergraph is unimodular (i.e., its incidence matrix is totally unimodular), then even a stable $b$-matching is guaranteed to exist. However, no polynomial-time algorithm is known for finding a stable matching or $b$-matching in unimodular hypergraphs.We identify subclasses of unimodular hypergraphs where SHM and SH$b$M are tractable such as laminar hypergraphs or so-called subpath hypergraphs with bounded-size hyperedges; for the latter case, even a maximum-weight stable $b$-matching can be found efficiently. We complement our algorithms by showing that optimizing over stable matchings is NP-hard even in laminar hypergraphs. As a practically important special case of SH$b$M for unimodular hypergraphs, we investigate a tripartite stable matching problem with students, schools, and companies as agents, called the University Dual Admission problem, which models real-world scenarios in higher education admissions.Finally, we examine a superclass of subpath hypergraphs that are normal but necessarily not unimodular, namely subtree hypergraphs where hyperedges correspond to subtrees of a tree. We establish that for such hypergraphs, stable matchings can be found in polynomial time but, in the setting with capacities, finding a stable $b$-matching is NP-hard."
2502.08898,"We consider learning outcomes in games with carryover effects between rounds: when outcomes in the present round affect the game in the future. An important example of such systems is routers in networking, as they use simple learning algorithms to find the best way to deliver packets to their desired destination. This simple, myopic, and distributed decision process makes large queuing systems easy to operate, but at the same time, the system needs more capacity than would be required if all traffic were centrally coordinated. Gaitonde and Tardos (EC 2020 and JACM 2023) initiated the study of such systems, modeling them as an infinitely repeated game in which routers compete for servers and the system maintains a state (the number of packets held at each queue) that results from outcomes of previous rounds. However, their model assumes that servers have no buffers at all, so routers have to resend all packets that were not served successfully, which makes their system model unrealistic. They show that in their model, even with hugely increased server capacity relative to what is needed in the centrally coordinated case, ensuring that the system is stable requires the use of timestamps and priority for older packets.We consider a system with two important changes, which make the model more realistic and allow for much higher traffic rates: first, we add a very small buffer to each server, allowing the server to hold on to a single packet to be served later (if it fails to serve it immediately), and second, we do not require timestamps or priority to older packets. Using theoretical analysis and simulations, we show that when queues are learning, a small constant-factor increase in server capacity, compared to what would be needed if centrally coordinating, suffices to keep the system stable, even if servers select randomly among packets arriving simultaneously."
2502.08976,"A decisionmaker faces $n$ alternatives, each of which represents a potential reward. After investing costly resources into investigating the alternatives, the decisionmaker may select one, or more generally a feasible subset, and obtain the associated reward(s). The objective is to maximize the sum of rewards minus total costs invested. We consider this problem under a general model of an alternative as a ""Markov Search Process,"" a type of undiscounted Markov Decision Process on a finite acyclic graph. Even simple cases generalize NP-hard problems such as Pandora's Box with nonobligatory inspection.Despite the apparently adaptive and interactive nature of the problem, we prove optimal prophet inequalities for this problem under a variety of combinatorial constraints. That is, we give approximation algorithms that interact with the alternatives sequentially, where each must be fully explored and either selected or else discarded before the next arrives. In particular, we obtain a computationally efficient $\frac{1}{2}-\epsilon$ prophet inequality for Combinatorial Markov Search subject to any matroid constraint. This result implies incentive-compatible mechanisms with constant Price of Anarchy for serving single-parameter agents when the agents strategically conduct independent, costly search processes to discover their values."
2502.09006,"We explore solutions for fairly allocating indivisible items among agents assigned weights representing their entitlements. Our fairness goal is weighted-envy-freeness (WEF), where each agent deems their allocated portion relative to their entitlement at least as favorable as any others relative to their own. Often, achieving WEF necessitates monetary transfers, which can be modeled as third-party subsidies. The goal is to attain WEF with bounded subsidies.Previous work relied on characterizations of unweighted envy-freeness (EF), that fail in the weighted setting. This makes our new setting challenging. We present polynomial-time algorithms that compute WEF allocations with a guaranteed upper bound on total subsidy for monotone valuations and various subclasses thereof.We also present an efficient algorithm to compute a fair allocation of items and money, when the budget is not enough to make the allocation WEF. This algorithm is new even for the unweighted setting."
2502.09014,"This paper explores the design of contests involving $n$ contestants, focusing on how the designer decides on the number of contestants allowed and the prize structure with a fixed budget. We characterize the unique symmetric Bayesian Nash equilibrium of contestants and find the optimal contests design for the maximum individual effort objective and the total effort objective."
2502.09129,"This paper proposes a new differentially private distributed Nash equilibrium seeking algorithm for aggregative games under time-varying unbalanced directed communication graphs. Random independent Laplace noises are injected into the transmitted information to protect players' sensitive information. Then, the push-sum consensus protocol is utilized to estimate the aggregate function with the perturbed information under the time-varying topologies. The weakening factor and the momentum term are designed to attenuate the negative affect of the noise and guarantee the convergence of the algorithm, respectively. The algorithm is then proven to ensure the almost sure convergence, as well as rigorous differential privacy with a finite cumulative privacy budget, without requiring a trade-off between provable convergence and differential privacy. Finally, the simulation is provided to demonstrate the effectiveness of the proposed algorithm."
2502.09265,"Choice correspondences are crucial in decision-making, especially when faced with indifferences or ties. While tie-breaking can transform a choice correspondence into a choice function, it often introduces inefficiencies. This paper introduces a novel notion of path-independence (PI) for choice correspondences, extending the existing concept of PI for choice functions. Intuitively, a choice correspondence is PI if any consistent tie-breaking produces a PI choice function. This new notion yields several important properties. First, PI choice correspondences are rationalizabile, meaning they can be represented as the maximization of a utility function. This extends a core feature of PI in choice functions. Second, we demonstrate that the set of choices selected by a PI choice correspondence for any subset forms a generalized matroid. This property reveals that PI choice correspondences exhibit a nice structural property. Third, we establish that choice correspondences rationalized by ordinally concave functions inherently satisfy the PI condition. This aligns with recent findings that a choice function satisfies PI if and only if it can be rationalized by an ordinally concave function. Building on these theoretical foundations, we explore stable and efficient matchings under PI choice correspondences. Specifically, we investigate constrained efficient matchings, which are efficient (for one side of the market) within the set of stable matchings. Under responsive choice correspondences, such matchings are characterized by cycles. However, this cycle-based characterization fails in more general settings. We demonstrate that when the choice correspondence of each school satisfies both PI and monotonicity conditions, a similar cycle-based characterization is restored. These findings provide new insights into the matching theory and its practical applications."
2502.09377,"We introduce and formalize the notion of resource augmentation for maximin share allocations -- an idea that can be traced back to the seminal work of Budish [JPE 2011]. Specifically, given a fair division instance with $m$ goods and $n$ agents, we ask how many copies of the goods should be added in order to guarantee that each agent receives at least their original maximin share, or an approximation thereof. We establish a tight bound of $m/e$ copies for arbitrary monotone valuations. For additive valuations, we show that at most $\min\{n-2,\lfloor \frac{m}{3}\rfloor (1+o(1))\}$ copies suffice. For approximate-MMS in ordered instances, we give a tradeoff between the number of copies needed and the approximation guarantee. In particular, we prove that $\lfloor n/2 \rfloor$ copies suffice to guarantee a $6/7$-approximation to the original MMS, and $\lfloor n/3 \rfloor$ copies suffice for a $4/5$-approximation. Both results improve upon the best known approximation guarantees for additive valuations in the absence of copies."
2502.09777,"We study the problem of ""fairly"" dividing indivisible goods to several agents that have valuation set functions over the sets of goods. As fair we consider the allocations that are envy-free up to any good (EFX), i.e., no agent envies any proper subset of the goods given to any other agent. The existence or not of EFX allocations is a major open problem in Fair Division, and there are only positive results for special cases.[George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023] introduced a restriction on the agents' valuations according to a graph structure: the vertices correspond to agents and the edges to goods, and each vertex/agent has zero marginal value (or in other words, they are indifferent) for the edges/goods that are not adjacent to them. The existence of EFX allocations has been shown for simple graphs with general monotone valuations [George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023], and for multigraphs for restricted additive valuations [Alireza Kaviani, Masoud Seddighin, Amir Mohammad Shahrezaei 2024].In this work, we push the state-of-the-art further, and show that the EFX allocations always exists in multigraphs and general monotone valuations if any of the following three conditions hold: either (a) the multigraph is bipartite, or (b) each agent has at most $\lceil \frac{n}{4} \rceil -1$ neighbors, where $n$ is the total number of agents, or (c) the shortest cycle with non-parallel edges has length at least 6."
2502.09907,"First-price auctions are one of the most popular mechanisms for selling goods and services, with applications ranging from display advertising to timber sales. Unlike their close cousin, the second-price auction, first-price auctions do not admit a dominant strategy. Instead, each buyer must design a bidding strategy that maps values to bids -- a task that is often challenging due to the lack of prior knowledge about competing bids. To address this challenge, we conduct a principled analysis of prior-independent bidding strategies for first-price auctions using worst-case regret as the performance measure. First, we develop a technique to evaluate the worst-case regret for (almost) any given value distribution and bidding strategy, reducing the complex task of ascertaining the worst-case competing-bid distribution to a simple line search. Next, building on our evaluation technique, we minimize worst-case regret and characterize a minimax-optimal bidding strategy for every value distribution. We achieve it by explicitly constructing a bidding strategy as a solution to an ordinary differential equation, and by proving its optimality for the intricate infinite-dimensional minimax problem underlying worst-case regret minimization. Our construction provides a systematic and computationally-tractable procedure for deriving minimax-optimal bidding strategies. When the value distribution is continuous, it yields a deterministic strategy that maps each value to a single bid. We also show that our minimax strategy significantly outperforms the uniform-bid-shading strategies advanced by prior work. Our result allows us to precisely quantify, through minimax regret, the performance loss due to a lack of knowledge about competing bids. We leverage this to analyze the impact of the value distribution on the performance loss, and find that it decreases as the buyer's values become more dispersed."
2502.09962,"We consider a two-sided matching problem in which the agents on one side have dichotomous preferences and the other side representing institutions has strict preferences (priorities). It captures several important applications in matching market design in which the agents are only interested in getting matched to an acceptable institution. These include centralized daycare assignment and healthcare rationing. We present a compelling new mechanism that satisfies many prominent and desirable properties including individual rationality, maximum size, fairness, Pareto-efficiency on both sides, strategyproofness on both sides, non-bossiness and having polynomial time running time. As a result, we answer an open problem whether there exists a mechanism that is agent-strategyproof, maximum, fair and non-bossy."
2502.10068,We show that the proportional clustering problem using the Droop quota for $k = 1$ is equivalent to the $\beta$-plurality problem. We also show that the Plurality Veto rule can be used to select ($\sqrt{5} - 2$)-plurality points using only ordinal information about the metric space and resolve an open question of Kalayci et al. (AAAI 2024) by proving that $(2+\sqrt{5})$-proportionally fair clusterings can be found using purely ordinal information.
2502.10149,"Recent advancements in intelligent reflecting surfaces (IRS) and mobile edge computing (MEC) offer new opportunities to enhance the performance of vehicular networks. However, meeting the computation-intensive and latency-sensitive demands of vehicles remains challenging due to the energy constraints and dynamic environments. To address this issue, we study an IRS-assisted MEC architecture for vehicular networks. We formulate a multi-objective optimization problem aimed at minimizing the total task completion delay and total energy consumption by jointly optimizing task offloading, IRS phase shift vector, and computation resource allocation. Given the mixed-integer nonlinear programming (MINLP) and NP-hard nature of the problem, we propose a generative diffusion model (GDM)-based Stackelberg game (GDMSG) approach. Specifically, the problem is reformulated within a Stackelberg game framework, where generative GDM is integrated to capture complex dynamics to efficiently derive optimal solutions. Simulation results indicate that the proposed GDMSG achieves outstanding performance compared to the benchmark approaches."
2502.10304,"Although synergy is an important concept that is strongly ingrained in games, it has not been widely discussed by the games community. This is due to the vagueness of the concept and the fact that there is no clear agreement on what it means. To solve this, we present a strict definition of what is synergy. Then we propose a methodology to use this definition to analyze synergy in games. Applying this definition to various games (Chess, League of Legends, and Magic: The Gathering), we illustrate how it can be used to solve many of the practical problems related to synergy."
2502.10516,"A classical problem in combinatorics seeks colorings of low discrepancy. More concretely, the goal is to color the elements of a set system so that the number of appearances of any color among the elements in each set is as balanced as possible. We present a new lower bound for multi-color discrepancy, showing that there is a set system with $n$ subsets over a set of elements in which any $k$-coloring of the elements has discrepancy at least $\Omega\left(\sqrt{\frac{n}{\ln{k}}}\right)$. This result improves the previously best-known lower bound of $\Omega\left(\sqrt{\frac{n}{k}}\right)$ of Doerr and Srivastav [2003] and may have several applications. Here, we explore its implications on the feasibility of fair division concepts for instances with $n$ agents having valuations for a set of indivisible items. The first such concept is known as consensus $1/k$-division up to $d$ items (\cd$d$) and aims to allocate the items into $k$ bundles so that no matter which bundle each agent is assigned to, the allocation is envy-free up to $d$ items. The above lower bound implies that \cd$d$ can be infeasible for $d\in \Omega\left(\sqrt{\frac{n}{\ln{k}}}\right)$. We furthermore extend our proof technique to show that there exist instances of the problem of allocating indivisible items to $k$ groups of $n$ agents in total so that envy-freeness and proportionality up to $d$ items are infeasible for $d\in \Omega\left(\sqrt{\frac{n}{k\ln{k}}}\right)$ and $d\in \Omega\left(\sqrt{\frac{n}{k^3\ln{k}}}\right)$, respectively. The lower bounds for fair division improve the currently best-known ones by Manurangsi and Suksompong [2022]."
2502.10592,"Universities regularly face the challenging task of assigning classes to thousands of students while considering their preferences, along with course schedules and capacities. Ensuring the effectiveness and fairness of course allocation mechanisms is crucial to guaranteeing student satisfaction and optimizing resource utilization. We approach this problem from an economic perspective, using formal justice criteria to evaluate different algorithmic frameworks. To evaluate our frameworks, we conduct a large scale survey of university students at University of Massachusetts Amherst, collecting over 1,000 student preferences. This is, to our knowledge, the largest publicly available dataset of student preferences. We develop software for generating synthetic student preferences over courses, and implement four allocation algorithms: the serial dictatorship algorithm used by University of Massachusetts Amherst; Round Robin; an Integer Linear Program; and the Yankee Swap algorithm. We propose improvements to the Yankee Swap framework to handle scenarios with item multiplicities. Through experimentation with the Fall 2024 Computer Science course schedule at University of Massachusetts Amherst, we evaluate each algorithm's performance relative to standard justice criteria, providing insights into fair course allocation in large university settings."
2502.10765,"As the next-generation Internet paradigm, the metaverse can provide users with immersive physical-virtual experiences without spatial limitations. However, there are various concerns to be overcome, such as resource allocation, resource pricing, and transaction security issues. To address the above challenges, we integrate blockchain technology into the metaverse to manage and automate complex interactions effectively and securely utilizing the advantages of blockchain. With the objective of promoting the Quality of Experience (QoE), Metaverse Service Users (MSUs) purchase rendering and bandwidth resources from the Metaverse Service Provider (MSP) to access low-latency and high-quality immersive services. The MSP maximizes the profit by controlling the unit prices of resources. In this paper, we model the interaction between the MSP and MSUs as a Stackelberg game, in which the MSP acts as the leader and MSUs are followers. The existence of Stackelberg equilibrium is analyzed and proved mathematically. Besides, we propose an efficient greedy-and-search-based resource allocation and pricing algorithm (GSRAP) to solve the Stackelberg equilibrium (SE) point. Finally, we conduct extensive simulations to verify the effectiveness and efficiency of our designs. The experiment results show that our algorithm outperforms the baseline scheme in terms of improving the MSP's profit and convergence speed."
2502.11148,"We investigate the problem of designing randomized obviously strategy-proof (OSP) mechanisms in several canonical auction settings. Obvious strategy-proofness, introduced by Li [American Economic Review, 2017], strengthens the well-known concept of dominant-strategy incentive compatibility (DSIC). Loosely speaking, it ensures that even agents who struggle with contingent reasoning can identify that their dominant strategy is optimal.Thus, one would hope to design OSP mechanisms with good approximation guarantees. Unfortunately, Ron [SODA,2024] has shown that deterministic OSP mechanisms fail to achieve an approximation better than $\min\{m,n\}$ where $m$ is the number of items and $n$ is the number of bidders, even for the simple settings of additive and unit-demand bidders. We circumvent these impossibilities by showing that randomized mechanisms that are obviously strategy-proof in the universal sense obtain a constant factor approximation for these classes. We show that this phenomenon occurs also for the setting of a multi-unit auction with single-minded bidders. Thus, our results provide a more positive outlook on the design of OSP mechanisms and exhibit a stark separation between the power of randomized and deterministic OSP mechanisms.To complement the picture, we provide impossibilities for randomized OSP mechanisms in each setting. While the deterministic VCG mechanism is well known to output an optimal allocation in dominant strategies, we show that even randomized OSP mechanisms cannot obtain more than $87.5\%$ of the optimal welfare. This further demonstrates that OSP mechanisms are significantly weaker than dominant-strategy mechanisms."
2502.11449,"We study Walrasian economies (or general equilibrium models) and their solution concept, the Walrasian equilibrium. A key challenge in this domain is identifying price-adjustment processes that converge to equilibrium. One such process, tâtonnement, is an auction-like algorithm first proposed in 1874 by Léon Walras. While continuous-time variants of tâtonnement are known to converge to equilibrium in economies satisfying the Weak Axiom of Revealed Preferences (WARP), the process fails to converge in a pathological Walrasian economy known as the Scarf economy. To address these issues, we analyze Walrasian economies using variational inequalities (VIs), an optimization framework. We introduce the class of mirror extragradient algorithms, which, under suitable Lipschitz-continuity-like assumptions, converge to a solution of any VI satisfying the Minty condition in polynomial time. We show that the set of Walrasian equilibria of any balanced economy-which includes among others Arrow-Debreu economies-corresponds to the solution set of an associated VI that satisfies the Minty condition but is generally discontinuous. Applying the mirror extragradient algorithm to this VI we obtain a class of tâtonnement-like processes, which we call the mirror extratâtonnement process. While our VI formulation is generally discontinuous, it is Lipschitz-continuous in variationally stable Walrasian economies with bounded elasticity-including those satisfying WARP and the Scarf economy-thus establishing the polynomial-time convergence of mirror extratâtonnement in these economies. We validate our approach through experiments on large Arrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well as the Scarf economy, demonstrating fast convergence in all cases without failure."
2502.11645,"Many real-world multi-agent or multi-task evaluation scenarios can be naturally modelled as normal-form games due to inherent strategic (adversarial, cooperative, and mixed motive) interactions. These strategic interactions may be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or complementary (e.g. niche finding and specialization). In such a formulation, it is the strategies (actions, policies, agents, models, tasks, prompts, etc.) that are rated. However, the rating problem is complicated by redundancy and complexity of N-player strategic interactions. Repeated or similar strategies can distort ratings for those that counter or complement them. Previous work proposed ``clone invariant'' ratings to handle such redundancies, but this was limited to two-player zero-sum (i.e. strictly competitive) interactions. This work introduces the first N-player general-sum clone invariant rating, called deviation ratings, based on coarse correlated equilibria. The rating is explored on several domains including LLMs evaluation."
2502.11661,"We study a Bayesian contract design problem in which a principal interacts with an unknown agent. We consider the single-parameter uncertainty model introduced by Alon et al. [2021], in which the agent's type is described by a single parameter, i.e., the cost per unit-of-effort. Despite its simplicity, several works have shown that single-dimensional contract design is not necessarily easier than its multi-dimensional counterpart in many respects. Perhaps the most surprising result is the reduction by Castiglioni et al . [2025] from multi- to single-dimensional contract design. However, their reduction preserves only multiplicative approximations, leaving open the question of whether additive approximations are easier to obtain than multiplicative ones. In this paper, we answer this question -- to some extent -- positively. In particular, we provide an additive PTAS for these problems while also ruling out the existence of an additive FPTAS. This, in turn, implies that no reduction from multi- to single-dimensional contracts can preserve additive approximations. Moreover, we show that single-dimensional contract design is fundamentally easier than its multi-dimensional counterpart from a learning perspective. Under mild assumptions, we show that optimal contracts can be learned efficiently, providing results on both regret and sample complexity."
2502.11797,"Participatory budgeting (PB) is a form of citizen participation that allows citizens to decide how public funds are spent. Through an election, citizens express their preferences on various projects (spending proposals). A voting mechanism then determines which projects will be approved. The Method of Equal Shares (MES) is the state of the art algorithm for a proportional, voting based approach to participatory budgeting and has been implemented in cities across Poland and Switzerland. A significant drawback of MES is that it is not \textit{exhaustive} meaning that it often leaves a portion of the budget unspent that could be used to fund additional projects. To address this, in practice the algorithm is combined with a completion heuristic - most often the ``add-one"" heuristic which artificially increases the budget until a heuristically chosen threshold. This heuristic is computationally inefficient and will become computationally impractical if PB is employed on a larger scale. We propose the more efficient \textsc{add-opt} heuristic for Exact Equal Shares (EES), a variation of MES that is known to retain many of its desirable properties. We solve the problem of identifying the next budget for which the outcome for EES changes in $O(mn)$ time for cardinal utilities and $O(m^2n)$ time for uniform utilities, where $m$ is the number of projects and $n$ is the number of voters. Our solution to this problem inspires the efficient \textsc{add-opt} heuristic which bypasses the need to search through each intermediary budget. We perform comprehensive experiments on real-word PB instances from Pabulib and show that completed EES outcomes usually match the proportion of budget spent by completed MES outcomes. Furthermore, the \textsc{add-opt} heuristic matches the proportion of budget spend by add-one for EES."
2502.11822,"Tradable credit schemes (TCS) have been attracting interest from the transportation research community as an appealing alternative to congestion pricing, due to the advantages of revenue neutrality and equity. Nonetheless, existing research has largely employed network and market equilibrium approaches with simplistic characterizations of transportation demand, supply, credit market operations, and market behavior. Agent- and activity-based simulation affords a natural means to comprehensively assess TCS by more realistically modeling demand, supply, and individual market interactions. We propose an integrated simulation framework for modeling a TCS, and implements it within the state-of-the-art open-source urban simulation platform SimMobility, including: (a) a flexible TCS design that considers multiple trips and explicitly accounts for individual trading behaviors; (b) a simulation framework that captures the complex interactions between a TCS regulator, the traveler, and the TCS market itself, with the flexibility to test future TCS designs and relevant mobility models; and (c) a set of simulation experiments on a large mesoscopic multimodal network combined with a Bayesian Optimization approach for TCS optimal design. The experiment results indicate network and market performance to stabilize over the day-to-day process, showing the alignment of our agent-based simulation with the known theoretical properties of TCS. We confirm the efficiency of TCS in reducing congestion under the adopted market behavioral assumptions and open the door for simulating different individual behaviors. We measure how TCS impacts differently the local network, heterogeneous users, the different travel behaviors, and how testing different TCS designs can avoid negative market trading behaviors."
2502.11964,"Given the low throughput of blockchains like Bitcoin and Ethereum, scalability - the ability to process an increasing number of transactions - has become a central focus of blockchain research. One promising approach is the parallelization of transaction execution across multiple threads. However, achieving efficient parallelization requires a redesign of the incentive structure within the fee market. Currently, the fee market does not differentiate between transactions that access multiple high-demand storage keys (i.e., unique identifiers for individual data entries) versus a single low-demand one, as long as they require the same computational effort. Addressing this discrepancy is crucial for enabling more effective parallel execution.In this work, we aim to bridge the gap between the current fee market and the need for parallel execution by exploring alternative fee market designs. To this end, we propose a framework consisting of two key components: a Gas Computation Mechanism (GCM), which quantifies the load a transaction places on the network in terms of parallelization and computation, measured in units of gas, and a Transaction Fee Mechanism (TFM), which assigns a price to each unit of gas. We additionally introduce a set of desirable properties for a GCM, propose several candidate mechanisms, and evaluate them against these criteria. Our analysis highlights two strong candidates: the weighted area GCM, which integrates smoothly with existing TFMs such as EIP-1559 and satisfies a broad subset of the outlined properties, and the time-proportional makespan GCM, which assigns gas costs based on the context of the entire block's schedule and, through this dependence on the overall execution outcome, captures the dynamics of parallel execution more accurately."
2502.12042,"Coordinating the behaviour of self-interested agents in the presence of multiple Nash equilibria is a major research challenge for multi-agent systems. Pre-game communication between all the players can aid coordination in cases where the Pareto-optimal payoff is unique, but can lead to deadlocks when there are multiple payoffs on the Pareto frontier. We consider a communication partition, where only players within the same coalition can communicate with each other, and they can establish an agreement (a coordinated joint-action) if it is envy-free, credible, and Pareto-optimal. We show that under a natural assumption about symmetry, certain communication partitions can induce social optimal outcomes in singleton congestion games. This game is a reasonable model for a decentralised, anonymous system where players are required to choose from a range of identical resources, and incur costs that are increasing and convex in the total number of players sharing the same resource. The communication partition can be seen as a mechanism for inducing efficient outcomes in this context."
2502.12117,"Sellers often prescreen potential bidders, restricting participation to a select group of capable participants. Recent advances in machine learning and generative AI make this strategy increasingly viable by enabling the cost-effective identification of high-quality bidders. However, the practice departs from classic auction theory, which usually favors broad competition over selective exclusion. In this paper, we examine whether and under what conditions bidder prescreening can be justified. We analyze a setting in which bidders have independent and identically distributed private valuations, and the seller observes noisy signals generated by a valuation predictor. The seller determines how many top bidders to admit and, after receiving signals, selects exactly that many with the highest signal-based rankings. We demonstrate that an auction with prescreening is equivalent to a standard auction (i.e., without prescreening) but with correlated valuations. Our analysis shows that, although admitting fewer bidders leads to revenue losses in both second-price and first-price auctions, a more accurate predictor can mitigate or even fully offset these losses. In contrast, prescreening can significantly boost revenue in all-pay auctions; notably, when the predictor is perfect, admitting only two bidders is optimal. All results remain valid in the presence of reserve prices."
2502.12306,"One of the main challenges in mechanism design is to carefully engineer incentives ensuring truthfulness while maintaining strong social welfare approximation guarantees. But these objectives are often in conflict, making it impossible to design effective mechanisms. An important class of mechanism design problems that belong to this category are budget-feasible mechanisms. Here, the designer needs to procure services of maximum value from a set of agents while being on a budget, i.e., having a limited budget to enforce truthfulness. However, as empirical studies suggest, factors like limited information and bounded rationality question the idealized assumption that the agents behave perfectly rationally. Motivated by this, Troyan and Morill in 2022 introduced non-obvious manipulability (NOM) as a more lenient incentive compatibility notion. In this paper, we investigate whether resorting to NOM enables us to derive improved mechanisms in budget-feasible domains. We establish a tight bound of 2 on the approximation guarantee of budget-feasible mechanisms satisfying NOM for the general class of monotone subadditive valuation functions. Our result thus establishes a clear separation between the achievable guarantees for DSIC (perfectly rational agents) and NOM (imperfectly rational agents) as no truthful mechanism can achieve a guarantee better than 2.41. Along the way, we fully characterize BNOM and WNOM (which together form NOM) and derive matching upper and lower bounds, respectively. Conceptually, our characterization results suggest ""Golden Tickets"" and ""Wooden Spoons"" as natural means to realize BNOM and WNOM, respectively. Additionally, we show that randomized budget-feasible mechanisms satisfying BNOM can achieve an expected approximation ratio arbitrarily close to 1."
2502.12313,"We consider the fundamental scenario where a single item is to be sold to one of two agents. Both agents draw their valuation for the item from the same probability distribution. However, only one of them submits a bid to the mechanism. The other agent is profiled, i.e., the mechanism receives a prediction for her valuation, which can be true or false. Our goal is to design mechanisms for selling the item that make as much revenue as possible in cases of a correct or incorrect prediction. As a benchmark for proving our revenue-approximation guarantees, we use the maximum expected revenue that can be obtained by a strategic and an honest bidder. We study two mechanisms. The first one yields optimal revenue when the prediction is guaranteed to be correct and a constant revenue approximation when the prediction is incorrect, assuming that the agent valuations are drawn from a monotone hazard rate (MHR) distribution. The second mechanism ignores the prediction for the second agent and simulates the revenue-optimal mechanism when no bid information for the bidders is available. We prove, again assuming that valuations are drawn from MHR distributions, that this mechanism achieves a constant revenue approximation guarantee compared to our revenue benchmark. The MHR assumption is necessary; we show that there are non-MHR but regular probability distributions for which no constant approximation of our revenue benchmark is possible."
2502.12542,"Aggregating preferences under incomplete or constrained feedback is a fundamental problem in social choice and related domains. While prior work has established strong impossibility results for pairwise comparisons, this paper extends the inquiry to improvement feedback, where voters express incremental adjustments rather than complete preferences. We provide a complete characterization of the positional scoring rules that can be computed given improvement feedback. Interestingly, while plurality is learnable under improvement feedback--unlike with pairwise feedback--strong impossibility results persist for many other positional scoring rules. Furthermore, we show that improvement feedback, unlike pairwise feedback, does not suffice for the computation of any Condorcet-consistent rule. We complement our theoretical findings with experimental results, providing further insights into the practical implications of improvement feedback for preference aggregation."
2502.12644,"Envy-freeness is one of the most prominent fairness concepts in the allocation of indivisible goods. Even though trivial envy-free allocations always exist, rich literature shows this is not true when one additionally requires some efficiency concept (e.g., completeness, Pareto-efficiency, or social welfare maximization). In fact, in such case even deciding the existence of an efficient envy-free allocation is notoriously computationally hard. In this paper, we explore the limits of efficient computability by relaxing standard efficiency concepts and analyzing how this impacts the computational complexity of the respective problems. Specifically, we allow partial allocations (where not all goods are allocated) and impose only very mild efficiency constraints, such as ensuring each agent receives a bundle with positive utility. Surprisingly, even such seemingly weak efficiency requirements lead to a diverse computational complexity landscape. We identify several polynomial-time solvable or fixed-parameter tractable cases for binary utilities, yet we also find NP-hardness in very restricted scenarios involving ternary utilities."
2502.12766,"With the rise of online applications, recommender systems (RSs) often encounter constraints in balancing exploration and exploitation. Such constraints arise when exploration is carried out by agents whose utility must be taken into account when optimizing overall welfare. A recent work by Bahar et al. (2020) suggests that recommendations should be \emph{mechanism-informed individually rational} (MIR). Specifically, if agents have a default arm they would use, relying on the RS should yield each agent at least the reward of the default arm, conditioned on the information available to the RS. Under the MIR constraint, striking a balance between exploration and exploitation becomes a complex planning problem. To that end, Bahar et al. propose an approximately optimal yet inefficient planning algorithm that runs in $O(2^K K^2 H^2)$, where $K$ is the number of arms and $H$ is the size of the support of the reward distributions. In this paper, we make a significant improvement for a special yet practical case, removing both the dependence on $H$ and the exponential dependence on $K$. We assume a stochastic order of the rewards (e.g., Gaussian with unit variance, Bernoulli, etc.), and devise an asymptotically optimal algorithm with a runtime of $O(K \log K)$. Our technique is based on formulating a Goal Markov Decision Process (GMDP), establishing an optimal dynamic programming procedure, and then unveiling its crux -- fleshing out a simple index-based structure that facilitates efficient computation. Additionally, we present an incentive-compatible version of our algorithm."
2502.12798,"Explore-and-exploit tradeoffs play a key role in recommendation systems (RSs), aiming at serving users better by learning from previous interactions. Despite their commercial success, the societal effects of explore-and-exploit mechanisms are not well understood, especially regarding the utility discrepancy they generate between different users. In this work, we measure such discrepancy using the economic notion of envy. We present a multi-armed bandit-like model in which every round consists of several sessions, and rewards are realized once per round. We call the latter property reward consistency, and show that the RS can leverage this property for better societal outcomes. On the downside, doing so also generates envy, as late-to-arrive users enjoy the information gathered by early-to-arrive users. We examine the generated envy under several arrival order mechanisms and virtually any anonymous algorithm, i.e., any algorithm that treats all similar users similarly without leveraging their identities. We provide tight envy bounds on uniform arrival and upper bound the envy for nudged arrival, in which the RS can affect the order of arrival by nudging its users. Furthermore, we study the efficiency-fairness trade-off by devising an algorithm that allows constant envy and approximates the optimal welfare in restricted settings. Finally, we validate our theoretical results empirically using simulations."
2502.13122,"We study the social efficiency of bilateral trade between a seller and a buyer. In the classical Bayesian setting, the celebrated Myerson-Satterthwaite impossibility theorem states that no Bayesian incentive-compatible, individually rational, and budget-balanced mechanism can achieve full efficiency. As a counterpoint, Deng, Mao, Sivan, and Wang (STOC 2022) show that if pricing power is delegated to the right person (either the seller or the buyer), the resulting mechanism can guarantee at least a constant fraction of the ideal (yet unattainable) gains from trade.In practice, the agent with pricing power may not have perfect knowledge of the value distribution of the other party, and instead may rely on samples of that distribution to set a price. We show that for a broad class of sampling and pricing behaviors, the resulting market still guarantees a constant fraction of the ideal gains from trade in expectation. Our analysis hinges on the insight that social welfare under sample-based pricing approximates the seller's optimal revenue -- a result we establish via a reduction to a random walk."
2502.13334,"Motivated by the recent popularity of machine learning training services, we introduce a contract design problem in which a provider sells a service that results in an outcome of uncertain quality for the buyer. The seller has a set of actions that lead to different distributions over outcomes. We focus on a setting in which the seller has the ability to commit to an action and the buyer is free to accept or reject the outcome after seeing its realized quality. We propose a two-stage payment scheme where the seller designs a menu of contracts, each of which specifies an action, an upfront price and a vector of outcome-dependent usage prices. Upon selecting a contract, the buyer pays the upfront price, and after observing the realized outcome, the buyer either accepts and pays the corresponding usage price, or rejects and is exempt from further payment. We show that this two-stage payment structure is necessary to maximize profit: only upfront prices or only usage prices is insufficient. We then study the computational complexity of computing a profit-maximizing menu in our model. While computing the exact maximum seller profit is NP-hard even for two buyer types, we derive a fully-polynomial time approximation scheme (FPTAS) for the maximum profit for a constant number of buyer types. Finally, we prove that in the single-parameter setting in which buyers' valuations are parametrized by a single real number that seller revenue can be maximized using a menu consisting of a single contract."
2502.13375,"Schelling games use a game-theoretic approach to study the phenomenon of residential segregation as originally modeled by Schelling. Inspired by the recent increase in the number of people and businesses preferring and promoting diversity, we propose swap games under three diversity-seeking utility functions: the binary utility of an agent is 1 if it has a neighbor of a different type, and 0 otherwise; the difference-seeking utility of an agent is equal to the number of its neighbors of a different type; the variety-seeking utility of an agent is equal to the number of types different from its own in its neighborhood. We consider four global measures of diversity: degree of integration, number of colorful edges, neighborhood variety, and evenness, and prove asymptotically tight or almost tight bounds on the price of anarchy with respect to these measures on both general graphs, as well as on cycles, cylinders, and tori that model residential neighborhoods. We complement our theoretical results with simulations of our swap games starting either from random placements of agents, or from segregated placements. Our simulation results are generally consistent with our theoretical results, showing that segregation is effectively removed when agents are diversity-seeking; however strong diversity, such as measured by neighborhood variety and evenness, is harder to achieve by our swap games."
2502.1341,"Common sense suggests that when individuals explain why they believe something, we can arrive at more accurate conclusions than when they simply state what they believe. Yet, there is no known mechanism that provides incentives to elicit explanations for beliefs from agents. This likely stems from the fact that standard Bayesian models make assumptions (like conditional independence of signals) that preempt the need for explanations, in order to show efficient information aggregation. A natural justification for the value of explanations is that agents' beliefs tend to be drawn from overlapping sources of information, so agents' belief reports do not reveal all that needs to be known. Indeed, this work argues that rationales-explanations of an agent's private information-lead to more efficient aggregation by allowing agents to efficiently identify what information they share and what information is new. Building on this model of rationales, we present a novel 'deliberation mechanism' to elicit rationales from agents in which truthful reporting of beliefs and rationales is a perfect Bayesian equilibrium."
2502.13541,"We consider fair allocation of $m$ indivisible items to $n$ agents of equal entitlements, with submodular valuation functions. Previously, Seddighin and Seddighin [{\em Artificial Intelligence} 2024] proved the existence of allocations that offer each agent at least a $\frac{1}{c \log n \log\log n}$ fraction of her maximin share (MMS), where $c$ is some large constant (over 1000, in their work). We modify their algorithm and improve its analysis, improving the ratio to $\frac{1}{14 \log n}$.Some of our improvement stems from tighter analysis of concentration properties for the value of any subadditive valuation function $v$, when considering a set $S' \subseteq S$ of items, where each item of $S$ is included in $S'$ independently at random (with possibly different probabilities). In particular, we prove that up to less than the value of one item, the median value of $v(S')$, denoted by $M$, is at least two-thirds of the expected value, $M \geq \frac{2}{3}\E[v(S')] - \frac{11}{12}\max_{e \in S} v(e)$."
2502.13671,"We study a fair division problem in (multi)graphs where $n$ agents (vertices) are pairwise connected by items (edges), and each agent is only interested in its incident items. We consider how to allocate items to incident agents in an envy-free manner, i.e., envy-free orientations, while minimizing the overall payment, i.e., subsidy. We first prove that computing an envy-free orientation with the minimum subsidy is NP-hard, even when the graph is simple and the agents have bi-valued additive valuations. We then bound the worst-case subsidy. We prove that for any multigraph (i.e., allowing parallel edges) and monotone valuations where the marginal value of each good is at most \$1 for each agent, \$1 each (a total subsidy of $n-1$, where $n$ is the number of agents) is sufficient. This is one of the few cases where linear subsidy $\Theta(n)$ is known to be necessary and sufficient to guarantee envy-freeness when agents have monotone valuations. When the valuations are additive (while the graph may contain parallel edges) and when the graph is simple (while the valuations may be monotone), we improve the bound to $n/2$ and $n-2$, respectively. Moreover, these two bounds are tight."
2502.13703,"Hedonic games model settings in which a set of agents have to be partitioned into groups which we call coalitions. In the enemy aversion model, each agent has friends and enemies, and an agent prefers to be in a coalition with as few enemies as possible and, subject to that, as many friends as possible. A partition should be stable, i.e., no subset of agents prefer to be together rather than being in their assigned coalition under the partition. We look at two stability concepts: core stability and strict core stability. This yields several algorithmic problems: determining whether a (strictly) core stable partition exists, finding such a partition, and checking whether a given partition is (strictly) core stable. Several of these problems have been shown to be NP-complete, or even beyond NP. This motivates the study of parameterized complexity. We conduct a thorough computational study using several parameters: treewidth, number of friends, number of enemies, partition size, and coalition size. We give polynomial algorithms for restricted graph classes as well as FPT algorithms with respect to the number of friends an agent may have and the treewidth of the graph representing the friendship or enemy relations. We show W[1]-hardness or para-NP-hardness with respect to the other parameters.We conclude this paper with results in the setting in which agents can have neutral relations with each other, including hardness-results for very restricted cases."
2502.13772,"We initiate a novel direction in randomized social choice by proposing a new definition of agent utility for randomized outcomes. Each agent has a preference over all outcomes and a {\em quantile} parameter. Given a {\em lottery} over the outcomes, an agent gets utility from a particular {\em representative}, defined as the least preferred outcome that can be realized so that the probability that any worse-ranked outcome can be realized is at most the agent's quantile value.In contrast to other utility models that have been considered in randomized social choice (e.g., stochastic dominance, expected utility), our {\em quantile agent utility} compares two lotteries for an agent by just comparing the representatives, as is done for deterministic outcomes.We revisit questions in randomized social choice using the new utility definition. We study the compatibility of efficiency and strategyproofness for randomized voting rules, efficiency and fairness for randomized one-sided matching mechanisms, and efficiency, stability, and strategyproofness for lotteries over two-sided matchings. In contrast to well-known impossibilities in randomized social choice, we show that satisfying the above properties simultaneously can be possible."
2502.13933,"In games with imperfect recall, players may forget the sequence of decisions they made in the past. When players also forget whether they have already encountered their current decision point, they are said to be absent-minded. Solving one-player imperfect recall games is known to be NP-hard, even when the players are not absent-minded. This motivates the search for polynomial-time solvable subclasses. A special type of imperfect recall, called A-loss recall, is amenable to efficient polynomial-time algorithms. In this work, we present novel techniques to simplify non-absent-minded imperfect recall games into equivalent A-loss recall games. The first idea involves shuffling the order of actions, and leads to a new polynomial-time solvable class of imperfect recall games that extends A-loss recall. The second idea generalises the first one, by constructing a new set of action sequences which can be ""linearly combined"" to give the original game. The equivalent game has a simplified information structure, but it could be exponentially bigger in size (in accordance with the NP-hardness). We present an algorithm to generate an equivalent A-loss recall game with the smallest size."
2502.14078,"Learning or estimating game models from data typically entails inducing separate models for each setting, even if the games are parametrically related. In empirical mechanism design, for example, this approach requires learning a new game model for each candidate setting of the mechanism parameter. Recent work has shown the data efficiency benefits of learning a single parameterized model for families of related games. In Bayesian games - a typical model for mechanism design - payoffs depend on both the actions and types of the players. We show how to exploit this structure by learning an interim game-family model that conditions on a single player's type. We compare this approach to the baseline approach of directly learning the ex ante payoff function, which gives payoffs in expectation of all player types. By marginalizing over player type, the interim model can also provide ex ante payoff predictions. This dual capability not only facilitates Bayes-Nash equilibrium approximation, but also enables new types of analysis using the conditional model. We validate our method through a case study of a dynamic sponsored search auction. In our experiments, the interim model more reliably approximates equilibria than the ex ante model and exhibits effective parameter extrapolation. With local search over the parameter space, the learned game-family model can be used for mechanism design. Finally, without any additional sample data, we leverage the interim model to compute piecewise best-response strategies and refine our model to incorporate these strategies, enabling an iterative approach to empirical mechanism design."
2502.1416,"In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to find parameters of a game's payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as generative-adversarial (i.e., min-max) optimization problems, for which we develop polynomial-time algorithms to solve, the former of which relies on an exact first-order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent simulacral learning in polynomial time and number of samples. In these problems, we seek a simulacrum, meaning parameters and an associated equilibrium that replicate the given observations in expectation. We find that our approach outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data."
2502.14371,"We consider a one-sided matching problem where agents who are partitioned into disjoint classes and each class must receive fair treatment in a desired matching. This model, proposed by Benabbou et al. [2019], aims to address various real-life scenarios, such as the allocation of public housing and medical resources across different ethnic, age, and other demographic groups. Our focus is on achieving class envy-free matchings, where each class receives a total utility at least as large as the maximum value of a matching they would achieve from the items matched to another class. While class envy-freeness for worst-case utilities is unattainable without leaving some valuable items unmatched, such extreme cases may rarely occur in practice. To analyze the existence of a class envy-free matching in practice, we study a distributional model where agents' utilities for items are drawn from a probability distribution. Our main result establishes the asymptotic existence of a desired matching, showing that a round-robin algorithm produces a class envy-free matching as the number of agents approaches infinity."
2502.14624,"We consider the fundamental problem of allocating $T$ indivisible items that arrive over time to $n$ agents with additive preferences, with the goal of minimizing envy. This problem is tightly connected to online multicolor discrepancy: vectors $v_1, \dots, v_T \in \mathbb{R}^d$ with $\| v_i \|_2 \leq 1$ arrive over time and must be, immediately and irrevocably, assigned to one of $n$ colors to minimize $\max_{i,j \in [n]} \| \sum_{v \in S_i} v - \sum_{v \in S_j} v \|_{\infty}$ at each step, where $S_\ell$ is the set of vectors that are assigned color $\ell$. The special case of $n = 2$ is called online vector balancing. Any bound for multicolor discrepancy implies the same bound for envy minimization. Against an adaptive adversary, both problems have the same optimal bound, $\Theta(\sqrt{T})$, but whether this holds for weaker adversaries is unknown.Against an oblivious adversary, Alweiss et al. give a $O(\log T)$ bound, with high probability, for multicolor discrepancy. Kulkarni et al. improve this to $O(\sqrt{\log T})$ for vector balancing and give a matching lower bound. Whether a $O(\sqrt{\log T})$ bound holds for multicolor discrepancy remains open. These results imply the best-known upper bounds for envy minimization (for an oblivious adversary) for $n$ and two agents, respectively; whether better bounds exist is open.In this paper, we resolve all aforementioned open problems. We prove that online envy minimization and multicolor discrepancy are equivalent against an oblivious adversary: we give a $O(\sqrt{\log T})$ upper bound for multicolor discrepancy, and a $\Omega(\sqrt{\log T})$ lower bound for envy minimization. For a weaker, i.i.d. adversary, we prove a separation: For online vector balancing, we give a $\Omega\left(\sqrt{\frac{\log T}{\log \log T}}\right)$ lower bound, while for envy minimization, we give an algorithm that guarantees a constant upper bound."
2502.14639,"We consider voting on multiple independent binary issues. In addition, a weighting vector for each voter defines how important they consider each issue. The most natural way to aggregate the votes into a single unified proposal is issue-wise majority (IWM): taking a majority opinion for each issue. However, in a scenario known as Ostrogorski's Paradox, an IWM proposal may not be a Condorcet winner, or it may even fail to garner majority support in a special case known as Anscombe's Paradox.We show that it is co-NP-hard to determine whether there exists a Condorcet-winning proposal even without weights. In contrast, we prove that the single-switch condition provides an Ostrogorski-free voting domain under identical weighting vectors. We show that verifying the condition can be achieved in linear time and no-instances admit short, efficiently computable proofs in the form of forbidden substructures. On the way, we give the simplest linear-time test for the voter/candidate-extremal-interval condition in approval voting and the simplest and most efficient algorithm for recognizing single-crossing preferences in ordinal voting.We then tackle Anscombe's Paradox. Under identical weight vectors, we can guarantee a majority-supported proposal agreeing with IWM on strictly more than half of the overall weight, while with two distinct weight vectors, such proposals can get arbitrarily far from IWM. The severity of such examples is controlled by the maximum average topic weight $\tilde{w}_{max}$: a simple bound derived from a partition-based approach is tight on a large portion of the range $\tilde{w}_{max} \in (0,1)$. Finally, we extend Wagner's rule to the weighted setting: an average majority across topics of at least $\frac{3}{4}$'s precludes Anscombe's paradox from occurring."
2502.1469,"We study two-sided many-to-one matching problems under a novel type of distributional constraints, resource-regional caps. In the context of college admissions, under resource-regional caps, an admitted student may be provided with a unit of some resource through a college, which belongs to a region possessing some amount of this resource. A student may be admitted to a college with at most one unit of any resource, i.e., all resources are close substitutes, e.g., dorms on the campus, dorms outside the campus, subsidies for renting a room, etc. The core feature of our model is that students are allowed to be admitted without any resource, which breaks heredity property of previously studied models with regions.It is well known that a stable matching may not exist under markets with regional constraints. Thus, we focus on three weakened versions of stability that restore existence under resource-regional caps: envyfreeness plus resource-efficiency, non-wastefulness, and novel direct-envy stability. For each version of stability we design corresponding matching mechanism(s). Finally, we compare stability performances of constructed mechanisms on an exhaustive collection of synthetic markets, and conclude that the most sophisticated direct-envy stable mechanism is the go-to mechanism for maximal stability of the resulting matching under resource-regional caps."
2502.14732,"The Facility Location Problem (FLP) is a well-studied optimization problem with applications in many real-world scenarios. Past literature has explored the solutions from different perspectives to tackle FLPs. These include investigating FLPs under objective functions such as utilitarian, egalitarian, Nash welfare, etc. Also, there is no treatment for asymmetric welfare functions around the facility. We propose a unified framework, FLIGHT, to accommodate a broad class of welfare notions. The framework undergoes rigorous theoretical analysis, and we prove some structural properties of the solution to FLP. Additionally, we provide approximation bounds, which provide insight into an interesting fact: as the number of agents arbitrarily increases, the choice of welfare notion is irrelevant. Furthermore, the paper also includes results around concentration bounds under certain distributional assumptions over the preferred locations of agents."
2502.14812,"We introduce the Byzantine Selection Problem, living at the intersection of game theory and fault-tolerant distributed computing. Here, an event organizer is presented with a group of $n$ agents, and wants to select $\ell < n$ of them to form a team. For these purposes, each agent $i$ self-reports a positive skill value $v_i$, and a team's value is the sum of its members' skill values. Ideally, the value of the team should be as large as possible, which can be easily achieved by selecting agents with the highest $\ell$ skill values. However, an unknown subset of at most $t < n$ agents are byzantine and hence not to be trusted, rendering their true skill values as $0$. In the spirit of the distributed computing literature, the identity of the byzantine agents is not random but instead chosen by an adversary aiming to minimize the value of the chosen team. Can we still select a team with good guarantees in this adversarial setting? As it turns out, deterministically, it remains optimal to select agents with the highest $\ell$ values. Yet, if $t \geq \ell$, the adversary can choose to make all selected agents byzantine, leading to a team of value zero. To provide meaningful guarantees, one hence needs to allow for randomization, in which case the expected value of the selected team needs to be maximized, assuming again that the adversary plays to minimize it. For this case, we provide linear-time randomized algorithms that maximize the expected value of the selected team."
2502.15283,"Differentiable economics -- the use of deep learning for auction design -- has driven progress in the automated design of multi-item auctions with additive or unit-demand valuations. However, little progress has been made for optimal combinatorial auctions (CAs), even for the single bidder case, because we need to overcome the challenge of the bundle space growing exponentially with the number of items. For example, when learning a menu of allocation-price choices for a bidder in a CA, each menu element needs to efficiently and flexibly specify a probability distribution on bundles. In this paper, we solve this problem in the single-bidder CA setting by generating a bundle distribution through an ordinary differential equation (ODE) applied to a tractable initial distribution, drawing inspiration from generative models, especially score-based diffusion models and continuous normalizing flow. Our method, BundleFlow, uses deep learning to find suitable ODE-based transforms of initial distributions, one transform for each menu element, so that the overall menu achieves high expected revenue. Our method achieves 1.11$-$2.23$\times$ higher revenue compared with automated mechanism design baselines on the single-bidder version of CATS, a standard CA testbed, and scales to problems with up to 150 items. Relative to a baseline that also learns allocations in menu elements, our method reduces the training iterations by 3.6$-$9.5$\times$ and cuts training time by about 80% in settings with 50 and 100 items."
2502.1542,"In the context of blockchain, MEV refers to the maximum value that can be extracted from block production through the inclusion, exclusion, or reordering of transactions. Searchers often participate in order flow auctions (OFAs) to obtain exclusive rights to private transactions, available through entities called matchmakers, also known as order flow providers (OFPs). Most often, redistributing the revenue generated through such auctions among transaction creators is desirable. In this work, we formally introduce the matchmaking problem in MEV, its desirable properties, and associated challenges. Using cooperative game theory, we formalize the notion of fair revenue redistribution in matchmaking and present its potential possibilities and impossibilities. Precisely, we define a characteristic form game, referred to as RST-Game, for the transaction creators. We propose to redistribute the revenue using the Shapley value of RST-Game. We show that the corresponding problem could be SUBEXP (i.e. $2^{o(n)}$, where $n$ is the number of transactions); therefore, approximating the Shapley value is necessary. Further, we propose a randomized algorithm for computing the Shapley value in RST-Game and empirically verify its efficacy."
2502.15523,"Principal-agent problems model scenarios where a principal incentivizes an agent to take costly, unobservable actions through the provision of payments. Such problems are ubiquitous in several real-world applications, ranging from blockchain to the delegation of machine learning tasks. In this paper, we initiate the study of hidden-action principal-agent problems under approximate best responses, in which the agent may select any action that is not too much suboptimal given the principal's payment scheme (a.k.a. contract). Our main result is a polynomial-time algorithm to compute an optimal contract under approximate best responses. This positive result is perhaps surprising, since, in Stackelberg games, computing an optimal commitment under approximate best responses is computationally intractable. We also investigate the learnability of contracts under approximate best responses, by providing a no-regret learning algorithm for a natural application scenario where the principal has no prior knowledge about the environment."
2502.16052,"We study a data marketplace where a broker intermediates between buyers, who seek to estimate the mean \(\mu\) of an unknown normal distribution \(\Ncal(\mu, \sigma^2)\), and contributors, who can collect data from this distribution at a cost. The broker delegates data collection work to contributors, aggregates reported datasets, sells it to buyers, and redistributes revenue as payments to contributors. We aim to maximize welfare or profit under key constraints: individual rationality for buyers and contributors, incentive compatibility (contributors are incentivized to comply with data collection instructions and truthfully report the collected data), and budget balance (total contributor payments equals total revenue). We first compute welfare/profit-optimal prices under truthful reporting; however, to incentivize data collection and truthful data reporting, we adjust them based on discrepancies in contributors' reported data. This yields a Nash equilibrium (NE) where the two lowest-cost contributors collect all data. We complement this with two hardness results: \emph{(i)} no nontrivial dominant-strategy incentive-compatible mechanism exists in this problem, and \emph{(ii)} no mechanism outperforms ours in a NE."
2502.1608,"In this paper, we study a generalization of Markov games and pseudo-games that we call Markov pseudo-games, which, like the former, captures time and uncertainty, and like the latter, allows for the players' actions to determine the set of actions available to the other players. In the same vein as Arrow and Debreu, we intend for this model to be rich enough to encapsulate a broad mathematical framework for modeling economies. We then prove the existence of a game-theoretic equilibrium in our model, which in turn implies the existence of a general equilibrium in the corresponding economies. Finally, going beyond Arrow and Debreu, we introduce a solution method for Markov pseudo-games and prove its polynomial-time convergence.We then provide an application of Markov pseudo-games to infinite-horizon Markov exchange economies, a stochastic economic model that extends Radner's stochastic exchange economy and Magill and Quinzii's infinite-horizon incomplete markets model. We show that under suitable assumptions, the solutions of any infinite-horizon Markov exchange economy (i.e., recursive Radner equilibria -- RRE) can be formulated as the solution to a concave Markov pseudo-game, thus establishing the existence of RRE and providing first-order methods for approximating RRE. Finally, we demonstrate the effectiveness of our approach in practice by building the corresponding generative adversarial policy neural network and using it to compute RRE in a variety of infinite-horizon Markov exchange economies."
2502.16363,"With the rapid development of Internet of Things (IoT) and artificial intelligence technologies, data has become an important strategic resource in the new era. However, the growing demand for data has exacerbated the issue of \textit{data silos}. Existing data pricing models primarily focus on single factors such as data quality or market demand, failing to adequately address issues such as data seller monopolies and the diverse needs of buyers, resulting in biased pricing that cannot meet the complexities of evolving transaction scenarios. To address these problems, this paper proposes a multi-party data pricing model based on the Rubinstein bargaining model. The model introduces buyer data utility indicators and data quality assessments, comprehensively considering factors such as the utility, accuracy, and timeliness of data sets, to more accurately evaluate their value to buyers. To overcome the limitations of single-factor models, this paper innovatively introduces the buyer data set satisfaction indicator, which reflects the overall satisfaction of buyers with data sets by integrating both data utility and quality assessments. Based on this, the model uses the Rubinstein bargaining model to simulate the pricing process between multiple sellers and multiple buyers, yielding pricing results that better align with market demands. Experimental results show that the proposed model effectively addresses the pricing imbalance caused by data monopolies and demonstrates good applicability and accuracy in multi-seller, multi-buyer transaction environments. This research provides an effective pricing mechanism for complex data trading markets and has significant theoretical and practical value in solving pricing issues in actual data transactions."
2502.1696,We propose an $O(n^2)$-time algorithm to determine whether a given matching is efficient in the roommates problem.
2502.16973,"We study two axioms for social choice functions that capture the impact of similar candidates: independence of clones (IoC) and composition consistency (CC). We clarify the relationship between these axioms by observing that CC is strictly more demanding than IoC, and investigate whether common voting rules that are known to be independent of clones (such as STV, Ranked Pairs, Schulze, and Split Cycle) are composition-consistent. While for most of these rules the answer is negative, we identify a variant of Ranked Pairs that satisfies CC. Further, we show how to efficiently modify any (neutral) social choice function so that it satisfies CC, while maintaining its other desirable properties. Our transformation relies on the hierarchical representation of clone structures via PQ-trees. We extend our analysis to social preference functions. Finally, we interpret IoC and CC as measures of robustness against strategic manipulation by candidates, with IoC corresponding to strategy-proofness and CC corresponding to obvious strategy-proofness."
2502.17809,"We study multi-product monopoly pricing where the seller jointly designs the selling mechanism and the information structure for the buyer to learn his values. Unlike the case with exogenous information, we show that when the seller controls information, even uniform pricing guarantees at least half of the optimal revenue. Moreover, for negatively affiliated or exchangeable value distributions, deterministic pricing is revenue-optimal. Our results highlight the power of information design in making pricing mechanisms approximately optimal in multi-dimensional settings."
2502.17869,"We propose a new model for aggregating preferences over a set of indivisible items based on a quantile value. In this model, each agent is endowed with a specific quantile, and the value of a given bundle is defined by the corresponding quantile of the individual values of the items within it. Our model captures the diverse ways in which agents may perceive a bundle, even when they agree on the values of individual items. It enables richer behavioral modeling that cannot be easily captured by additive valuation functions. We study the problem of maximizing utilitarian and egalitarian welfare within the quantile-based valuation setting. For each of the welfare functions, we analyze the complexity of the objectives. Interestingly, our results show that the complexity of both objectives varies significantly depending on whether the allocation is required to be balanced. We provide near-optimal approximation algorithms for utilitarian welfare, and for egalitarian welfare, we present exact algorithms whenever possible."
2502.18265,"We consider online procurement auctions, where the agents arrive sequentially, in random order, and have private costs for their services. The buyer aims to maximize a monotone submodular value function for the subset of agents whose services are procured, subject to a budget constraint on their payments. We consider a posted-price setting where upon each agent's arrival, the buyer decides on a payment offered to them. The agent accepts or rejects the offer, depending on whether the payment exceeds their cost, without revealing any other information about their private costs whatsoever. We present a randomized online posted-price mechanism with constant competitive ratio, thus resolving the main open question of (Badanidiyuru, Kleinberg and Singer, EC 2012). Posted-price mechanisms for online procurement typically operate by learning an estimation of the optimal value, denoted as OPT, and using it to determine the payments offered to the agents. The main challenge is to learn OPT within a constant factor from the agents' accept / reject responses to the payments offered. Our approach is based on an online test of whether our estimation is too low compared against OPT and a carefully designed adaptive search that gradually refines our estimation."
2502.18296,"We consider multi-dimensional payoff functions in partially observable Markov decision processes. We study the structure of the set of expected payoff vectors of all strategies (policies) and study what kind are needed to achieve a given expected payoff vector. In general, pure strategies (i.e., not resorting to randomisation) do not suffice for this problem.We prove that for any payoff for which the expectation is well-defined under all strategies, it is sufficient to mix (i.e., randomly select a pure strategy at the start of a play and committing to it for the rest of the play) finitely many pure strategies to approximate any expected payoff vector up to any precision. Furthermore, for any payoff for which the expected payoff is finite under all strategies, any expected payoff can be obtained exactly by mixing finitely many strategies."
2502.18605,"Variational inequalities (VIs) encompass many fundamental problems in diverse areas ranging from engineering to economics and machine learning. However, their considerable expressivity comes at the cost of computational intractability. In this paper, we introduce and analyze a natural relaxation -- which we refer to as expected variational inequalities (EVIs) -- where the goal is to find a distribution that satisfies the VI constraint in expectation. By adapting recent techniques from game theory, we show that, unlike VIs, EVIs can be solved in polynomial time under general (nonmonotone) operators. EVIs capture the seminal notion of correlated equilibria, but enjoy a greater reach beyond games. We also employ our framework to capture and generalize several existing disparate results, including from settings such as smooth games, and games with coupled constraints or nonconcave utilities."
2502.18805,"The classic notion of \emph{truthfulness} requires that no agent has a profitable manipulation -- an untruthful report that, for \emph{some} combination of reports of the other agents, increases her utility. This strong notion implicitly assumes that the manipulating agent either knows what all other agents are going to report, or is willing to take the risk and act as-if she knows their reports.Without knowledge of the others' reports, most manipulations are \emph{risky} -- they might decrease the manipulator's utility for some other combinations of reports by the other agents. Accordingly, a recent paper (Bu, Song and Tao, ``On the existence of truthful fair cake cutting mechanisms'', Artificial Intelligence 319 (2023), 103904) suggests a relaxed notion, which we refer to as \emph{risk-avoiding truthfulness (RAT)}, which requires only that no agent can gain from a \emph{safe} manipulation -- one that is sometimes beneficial and never harmful.Truthfulness and RAT are two extremes: the former considers manipulators with complete knowledge of others, whereas the latter considers manipulators with no knowledge at all. In reality, agents often know about some -- but not all -- of the other agents. This paper introduces the \emph{RAT-degree} of a mechanism, defined as the smallest number of agents whose reports, if known, may allow another agent to safely manipulate, or $n$ if there is no such number. This notion interpolates between classic truthfulness (degree $n$) and RAT (degree at least $1$): a mechanism with a higher RAT-degree is harder to manipulate safely.To illustrate the generality and applicability of this concept, we analyze the RAT-degree of prominent mechanisms across various social choice settings, including auctions, indivisible goods allocations, cake-cutting, voting, and two-sided matching."
2502.18988,"Varied real world systems such as transportation networks, supply chains and energy grids present coordination problems where many agents must learn to share resources. It is well known that the independent and selfish interactions of agents in these systems may lead to inefficiencies, often referred to as the `Price of Anarchy'. Effective interventions that reduce the Price of Anarchy while preserving individual autonomy are of great interest. In this paper we explore recommender systems as one such intervention mechanism. We start with the Braess Paradox, a congestion game model of a routing problem related to traffic on roads, packets on the internet, and electricity on power grids. Following recent literature, we model the interactions of agents as a repeated game between $Q$-learners, a common type of reinforcement learning agents. This work introduces the Learning Dynamic Manipulation Problem, where an external recommender system can strategically trigger behavior by picking the states observed by $Q$-learners during learning. Our computational contribution demonstrates that appropriately chosen recommendations can robustly steer the system towards convergence to the social optimum, even for many players. Our theoretical and empirical results highlight that increases in the recommendation space can increase the steering potential of a recommender system, which should be considered in the design of recommender systems."
2502.1905,"We consider the impact of fairness requirements on the social efficiency of truthful mechanisms for trade, focusing on Bayesian bilateral-trade settings. Unlike the full information case in which all gains-from-trade can be realized and equally split between the two parties, in the private information setting, equitability has devastating welfare implications (even if only required to hold ex-ante). We thus search for an alternative fairness notion and suggest requiring the mechanism to be KS-fair: it must ex-ante equalize the fraction of the ideal utilities of the two traders. We show that there is always a KS-fair (simple) truthful mechanism with expected gains-from-trade that are half the optimum, but always ensuring any better fraction is impossible (even when the seller value is zero). We then restrict our attention to trade settings with a zero-value seller and a buyer with value distribution that is Regular or MHR, proving that much better fractions can be obtained under these conditions."
2502.19317,"We study the problem of finding the optimal bidding strategy for an advertiser in a multi-platform auction setting. The competition on a platform is captured by a value and a cost function, mapping bidding strategies to value and cost respectively. We assume a diminishing returns property, whereby the marginal cost is increasing in value. The advertiser uses an autobidder that selects a bidding strategy for each platform, aiming to maximize total value subject to budget and return-on-spend constraint. The advertiser has no prior information and learns about the value and cost functions by querying a platform with a specific bidding strategy. Our goal is to design algorithms that find the optimal bidding strategy with a small number of queries.We first present an algorithm that requires \(O(m \log (mn) \log n)\) queries, where $m$ is the number of platforms and $n$ is the number of possible bidding strategies in each platform. Moreover, we adopt the learning-augmented framework and propose an algorithm that utilizes a (possibly erroneous) prediction of the optimal bidding strategy. We provide a $O(m \log (m\eta) \log \eta)$ query-complexity bound on our algorithm as a function of the prediction error $\eta$. This guarantee gracefully degrades to \(O(m \log (mn) \log n)\). This achieves a ``best-of-both-worlds'' scenario: \(O(m)\) queries when given a correct prediction, and \(O(m \log (mn) \log n)\) even for an arbitrary incorrect prediction."
2502.19744,"We study a two-sided matching model where one side of the market (hospitals) has combinatorial preferences over the other side (doctors). Specifically, we consider the setting where hospitals have matroid rank valuations over the doctors, and doctors have either ordinal or cardinal unit-demand valuations over the hospitals. While this setting has been extensively studied in the context of one-sided markets, it remains unexplored in the context of two-sided markets.When doctors have ordinal preferences over hospitals, we present simple sequential allocation algorithms that guarantee stability, strategyproofness for doctors, and approximate strategyproofness for hospitals. When doctors have cardinal utilities over hospitals, we present an algorithm that finds a stable allocation maximizing doctor welfare; subject to that, we show how one can maximize either the hospital utilitarian or hospital Nash welfare. Moreover, we show that it is NP-hard to compute stable allocations that approximately maximize hospital Nash welfare."
2502.19791,"This paper studies cooperative games where coalitions are formed online and the value generated by the grand coalition must be irrevocably distributed among the players at each timestep. We investigate the fundamental issue of strategic pariticipation incentives and address these concerns by formalizing natural participation incentive axioms. Our analysis reveals that existing value-sharing mechanisms fail to meet these criteria. Consequently, we propose several new mechanisms that not only fulfill these desirable participation incentive axioms but also satisfy the early arrival incentive for general valuation functions. Additionally, we refine our mechanisms under superadditive valuations to ensure individual rationality while preserving the previously established axioms."
2502.20031,"The ``EIP-1599 algorithm'' is used by the Ethereum blockchain to assemble transactions into blocks. While prior work has studied it under the assumption that bidders are ``impatient'', we analyze it under the assumption that bidders are ``patient'', which better corresponds to the fact that unscheduled transactions remain in the mempool and can be scheduled at a later time. We show that with ``patient'' bidders, this algorithm produces schedules of near-optimal welfare, provided it is given a mild resource augmentation (that does not increase with the time horizon). We prove some generalizations of the basic theorem, establish lower bounds that rule out several candidate improvements and extensions, and propose several questions for future work."
2502.20063,"We study the impact of strategic behavior in a setting where firms compete to hire from a shared pool of applicants, and firms use a common algorithm to evaluate them. Each applicant is associated with a scalar score that is observed by all firms, provided by the algorithm. Firms simultaneously make interview decisions, where the number of interviews is capacity-constrained. Job offers are given to those who pass the interview, and an applicant who receives multiple offers accepts one of them uniformly at random. We fully characterize the set of Nash equilibria under this model. Defining social welfare as the total number of applicants who find a job, we then compare the social welfare at a Nash equilibrium to a naive baseline where all firms interview applicants with the highest scores. We show that the Nash equilibrium greatly improves upon social welfare compared to the naive baseline, especially when the interview capacity is small and the number of firms is large. We also show that the price of anarchy is small, providing further appeal for the equilibrium solution.We then study how the firms may converge to a Nash equilibrium. We show that when firms make interview decisions sequentially and each firm takes the best response action assuming they are the last to act, this process converges to an equilibrium when interview capacities are small. However, we show that the task of computing the best response is difficult if firms have to use its own historical samples to estimate it, while this task becomes trivial if firms have information on the degree of competition for each applicant. Therefore, converging to an equilibrium can be greatly facilitated if firms have information on the level of competition for each applicant."
2502.2017,"Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development."
2502.20229,"Swap regret is a notion that has proven itself to be central to the study of general-sum normal-form games, with swap-regret minimization leading to convergence to the set of correlated equilibria and guaranteeing non-manipulability against a self-interested opponent. However, the situation for more general classes of games -- such as Bayesian games and extensive-form games -- is less clear-cut, with multiple candidate definitions for swap-regret but no known efficiently minimizable variant of swap regret that implies analogous non-manipulability guarantees.In this paper, we present a new variant of swap regret for polytope games that we call ``profile swap regret'', with the property that obtaining sublinear profile swap regret is both necessary and sufficient for any learning algorithm to be non-manipulable by an opponent (resolving an open problem of Mansour et al., 2022). Although we show profile swap regret is NP-hard to compute given a transcript of play, we show it is nonetheless possible to design efficient learning algorithms that guarantee at most $O(\sqrt{T})$ profile swap regret. Finally, we explore the correlated equilibrium notion induced by low-profile-swap-regret play, and demonstrate a gap between the set of outcomes that can be implemented by this learning process and the set of outcomes that can be implemented by a third-party mediator (in contrast to the situation in normal-form games)."
2502.20334,"Optimistic rollups rely on fraud proofs -- interactive protocols executed on Ethereum to resolve conflicting claims about the rollup's state -- to scale Ethereum securely.To mitigate against potential censorship of protocol moves, fraud proofs grant participants a significant time window, known as the challenge period, to ensure their moves are processed on chain. Major optimistic rollups today set this period at roughly one week, mainly to guard against strong censorship that undermines Ethereum's own crypto-economic security. However, other forms of censorship are possible, and their implication on optimistic rollup security is not well understood.This paper considers economic censorship attacks, where an attacker censors the defender's transactions by bribing block proposers. At each step, the attacker can either censor the defender -- depleting the defender's time allowance at the cost of the bribe -- or allow the current transaction through while conserving funds for future censorship.We analyze three game theoretic models of these dynamics and determine the challenge period length required to ensure the defender's success, as a function of the number of required protocol moves and the players' available budgets."
2502.20346,"We envision a marketplace where diverse entities offer specialized ""modules"" through APIs, allowing users to compose the outputs of these modules for complex tasks within a given budget. This paper studies the market design problem in such an ecosystem, where module owners strategically set prices for their APIs (to maximize their profit) and a central platform orchestrates the aggregation of module outputs at query-time. One can also think about this as a first-price procurement auction with budgets. The first observation is that if the platform's algorithm is to find the optimal set of modules then this could result in a poor outcome, in the sense that there are price equilibria which provide arbitrarily low value for the user. We show that under a suitable version of the ""bang-per-buck"" algorithm for the knapsack problem, an $\varepsilon$-approximate equilibrium always exists, for any arbitrary $\varepsilon > 0$. Further, our first main result shows that with this algorithm any such equilibrium provides a constant approximation to the optimal value that the buyer could get under various constraints including (i) a budget constraint and (ii) a budget and a matroid constraint. Finally, we demonstrate that these efficient equilibria can be learned through decentralized price adjustments by module owners using no-regret learning algorithms."
2502.2036,"Selfish miners selectively withhold blocks to earn disproportionately high revenue. The vast majority of the selfish mining literature focuses exclusively on block rewards. Carlsten et al. [2016] is a notable exception, observing that similar strategic behavior is profitable in a zero-block-reward regime (the endgame for Bitcoin's quadrennial halving schedule) if miners are compensated with transaction fees alone. Neither model fully captures miner incentives today. The block reward remains 3.125 BTC, yet some blocks yield significantly higher revenue. For example, congestion during the launch of the Babylon protocol in August 2024 caused transaction fees to spike to 9.52 BTC. Our results are both practical and theoretical. Of practical interest, we study selfish mining profitability under a combined reward function that more accurately models miner incentives. This analysis enables us to make quantitative claims about protocol risk (e.g., the mining power at which a selfish strategy becomes profitable is reduced by 22% when optimizing over the combined reward function versus block rewards alone) and qualitative observations (e.g., a miner considering both block rewards and transaction fees will mine more or less aggressively respectively). These practical results follow from our novel model and methodology, which constitute our theoretical contributions. We model general, time-accruing stochastic rewards, which requires explicit treatment of difficult adjustment and randomness; we characterize reward function structure through a set of properties (e.g., that rewards accrue only as a function of time). We present a new methodology to analytically calculate expected selfish miner rewards under a broad class of stochastic reward functions and validate our method numerically by comparing it with the existing literature and simulating the combined reward sources directly."
2502.20466,"Projected gradient ascent is known to satisfy no-external regret as a learning algorithm. However, recent empirical work shows that projected gradient ascent often finds the Nash equilibrium in settings beyond two-player zero-sum interactions or potential games, including those where the set of coarse correlated equilibria is very large. We show that gradient ascent in fact satisfies a stronger class of linear $\Phi$-regret in normal-form games; resulting in a refined solution concept which we dub semicoarse correlated equilibria. Our theoretical analysis of the discretised Bertrand competition mirrors those recently established for mean-based learning in first-price auctions. With at least two firms of lowest marginal cost, Nash equilibria emerge as the only semicoarse equilibria under concavity conditions on firm profits. In first-price auctions, the granularity of the bid space affects semicoarse equilibria, but finer granularity for lower bids also induces convergence to Nash equilibria. Unlike previous work that aims to prove convergence to a Nash equilibrium that often relies on epoch based analysis and probability theoretic machinery, our LP-based duality approach enables a simple and tractable analysis of equilibrium selection under gradient-based learning."
2502.2077,"We consider the problem of learning to exploit learning algorithms through repeated interactions in games. Specifically, we focus on the case of repeated two player, finite-action games, in which an optimizer aims to steer a no-regret learner to a Stackelberg equilibrium without knowledge of its payoffs. We first show that this is impossible if the optimizer only knows that the learner is using an algorithm from the general class of no-regret algorithms. This suggests that the optimizer requires more information about the learner's objectives or algorithm to successfully exploit them. Building on this intuition, we reduce the problem for the optimizer to that of recovering the learner's payoff structure. We demonstrate the effectiveness of this approach if the learner's algorithm is drawn from a smaller class by analyzing two examples: one where the learner uses an ascent algorithm, and another where the learner uses stochastic mirror ascent with known regularizer and step sizes."
2502.20783,"In the digital economy, technological innovations make it cheaper to produce high-quality content. For example, generative AI tools reduce costs for creators who develop content to be distributed online, but can also reduce production costs for the users who consume that content. These innovations can thus lead to disintermediation, since consumers may choose to use these technologies directly, bypassing intermediaries. To investigate when technological improvements lead to disintermediation, we study a game with an intermediary, suppliers of a production technology, and consumers. First, we show disintermediation occurs whenever production costs are too high or too low. We then investigate the consequences of disintermediation for welfare and content quality at equilibrium. While the intermediary is welfare-improving, the intermediary extracts all gains to social welfare and its presence can raise or lower content quality. We further analyze how disintermediation is affected by the level of competition between suppliers and the intermediary's fee structure. More broadly, our results take a step towards assessing how production technology innovations affect the survival of intermediaries and impact the digital economy."
2502.21084,"In the metric distortion problem, a set of voters and candidates lie in a common metric space, and a committee of $k$ candidates must be elected. The objective is to minimize a social cost, defined as a function of the distances between voters and their chosen representatives, while the voting rule only has access to ordinal preferences. The distortion of a rule is the worst-case ratio between the social cost of its outcome and that of the optimal committee, taken over all consistent preferences and metrics.We initiate the study of metric distortion in peer selection, where voters and candidates coincide. We consider four objectives, obtained by combining two aggregation rules with two types of social cost. Under additive aggregation, an individual's cost is the sum of their distances to all committee members; under $q$-cost, it is their distance to the $q$th closest member. The overall social cost is either utilitarian, given by the sum of all individual costs, or egalitarian, given by the maximum individual cost. Surprisingly, we find that even on the line metric, peer selection retains much of the hardness of the general case: Lower bounds remain strictly larger than one for all objectives, and cases where bounded distortion is impossible in general remain so here as well. On a positive note, cases with bounded distortion in the general setting achieve better constants in peer selection. For utilitarian cost, selecting the $k$ middle agents achieves a distortion between $1$ and $2$ under additive aggregation. Under $q$-cost, we show positive results for $q=k=2$, but impossibility results largely carry over. For egalitarian cost, selecting the extremes yields an optimal distortion of $2$ under additive aggregation and for $q$-cost with $q>k/3$. Thus, while peer selection on the line metric allows better constants, fundamental hardness barriers persist."
2503.00078,"In this paper we consider non-atomic games in populations that are provided with a choice of preventive policies to act against a contagion spreading amongst interacting populations, be it biological organisms or connected computing devices. The spreading model of the contagion is the standard SIR model. Each participant of the population has a choice from amongst a set of precautionary policies with each policy presenting a payoff or utility, which we assume is the same within each group, the risk being the possibility of infection. The policy groups interact with each other. We also define a network model to model interactions between different population sets. The population sets reside at nodes of the network and follow policies available at that node. We define game-theoretic models and study the inefficiency of allowing for individual decision making, as opposed to centralized control. We study the computational aspects as well."
2503.0017,"Many blockchain-based decentralized services require their validators (operators) to deposit stake (collateral), which is forfeited (slashed) if they misbehave. Restaking networks let validators secure multiple services by reusing stake. These networks have quickly gained traction, leveraging over \$20 billion in stake. However, restaking introduces a new attack vector where validators can coordinate to misbehave across multiple services simultaneously, extracting digital assets while forfeiting their stake only once.Previous work focused either on preventing coordinated misbehavior or on protecting services if all other services are Byzantine and might unjustly cause slashing due to bugs or malice. The first model overlooks how a single Byzantine service can collapse the network, while the second ignores shared-stake benefits.To bridge the gap, we analyze the system as a strategic game of coordinated misbehavior, when a given fraction of the services are Byzantine. We introduce elastic restaking networks, where validators can allocate portions of their stake that may cumulatively exceed their total stake, and when allocations are lost, the remaining stake stretches to cover remaining allocations. We show that elastic networks exhibit superior robustness compared to previous approaches, and demonstrate a synergistic effect where an elastic restaking network enhances its blockchain's security, contrary to community concerns of an opposite effect in existing networks. We then design incentives for tuning validators' allocations.Our elastic restaking system and incentive design have immediate practical implications for deployed restaking networks."
2503.00227,"This work introduces a unified framework for analyzing games in greater depth. In the existing literature, players' strategies are typically assigned scalar values, and equilibrium concepts are used to identify compatible choices. However, this approach neglects the internal structure of players, thereby failing to accurately model observed behaviors.To address this limitation, we propose an abstract definition of a player, consistent with constructions in reinforcement learning. Instead of defining games as external settings, our framework defines them in terms of the players themselves. This offers a language that enables a deeper connection between games and learning. To illustrate the need for this generality, we study a simple two-player game and show that even in basic settings, a sophisticated player may adopt dynamic strategies that cannot be captured by simpler models or compatibility analysis.For a general definition of a player, we discuss natural conditions on its components and define competition through their behavior. In the discrete setting, we consider players whose estimates largely follow the standard framework from the literature. We explore connections to correlated equilibrium and highlight that dynamic programming naturally applies to all estimates. In the mean-field setting, we exploit symmetry to construct explicit examples of equilibria. Finally, we conclude by examining relations to reinforcement learning."
2503.00788,"Markov decision processes (MDPs) are a canonical model to reason about decision making within a stochastic environment. We study a fundamental class of infinite MDPs: one-counter MDPs (OC-MDPs). They extend finite MDPs via an associated counter taking natural values, thus inducing an infinite MDP over the set of configurations (current state and counter value). We consider two characteristic objectives: reaching a target state (state-reachability), and reaching a target state with counter value zero (selective termination). The synthesis problem for the latter is not known to be decidable and connected to major open problems in number theory. Furthermore, even seemingly simple strategies (e.g., memoryless ones) in OC-MDPs might be impossible to build in practice (due to the underlying infinite configuration space): we need finite, and preferably small, representations.To overcome these obstacles, we introduce two natural classes of concisely represented strategies based on a (possibly infinite) partition of counter values in intervals. For both classes, and both objectives, we study the verification problem (does a given strategy ensure a high enough probability for the objective?), and two synthesis problems (does there exist such a strategy?): one where the interval partition is fixed as input, and one where it is only parameterized. We develop a generic approach based on a compression of the induced infinite MDP that yields decidability in all cases, with all complexities within PSPACE."
2503.00885,"Approval voting is widely used for making multi-winner voting decisions. The canonical rule (also called Approval Voting) used in the setting aims to maximize social welfare by selecting candidates with the highest number of approvals. We revisit approval-based multi-winner voting in scenarios where the information regarding the voters' preferences is uncertain. We present several algorithmic results for problems related to social welfare maximization under uncertainty, including computing an outcome that is social welfare maximizing with the highest probability, computing the social welfare probability distribution of a given outcome, computing the probability that a given outcome is social welfare maximizing, and understanding how robust an outcome is with respect to social welfare maximizing."
2503.01368,"We initiate the study of computing envy-free allocations of indivisible items in the extension setting, i.e., when some part of the allocation is fixed and the task is to allocate the remaining items. Given the known NP-hardness of the problem, we investigate whether -- and under which conditions -- one can obtain fixed-parameter algorithms for computing a solution in settings where most of the allocation is already fixed. Our results provide a broad complexity-theoretic classification of the problem which includes: (a) fixed-parameter algorithms tailored to settings with few distinct types of agents or items; (b) lower bounds which exclude the generalization of these positive results to more general settings. We conclude by showing that -- unlike when computing allocations from scratch -- the non-algorithmic question of whether more relaxed EFX allocations exist can be completely resolved in the extension setting."
2503.01529,"We study a repeated trading problem in which a mechanism designer facilitates trade between a single seller and multiple buyers. Our model generalizes the classic bilateral trade setting to a multi-buyer environment. Specifically, the mechanism designer runs a second-price auction among the buyers -- extending the fixed-price mechanism used in bilateral trade -- before proposing a price to the seller. While this setting introduces new challenges compared to bilateral trade, it also provides an informational advantage. Indeed, the presence of multiple buyers enhances competition, inducing them to reveal their valuations in order to win the auction. However, as in bilateral trade, the seller faces a binary decision: whether to accept the proposed price or not. We show that this asymmetric feedback, which is more informative than in bilateral trade, allows us to break some lower bounds on regret minimization with a single buyer. In particular, we provide a $\tilde O(T^{2/3})$ regret upper bound with respect to an optimal strong budget-balanced mechanism, without any assumptions on the distribution of valuations. Our main tool for achieving this result is the design of an adaptive grid that approximates the optimal gain from trade across the continuum of possible mechanisms. Furthermore, we attain the same regret bound with respect to an optimal global budget-balanced mechanism, under two possible conditions: (i) buyers' and seller's valuations are independent, or (ii) valuations are drawn from a distribution with bounded density. In doing so, we provide some novel technical results on constrained MABs with feedback graphs, which may be of independent interest."
2503.01701,"Most microeconomic models of interest involve optimizing a piecewise linear function. These include contract design in hidden-action principal-agent problems, selling an item in posted-price auctions, and bidding in first-price auctions. When the relevant model parameters are unknown and determined by some (unknown) probability distributions, the problem becomes learning how to optimize an unknown and stochastic piecewise linear reward function. Such a problem is usually framed within an online learning framework, where the decision-maker (learner) seeks to minimize the regret of not knowing an optimal decision in hindsight. This paper introduces a general online learning framework that offers a unified approach to tackle regret minimization for piecewise linear rewards, under a suitable monotonicity assumption commonly satisfied by microeconomic models. We design a learning algorithm that attains a regret of $\widetilde{O}(\sqrt{nT})$, where $n$ is the number of ``pieces'' of the reward function and $T$ is the number of rounds. This result is tight when $n$ is \emph{small} relative to $T$, specifically when $n \leq T^{1/3}$. Our algorithm solves two open problems in the literature on learning in microeconomic settings. First, it shows that the $\widetilde{O}(T^{2/3})$ regret bound obtained by Zhu et al. [Zhu+23] for learning optimal linear contracts in hidden-action principal-agent problems is not tight when the number of agent's actions is small relative to $T$. Second, our algorithm demonstrates that, in the problem of learning to set prices in posted-price auctions, it is possible to attain suitable (and desirable) instance-independent regret bounds, addressing an open problem posed by Cesa-Bianchi et al. [CBCP19]."
2503.01976,"We study the problem of learning the utility functions of agents in a normal-form game by observing the agents play the game repeatedly. Differing from most prior literature, we introduce a principal with the power to observe the agents playing the game, send the agents signals, and send the agents payments as a function of their actions. Under reasonable behavioral models for the agents such as iterated dominated action removal or a no-regret assumption, we show that the principal can, using a number of rounds polynomial in the size of the game, learn the utility functions of all agents to any desirable precision $\varepsilon > 0$. We also show lower bounds in both models, which nearly match the upper bounds in the former model and also strictly separate the two models: the principal can learn strictly faster in the iterated dominance model. Finally, we discuss implications for the problem of steering agents to a desired equilibrium: in particular, we introduce, using our utility-learning algorithm as a subroutine, the first algorithm for steering learning agents without prior knowledge of their utilities."
2503.01985,"Consider the decision-making setting where agents elect a panel by expressing both positive and negative preferences. Prominently, in constitutional AI, citizens democratically select a slate of ethical preferences on which a foundation model is to be trained. There, in practice, agents may both approve and disapprove of different ethical principles. Proportionality has been well-studied in computational social choice for approval ballots, but its meaning remains unclear when negative sentiments are also considered. In this work, we propose two conceptually distinct approaches to interpret proportionality in the presence of up and down votes. The first approach treats the satisfaction from electing candidates and the impact of vetoing them as comparable, leading to combined proportionality guarantees. The second approach considers veto power separately, introducing guarantees distinct from traditional proportionality. We formalize axioms for each perspective and examine their satisfiability by suitable adaptations of Phragmén's rule, Proportional Approval Voting rule and the Method of Equal Shares."
2503.02088,"We investigate the problem of fairly allocating $m$ indivisible items among $n$ sequentially arriving agents with additive valuations, under the sought-after fairness notion of maximin share (MMS). We first observe a strong impossibility: without appropriate knowledge about the valuation functions of the incoming agents, no online algorithm can ensure any non-trivial MMS approximation, even when there are only two agents. Motivated by this impossibility, we introduce OnlineKTypeFD (online $k$-type fair division), a model that balances theoretical tractability with real-world applicability. In this model, each arriving agent belongs to one of $k$ types, with all agents of a given type sharing the same known valuation function. We do not constrain $k$ to be a constant. Upon arrival, an agent reveals her type, receives an irrevocable allocation, and departs. We study the ex-post MMS guarantees of online algorithms under two arrival models:1- Adversarial arrivals: In this model, an adversary determines the type of each arriving agent. We design a $\frac{1}{k}$-MMS competitive algorithm and complement it with a lower bound, ruling out any $\Omega(\frac{1}{\sqrt{k}})$-MMS-competitive algorithm, even for binary valuations.2- Stochastic arrivals: In this model, the type of each arriving agent is independently drawn from an underlying, possibly unknown distribution. Unlike the adversarial setting where the dependence on $k$ is unavoidable, we surprisingly show that in the stochastic setting, an asymptotic, arbitrarily close-to-$\frac{1}{2}$-MMS competitive guarantee is achievable under mild distributional assumptions.Our results extend naturally to a learning-augmented framework; when given access to predictions about valuation functions, we show that the competitive ratios of our algorithms degrade gracefully with multiplicative prediction errors."
2503.02089,"We study fair division of indivisible goods under the maximin share (MMS) fairness criterion in settings where agents are grouped into a small number of types, with agents within each type having identical valuations. For the special case of a single type, an exact MMS allocation is always guaranteed to exist. However, for two or more distinct agent types, exact MMS allocations do not always exist, shifting the focus to establishing the existence of approximate-MMS allocations. A series of works over the last decade has resulted in the best-known approximation guarantee of $\frac{3}{4} + \frac{3}{3836}$.In this paper, we improve the approximation guarantees for settings where agents are grouped into two or three types, a scenario that arises in many practical settings. Specifically, we present novel algorithms that guarantee a $\frac{4}{5}$-MMS allocation for two agent types and a $\frac{16}{21}$-MMS allocation for three agent types. Our approach leverages the MMS partition of the majority type and adapts it to provide improved fairness guarantees for all types."
2503.02326,"Ethics play an important role in determining the behavior of an individual under certain circumstances. Ethical or unethical behavior can be treated as a strategy of a player in a pay-off game. In this paper, we present two analytical solutions to studying time evolution of behavior of an individual from ethics perspective. We also present the effect of a third player as a perturbation to a two player game and develop a general approach for a $N$ player game. We demonstrate geometric modeling of behavioral characteristics of individuals as polytopes residing in $D$ dimensional space. We treat three player and two player games using set of differential equations that lead to time evolution of phase trajectories which reveal about the interdependencies and self dependencies of each player. We also demonstrate the effect of strategies of each player on other players in cardinal games."
2503.02429,"This article deals with ranking methods. We study the situation where a tournament between $n$ players $P_1$, $P_2$, \ldots $P_n$ gives the ranking $P_1 \succ P_2 \succ \cdots \succ P_n$, but, if the results of $P_n$ are no longer taken into account (for example $P_n$ is suspended for doping), then the ranking becomes $P_{n-1} \succ P_{n-2} \succ \cdots \succ P_2 \succ P_1$. If such a situation arises, we call it an inversion paradox. In this article, we give a sufficient condition for the inversion paradox to occur. More precisely, we give an impossibility theorem. We prove that if a ranking method satisfies three reasonable properties (the ranking must be natural, reducible by Condorcet tournaments and satisfies the long tournament property) then we cannot avoid the inversion paradox, i.e., there are tournaments where the inversion paradox occurs. We then show that this paradox can occur when we use classical methods, e.g., Borda, Massey, Colley and Markov methods."
2503.02592,"Real-world contracts are often ambiguous. Recent work by Dütting et al. (EC 2023, Econometrica 2024) models ambiguous contracts as a collection of classic contracts, with the agent choosing an action that maximizes his worst-case utility. In this model, optimal ambiguous contracts have been shown to be ``simple"" in that they consist of single-outcome payment (SOP) contracts, and can be computed in polynomial-time. However, this simplicity is challenged by the potential need for many classic contracts. Motivated by this, we explore \emph{succinct} ambiguous contracts, where the ambiguous contract is restricted to consist of at most $k$ classic contracts. Unlike in the unrestricted case, succinct ambiguous contracts are no longer composed solely of SOP contracts, making both their structure and computation more complex.We show that, despite this added complexity, optimal succinct ambiguous contracts are governed by a simple divide-and-conquer principle, showing that they consist of ``shifted min-pay contracts"" for a suitable partition of the actions. This structural insight implies a characterization of implementability by succinct ambiguous contracts, and can be leveraged to devise an algorithm for the optimal succinct ambiguous contract. While this algorithm is polynomial for $k$ sufficiently close to $n$, for smaller values of $k$, this algorithm is exponential, and we show that this is inevitable (unless P=NP) by establishing NP-hardness for any constant $k$, or $k=\beta n$ for some $\beta\in(0,1)$. Finally, we introduce the succinctness gap measure to quantify the loss incurred due to succinctness, and provide upper and lower bounds on this gap. Interestingly, in the case where we are missing just a single contract from the number sufficient to obtain the utility of the unrestricted case, the principal's utility drops by a factor of $2$, and this is tight."
2503.03082,"We introduces a general linear framework that unifies the study of multi-winner voting rules and proportionality axioms, demonstrating that many prominent multi-winner voting rules-including Thiele methods, their sequential variants, and approval-based committee scoring rules-are linear. Similarly, key proportionality axioms such as Justified Representation (JR), Extended JR (EJR), and their strengthened variants (PJR+, EJR+), along with core stability, can fit within this linear structure as well.Leveraging PAC learning theory, we establish general and novel upper bounds on the sample complexity of learning linear mappings. Our approach yields near-optimal guarantees for diverse classes of rules, including Thiele methods and ordered weighted average rules, and can be applied to analyze the sample complexity of learning proportionality axioms such as approximate core stability. Furthermore, the linear structure allows us to leverage prior work to extend our analysis beyond worst-case scenarios to study the likelihood of various properties of linear rules and axioms. We introduce a broad class of distributions that extend Impartial Culture for approval preferences, and show that under these distributions, with high probability, any Thiele method is resolute, CORE is non-empty, and any Thiele method satisfies CORE, among other observations on the likelihood of commonly-studied properties in social choice.We believe that this linear theory offers a new perspective and powerful new tools for designing and analyzing multi-winner rules in modern social choice applications."
2503.03676,"In this work, we develop a reward design framework for installing a desired behavior as a strict equilibrium across standard solution concepts: dominant strategy equilibrium, Nash equilibrium, correlated equilibrium, and coarse correlated equilibrium. We also extend our framework to capture the Markov-perfect equivalents of each solution concept. Central to our framework is a comprehensive mathematical characterization of strictly installable, based on the desired solution concept and the behavior's structure. These characterizations lead to efficient iterative algorithms, which we generalize to handle optimization objectives through linear programming. Finally, we explore how our results generalize to bounded rational agents."
2503.04202,"Online learning algorithms are widely used in strategic multi-agent settings, including repeated auctions, contract design, and pricing competitions, where agents adapt their strategies over time. A key question in such environments is how an optimizing agent can best respond to a learning agent to improve its own long-term outcomes. While prior work has developed efficient algorithms for the optimizer in special cases - such as structured auction settings or contract design - no general efficient algorithm is known.In this paper, we establish a strong computational hardness result: unless $\mathsf{P} = \mathsf{NP}$, no polynomial-time optimizer can compute a near-optimal strategy against a learner using a standard no-regret algorithm, specifically Multiplicative Weights Update (MWU). Our result proves an $\Omega(T)$ hardness bound, significantly strengthening previous work that only showed an additive $\Theta(1)$ impossibility result. Furthermore, while the prior hardness result focused on learners using fictitious play - an algorithm that is not no-regret - we prove intractability for a widely used no-regret learning algorithm. This establishes a fundamental computational barrier to finding optimal strategies in general game-theoretic settings."
2503.04542,"Professional networks are a key determinant of individuals' labor market outcomes. They may also play a role in either exacerbating or ameliorating inequality of opportunity across demographic groups. In a theoretical model of professional network formation, we show that inequality can increase even without exogenous in-group preferences, confirming and complementing existing theoretical literature. Increased inequality emerges from the differential leverage privileged and unprivileged individuals have in forming connections due to their asymmetric ex ante prospects. This is a formalization of a source of inequality in the labor market which has not been previously explored.We next show how inequality-aware platforms may reduce inequality by subsidizing connections, through link recommendations that reduce costs, between privileged and unprivileged individuals. Indeed, mixed-privilege connections turn out to be welfare improving, over all possible equilibria, compared to not recommending links or recommending some smaller fraction of cross-group links. Taken together, these two findings reveal a stark reality: professional networking platforms that fail to foster integration in the link formation process risk reducing the platform's utility to its users and exacerbating existing labor market inequality."
2503.04661,"The traditional election control problem focuses on the use of control to promote a single candidate. In parliamentary elections, however, the focus shifts: voters care no less about the overall governing coalition than the individual parties' seat count. This paper introduces a new problem: controlling parliamentary elections, where the goal extends beyond promoting a single party to influencing the collective seat count of coalitions of parties.We focus on plurality rule and control through the addition or deletion of parties. Our analysis reveals that, without restrictions on voters' preferences, these control problems are W[1]-hard. In some cases, the problems are immune to control, making such efforts ineffective.We then study the special case where preferences are symmetric single-peaked. We show that in the single-peaked setting, aggregation of voters into types allows for a compact representation of the problem. Our findings show that for the single-peaked setting, some cases are solvable in polynomial time, while others are NP-hard for the compact representation - but admit a polynomial algorithm for the extensive representation."
2503.05338,"In the context of decentralized blockchains, accurately simulating the outcome of order flow auctions (OFAs) off-chain is challenging due to adversarial sequencing, encrypted bids, and frequent state changes. Existing approaches, such as deterministic sorting via consensus layer modifications (e.g., MEV taxes) (Robinson and White 2024) and BRAID (Resnick 2024) or atomic execution of aggregated bids (e.g., Atlas) (Watts et al. 2024), remain vulnerable in permissionless settings where limited throughput allows rational adversaries to submit ""spoof"" bids that block their competitors' access to execution. We propose a new failure cost penalty that applies only when a solution is executed but does not pay its bid or fulfill the order. Combined with an on-chain escrow system, this mechanism empowers applications to asynchronously issue their users a guaranteed minimum outcome before the execution results are finalized. It implies a direct link between blockchain throughput, censorship resistance, and the capital efficiency of auction participants (e.g., solvers), which intuitively extends to execution quality. At equilibrium, bids fully reflect the potential for price improvement between bid submission and execution, but only partially reflect the potential for price declines. This asymmetry unbounded upside for winning bids, limited downside for failed bids, and no loss for losing bids - ultimately benefits users."
2503.05695,"We revisit the setting of fair allocation of indivisible items among agents with heterogeneous, non-monotone valuations. We explore the existence and efficient computation of allocations that approximately satisfy either envy-freeness or equity constraints. Approximate envy-freeness ensures that each agent values her bundle at least as much as those given to the others, after some (or any) item removal, while approximate equity guarantees roughly equal valuations among agents, under similar adjustments. As a key technical contribution of this work, by leveraging fixed-point theorems (such as Sperner's Lemma and its variants), we establish the existence of {\em envy-free-up-to-one-good-and-one-chore} ($\text{EF1}^c_g$) and {\em equitable-up-to-one-good-and-one-chore} ($\text{EQ1}^c_g$) allocations, for non-monotone valuations that are always either non-negative or non-positive. These notions represent slight relaxations of the well-studied {\em envy-free-up-to-one-item} (EF1) and {\em equitable-up-to-one-item} (EQ1) guarantees, respectively. Our existential results hold even when items are arranged in a path and bundles must form connected sub-paths. The case of non-positive valuations, in particular, has been solved by proving a novel multi-coloring variant of Sperner's Lemma that constitutes a combinatorial result of independent interest. In addition, we also design a polynomial-time dynamic programming algorithm that computes an $\text{EQ1}^c_g$ allocation. For monotone non-increasing valuations and path-connected bundles, all the above results can be extended to EF1 and EQ1 guarantees as well. Finally, we provide existential and computational results for certain stronger {\em up-to-any-item} equity notions under objective valuations, where items are partitioned into goods and chores."
2503.06017,"Partitioning a set of $n$ items or agents while maximizing the value of the partition is a fundamental algorithmic task. We study this problem in the specific setting of maximizing social welfare in additively separable hedonic games. Unfortunately, this task faces strong computational boundaries: Extending previous results, we show that approximating welfare by a factor of $n^{1-\epsilon}$ is NP-hard, even for severely restricted weights. However, we can obtain a randomized $\log n$-approximation on instances for which the sum of input valuations is nonnegative. Finally, we study two stochastic models of aversion-to-enemies games, where the weights are derived from Erdős-Rényi or multipartite graphs. We obtain constant-factor and logarithmic-factor approximations with high probability."
2503.06942,"A typical real-time ad-serving funnel comprises ad targeting, conversion modeling (e.g., click-through rate prediction), budget pacing (bidding), and auction processes. While there is a wealth of research and articles on ad targeting and conversion modeling, budget pacing,a crucial component,lacks a systematic treatment specifically tailored for engineers in existing literature. This book aims to provide engineers with a practical yet relatively comprehensive introduction to budget pacing algorithms within the digital advertising domain."
2503.07387,"A seminal result of [Fleischer et al. and Karakostas and Kolliopulos, both FOCS 2004] states that system optimal multi-commodity static network flows are always implementable as tolled Wardrop equilibrium flows even if users have heterogeneous value-of-time sensitivities. Their proof uses LP-duality to characterize the general implementability of network flows by tolls. For the much more complex setting of $\textit{dynamic flows}$, [Graf et al., SODA 2025] identified necessary and sufficient conditions for a dynamic $s$-$d$ flow to be implementable as a tolled dynamic equilibrium. They used the machinery of (infinite-dimensional) strong duality to obtain their characterizations. Their work, however, does not answer the question of whether system optimal dynamic network flows are implementable by tolls.We consider this question for a general dynamic flow model involving multiple commodities with individual source-destination pairs, fixed inflow rates and heterogeneous valuations of travel time and money spent. We present both a positive and a, perhaps surprising, negative result: For the negative result, we provide a network with multiple source and destination pairs in which under the Vickrey queuing model no system optimal flow is implementable -- even if all users value travel times and spent money the same. Our counter-example even shows that the ratio of the achievable equilibrium travel times by using tolls and of the system optimal travel times can be unbounded. For the single-source, single-destination case, we show that if the traversal time functions are suitably well-behaved (as is the case, for example, in the Vickrey queuing model), any system optimal flow is implementable."
2503.07558,"We introduce the first formal model capturing the elicitation of unverifiable information from a party (the ""source"") with implicit signals derived by other players (the ""observers""). Our model is motivated in part by applications in decentralized physical infrastructure networks (a.k.a. ""DePIN""), an emerging application domain in which physical services (e.g., sensor information, bandwidth, or energy) are provided at least in part by untrusted and self-interested parties. A key challenge in these signal network applications is verifying the level of service that was actually provided by network participants.We first establish a condition called source identifiability, which we show is necessary for the existence of a mechanism for which truthful signal reporting is a strict equilibrium. For a converse, we build on techniques from peer prediction to show that in every signal network that satisfies the source identifiability condition, there is in fact a strictly truthful mechanism, where truthful signal reporting gives strictly higher total expected payoff than any less informative equilibrium. We furthermore show that this truthful equilibrium is in fact the unique equilibrium of the mechanism if there is positive probability that any one observer is unconditionally honest (e.g., if an observer were run by the network owner). Also, by extending our condition to coalitions, we show that there are generally no collusion-resistant mechanisms in the settings that we consider.We apply our framework and results to two DePIN applications: proving location, and proving bandwidth. In the location-proving setting observers learn (potentially enlarged) Euclidean distances to the source. Here, our condition has an appealing geometric interpretation, implying that the source's location can be truthfully elicited if and only if it is guaranteed to lie inside the convex hull of the observers."
2503.08385,"The allocation of tasks to a large number of distributed satellites is a difficult problem owing to dynamic changes in massive tasks and the complex matching of tasks to satellites. To reduce the complexity of the problem, tasks that are geographically close can be divided into a predefined grid with a specific time window and processed together. The problem then becomes a dynamic grid with time-window allocation problem (DGAP). To ensure consistent visibility between satellites and grids, the timeline of the DGAP is partitioned into several decision-making stages that are determined by dynamic changes in the time window. Subsequently, the DGAP can be resolved progressively adopting the potential game approach in the single-stage DGAP (sDGAP). First, to solve the discontinuity in the goal of the sDGAP, we approximate the goal by a smooth exponential sum function that we regard as the global utility function. Second, a potential game theoretic framework is constructed by decomposing this global utility function into the local utility functions of individuals. We prove that each Nash equilibrium of the proposed potential game is the optimal solution of the sDGAP. Third, to solve the potential game, a distributed algorithm, referred to as the selective time-variant better reply process (SeTVBRP) algorithm, is proposed and its convergence is proved. The SeTVBRP algorithm is an improved algorithm based on the better reply process algorithm, where two improvement methods (i.e., the selective action method and time-variant parameter method) are introduced. Through factor analysis, we demonstrate the effectiveness of the two improvement methods for the sDGAP. Last, numerical results show that the proposed algorithm outperforms existing learning algorithms and is effective in solving the DGAP."
2503.08416,"In the context of Vehicular ad-hoc networks (VANETs), the hierarchical management of intelligent vehicles, based on clustering methods, represents a well-established solution for effectively addressing scalability and reliability issues. The previous studies have primarily focused on centralized clustering problems with a single objective. However, this paper investigates the distributed clustering problem that simultaneously optimizes two objectives: the cooperative capacity and management overhead of cluster formation, under dynamic network conditions. Specifically, the clustering problem is formulated within a coalition formation game framework to achieve both low computational complexity and automated decision-making in cluster formation. Additionally, we propose a distributed clustering algorithm (DCA) that incorporates three innovative operations for forming/breaking coalition, facilitating collaborative decision-making among individual intelligent vehicles. The convergence of the DCA is proven to result in a Nash stable partition, and extensive simulations demonstrate its superior performance compared to existing state-of-the-art approaches for coalition formation."
2503.09039,"Federated learning offers a decentralized approach to machine learning, where multiple agents collaboratively train a model while preserving data privacy. In this paper, we investigate the decision-making and equilibrium behavior in federated learning systems, where agents choose between participating in global training or conducting independent local training. The problem is first modeled as a stage game and then extended to a repeated game to analyze the long-term dynamics of agent participation. For the stage game, we characterize the participation patterns and identify Nash equilibrium, revealing how data heterogeneity influences the equilibrium behavior-specifically, agents with similar data qualities will participate in FL as a group. We also derive the optimal social welfare and show that it coincides with Nash equilibrium under mild assumptions. In the repeated game, we propose a privacy-preserving, computationally efficient myopic strategy. This strategy enables agents to make practical decisions under bounded rationality and converges to a neighborhood of Nash equilibrium of the stage game in finite time. By combining theoretical insights with practical strategy design, this work provides a realistic and effective framework for guiding and analyzing agent behaviors in federated learning systems."
2503.09538,"We study equilibrium finding in polymatrix games under differential privacy constraints. To start, we show that high accuracy and asymptotically vanishing differential privacy budget (as the number of players goes to infinity) cannot be achieved simultaneously under either of the two settings: (i) We seek to establish equilibrium approximation guarantees in terms of Euclidean distance to the equilibrium set, and (ii) the adversary has access to all communication channels. Then, assuming the adversary has access to a constant number of communication channels, we develop a novel distributed algorithm that recovers strategies with simultaneously vanishing Nash gap (in expected utility, also referred to as exploitability and privacy budget as the number of players increases."
2503.10185,"Following the publication of Bitcoin's arguably most famous attack, selfish mining, various works have introduced mechanisms to enhance blockchain systems' game theoretic resilience. Some reward mechanisms, like FruitChains, have been shown to be equilibria in theory. However, their guarantees assume non-realistic parameters and their performance degrades significantly in a practical deployment setting. In this work we introduce a reward allocation mechanism, called Proportional Splitting (PRS), which outperforms existing state of the art. We show that, for large enough parameters, PRS is an equilibrium, offering the same theoretical guarantees as the state of the art. In addition, for practical, realistically small, parameters, PRS outperforms all existing reward mechanisms across an array of metrics. We implement PRS on top of a variant of PoEM, a Proof-of-Work (PoW) protocol that enables a more accurate estimation of each party's mining power compared to e.g., Bitcoin. We then evaluate PRS both theoretically and in practice. On the theoretical side, we show that our protocol combined with PRS is an equilibrium and guarantees fairness, similar to FruitChains. In practice, we compare PRS with an array of existing reward mechanisms and show that, assuming an accurate estimation of the mining power distribution, it outperforms them across various well-established metrics. Finally, we realize this assumption by approximating the power distribution via low-work objects called ""workshares"" and quantify the tradeoff between the approximation's accuracy and storage overhead."
2503.10513,"We consider the problem of fair allocation of $m$ indivisible goods to $n$ agents with either subadditive or XOS valuations, in the arbitrary entitlement case. As fairness notions, we consider the anyprice share (APS) ex-post, and the maximum expectation share (MES) ex-ante.We observe that there are randomized allocations that ex-ante are at least $\frac{1}{2}$-MES in the subadditive case and $(1-\frac{1}{e})$-MES in the XOS case. Our more difficult results concern ex-post guarantees. We show that $(1 - o(1))\frac{\log\log m}{\log m}$-APS allocations exist in the subadditive case, and $\frac{1}{6}$-APS allocations exist in the XOS case. For the special case of equal entitlements, we show $\frac{4}{17}$-APS allocations for XOS.Our results are the first for subadditive and XOS valuations in the arbitrary entitlement case, and also improve over the previous best results for the equal entitlement case."
2503.1083,"In the recently introduced model of fair partitioning of friends, there is a set of agents located on the vertices of an underlying graph that indicates the friendships between the agents. The task is to partition the graph into $k$ balanced-sized groups, keeping in mind that the value of an agent for a group equals the number of edges they have in that group. The goal is to construct partitions that are ""fair"", i.e., no agent would like to replace an agent in a different group. We generalize the standard model by considering utilities for the agents that are beyond binary and additive. Having this as our foundation, our contribution is threefold (a) we adapt several fairness notions that have been developed in the fair division literature to our setting; (b) we give several existence guarantees supported by polynomial-time algorithms; (c) we initiate the study of the computational (and parameterized) complexity of the model and provide an almost complete landscape of the (in)tractability frontier for our fairness concepts."
2503.1091,"We study sequential procurement auctions where the sellers are provided with a ``best and final offer'' (BAFO) strategy. This strategy allows each seller $i$ to effectively ``freeze'' their price while remaining active in the auction, and it signals to the buyer, as well as all other sellers, that seller $i$ would reject any price lower than that. This is in contrast to prior work, e.g., on descending auctions, where the options provided to each seller are to either accept a price reduction or reject it and drop out. As a result, the auctions that we consider induce different extensive form games and our goal is to study the subgame perfect equilibria of these games. We focus on settings involving multiple sellers who have full information regarding each other's cost (i.e., the minimum price that they can accept) and a single buyer (the auctioneer) who has no information regarding these costs. Our main result shows that the auctions enhanced with the BAFO strategy can guarantee efficiency in every subgame perfect equilibrium, even if the buyer's valuation function is an arbitrary monotone function. This is in contrast to prior work which required that the buyer's valuation satisfies restrictive properties, like gross substitutes, to achieve efficiency. We then also briefly analyze the seller's cost in the subgame perfect equilibria of these auctions and we show that even if the auctions all return the same outcome, the cost that they induce for the buyer can vary significantly."
2503.1099,"Aligning large language models (LLMs) with diverse human preferences is critical for ensuring fairness and informed outcomes when deploying these models for decision-making. In this paper, we seek to uncover fundamental statistical limits concerning aligning LLMs with human preferences, with a focus on the probabilistic representation of human preferences and the preservation of diverse preferences in aligned LLMs. We first show that human preferences can be represented by a reward model if and only if the preference among LLM-generated responses is free of any Condorcet cycle. Moreover, we prove that Condorcet cycles exist with probability converging to one exponentially fast under a probabilistic preference model, thereby demonstrating the impossibility of fully aligning human preferences using reward-based approaches such as reinforcement learning from human feedback. Next, we explore the conditions under which LLMs would employ mixed strategies -- meaning they do not collapse to a single response -- when aligned in the limit using a non-reward-based approach, such as Nash learning from human feedback (NLHF). We identify a necessary and sufficient condition for mixed strategies: the absence of a response that is preferred over all others by a majority. As a blessing, we prove that this condition holds with high probability under the probabilistic preference model, thereby highlighting the statistical possibility of preserving minority preferences without explicit regularization in aligning LLMs. Finally, we leverage insights from our statistical results to design a novel, computationally efficient algorithm for finding Nash equilibria in aligning LLMs with NLHF. Our experiments show that Llama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\% against the base model."
2503.13232,"Consider an M/M/1-type queue where joining attains a known reward, but a known waiting cost is paid per time unit spent queueing. In the 1960s, Naor showed that any arrival optimally joins the queue if its length is less than a known threshold. Yet acquiring knowledge of the queue length often brings an additional cost, e.g., website loading time or data roaming charge. Therefore, our model presents any arrival with three options: join blindly, balk blindly, or pay a known inspection cost to make the optimal joining decision by comparing the queue length to Naor's threshold. In a recent paper, Hassin and Roet-Green prove that a unique Nash equilibrium always exists and classify regions where the equilibrium probabilities are non-zero. We complement these findings with new closed-form expressions for the equilibrium probabilities in the majority of cases. Further, Hassin and Roet-Green show that minimizing inspection cost maximises social welfare. Envisaging a queue operator choosing where to invest, we compare the effects of lowering inspection cost and increasing the queue-joining reward on social welfare. We prove that the former dominates and that the latter can even have a detrimental effect on social welfare."
2503.14316,"Airdrops issued by platforms are to distribute tokens, drive user adoption, and promote decentralized services. The distributions attract airdrop hunters (attackers), who exploit the system by employing Sybil attacks, i.e., using multiple identities to manipulate token allocations to meet eligibility criteria. While debates around airdrop hunting question the potential benefits to the ecosystem, exploitative behaviors like Sybil attacks clearly undermine the system's integrity, eroding trust and credibility. Despite the increasing prevalence of these tactics, a gap persists in the literature regarding systematic modeling of airdrop hunters' costs and returns, alongside the theoretical models capturing the interactions among all roles for airdrop mechanism design. Our study first conducts an empirical analysis of transaction data from the Hop Protocol and LayerZero, identifying prevalent attack patterns and estimating hunters' expected profits. Furthermore, we develop a game-theory model that simulates the interactions between attackers, organizers, and bounty hunters, proposing optimal incentive structures that enhance detection while minimizing organizational costs."
2503.1469,"Finite-horizon probabilistic multiagent concurrent game systems, also known as finite multiplayer stochastic games, are a well-studied model in computer science due to their ability to represent a wide range of real-world scenarios involving strategic interactions among agents over a finite amount of iterations (given by the finite-horizon). The analysis of these games typically focuses on evaluating (verifying) and computing (synthesizing/realizing) which strategy profiles (functions that represent the behavior of each agent) qualify as equilibria. The two most prominent equilibrium concepts are the Nash equilibrium and the subgame perfect equilibrium, with the latter considered a conceptual refinement of the former. However, computing these equilibria from scratch is often computationally infeasible. Therefore, recent attention has shifted to the verification problem, where a given strategy profile must be evaluated to determine whether it satisfies equilibrium conditions. In this paper, we demonstrate that the verification problem for subgame perfect equilibria lies in PSPACE, while for Nash equilibria, it is EXPTIME-complete. This is a highly counterintuitive result since subgame perfect equilibria are often seen as a strict strengthening of Nash equilibria and are intuitively seen as more complicated."
2503.14707,"We study the computational complexity of bribery in parliamentary voting, in settings where the briber is (also) interested in the success of an entire set of political parties - a ``coalition'' - rather than an individual party. We introduce two variants of the problem: the Coalition-Bribery Problem (CB) and the Coalition-Bribery-with-Preferred-party Problem (CBP). In CB, the goal is to maximize the total number of seats held by a coalition, while in CBP, there are two objectives: to maximize the votes for the preferred party, while also ensuring that the total number of seats held by the coalition is above the target support (e.g. majority).We study the complexity of these bribery problems under two positional scoring functions - Plurality and Borda - and for multiple bribery types - $1$-bribery, $\$$-bribery, swap-bribery, and coalition-shift-bribery. We also consider both the case where seats are only allotted to parties whose number of votes passes some minimum support level and the case with no such minimum. We provide polynomial-time algorithms to solve some of these problems and prove that the others are NP-hard."
2503.15184,"To address the risks of validator centralization, Proposer-Builder Separation (PBS) was introduced in Ethereum to divide the roles of block building and block proposing, fostering a more equitable and decentralized block production environment. PBS creates a two-sided market in which searchers submit valuable bundles to builders for inclusion in blocks, while builders compete in auctions for block proposals. In this paper, we formulate and analyze a role-selection game that models how profit-seeking participants in PBS strategically choose between acting as searchers or builders, using a co-evolutionary framework to capture the complex interactions and payoff dynamics in this market. Through agent-based simulations, we demonstrate that agents' optimal role-acting as searcher or builder-responds dynamically to the probability of conflict between bundles. Our empirical game-theoretic analysis quantifies the equilibrium frequencies of role selection under different market conditions, revealing that low conflict probabilities lead to equilibria dominated by searchers, while higher probabilities shift equilibrium toward builders. Additionally, bundle conflicts have non-monotonic effects on agent payoffs and strategy evolution. Our results advance the understanding of decentralized block building and provide guidance for designing fairer and more robust block production mechanisms in blockchain systems."
2503.15346,"This paper investigates properties of Blackwell $\epsilon$-optimal strategies in zero-sum stochastic games when the adversary is restricted to stationary strategies, motivated by applications to robust Markov decision processes. For a class of absorbing games, we show that Markovian Blackwell $\epsilon$-optimal strategies may fail to exist, yet we prove the existence of Blackwell $\epsilon$-optimal strategies that can be implemented by a two-state automaton whose internal transitions are independent of actions. For more general absorbing games, however, there need not exist Blackwell $\epsilon$-optimal strategies that are independent of the adversary's decisions. Our findings point to a contrast between absorbing games and generalized Big Match games, and provide new insights into the properties of optimal policies for robust Markov decision processes."
2503.15486,"Non-cooperative dynamic game theory provides a principled approach to modeling sequential decision-making among multiple noncommunicative agents. A key focus has been on finding Nash equilibria in two-agent zero-sum dynamic games under various information structures. A well-known result states that in linear-quadratic games, unique Nash equilibria under feedback and open-loop information structures yield identical trajectories. Motivated by two key perspectives -- (i) many real-world problems extend beyond linear-quadratic settings and lack unique equilibria, making only local Nash equilibria computable, and (ii) local open-loop Nash equilibria (OLNE) are easier to compute than local feedback Nash equilibria (FBNE) -- it is natural to ask whether a similar result holds for local equilibria in zero-sum games. To this end, we establish that for a broad class of zero-sum games with potentially nonconvex-nonconcave objectives and nonlinear dynamics: (i) the state/control trajectory of a local FBNE satisfies local OLNE first-order optimality conditions, and vice versa, (ii) a local FBNE trajectory satisfies local OLNE second-order necessary conditions, (iii) a local FBNE trajectory satisfying feedback sufficiency conditions also constitutes a local OLNE, and (iv) with additional hard constraints on agents' actuations, a local FBNE where strict complementarity holds also satisfies local OLNE first-order optimality conditions, and vice versa."
2503.15634,"Firms' algorithm development practices are often homogeneous. Whether firms train algorithms on similar data, aim at similar benchmarks, or rely on similar pre-trained models, the result is correlated predictions. We model the impact of correlated algorithms on competition in the context of personalized pricing. Our analysis reveals that (1) higher correlation diminishes consumer welfare and (2) as consumers become more price sensitive, firms are increasingly incentivized to compromise on the accuracy of their predictions in exchange for coordination. We demonstrate our theoretical results in a stylized empirical study where two firms compete using personalized pricing algorithms. Our results underscore the ease with which algorithms facilitate price correlation without overt communication, which raises concerns about a new frontier of anti-competitive behavior. We analyze the implications of our results on the application and interpretation of US antitrust law."
2503.16002,"Distributing services, goods, and tasks in the gig economy heavily relies upon on-demand workers (aka agents), leading to new challenges varying from logistics optimization to the ethical treatment of gig workers. We focus on fair and efficient distribution of delivery tasks -- placed on the vertices of a graph -- among a fixed set of agents. We consider the fairness notion of minimax share (MMS), which aims to minimize the maximum (submodular) cost among agents and is particularly appealing in applications without monetary transfers. We propose a novel efficiency notion -- namely non-wastefulness -- that is desirable in a wide range of scenarios and, more importantly, does not suffer from computational barriers. Specifically, given a distribution of tasks, we can, in polynomial time, i) verify whether the distribution is non-wasteful and ii) turn it into an equivalent non-wasteful distribution. Moreover, we investigate several fixed-parameter tractable and polynomial-time algorithms and paint a complete picture of the (parameterized) complexity of finding fair and efficient distributions of tasks with respect to both the structure of the topology and natural restrictions of the input. Finally, we highlight how our findings shed light on computational aspects of other well-studied fairness notions, such as envy-freeness and its relaxations."
2503.16052,"The stable roommates problem is a non-bipartite version of the well-known stable matching problem. Teo and Sethuraman proved that, for each instance of the stable roommates problem in a complete graph, there exists a linear inequality system such that there exists a feasible solution to this system if and only if there exists a stable matching in the given instance. The aim of this paper is to extend the result of Teo and Sethuraman to the stable roommates problem with ties. More concretely, we prove that, for each instance of the stable roommates problem with ties in a complete graph, there exists a linear inequality system such that there exists a feasible solution to this system if and only if there exists a super-stable matching in the given instance."
2503.1628,"Theoretical guarantees about peer prediction mechanisms typically rely on the discreteness of the signal and report space. However, we posit that a discrete signal model is not realistic: in practice, agents observe richer information and map their signals to a discrete report. In this paper, we formalize a model with real-valued signals and binary reports. We study a natural class of symmetric strategies where agents map their information to a binary value according to a single real-valued threshold. We characterize equilibria for several well-known peer prediction mechanisms which are known to be truthful under the binary report model. In general, even when every threshold would correspond to a truthful equilibrium in the binary signal model, only certain thresholds remain equilibria in our model. Furthermore, by studying the dynamics of this threshold, we find that some of these equilibria are unstable. These results suggest important limitations for the deployment of existing peer prediction mechanisms in practice."
2503.16285,"Understanding the convergence landscape of multi-agent learning is a fundamental problem of great practical relevance in many applications of artificial intelligence and machine learning. While it is known that learning dynamics converge to Nash equilibrium in potential games, the behavior of dynamics in many important classes of games that do not admit a potential is poorly understood. To measure how ''close'' a game is to being potential, we consider a distance function, that we call ''potentialness'', and which relies on a strategic decomposition of games introduced by Candogan et al. (2011). We introduce a numerical framework enabling the computation of this metric, which we use to calculate the degree of ''potentialness'' in generic matrix games, as well as (non-generic) games that are important in economic applications, namely auctions and contests. Understanding learning in the latter games has become increasingly important due to the wide-spread automation of bidding and pricing with no-regret learning algorithms. We empirically show that potentialness decreases and concentrates with an increasing number of agents or actions; in addition, potentialness turns out to be a good predictor for the existence of pure Nash equilibria and the convergence of no-regret learning algorithms in matrix games. In particular, we observe that potentialness is very low for complete-information models of the all-pay auction where no pure Nash equilibrium exists, and much higher for Tullock contests, first-, and second-price auctions, explaining the success of learning in the latter. In the incomplete-information version of the all-pay auction, a pure Bayes-Nash equilibrium exists and it can be learned with gradient-based algorithms. Potentialness nicely characterizes these differences to the complete-information version."
2503.16414,"Lindahl equilibrium is a solution concept for allocating a fixed budget across several divisible public goods. It always lies in the weak core, meaning that the equilibrium allocation satisfies desirable stability and proportional fairness properties. We consider a model where agents have separable linear utility functions over the public goods, and the output assigns to each good an amount of spending, summing to at most the available budget.In the uncapped setting, each of the public goods can absorb any amount of funding. In this case, it is known that Lindahl equilibrium is equivalent to maximizing Nash social welfare, and this allocation can be computed by a public-goods variant of the proportional response dynamics. We introduce a new convex programming formulation for computing this solution and show that it is related to Nash welfare maximization through double duality and reformulation. We then show that the proportional response dynamics is equivalent to running mirror descent on our new formulation. Our new formulation has similarities to Shmyrev's convex program for Fisher market equilibrium.In the capped setting, each public good has an upper bound on the amount of funding it can receive, which is a type of constraint that appears in fractional committee selection and participatory budgeting. In this setting, existence of Lindahl equilibrium was only known via fixed-point arguments. The existence of an efficient algorithm computing one has been a long-standing open question. We prove that our new convex program continues to work when the cap constraints are added, and its optimal solutions are Lindahl equilibria. Thus, we establish that approximate Lindahl equilibrium can be efficiently computed. Our result also implies that approximately core-stable allocations can be efficiently computed for the class of separable piecewise-linear concave (SPLC) utilities."
2503.16781,"We propose a variant of Nim, named StrNim. Whereas a position in Nim is a tuple of non-negative integers, that in StrNim is a string, a sequence of characters. In every turn, each player shrinks the string, by removing a substring repeating the same character. As a first study on this new game, we present some sufficient conditions for the positions to be P-positions."
2503.17156,"In many proportional parliamentary elections, electoral thresholds (typically 3-5%) are used to promote stability and governability by preventing the election of parties with very small representation. However, these thresholds often result in a significant number of ""wasted votes"" cast for parties that fail to meet the threshold, which reduces representativeness. One proposal is to allow voters to specify replacement votes, by either indicating a second choice party or by ranking a subset of the parties, but there are several ways of deciding on the scores of the parties (and thus the composition of the parliament) given those votes. We introduce a formal model of party voting with thresholds, and compare a variety of party selection rules axiomatically, and experimentally using a dataset we collected during the 2024 European election in France. We identify three particularly attractive rules, called Direct Winners Only (DO), Single Transferable Vote (STV) and Greedy Plurality (GP)."
2503.17294,"We introduce the concept of a \emph{cycle pattern} for directed graphs as functions from the set of cycles to the set $\{-,0,+\}$. The key example for such a pattern is derived from a weight function, giving rise to the sign of the total weight of the edges for each cycle. Hence, cycle patterns describe a fundamental structure of a weighted digraph, and they arise naturally in games on graphs, in particular parity games, mean payoff games, and energy games.Our contribution is threefold: we analyze the structure and derive hardness results for the realization of cycle patterns by weight functions. Then we use them to show hardness of solving games given the limited information of a cycle pattern. Finally, we identify a novel geometric hardness measure for solving mean payoff games (MPG) using the framework of linear decision trees, and use cycle patterns to derive lower bounds with respect to this measure, for large classes of algorithms for MPGs."
2503.18237,"Lending within decentralized finance (DeFi) has facilitated over \$100 billion of loans since 2020. A long-standing inefficiency in DeFi lending protocols such as Aave is the use of static pricing mechanisms for loans. These mechanisms have been shown to maximize neither welfare nor revenue for participants in DeFi lending protocols. Recently, adaptive supply models pioneered by Morpho and Euler have become a popular means of dynamic pricing for loans. This pricing is facilitated by agents known as curators, who bid to match supply and demand. We construct and analyze an online learning model for static and dynamic pricing models within DeFi lending. We show that when loans are small and have a short duration relative to an observation time $T$, adaptive supply models achieve $O(\log T)$ regret, while static models cannot achieve better than $\Omega(\sqrt{T})$ regret. We then study competitive behavior between curators, demonstrating that adaptive supply mechanisms maximize revenue and welfare for both borrowers and lenders."
2503.18398,"Global cooperation often falters despite shared objectives, as misaligned interests and unequal incentives undermine collective efforts, such as those in international climate change collaborations. To tackle this issue, this paper introduces a multi-level game-theoretic model to analyze the dynamics of complex interactions within hierarchical systems. The model consists of global, local, and pairwise games, and two strategy types, binary and level-based strategies, are explored under varying parameter conditions. Using computational simulations and numerical analysis, we examine how factors across different levels influence player decisions, game dynamics and population phase transitions during the evolutionary process. Our findings reveal that although the increase of profit rates at local and pairwise games enhances cooperation within the population, the global game exerts minimal influence on player decisions and population states under both strategy settings. Particularly, analytical and simulation results show that, under binary strategies, global profit does not influence localized decision-making of players, while under level-based strategies, players cooperating at the global level are eventually outcompeted due to the evolutionary disadvantage even when global profit is substantial. These insights contribute to a theoretical understanding of cooperation dynamics in multi-level systems and may offer implications for fostering global collaboration on challenges like climate change."
2503.19156,"Big Boss Games represent a specific class of cooperative games where a single veto player, known as the Big Boss, plays a central role in determining resource allocation and maintaining coalition stability. In this paper, we introduce a novel allocation scheme for Big Boss games, based on two classical solution concepts: the Shapley value and the $\tau$-value. This scheme generates a coalitionally stable allocation that effectively accounts for the contributions of weaker players. Specifically, we consider a diagonal of the core that includes the Big Boss's maximum aspirations, the $\tau$-value, and those of the weaker players. From these allocations, we select the one that is closest to the Shapley value, referred to as the Projected Shapley Value allocation (PSV allocation). Through our analysis, we identify a new property of Big Boss games, particularly the relationship between the allocation discrepancies assigned by the $\tau$-value and the Shapley value, with a particular focus on the Big Boss and the other players. Additionally, we provide a new characterization of convexity within this context. Finally, we conduct a statistical analysis to assess the position of the PSV allocation within the core, especially in cases where computing the Shapley value is computationally challenging."
2503.192,"Game-theoretic approaches and Nash equilibrium have been widely applied across various engineering domains. However, practical challenges such as disturbances, delays, and actuator limitations can hinder the precise execution of Nash equilibrium strategies. This work investigates the impact of such implementation imperfections on game trajectories and players' costs in the context of a two-player finite-horizon linear quadratic (LQ) nonzero-sum game. Specifically, we analyze how small deviations by one player, measured or estimated at each stage, affect the state and cost function of the other player. To mitigate these effects, we propose an adjusted control policy that optimally compensates for the deviations under the stated information structure and can, under certain conditions, exploit them to improve performance. Rigorous mathematical analysis and proofs are provided, and the effectiveness of the proposed method is demonstrated through a representative numerical example."
2503.2026,"We study the problem of fairly allocating indivisible items under category constraints. Specifically, there are $n$ agents and $m$ indivisible items which are partitioned into categories with associated capacities. An allocation is considered feasible if each bundle satisfies the capacity constraints of its respective categories. For the case of two agents, Shoshan et al. (2023) recently developed a polynomial-time algorithm to find a Pareto-optimal allocation satisfying a relaxed version of envy-freeness, called EF$[1,1]$. In this paper, we extend the result of Shoshan et al. to $n$ agents, proving the existence of a Pareto-optimal allocation where each agent can be made envy-free by reallocating at most ${n(n-1)}$ items. Furthermore, we present a polynomial-time algorithm to compute such an allocation when the number $n$ of agents is constant."
2503.20848,"Recent policy proposals aim to improve the safety of general-purpose AI, but there is little understanding of the efficacy of different regulatory approaches to AI safety. We present a strategic model that explores the interactions between safety regulation, the general-purpose AI creators, and domain specialists--those who adapt the technology for specific applications. Our analysis examines how different regulatory measures, targeting different parts of the AI development chain, affect the outcome of this game. In particular, we assume AI technology is characterized by two key attributes: safety and performance. The regulator first sets a minimum safety standard that applies to one or both players, with strict penalties for non-compliance. The general-purpose creator then invests in the technology, establishing its initial safety and performance levels. Next, domain specialists refine the AI for their specific use cases, updating the safety and performance levels and taking the product to market. The resulting revenue is then distributed between the specialist and generalist through a revenue-sharing parameter. Our analysis reveals two key insights: First, weak safety regulation imposed predominantly on domain specialists can backfire. While it might seem logical to regulate AI use cases, our analysis shows that weak regulations targeting domain specialists alone can unintentionally reduce safety. This effect persists across a wide range of settings. Second, in sharp contrast to the previous finding, we observe that stronger, well-placed regulation can in fact mutually benefit all players subjected to it. When regulators impose appropriate safety standards on both general-purpose AI creators and domain specialists, the regulation functions as a commitment device, leading to safety and performance gains, surpassing what is achieved under no regulation or regulating one player alone."
2503.20918,"Integer programming games (IPGs) are n-person games with integer strategy spaces. These games are used to model non-cooperative combinatorial decision-making and are used in domains such as cybersecurity and transportation. The prevalent solution concept for IPGs, Nash equilibrium, is difficult to compute and even showing whether such an equilibrium exists is known to be Sp2-complete. In this work, we introduce a class of relaxed solution concepts for IPGs called locally optimal integer solutions (LOIS) that are simpler to obtain than pure Nash equilibria. We demonstrate that LOIS are not only faster and more readily scalable in large-scale games but also support desirable features such as equilibrium enumeration and selection. We also show that these solutions can model a broader class of problems including Stackelberg, Stackelberg-Nash, and generalized IPGs. Finally, we provide initial comparative results in a cybersecurity game called the Critical Node game, showing the performance gains of LOIS in comparison to the existing Nash equilibrium solution concept."
2503.20975,"In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new N-player K-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on same arms and time-varying nature of arm rewards make the policy analysis more involved than existing studies for myopic players. We explicitly analyze the threshold-based structures of social optimum and existing selfish policy, showing that the latter causes prolonged convergence time $\Omega(\frac{K}{\eta^2}\ln({\frac{KN}{\delta}}))$, while socially optimal policy with coordinated communication reduces it to $\mathcal{O}(\frac{K}{N\eta^2}\ln{(\frac{K}{\delta})})$. Based on the comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as the strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for social planner and ensures truthful reporting from players, achieving the minimum PoA=1 and same convergence time as social optimum."
2503.22726,"In online advertising systems, publishers often face a trade-off in information disclosure strategies: while disclosing more information can enhance efficiency by enabling optimal allocation of ad impressions, it may lose revenue potential by decreasing uncertainty among competing advertisers. Similar to other challenges in market design, understanding this trade-off is constrained by limited access to real-world data, leading researchers and practitioners to turn to simulation frameworks. The recent emergence of large language models (LLMs) offers a novel approach to simulations, providing human-like reasoning and adaptability without necessarily relying on explicit assumptions about agent behavior modeling. Despite their potential, existing frameworks have yet to integrate LLM-based agents for studying information asymmetry and signaling strategies, particularly in the context of auctions. To address this gap, we introduce InfoBid, a flexible simulation framework that leverages LLM agents to examine the effects of information disclosure strategies in multi-agent auction settings. Using GPT-4o, we implemented simulations of second-price auctions with diverse information schemas. The results reveal key insights into how signaling influences strategic behavior and auction outcomes, which align with both economic and social learning theories. Through InfoBid, we hope to foster the use of LLMs as proxies for human economic and social agents in empirical studies, enhancing our understanding of their capabilities and limitations. This work bridges the gap between theoretical market designs and practical applications, advancing research in market simulations, information design, and agent-based reasoning while offering a valuable tool for exploring the dynamics of digital economies."
2503.23139,"Motivated by applications such as urban traffic control and make-to-order systems, we study a fluid model of a single-server, on-off system that can accommodate multiple queues. The server visits each queue in order: when a queue is served, it is ""on"", and when the server is serving another queue or transitioning between queues, it is ""off"". Customers arrive over time, observe the state of the system, and decide whether to join. We consider two regimes for the formation of the on and off durations. In the exogenous setting, each queue's on and off durations are predetermined. We explicitly characterize the equilibrium outcome in closed form and give a compact linear program to compute the optimal on-off durations that maximizes total reward collected from serving customers. In the endogenous setting, the durations depend on customers' joining decisions under an exhaustive service policy where the server never leaves a non-empty queue. We show that an optimal policy in this case extends service beyond the first clearance for at most one queue. Using this property, we introduce a closed-form procedure that computes an optimal policy in no more than 2n steps for a system with n queues."
2503.23991,"We investigate the relationship between the team-optimal solution and the Nash equilibrium (NE) to assess the impact of strategy deviation on team performance. As a working use case, we focus on a class of flow assignment problems in which each source node acts as a cooperating decision maker (DM) within a team that minimizes the team cost based on the team-optimal strategy. In practice, some selfish DMs may prioritize their own marginal cost and deviate from NE strategies, thus potentially degrading the overall performance. To quantify this deviation, we explore the deviation bound between the team-optimal solution and the NE in two specific scenarios: (i) when the team-optimal solution is unique and (ii) when multiple solutions do exist. This helps DMs analyze the factors influencing the deviation and adopting the NE strategy within a tolerable range. Furthermore, in the special case of a potential game model, we establish the consistency between the team-optimal solution and the NE. Once the consistency condition is satisfied, the strategy deviation does not alter the total cost, and DMs do not face a strategic trade-off. Finally, we validate our theoretical analysis through some simulation studies."
2503.24179,"In this study, we focus on a form of joint transportation called mixed transportation and enumerate the combinations with high cooperation effects from among a number of transport lanes registered in a database (logistics big data). As a measure of the efficiency of mixed transportation, we consider the reduction rate that represents how much the total distance of loading trips is shortened by cooperation. The proposed algorithm instantly presents the set of all mixed transports with a reduction rate of a specified value or less. This algorithm is more than 7,000 times faster than simple brute force."
2503.2434,"We establish the first uncoupled learning algorithm that attains $O(n \log^2 d \log T)$ per-player regret in multi-player general-sum games, where $n$ is the number of players, $d$ is the number of actions available to each player, and $T$ is the number of repetitions of the game. Our results exponentially improve the dependence on $d$ compared to the $O(n\, d \log T)$ regret attainable by Log-Regularized Lifted Optimistic FTRL [Far+22c], and also reduce the dependence on the number of iterations $T$ from $\log^4 T$ to $\log T$ compared to Optimistic Hedge, the previously well-studied algorithm with $O(n \log d \log^4 T)$ regret [DFG21]. Our algorithm is obtained by combining the classic Optimistic Multiplicative Weights Update (OMWU) with an adaptive, non-monotonic learning rate that paces the learning process of the players, making them more cautious when their regret becomes too negative."
2504.01192,"Traditional approaches to modeling and predicting traffic behavior often rely on Wardrop Equilibrium (WE), assuming non-atomic traffic demand and neglecting correlations in individual decisions. However, the growing role of real-time human feedback and adaptive recommendation systems calls for more expressive equilibrium concepts that better capture user preferences and the stochastic nature of routing behavior. In this paper, we introduce a preference-centric route recommendation framework grounded in the concept of Borda Coarse Correlated Equilibrium (BCCE), wherein users have no incentive to deviate from recommended strategies when evaluated by Borda scores-pairwise comparisons encoding user preferences. We develop an adaptive algorithm that learns from dueling feedback and show that it achieves $\mathcal{O}(T^{\frac{2}{3}})$ regret, implying convergence to the BCCE under mild assumptions. We conduct empirical evaluations using a case study to illustrate and justify our theoretical analysis. The results demonstrate the efficacy and practical relevance of our approach."
2504.01773,"The problem of computing near-optimal contracts in combinatorial settings has recently attracted significant interest in the computer science community. Previous work has provided a rich body of structural and algorithmic insights into this problem. However, most of these results rely on the assumption that the principal has an unlimited budget for incentivizing agents, an assumption that is often unrealistic in practice. This motivates the study of the optimal contract problem under budget constraints.In this work, we study multi-agent contracts with binary actions under budget constraints. Our contribution is threefold. First, we show that all previously known approximation guarantees on the principal's utility extend (asymptotically) to budgeted settings. Second, through the lens of budget constraints, we uncover insightful connections between the standard objective of maximizing the principal's utility and other relevant objectives. Specifically, we identify a broad class of objectives, which we term BEST (BEyond STandard) objectives, including reward, social welfare, and principal's utility, and show that they are all equivalent (up to a constant factor), leading to approximation guarantees for all BEST objectives. Third, we introduce the price of frugality, which quantifies the loss due to budget constraints, and establish near-tight bounds on this measure, providing deeper insights into the tradeoffs between budgets and incentives."
2504.02346,"The on-demand ride-hailing industry has experienced rapid growth, transforming transportation norms worldwide. Despite improvements in efficiency over traditional taxi services, significant challenges remain, including drivers' strategic repositioning behavior, customer abandonment, and inefficiencies in dispatch algorithms. To address these issues, we introduce a comprehensive mean field game model that systematically analyzes the dynamics of ride-hailing platforms by incorporating driver repositioning across multiple regions, customer abandonment behavior, and platform dispatch algorithms. Using this framework, we identify all possible mean field equilibria as the Karush-Kuhn-Tucker (KKT) points of an associated optimization problem. Our analysis reveals the emergence of multiple equilibria, including the inefficient ""Wild Goose Chase"" one, characterized by drivers pursuing distant requests, leading to suboptimal system performance. To mitigate these inefficiencies, we propose a novel two-matching-radius nearest-neighbor dispatch algorithm that eliminates undesirable equilibria and ensures a unique mean field equilibrium for multi-region systems. The algorithm dynamically adjusts matching radii based on driver supply rates, optimizing pick-up times and waiting times for drivers while maximizing request completion rates. Numerical experiments and simulation results show that our proposed algorithm reduces customer abandonment, minimizes waiting times for both customers and drivers, and improves overall platform efficiency."
2504.03456,"We use vector bundles to study the locus of totally mixed Nash equilibria of an $n$-player game in normal form, which we call Nash equilibrium scheme. When the payoff tensor format is balanced, we study the Nash discriminant variety, i.e., the algebraic variety of games whose Nash equilibrium scheme is nonreduced or has a positive dimensional component. We prove that this variety has codimension one. We classify all components of the Nash equilibrium scheme of binary three-player games. We prove that if the payoff tensor is of boundary format, then the Nash discriminant variety has two components: an irreducible hypersurface and a larger-codimensional component. A generic game with an unbalanced payoff tensor format does not admit totally mixed Nash equilibria. We define the Nash resultant variety of games admitting a positive number of totally mixed Nash equilibria. We prove that it is irreducible and determine its codimension and degree."
2504.03618,"We study the problem of position allocation in job marketplaces, where the platform determines the ranking of the jobs for each seeker. The design of ranking mechanisms is critical to marketplace efficiency, as it influences both short-term revenue from promoted job placements and long-term health through sustained seeker engagement. Our analysis focuses on the tradeoff between revenue and relevance, as well as the innovations in job auction design. We demonstrated two ways to improve relevance with minimal impact on revenue: incorporating the seekers preferences and applying position-aware auctions."
2504.03951,"Envy-freeness up to any good (EFX) is a popular and important fairness property in the fair allocation of indivisible goods, of which its existence in general is still an open question. In this work, we investigate the problem of determining the minimum number of EFX allocations for a given instance, arguing that this approach may yield valuable insights into the existence and computation of EFX allocations. We focus on restricted instances where the number of goods slightly exceeds the number of agents, and extend our analysis to weighted EFX (WEFX) and a novel variant of EFX for general monotone valuations, termed EFX+. In doing so, we identify the transition threshold for the existence of allocations satisfying these fairness notions. Notably, we resolve open problems regarding WEFX by proving polynomial-time computability under binary additive valuations, and establishing the first constant-factor approximation for two agents."
2504.04349,"We examine fixed-price mechanisms in bilateral trade through the lens of regret minimization. Our main results are twofold. (i) For independent values, a near-optimal $\widetilde{\Theta}(T^{2/3})$ tight bound for $\textsf{Global Budget Balance}$ fixed-price mechanisms with two-bit/one-bit feedback. (ii) For correlated/adversarial values, a near-optimal $\Omega(T^{3/4})$ lower bound for $\textsf{Global Budget Balance}$ fixed-price mechanisms with two-bit/one-bit feedback, which improves the best known $\Omega(T^{5/7})$ lower bound obtained in the work [BCCF24] and, up to polylogarithmic factors, matches the $\widetilde{\mathcal{O}}(T^{3 / 4})$ upper bound obtained in the same work. Our work in combination with the previous works [CCCFL24mor, CCCFL24jmlr, AFF24, BCCF24] (essentially) gives a thorough understanding of regret minimization for fixed-price bilateral trade.En route, we have developed two technical ingredients that might be of independent interest: (i) A novel algorithmic paradigm, called $\textit{fractal elimination}$, to address one-bit feedback and independent values. (ii) A new $\textit{lower-bound construction}$ with novel proof techniques, to address the $\textsf{Global Budget Balance}$ constraint and correlated values."
2504.05094,"Blockchain systems, such as Ethereum, are increasingly adopting layer-2 scaling solutions to improve transaction throughput and reduce fees. One popular layer-2 approach is the Optimistic Rollup, which relies on a mechanism known as a dispute game for block proposals. In these systems, validators can challenge blocks that they believe contain errors, and a successful challenge results in the transfer of a portion of the proposer's deposit as a reward. In this paper, we reveal a structural vulnerability in the mechanism: validators may not be awarded a proper profit despite winning a dispute challenge. We develop a formal game-theoretic model of the dispute game and analyze several scenarios, including cases where the proposer controls some validators and cases where a secondary auction mechanism is deployed to induce additional participation. Our analysis demonstrates that under current designs, the competitive pressure from validators may be insufficient to deter malicious behavior. We find that increased validator competition, paradoxically driven by higher rewards or participation, can allow a malicious proposer to significantly lower their net loss by capturing value through mechanisms like auctions. To address this, we propose countermeasures such as an escrowed reward mechanism and a commit-reveal protocol. Our findings provide critical insights into enhancing the economic security of layer-2 scaling solutions in blockchain networks."
2504.05563,"As large language models increasingly rely on external data sources, compensating data contributors has become a central concern. But how should these payments be devised? We revisit data valuations from a $\textit{market-design perspective}$ where payments serve to compensate data owners for the $\textit{private}$ heterogeneous costs they incur for collecting and sharing data. We show that popular valuation methods-such as Leave-One-Out and Data Shapley-make for poor payments. They fail to ensure truthful reporting of the costs, leading to $\textit{inefficient market}$ outcomes. To address this, we adapt well-established payment rules from mechanism design, namely Myerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We show that Myerson payment is the minimal truthful mechanism, optimal from the buyer's perspective. Additionally, we identify a condition under which both data buyers and sellers are utility-satisfied, and the market achieves efficiency. Our findings highlight the importance of incorporating incentive compatibility into data valuation design, paving the way for more robust and efficient data markets. Our data market framework is readily applicable to real-world scenarios. We illustrate this with simulations of contributor compensation in an LLM based retrieval-augmented generation (RAG) marketplace tasked with challenging medical question answering."
2504.05891,"Individuals often aim to reverse undesired outcomes in interactions with automated systems, like loan denials, by either implementing system-recommended actions (recourse), or manipulating their features. While providing recourse benefits users and enhances system utility, it also provides information about the decision process that can be used for more effective strategic manipulation, especially when the individuals collectively share such information with each other.We show that this tension leads rational utility-maximizing systems to frequently withhold recourse, resulting in decreased population utility, particularly impacting sensitive groups.To mitigate these effects, we explore the role of recourse subsidies, finding them effective in increasing the provision of recourse actions by rational systems, as well as lowering the potential social cost and mitigating unfairness caused by recourse withholding."
2504.07435,"This paper introduces a game-theoretic model tailored for reward distribution on crowd-sourced computing platforms. It explores a repeated game framework where miners, as computation providers, decide their computation power contribution in each round, guided by the platform's designed reward distribution mechanism. The reward for each miner in every round is based on the platform's randomized task payments and the miners' computation transcripts. Specifically, it defines Opportunity-Cost-Driven Incentive Compatibility (OCD-IC) and Dynamic OCD-IC (DOCD-IC) for scenarios where strategic miners might allocate some computation power to more profitable activities, such as Bitcoin mining. The platform must also achieve Budget Balance (BB), aiming for a non-negative total income over the long term. This paper demonstrates that traditional Pay-Per-Share (PPS) reward schemes require assumptions about task demand and miners' opportunity costs to ensure OCD-IC and BB, yet they fail to satisfy DOCD-IC. The paper then introduces Pay-Per-Share with Subsidy (PPSS), a new reward mechanism that allows the platform to provide subsidies to miners, thus eliminating the need for assumptions on opportunity cost to achieve OCD-IC, DOCD-IC, and long-term BB."
2504.09006,"We study structured Stackelberg games, in which both players (the leader and the follower) observe contextual information about the state of the world at time of play. The leader plays against one of a finite number of followers, but the follower's type is not known until after the game has ended. Importantly, we assume a fixed relationship between the contextual information and the follower's type, thereby allowing the leader to leverage this additional structure when deciding her strategy. Under this setting, we find that standard learning theoretic measures of complexity do not characterize the difficulty of the leader's learning task. Instead, we introduce a new notion of dimension, the Stackelberg-Littlestone dimension, which we show characterizes the instance-optimal regret of the leader in the online setting. Based on this, we also provide a provably optimal learning algorithm. We extend our results to the distributional setting, where we use two new notions of dimension, the $\gamma$-Stackelberg-Natarajan dimension and $\gamma$-Stackelberg-Graph dimension. We prove that these control the sample complexity lower and upper bounds respectively, and we design a simple, improper algorithm that achieves the upper bound."
2504.09669,"We study the problem of allocating items to agents with submodular valuations with the goal of maximizing the weighted Nash social welfare (NSW). The best-known results for unweighted and weighted objectives are the $(4+\epsilon)$ approximation given by Garg, Husic, Li, Végh, and Vondrák~[STOC 2023] and the $(233+\epsilon)$ approximation given by Feng, Hu, Li, and Zhang~[STOC 2025], respectively.In this work, we present a $(3.56+\epsilon)$-approximation algorithm for weighted NSW maximization with submodular valuations, simultaneously improving the previous approximation ratios of both the weighted and unweighted NSW problems. Our algorithm solves the configuration LP of Feng, Hu, Li, and Zhang~[STOC 2025] via a stronger separation oracle that loses an $e/(e-1)$ factor only on small items, and then rounds the solution via a new bipartite multigraph construction. Some key technical ingredients of our analysis include a greedy proxy function, additive within each configuration, that preserves the LP value while lower-bounding the rounded solution, together with refined concentration bounds and a series of mathematical programs analyzed partly by computer assistance.On the hardness side, we prove that the configuration LP for weighted NSW with submodular valuations has an integrality gap of at least $(2^{\ln 2}-\epsilon) \approx 1.617 - \epsilon$, which is larger than the current best-known $e/(e-1)-\epsilon \approx 1.582-\epsilon$ hardness~[SODA 2020]. For additive valuations, we show an integrality gap of $(e^{1/e}-\epsilon)$, which proves the tightness of the approximation ratio in~[ICALP 2024] for algorithms based on the configuration LP. For unweighted NSW with additive valuations, we show an integrality gap of $(2^{1/4}-\epsilon) \approx 1.189-\epsilon$, again larger than the current best-known $\sqrt{8/7} \approx 1.069$-hardness~[Math. Oper. Res. 2024]."
2504.09716,"Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the ""All In or Fold"" No-Limit Texas Hold'em poker variant."
2504.10232,"We propose a new fairness notion, motivated by the practical challenge of allocating teaching assistants (TAs) to courses in a department. Each course requires a certain number of TAs and each TA has preferences over the courses they want to assist. Similarly, each course instructor has preferences over the TAs who applied for their course. We demand fairness and efficiency for both sides separately, giving rise to the following criteria: (i) every course gets the required number of TAs and the average utility of the assigned TAs meets a threshold; (ii) the allocation of courses to TAs is envy-free, where a TA envies another TA if the former prefers the latter's course and has a higher or equal grade in that course. Note that the definition of envy-freeness here differs from the one in the literature, and we call it merit-based envy-freeness.We show that the problem of finding a merit-based envy-free and efficient matching is NP-hard even for very restricted settings, such as two courses and uniform valuations; constant degree, constant capacity of TAs for every course, valuations in the range {0,1,2,3}, identical valuations from TAs, and even more. To find tractable results, we consider some restricted instances, such as, strict valuation of TAs for courses, the difference between the number of positively valued TAs for a course and the capacity, the number of positively valued TAs/courses, types of valuation functions, and obtained some polynomial-time solvable cases, showing the contrast with intractable results. We further studied the problem in the paradigm of parameterized algorithms and designed some exact and approximation algorithms."
2504.10459,"In many decision-making scenarios, individuals strategically choose what information to disclose to optimize their own outcomes. It is unclear whether such strategic information disclosure can lead to good societal outcomes. To address this question, we consider a competitive Bayesian persuasion model in which multiple agents selectively disclose information about their qualities to a principal, who aims to choose the candidates with the highest qualities. Using the price-of-anarchy framework, we quantify the inefficiency of such strategic disclosure. We show that the price of anarchy is at most a constant when the agents have independent quality distributions, even if their utility functions are heterogeneous. This result provides the first theoretical guarantee on the limits of inefficiency in Bayesian persuasion with competitive information disclosure."
2504.10728,"The shortest-time route recommendations offered by modern navigation systems fuel selfish routing in urban vehicular traffic networks and are therefore one of the main reasons for the growth of congestion. In contrast, intelligent transportation systems (ITS) prefer to steer driver-vehicle systems (DVS) toward system-optimal route recommendations, which are primarily designed to mitigate network congestion. However, due to the misalignment in motives, drivers exhibit a lack of trust in the ITS. This paper models the interaction between a DVS and an ITS as a novel, multi-stage routing game where the DVS exhibits dynamics in its trust towards the recommendations of ITS based on counterfactual and observed game outcomes. Specifically, DVS and ITS are respectively modeled as a travel-time minimizer and network congestion minimizer, each having nonidentical prior beliefs about the network state. A novel approximate algorithm to compute the Bayesian Nash equilibrium, called ROSTER(Recommendation Outcome Sampling with Trust Estimation and Re-evaluation), is proposed based on Monte Carlo sampling with trust belief updating to determine the best response route recommendations of the ITS at each stage of the game. Simulation results demonstrate that the trust prediction error in the proposed algorithm converges to zero with a growing number of multi-stage DVS-ITS interactions and is effectively able to both mitigate congestion and reduce driver travel times when compared to alternative route recommendation strategies."
2504.1091,"Nearly all living systems, especially humans, depend on collective cooperation for survival and prosperity. However, the mechanisms driving the evolution of cooperative behavior remain poorly understood, particularly in the context of simultaneous interactions involving multiple individuals, repeated encounters, and complex interaction structures. Here, we introduce a novel framework for studying repeated multi-player interactions in structured populations -- repeated multi-player games on hypergraphs -- where multiple individuals within each hyperedge engage in a repeated game, and each player can simultaneously participate in many games. We focus on public goods games, where individuals differ in their initial endowments, their allocation of endowments across games, and their productivity, which determines the impact of their contributions. Through Nash equilibrium analysis, we reveal the intricate interplay between full cooperation (all individuals contribute their entire endowments, maximizing collective benefits) and key factors such as initial endowments, productivity, contribution strategies, and interaction structure. Notably, while equal endowments are most effective in promoting full cooperation in homogeneous hypergraphs, they can hinder cooperation in heterogeneous hypergraphs, suggesting that equal endowments are not universally optimal. To address this, we propose two optimization strategies: one for policymakers to adjust endowment distributions and another for players to modify their contribution strategies. Both approaches successfully promote full cooperation across all studied hypergraphs. Our findings provide novel insights into the emergence of full cooperation, offering valuable guidance for both players and policymakers in fostering collective cooperation."
2504.11149,"Natural and political disasters, including earthquakes, hurricanes, and tsunamis, but also migration and refugees crisis, need quick and coordinated responses in order to support vulnerable populations. In such disasters, nongovernmental organizations compete with each other for financial donations, while people who need assistance suffer a lack of coordination, congestion in terms of logistics, and duplication of services. From a theoretical point of view, this problem can be formalized as a Generalized Nash Equilibrium (GNE) problem. This is a generalization of the Nash equilibrium problem, where the agents' strategies are not fixed but depend on the other agents' strategies. In this paper, we show that Membrane Computing can model humanitarian relief as a GNE problem. We propose a family of P systems that compute GNE in this context, and we illustrate their capabilities with Hurricane Katrina in 2005 as a case study."
2504.11854,"With the rise of smart contracts, decentralized autonomous organizations (DAOs) have emerged in public good auctions, allowing ""small"" bidders to gather together and enlarge their influence in high-valued auctions. However, models and mechanisms in the existing research literature do not guarantee non-excludability, which is a main property of public goods. As such, some members of the winning DAO may be explicitly prevented from accessing the public good. This side effect leads to regrouping of small bidders within the DAO to have a larger say in the final outcome. In particular, we provide a polynomial-time algorithm to compute the best regrouping of bidders that maximizes the total bidding power of a DAO. We also prove that such a regrouping is less-excludable, better aligning the needs of the entire DAO and the nature of public goods. Next, notice that members of a DAO in public good auctions often have a positive externality among themselves. Thus we introduce a collective factor into the members' utility functions. We further extend the mechanism's allocation for each member to allow for partial access to the public good. Under the new model, we propose a mechanism that is incentive compatible in generic games and achieves higher social welfare as well as less-excludable allocations."
2504.12859,"This study explores the application of Quadratic Voting (QV) and its generalization to improve decentralization and effectiveness in blockchain governance systems. The conducted research identified three main types of quadratic (square root) voting. Two of them pertain to voting with a split stake, and one involves voting without splitting. In split stakes, Type 1 QV applies the square root to the total stake before distributing it among preferences, while Type 2 QV distributes the stake first and then applies the square root. In unsplit stakes (Type 3 QV), the square root of the total stake is allocated entirely to each preference. The presented formal proofs confirm that Types 2 and 3 QV, along with generalized models, enhance decentralization as measured by the Gini and Nakamoto coefficients. A pivotal discovery is the existence of a threshold stakeholder whose relative voting ratio increases under QV compared to linear voting, while smaller stakeholders also gain influence. The generalized QV model allows flexible adjustment of this threshold, enabling tailored decentralization levels. Maintaining fairness, QV ensures that stakeholders with higher stakes retain a proportionally greater voting ratio while redistributing influence to prevent excessive concentration. It is shown that to preserve fairness and robustness, QV must be implemented alongside privacy-preserving cryptographic voting protocols, as voters casting their ballots last could otherwise manipulate outcomes. The generalized QV model, proposed in this paper, enables algorithmic parametrization to achieve desired levels of decentralization for specific use cases. This flexibility makes it applicable across diverse domains, including user interaction with cryptocurrency platforms, facilitating community events and educational initiatives, and supporting charitable activities through decentralized decision-making."
2504.1343,"We study the online allocation of divisible items to $n$ agents with additive valuations for $p$-mean welfare maximization, a problem introduced by Barman, Khan, and Maiti~(2022). Our algorithmic and hardness results characterize the optimal competitive ratios for the entire spectrum of $-\infty \le p \le 1$. Surprisingly, our improved algorithms for all $p \le \frac{1}{\log n}$ are simply the greedy algorithm for the Nash welfare, supplemented with two auxiliary components to ensure all agents have non-zero utilities and to help a small number of agents with low utilities. In this sense, the long arm of Nashian allocation achieves near-optimal competitive ratios not only for Nash welfare but also all the way to egalitarian welfare."
2504.14163,"We study a system composed of multiple distinct service locations that aims to convince customers to join the system by providing information to customers. We cast the system's information design problem in the framework of Bayesian persuasion and describe centralized and decentralized signaling. We provide efficient methods for computing the system's optimal centralized and decentralized signaling mechanisms and derive a performance guarantee for decentralized signaling when the locations' states are independent. The guarantee states that the probability that a customer joins under optimal decentralized signaling is bounded below by the product of a strictly positive constant and the probability that a customer joins under optimal centralized signaling. The constant depends only on the number of service locations. We provide an example that shows that the constant cannot be improved. We consider an extension to more-general objectives for the system and establish that the same guarantee continues to hold. We also extend our analysis to systems where the locations' states are correlated, and again derive a performance guarantee for decentralized signaling in that setting. For the correlated setting, we prove that the guarantee's asymptotic dependence upon the number of locations cannot be substantially improved. A comparison of our guarantees for independent locations and for correlated locations reveals the influence of dependence on the performance of decentralized signaling."
2504.14195,"We introduce River, a novel Condorcet-consistent voting method that is based on pairwise majority margins and can be seen as a simplified variation of Tideman's Ranked Pairs method. River is simple to explain, simple to compute even 'by hand', and gives rise to an easy-to-interpret certificate in the form of a directed tree. Like Ranked Pairs and Schulze's Beat Path method, River is a refinement of the Split Cycle method and shares with those many desirable properties, including independence of clones. Unlike the other three methods, River satisfies a strong form of resistance to agenda-manipulation that is known as independence of Pareto-dominated alternatives."
2504.14793,"Platforms design the form of presentation by which sellers are shown to the buyers. This design not only shapes the buyers' experience but also leads to different market equilibria or dynamics. One component in this design is through the platform's mediation of the search frictions experienced by the buyers for different sellers. We take a model of monopolistic competition and show that, on one hand, when all sellers have the same inspection costs, the market sees no stable price since the sellers always have incentives to undercut each other, and, on the other hand, the platform may stabilize the price by giving prominence to one seller chosen by a carefully designed mechanism. This calls to mind Amazon's Buy Box. We study natural mechanisms for choosing the prominent seller, characterize the range of equilibrium prices implementable by them, and find that in certain scenarios the buyers' surplus improves as the search friction increases."
2504.14823,"The rapid advancement of AI and other emerging technologies has triggered exponential growth in computing resources demand. Faced with prohibitive infrastructure costs for large-scale computing clusters, users are increasingly resorting to leased computing resources from third-party providers. However, prevalent overestimation of operational requirements frequently leads to substantial underutilization of the computing resources. To mitigate such inefficiency, we propose a contract-based incentive framework for computing resources repurchasing. Comparing to auction mechanisms, our design enables providers to reclaim and reallocate surplus computing resources through market-driven incentives. Our framework operates in a multi-parameter environment where both clients' idle resource capacities and their unit valuations of retained resources are private information, posing a significant challenge to contract design. Two scenarios are considered based on whether all clients possess the same amount of idle resource capacity. By transforming the contract design problem into solving a mathematical program, we obtain the optimal contracts for each scenario, which can maximize the utility of computing resources providers while ensuring the requirements of incentive compatibility (IC) and individual rationality (IR). This innovative design not only provides an effective approach to reduce the inefficient utilization of computing resources, but also establishes a market-oriented paradigm for sustainable computing ecosystems."
2504.14824,"The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the ``value of information"" as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6G networks."
2504.14948,"This paper studies mechanism design for auctions with externalities on budgets, a novel setting where the budgets that bidders commit are adjusted due to the externality of the competitors' allocation outcomes-a departure from traditional auctions with fixed budgets. This setting is motivated by real-world scenarios, for example, participants may increase their budgets in response to competitors' obtained items. We initially propose a general framework with homogeneous externalities to capture the interdependence between budget updates and allocation, formalized through a budget response function that links each bidder's effective budget to the amount of items won by others.The main contribution of this paper is to propose a truthful and individual rational auction mechanism for this novel auction setting, which achieves an approximation ratio of $1/3$ with respect to the liquid welfare. This mechanism is inspired by the uniform-price auction, in which an appropriate uniform price is selected to allocate items, ensuring the monotonicity of the allocation rule while accounting for budget adjustments. Additionally, this mechanism guarantees a constant approximation ratio by setting a purchase limit. Complementing this result, we establish an upper bound: no truthful mechanism can achieve an approximation ratio better than $1/2$. This work offers a new perspective to study the impact of externalities on auctions, providing an approach to handle budget externalities in multi-agent systems."
2504.15157,"An important desideratum in approval-based multiwinner voting is proportionality. We study the problem of reconfiguring proportional committees: given two proportional committees, is there a transition path that consists only of proportional committees, where each transition involves replacing one candidate with another candidate? We show that the set of committees satisfying the proportionality axiom of justified representation (JR) is not always connected, and it is PSPACE-complete to decide whether two such committees are connected. On the other hand, we prove that any two JR committees can be connected by committees satisfying a $2$-approximation of JR. We also obtain similar results for the stronger axiom of extended justified representation (EJR). In addition, we demonstrate that the committees produced by several well-known voting rules are connected or at least not isolated, and investigate the reconfiguration problem in restricted preference domains."
2504.15438,"Blockchains have block-size limits to ensure the entire cluster can keep up with the tip of the chain. These block-size limits are usually single-dimensional, but richer multidimensional constraints allow for greater throughput. The potential for performance improvements from multidimensional resource pricing has been discussed in the literature, but exactly how big those performance improvements are remains unclear. In order to identify the magnitude of additional throughput that multi-dimensional transaction fees can unlock, we introduce the concept of an $\alpha$-approximation. A constraint set $C_1$ is $\alpha$-approximated by $C_2$ if every block feasible under $C_1$ is also feasible under $C_2$ once all resource capacities are scaled by a factor of $\alpha$ (e.g., $\alpha =2$ corresponds to doubling all available resources). We show that the $\alpha$-approximation of the optimal single-dimensional gas measure corresponds to the value of a specific zero-sum game. However, the more general problem of finding the optimal $k$-dimensional approximation is NP-complete. Quantifying the additional throughput that multi-dimensional fees can provide allows blockchain designers to make informed decisions about whether the additional capacity unlocked by multidimensional constraints is worth the additional complexity they add to the protocol."
2504.15568,"In many settings of interest, a policy is set by one party, the leader, in order to influence the action of another party, the follower, where the follower's response is determined by some private information. A natural question to ask is, can the leader improve their strategy by learning about the unknown follower through repeated interactions? A well known folk theorem from dynamic pricing, a special case of this leader-follower setting, would suggest that the leader cannot learn effectively from the follower when the follower is fully strategic, leading to a large literature on learning in strategic settings that relies on limiting the strategic space of the follower in order to provide positive results. In this paper, we study dynamic Bayesian Stackelberg games, where a leader and a \emph{fully strategic} follower interact repeatedly, with the follower's type unknown. Contrary to existing results, we show that the leader can improve their utility through learning in repeated play. Using a novel average-case analysis, we demonstrate that learning is effective in these settings, without needing to weaken the follower's strategic space. Importantly, this improvement is not solely due to the leader's ability to commit, nor does learning simply substitute for communication between the parties. We provide an algorithm, based on a mixed-integer linear program, to compute the optimal leader policy in these games and develop heuristic algorithms to approximate the optimal dynamic policy more efficiently. Through simulations, we compare the efficiency and runtime of these algorithms against static policies."
2504.1648,"We study the problem of market equilibrium (ME) in future wireless networks, with multiple actors competing and negotiating for a pool of heterogeneous resources (communication and computing) while meeting constraints in terms of global cost. The latter is defined in a general way but is associated with energy and/or carbon emissions. In this direction, service providers competing for network resources do not acquire the latter, but rather the right to consume, given externally defined policies and regulations. We propose to apply the Fisher market model, and prove its convergence towards an equilibrium between utilities, regulatory constraints, and individual budgets. The model is then applied to an exemplary use case of access network, edge computing, and cloud resources, and numerical results assess the theoretical findings of convergence, under different assumptions on the utility function and more or less stringent constraints."
2504.16528,"This paper presents (permissive) \emph{Quantitative Strategy Templates} (QaSTels) to succinctly represent infinitely many winning strategies in two-player energy and mean-payoff games. This transfers the recently introduced concept of \emph{Permissive (qualitative) Strategy Templates} (PeSTels) for $\omega$-regular games to games with quantitative objectives. We provide the theoretical and algorithmic foundations of (i) QaSTel synthesis, and (ii) their (incremental) combination with PeSTels for games with mixed quantitative and qualitative objectives. Using a prototype implementation of our synthesis algorithms, we demonstrate empirically that QaSTels extend the advantageous properties of strategy templates over single winning strategies -- known from PeSTels -- to games with (additional) quantitative objectives. This includes (i) the enhanced robustness of strategies due to their runtime-adaptability, and (ii) the compositionality of templates w.r.t. incrementally arriving objectives. We use control-inspired examples to illustrate these superior properties of QaSTels for CPS design."
2504.16556,"Noncooperative games with uncertain payoffs have been classically studied under the expected-utility theory framework, which relies on the strong assumption that agents behave rationally. However, simple experiments on human decision makers found them to be not fully rational, due to their subjective risk perception. Prospect theory was proposed as an empirically-grounded model to incorporate irrational behaviours into game-theoretic models. But, how prospect theory shapes the set of Nash equilibria when considering irrational agents, is still poorly understood. To this end, we study how prospect theoretic transformations may generate new equilibria while eliminating existing ones. Focusing on aggregative games, we show that capturing users' irrationality can preserve symmetric equilibria while causing the vanishing of asymmetric equilibria. Further, there exist value functions which map uncountable sets of equilibria in the expected-utility maximization framework to finite sets. This last result may shape some equilibrium selection theories for human-in-the-loop systems where computing a single equilibrium is insufficient and comparison of equilibria is needed."
2504.16592,"The rise of algorithmic pricing in online retail platforms has attracted significant interest in how autonomous software agents interact under competition. This article explores the potential emergence of algorithmic collusion - supra-competitive pricing outcomes that arise without explicit agreements - as a consequence of repeated interactions between learning agents. Most of the literature focuses on oligopoly pricing environments modeled as repeated Bertrand competitions, where firms use online learning algorithms to adapt prices over time. While experimental research has demonstrated that specific reinforcement learning algorithms can learn to maintain prices above competitive equilibrium levels in simulated environments, theoretical understanding of when and why such outcomes occur remains limited. This work highlights the interdisciplinary nature of this challenge, which connects computer science concepts of online learning with game-theoretical literature on equilibrium learning. We examine implications for the Business & Information Systems Engineering (BISE) community and identify specific research opportunities to address challenges of algorithmic competition in digital marketplaces."
2504.16752,"This work addresses competitive resource allocation in a sequential setting, where two players allocate resources across objects or locations of shared interest. Departing from the simultaneous Colonel Blotto game, our framework introduces a sequential decision-making dynamic, where players act with partial or complete knowledge of previous moves. Unlike traditional approaches that rely on complex mixed strategies, we focus on deterministic pure strategies, streamlining computation while preserving strategic depth. Additionally, we extend the payoff structure to accommodate fractional allocations and payoffs, moving beyond the binary, all-or-nothing paradigm to allow more granular outcomes. We model this problem as an adversarial knapsack game, formulating it as a bilevel optimization problem that integrates the leader's objective with the follower's best-response. This knapsack-based approach is novel in the context of competitive resource allocation, with prior work only partially leveraging it for follower analysis. Our contributions include: (1) proposing an adversarial knapsack formulation for the sequential resource allocation problem, (2) developing efficient heuristics for fractional allocation scenarios, and (3) analyzing the 0-1 knapsack case, providing a computational hardness result alongside a heuristic solution."
2504.16852,"Rebuild and Divide is an urban renewal process that involves the demolition of old buildings and the construction of new ones. Original homeowners are compensated with upgraded apartments, while surplus units are sold for profit, so theoretically it is a win-win project for all parties involved. However, many rebuild-and-divide projects withheld or delayed due to disagreements over the assignment of new units, claiming they are not ""fair"". The goal of this research is to develop algorithms for envy-free allocation of the new units. The main challenge is that, in contrast to previous work on envy-free allocation, the envy depends also on the value of the old units, as people with more valuable old units are entitled to more valuable new units. We introduce three models that capture different notions of fairness: (1) the Difference Model, where agents evaluate their gains relative to others; (2) the Envy Sum Model, which permits some envy as long as the total envy does not exceed that of the original allocation; and (3) the Ratio Model, where fairness is assessed based on the proportional value of old apartments. For each model, we establish an envy criterion and seek a payment vector and allocation that ensure envy-freeness. These models present both theoretical challenges and intriguing insights. Additionally, within the Envy Sum Model, we present a mechanism that computes an allocation and payment scheme that minimizes total envy. We also analyze the mechanism's vulnerability to manipulation and identify conditions under which it is obviously manipulable."
2504.18075,"We study the long-term behavior of the fictitious play process in repeated extensive-form games of imperfect information with perfect recall. Each player maintains incorrect beliefs that the moves at all information sets, except the one at which the player is about to make a move, are made according to fixed random strategies, independently across all information sets. Accordingly, each player makes his moves at any of his information sets to maximize his expected payoff assuming that, at any other information set, the moves are made according to the empirical frequencies of the past moves. We extend the well-known Monderer-Shapley result [1] on the convergence of the empirical frequencies to the set of Nash equilibria to a certain class of extensive-form games with identical interests. We then strengthen this result by the use of inertia and fading memory, and prove the convergence of the realized play-paths to an essentially pure Nash equilibrium in all extensive-form games of imperfect information with identical interests."
2504.18214,"Blockchains rely on economic incentives to ensure secure and decentralised operation, making incentive compatibility a core design concern. However, protocols are rarely deployed in isolation. Applications interact with the underlying consensus and network layers, and multiple protocols may run concurrently on the same chain. These interactions give rise to complex incentive dynamics that traditional, isolated analyses often fail to capture.We propose the first compositional game-theoretic framework for blockchain protocols. Our model represents blockchain protocols as interacting games across layers -- application, network, and consensus. It enables formal reasoning about incentive compatibility under composition by introducing two key abstractions: the cross-layer game, which models how strategies in one layer influence others, and cross-application composition, which captures how application protocols interact concurrently through shared infrastructure.We illustrate our framework through case studies on HTLCs, Layer-2 protocols, and MEV, showing how compositional analysis reveals subtle incentive vulnerabilities and supports modular security proofs."
2504.18868,"Nash equilibrium is perhaps the best-known solution concept in game theory. Such a solution assigns a strategy to each player which offers no incentive to unilaterally deviate. While a Nash equilibrium is guaranteed to always exist, the problem of finding one in general-sum games is PPAD-complete, generally considered intractable. Regret minimization is an efficient framework for approximating Nash equilibria in two-player zero-sum games. However, in general-sum games, such algorithms are only guaranteed to converge to a coarse-correlated equilibrium (CCE), a solution concept where players can correlate their strategies. In this work, we use meta-learning to minimize the correlations in strategies produced by a regret minimizer. This encourages the regret minimizer to find strategies that are closer to a Nash equilibrium. The meta-learned regret minimizer is still guaranteed to converge to a CCE, but we give a bound on the distance to Nash equilibrium in terms of our meta-loss. We evaluate our approach in general-sum imperfect information games. Our algorithms provide significantly better approximations of Nash equilibria than state-of-the-art regret minimization techniques."
2504.18876,"A principal delegates a project to a team $S$ from a pool of $n$ agents. The project's value if all agents in $S$ exert costly effort is $f(S)$. To incentivize the agents to participate, the principal assigns each agent $i\in S$ a share $\rho_i\in [0,1]$ of the project's final value (i.e., designs $n$ linear contracts). The shares must be feasible -- their sum should not exceed $1$. It is well-understood how to design these contracts to maximize the principal's own expected utility, but what if the goal is to coordinate the agents toward maximizing social welfare?We initiate a systematic study of multi-agent contract design with objectives beyond principal's utility, including welfare maximization, for various classes of value functions $f$. Our exploration reveals an arguably surprising fact: If $f$ is up to XOS in the complement-free hierarchy of functions, then the optimal principal's utility is a constant-fraction of the optimal welfare. This is in stark contrast to the much larger welfare-utility gaps in auction design, and no longer holds above XOS in the hierarchy, where the gap can be unbounded.A constant bound on the welfare-utility gap immediately implies that existing algorithms for designing contracts with approximately-optimal principal's utility also guarantee approximately-optimal welfare. The downside of reducing welfare to utility is the loss of large constants. To obtain better guarantees, we develop polynomial-time algorithms directly for welfare, for different classes of value functions. These include a tight $2$-approximation to the optimal welfare for symmetric XOS functions.Finally, we extend our analysis beyond welfare to the project's value under general feasibility constraints. Our results immediately translate to budgeted welfare and utility."
2504.18917,"Regret minimization is a general approach to online optimization which plays a crucial role in many algorithms for approximating Nash equilibria in two-player zero-sum games. The literature mainly focuses on solving individual games in isolation. However, in practice, players often encounter a distribution of similar but distinct games. For example, when trading correlated assets on the stock market, or when refining the strategy in subgames of a much larger game. Recently, offline meta-learning was used to accelerate one-sided equilibrium finding on such distributions. We build upon this, extending the framework to the more challenging self-play setting, which is the basis for most state-of-the-art equilibrium approximation algorithms for domains at scale. When selecting the strategy, our method uniquely integrates information across all decision states, promoting global communication as opposed to the traditional local regret decomposition. Empirical evaluation on normal-form games and river poker subgames shows our meta-learned algorithms considerably outperform other state-of-the-art regret minimization algorithms."
2504.20161,"The fair division of indivisible goods is not only a subject of theoretical research, but also an important problem in practice, with solutions being offered on several online platforms. Little is known, however, about the characteristics of real-world allocation instances and how they compare to synthetic instances. Using dimensionality reduction, we compute a map of allocation instances: a 2-dimensional embedding such that an instance's location on the map is predictive of the instance's origin and other key instance features. Because the axes of this map closely align with the utility matrix's two largest singular values, we define a second, explicit map, which we theoretically characterize."
2504.20372,"Condorcet's paradox is a fundamental result in social choice theory which states that there exist elections in which, no matter which candidate wins, a majority of voters prefer a different candidate. In fact, even if we can select any $k$ winners, there still may exist another candidate that would beat each of the winners in a majority vote. That is, elections may require arbitrarily large dominating sets.We show that approximately dominating sets of constant size always exist. In particular, for every $\varepsilon > 0$, every election (irrespective of the number of voters or candidates) can select $O(\frac{1}{\varepsilon ^2})$ winners such that no other candidate beats each of the winners by a margin of more than $\varepsilon$ fraction of voters.Our proof uses a simple probabilistic construction using samples from a maximal lottery, a well-studied distribution over candidates derived from the Nash equilibrium of a two-player game. In stark contrast to general approximate equilibria, which may require support logarithmic in the number of pure strategies, we show that maximal lotteries can be approximated with constant support size. These approximate maximal lotteries may be of independent interest."
2504.20508,"Sortition is the practice of delegating public decision-making to randomly selected panels. Recently, it has gained momentum worldwide through its use in citizens' assemblies, sparking growing interest within the computer science community. One key appeal of sortition is that random panels tend to be more representative of the population than elected committees or parliaments. Our main conceptual contribution is a novel definition of representative panels, based on the Wasserstein distance from statistical learning theory. Using this definition, we develop a framework for analyzing the panel complexity problem -- determining the required panel size to ensure desirable properties. We focus on three key desiderata: (1) that efficiency at the panel level extends to the whole population, measured by social welfare; (2) that fairness guarantees for the panel translate to fairness for the population, captured by the core; and (3) that the probability of an outlier panel, for which the decision significantly deviates from the optimal one, remains low. We establish near-tight panel complexity guarantees for these desiderata across two fundamental social choice settings: participatory budgeting and facility location."
2504.20704,"When dividing items among agents, two of the most widely studied fairness notions are envy-freeness and proportionality. We consider a setting where $m$ chores are allocated to $n$ agents and the disutility of each chore for each agent is drawn from a probability distribution. We show that an envy-free allocation exists with high probability provided that $m \ge 2n$, and moreover, $m$ must be at least $n+\Theta(n)$ in order for the existence to hold. On the other hand, we prove that a proportional allocation is likely to exist as long as $m = \omega(1)$, and this threshold is asymptotically tight. Our results reveal a clear contrast with the allocation of goods, where a larger number of items is necessary to ensure existence for both notions."
2504.20774,"This paper examines the impact of agents' myopic optimization on the efficiency of systems comprised by many selfish agents. In contrast to standard congestion games where agents interact in a one-shot fashion, in our model each agent chooses an infinite sequence of actions and maximizes the total reward stream discounted over time under different ways of computing present values. Our model assumes that actions consume common resources that get congested, and the action choice by an agent affects the completion times of actions chosen by other agents, which in turn affects the time rewards are accrued and their discounted value. This is a mean-field game, where an agent's reward depends on the decisions of the other agents through the resulting action completion times. For this type of game we define stationary equilibria, and analyze their existence and price of anarchy (PoA). Overall, we find that the PoA depends entirely on the type of discounting rather than its specific parameters. For exponential discounting, myopic behaviour leads to extreme inefficiency: the PoA is infinity for any value of the discount parameter. For power law discounting, such inefficiency is greatly reduced and the PoA is 2 whenever stationary equilibria exist. This matches the PoA when there is no discounting and players maximize long-run average rewards. Additionally, we observe that exponential discounting may introduce unstable equilibria in learning algorithms, if action completion times are interdependent. In contrast, under no discounting all equilibria are stable."
2504.21728,"Several resource allocation settings involve agents with unequal entitlements represented by weights. We analyze weighted fair division from an asymptotic perspective: if $m$ items are divided among $n$ agents whose utilities are independently sampled from a probability distribution, when is it likely that a fair allocation exist? We show that if the ratio between the weights is bounded, a weighted envy-free allocation exists with high probability provided that $m = \Omega(n\log n/\log\log n)$, generalizing a prior unweighted result. For weighted proportionality, we establish a sharp threshold of $m = n/(1-\mu)$ for the transition from non-existence to existence, where $\mu\in (0,1)$ denotes the mean of the distribution. In addition, we prove that for two agents, a weighted envy-free (and weighted proportional) allocation is likely to exist if $m = \omega(\sqrt{r})$, where $r$ denotes the ratio between the two weights."
2504.21735,"Massage therapy training emphasizes hands-on techniques and effective therapist--patient communication. However, many educational programs struggle to provide realistic practice scenarios. To address this problem, we propose TheraQuest, a gamified, web-based simulation platform that employs large language models (LLMs) to generate diverse virtual patients with varying symptoms and cultural backgrounds. Through interactive dialogue, anatomical decision-making, and immediate assessment, trainees develop both diagnostic reasoning and empathetic communication skills in a low-risk environment. Unlike exclusively VR-based solutions, TheraQuest remains accessible via standard web browsers, mitigating the cost and discomfort associated with extended headset use. Preliminary testing suggests that integrating LLM-driven virtual patients with real-time skill metrics can enhance trainee engagement and help bridge the gap between theoretical knowledge and clinical proficiency."
2505.00405,"A competitive market is modeled as a game of incomplete information. One player observes some payoff-relevant state and can sell (possibly noisy) messages thereof to the other, whose willingness to pay is contingent on their own beliefs. We frame the decision of what information to sell, and at what price, as a product versioning problem. The optimal menu screens buyer types to maximize profit, which is the payment minus the externality induced by selling information to a competitor, that is, the cost of refining a competitor's beliefs. For a class of games with binary actions and states, we derive the following insights: (i) payments are necessary to provide incentives for information sharing amongst competing firms; (ii) the optimal menu benefits both the buyer and the seller; (iii) the seller cannot steer the buyer's actions at the expense of social welfare; (iv) as such, as competition grows fiercer it can be optimal to sell no information at all."
2505.0052,"Proportional representation plays a crucial role in electoral systems. In ordinal elections, where voters rank candidates based on their preferences, the Single Transferable Vote (STV) is the most widely used proportional voting method. STV is considered proportional because it satisfies an axiom requiring that large enough solid coalitions of voters are adequately represented. Using real-world data from local Scottish elections, we observe that solid coalitions of the required size rarely occur in practice. This observation challenges the importance of proportionality axioms and raises the question of how the proportionality of voting methods can be assessed beyond their axiomatic performance. We address these concerns by developing quantitative measures of proportionality. We apply these measures to evaluate the proportionality of voting rules on real-world election data. Besides STV, we consider SNTV, the Expanding Approvals Rule, and Sequential Ranked-Choice Voting. We also study the effects of ballot truncation by artificially completing truncated ballots and comparing the proportionality of outcomes under complete and truncated ballots."
2505.00783,"A safe Pareto improvement (SPI) [41] is a modification of a game that leaves all players better off with certainty. SPIs are typically proven under qualitative assumptions about the way different games are played. For example, we assume that strictly dominated strategies can be iteratively removed and that isomorphic games are played isomorphically. In this work, we study SPIs achieved through three types of ex post verifiable commitments -- promises about player behavior from which deviations can be detected by observing the game. First, we consider disarmament -- commitments not to play certain actions. Next, we consider SPIs based on token games. A token game is a game played by simply announcing an action (via cheap talk). As such, its outcome is intrinsically meaningless. However, we assume the players commit in advance to play specific (pure or correlated) strategy profiles in the original game as a function of the token game outcome. Under such commitments, the token game becomes a new, meaningful normal-form game. Finally, we consider default-conditional commitment: SPIs in settings where the players' default ways of playing the original game can be credibly revealed and hence the players can commit to act as a function of this default. We characterize the complexity of deciding whether SPIs exist in all three settings, giving a mixture of characterizations and efficient algorithms and NP- and Graph Isomorphism-hardness results."
2505.01395,"The proportional veto principle, which captures the idea that a candidate vetoed by a large group of voters should not be chosen, has been studied for ranked ballots in single-winner voting. We introduce a version of this principle for approval ballots, which we call flexible-voter representation (FVR). We show that while the approval voting rule and other natural scoring rules provide the optimal FVR guarantee only for some flexibility threshold, there exists a scoring rule that is FVR-optimal for all thresholds simultaneously. We also extend our results to multi-winner voting."
2505.01629,"We study the problem of fairly and efficiently allocating a set of items among strategic agents with additive valuations, where items are either all indivisible or all divisible. When items are goods, numerous positive and negative results are known regarding the fairness and efficiency guarantees achievable by truthful mechanisms, whereas our understanding of truthful mechanisms for chores remains considerably more limited. In this paper, we discover various connections between truthful good and chore allocations, greatly enhancing our understanding of the latter via tools from the former.For indivisible chores with two agents, by leveraging the observation that a simple bundle-swapping operation transforms several properties for goods including truthfulness to the corresponding properties for chores, we characterize truthful mechanisms and derive tight guarantees of various fairness notions achieved by truthful mechanisms. Moreover, for homogeneous divisible chores, by generalizing the above transformation to an arbitrary number of agents, we characterize truthful mechanisms with two agents, show that every truthful mechanism with two agents admits an efficiency ratio of $0$, and derive a large family of strictly truthful, envy-free (EF), and proportional mechanisms for an arbitrary number of agents. Finally, for indivisible chores with an arbitrary number of agents having bi-valued cost functions, we give an ex-ante truthful, ex-ante Pareto optimal, ex-ante EF, and ex-post envy-free up to one item mechanism, improving the best guarantees for bi-valued instances by prior works."
2505.02623,"We study the memory resources required for near-optimal play in two-player zero-sum stochastic games with the long-run average payoff. Although optimal strategies may not exist in such games, near-optimal strategies always do.Mertens and Neyman (1981) proved that in any stochastic game, for any $\varepsilon>0$, there exist uniform $\varepsilon$-optimal memory-based strategies -- i.e., strategies that are $\varepsilon$-optimal in all sufficiently long $n$-stage games -- that use at most $O(n)$ memory states within the first $n$ stages. We improve this bound on the number of memory states by proving that in any stochastic game, for any $\varepsilon>0$, there exist uniform $\varepsilon$-optimal memory-based strategies that use at most $O(\log n)$ memory states in the first $n$ stages. Moreover, we establish the existence of uniform $\varepsilon$-optimal memory-based strategies whose memory updating and action selection are time-independent and such that, with probability close to 1, for all $n$, the number of memory states used up to stage $n$ is at most $O(\log n)$.This result cannot be extended to strategies with bounded public memory -- even if time-dependent memory updating and action selection are allowed. This impossibility is illustrated in the Big Match -- a well-known stochastic game where the stage payoffs to Player 1 are 0 or 1. Although for any $\varepsilon > 0$, there exist strategies of Player 1 that guarantee a payoff {exceeding} $1/2 - \varepsilon$ in all sufficiently long $n$-stage games, we show that any strategy of Player 1 that uses a finite public memory fails to guarantee a payoff greater than $\varepsilon$ in any sufficiently long $n$-stage game."
2505.02796,"We study how a budget-constrained bidder should learn to adaptively bid in repeated first-price auctions to maximize her cumulative payoff. This problem arose due to an industry-wide shift from second-price auctions to first-price auctions in display advertising recently, which renders truthful bidding (i.e., always bidding one's private value) no longer optimal. We propose a simple dual-gradient-descent-based bidding policy that maintains a dual variable for budget constraint as the bidder consumes her budget. In analysis, we consider two settings regarding the bidder's knowledge of her private values in the future: (i) an uninformative setting where all the distributional knowledge (can be non-stationary) is entirely unknown to the bidder, and (ii) an informative setting where a prediction of the budget allocation in advance. We characterize the performance loss (or regret) relative to an optimal policy with complete information on the stochasticity. For uninformative setting, We show that the regret is \tilde{O}(\sqrt{T}) plus a variation term that reflects the non-stationarity of the value distributions, and this is of optimal order. We then show that we can get rid of the variation term with the help of the prediction; specifically, the regret is \tilde{O}(\sqrt{T}) plus the prediction error term in the informative setting."
2505.03078,"In this paper, we investigate the dynamics of coordinating and anti-coordinating agents in a coevolutionary model for actions and opinions. In the model, the individuals of a population interact on a two-layer network, sharing their opinions and observing others' action, while revising their own opinions and actions according to a game-theoretic mechanism, grounded in the social psychology literature. First, we consider the scenario of coordinating agents, where convergence to a Nash equilibrium (NE) is guaranteed. We identify conditions for reaching consensus configurations and establish regions of attraction for these equilibria. Second, we study networks of anti-coordinating agents. In this second scenario, we prove that all trajectories converge to a NE by leveraging potential game theory. Then, we establish analytical conditions on the network structure and model parameters to guarantee the existence of consensus and polarized equilibria, characterizing their regions of attraction."
2505.03391,"We study a truthful facility location problem where one out of $k\geq2$ available facilities must be built at a location chosen from a set of candidate ones in the interval $[0,1]$. This decision aims to accommodate a set of agents with private positions in $[0,1]$ and approval preferences over the facilities; the agents act strategically and may misreport their private information to maximize their utility, which depends on the chosen facility and their distance from it. We focus on strategyproof mechanisms that incentivize the agents to act truthfully and bound the best possible approximation of the optimal social welfare (the total utility of the agents) they can achieve. We first show that deterministic mechanisms have unbounded approximation ratio, and then present a randomized mechanism with approximation ratio $k$, which is tight even when agents may only misreport their positions. For the restricted setting where agents may only misreport their approval preferences, we design a deterministic mechanism with approximation ratio of roughly $2.325$, and establish lower bounds of $3/2$ and $6/5$ for deterministic and randomized mechanisms, respectively."
2505.03428,"Launching a new blockchain system or application is frequently facilitated by a so called airdrop, where the system designer chooses a pre-existing set of potentially interested parties and allocates newly minted tokens to them with the expectation that they will participate in the system - such engagement, especially if it is of significant level, facilitates the system and raises its value and also the value of its newly minted token, hence benefiting the airdrop recipients. A number of challenging questions befuddle designers in this setting, such as how to choose the set of interested parties and how to allocate tokens to them. To address these considerations we put forward a game-theoretic model for such airdrop games. Our model can be used to guide the designer's choices based on the way the system's value depends on participation (modeled by a ''technology function'' in our framework) and the costs that participants incur. We identify both bad and good equilibria and identify the settings and the choices that can be made where the designer can influence the players towards good equilibria in an expedient manner."
2505.04302,"This study investigates cooperation evolution mechanisms in the spatial public goods game. A novel deep reinforcement learning framework, Proximal Policy Optimization with Adversarial Curriculum Transfer (PPO-ACT), is proposed to model agent strategy optimization in dynamic environments. Traditional evolutionary game models frequently exhibit limitations in modeling long-term decision-making processes. Deep reinforcement learning effectively addresses this limitation by bridging policy gradient methods with evolutionary game theory. Our study pioneers the application of proximal policy optimization's continuous strategy optimization capability to public goods games through a two-stage adversarial curriculum transfer training paradigm. The experimental results show that PPO-ACT performs better in critical enhancement factor regimes. Compared to conventional standard proximal policy optimization methods, Q-learning and Fermi update rules, achieve earlier cooperation phase transitions and maintain stable cooperative equilibria. This framework exhibits better robustness when handling challenging scenarios like all-defector initial conditions. Systematic comparisons reveal the unique advantage of policy gradient methods in population-scale cooperation, i.e., achieving spatiotemporal payoff coordination through value function propagation. Our work provides a new computational framework for studying cooperation emergence in complex systems, algorithmically validating the punishment promotes cooperation hypothesis while offering methodological insights for multi-agent system strategy design."
2505.04422,"We study a game-theoretic model for pool formation in Proof of Stake blockchain protocols. In such systems, stakeholders can form pools as a means of obtaining regular rewards from participation in ledger maintenance, with the power of each pool being dependent on its collective stake. The question we are interested in is the design of mechanisms that suitably split rewards among pool members and achieve favorable properties in the resulting pool configuration. With this in mind, we initiate a non-cooperative game-theoretic analysis of the well known Shapley value scheme from cooperative game theory into the context of blockchains. In particular, we focus on the oceanic model of games, proposed by Milnor and Shapley (1978), which is suitable for populations where a small set of large players coexists with a big mass of rather small, negligible players. This provides an appropriate level of abstraction for pool formation processes among the stakeholders. We provide comparisons between the Shapley mechanism and the more standard proportional scheme, in terms of attained decentralization, via a Price of Stability analysis and in terms of susceptibility to Sybil attacks, i.e., the strategic splitting of a players' stake with the intention of participating in multiple pools for increased profit. Interestingly, while the widely deployed proportional scheme appears to have certain advantages, the Shapley value scheme, which rewards higher the most pivotal players, emerges as a competitive alternative, by being able to bypass some of the downsides of proportional sharing, while also not being far from optimal guarantees w.r.t. decentralization. Finally, we complement our study with some variations of proportional sharing, where the profit is split in proportion to a superadditive or a subadditive function of the stake, showing that the Shapley value scheme still maintains the same advantages."
2505.05014,"Dueling bandit is a variant of the Multi-armed bandit to learn the binary relation by comparisons. Most work on the dueling bandit has targeted transitive relations, that is, totally/partially ordered sets, or assumed at least the existence of a champion such as Condorcet winner and Copeland winner. This work develops an analysis of dueling bandits for non-transitive relations. Jan-ken (a.k.a. rock-paper-scissors) is a typical example of a non-transitive relation. It is known that a rational player chooses one of three items uniformly at random, which is known to be Nash equilibrium in game theory. Interestingly, any variant of Jan-ken with four items (e.g., rock, paper, scissors, and well) contains at least one useless item, which is never selected by a rational player. This work investigates a dueling bandit problem to identify whether all $n$ items are indispensable in a given win-lose relation. Then, we provide upper and lower bounds of the sample complexity of the identification problem in terms of the determinant of $A$ and a solution of $\mathbf{x}^{\top} A = \mathbf{0}^{\top}$ where $A$ is an $n \times n$ pay-off matrix that every duel follows."
2505.05211,"The article explores the emerging domain of incentive-aware machine learning (ML), which focuses on algorithmic decision-making in contexts where individuals can strategically modify their inputs to influence outcomes. It categorizes the research into three perspectives: robustness, aiming to design models resilient to ""gaming""; fairness, analyzing the societal impacts of such systems; and improvement/causality, recognizing situations where strategic actions lead to genuine personal or societal improvement. The paper introduces a unified framework encapsulating models for these perspectives, including offline, online, and causal settings, and highlights key challenges such as differentiating between gaming and improvement and addressing heterogeneity among agents. By synthesizing findings from diverse works, we outline theoretical advancements and practical solutions for robust, fair, and causally-informed incentive-aware ML systems."
2505.05353,"Envy-Freeness is one of the most fundamental and important concepts in fair allocation. Some recent studies have focused on the concept of weighted envy-freeness. Under this concept, each agent is assigned a weight, and their valuations are divided by their weights when assessing fairness. This concept can promote more fairness in some scenarios. But on the other hand, experimental research has shown that this weighted envy-freeness significantly reduces the likelihood of fair allocations. When we must allocate the resources, we may propose fairness concepts with lower requirements that are potentially more feasible to implement. In this paper, we revisit weighted envy-freeness and propose a new concept called SumAvg-envy-freeness, which substantially increases the existence of fair allocations. This new concept can be seen as a complement of the normal weighted envy-fairness. Furthermore, we systematically study the computational complexity of finding fair allocations under the old and new weighted fairness concepts in two types of classic problems: Indivisible Resource Allocation and House Allocation. Our study provides a comprehensive characterization of various properties of weighted envy-freeness."
2505.05708,"We study a budget aggregation setting where voters express their preferred allocation of a fixed budget over a set of alternatives, and a mechanism aggregates these preferences into a single output allocation. Motivated by scenarios in which the budget is not perfectly divisible, we depart from the prevailing literature by restricting the mechanism to output allocations that assign integral amounts. This seemingly minor deviation has significant implications for the existence of truthful mechanisms. Specifically, when voters can propose fractional allocations, we demonstrate that the Gibbard-Satterthwaite theorem can be extended to our setting. In contrast, when voters are restricted to integral ballots, we identify a class of truthful mechanisms by adapting moving-phantom mechanisms to our context. Moreover, we show that while a weak form of proportionality can be achieved alongside truthfulness, (stronger) proportionality notions derived from approval-based committee voting are incompatible with truthfulness."
2505.05765,"We introduce and analyze distance preservation games (DPGs). In DPGs, agents express ideal distances to other agents and need to choose locations in the unit interval while preserving their ideal distances as closely as possible. We analyze the existence and computation of location profiles that are jump stable (i.e., no agent can benefit by moving to another location) or welfare optimal for DPGs, respectively. Specifically, we prove that there are DPGs without jump stable location profiles and identify important cases where such outcomes always exist and can be computed efficiently. Similarly, we show that finding welfare optimal location profiles is NP-complete and present approximation algorithms for finding solutions with social welfare close to optimal. Finally, we prove that DPGs have a price of anarchy of at most $2$."
2505.05809,"Equitability is a well-studied fairness notion in fair division, where an allocation is equitable if all agents receive equal utility from their allocation. For indivisible items, an exactly equitable allocation may not exist, and a natural relaxation is EQ1, which stipulates that any inequitability should be resolved by the removal of a single item. In this paper, we study equitability in the context of randomized allocations. Specifically, we aim to achieve equitability in expectation (ex ante EQ) and require that each deterministic outcome in the support satisfies ex post EQ1. Such an allocation is commonly known as a `Best of Both Worlds' allocation, and has been studied, e.g., for envy-freeness and MMS.We characterize the existence of such allocations using a geometric condition on linear combinations of EQ1 allocations, and use this to give comprehensive results on both existence and computation. For two agents, we show that ex ante EQ and ex post EQ1 allocations always exist and can be computed in polynomial time. For three or more agents, however, such allocations may not exist. We prove that deciding existence of such allocations is strongly NP-complete in general, and weakly NP-complete even for three agents. We also present a pseudo-polynomial time algorithm for a constant number of agents. We show that when agents have binary valuations, best of both worlds allocations that additionally satisfy welfare guarantees exist and are efficiently computable."
2505.05842,"Online Federated Learning (OFL) is a real-time learning paradigm that sequentially executes parameter aggregation immediately for each random arriving client. To motivate clients to participate in OFL, it is crucial to offer appropriate incentives to offset the training resource consumption. However, the design of incentive mechanisms in OFL is constrained by the dynamic variability of Two-sided Incomplete Information (TII) concerning resources, where the server is unaware of the clients' dynamically changing computational resources, while clients lack knowledge of the real-time communication resources allocated by the server. To incentivize clients to participate in training by offering dynamic rewards to each arriving client, we design a novel Dynamic Bayesian persuasion pricing for online Federated learning (DaringFed) under TII. Specifically, we begin by formulating the interaction between the server and clients as a dynamic signaling and pricing allocation problem within a Bayesian persuasion game, and then demonstrate the existence of a unique Bayesian persuasion Nash equilibrium. By deriving the optimal design of DaringFed under one-sided incomplete information, we further analyze the approximate optimal design of DaringFed with a specific bound under TII. Finally, extensive evaluation conducted on real datasets demonstrate that DaringFed optimizes accuracy and converges speed by 16.99%, while experiments with synthetic datasets validate the convergence of estimate unknown values and the effectiveness of DaringFed in improving the server's utility by up to 12.6%."
2505.06028,"We study the probability that a given candidate is an alpha-winner, i.e. a candidate preferred to each other candidate j by a fraction alpha_j of the voters. This extends the classical notion of Condorcet winner, which corresponds to the case alpha = (1/2, ..., 1/2). Our analysis is conducted under the general assumption that voters have independent preferences, illustrated through applications to well-known models such as Impartial Culture and the Mallows model. While previous works use probabilistic arguments to derive the limiting probability as the number of voters tends to infinity, we employ techniques from the field of analytic combinatorics to compute convergence rates and provide a method for obtaining higher-order terms in the asymptotic expansion. In particular, we establish that the probability of a given candidate being the Condorcet winner in Impartial Culture is a_0 + a_{1, n} n^{-1/2} + O(n^{-1}), where we explicitly provide the values of the constant a_0 and the coefficient a_{1, n}, which depends solely on the parity of the number of voters n. Along the way, we derive technical results in multivariate analytic combinatorics that may be of independent interest."
2505.06163,"We study coalition formation in the framework of fractional hedonic games (FHGs). The objective is to maximize social welfare in an online model where agents arrive one by one and must be assigned to coalitions immediately and irrevocably. A recurrent theme in online coalition formation is that online matching algorithms, where coalitions are restricted to size at most $2$, yield good competitive ratios. For example, computing maximal matchings achieves the optimal competitive ratio for general online FHGs. However, this ratio is bounded only if agents' valuations are themselves bounded.We identify optimal algorithms with constant competitive ratios in two related settings, independent of the range of agent valuations. First, under random agent arrival, we present an asymptotically optimal $(\frac{1}{3}-\frac 1n)$-competitive algorithm, where $n$ is the number of agents. This result builds on our identification of an optimal matching algorithm in a general model of online matching with edge weights and an unknown number of agents. In this setting, we also achieve an asymptotically optimal competitive ratio of $\frac{1}{3}-\frac 1n$. Second, when agents arrive in an arbitrary order but algorithms are allowed to irrevocably and entirely dissolve coalitions, we show that another matching-based algorithm achieves an optimal competitive ratio of $\frac{1}{6 + 4\sqrt{2}}$."
2505.06312,"The responsibility gap is a set of outcomes of a collective decision-making mechanism in which no single agent is individually responsible. In general, when designing a decision-making process, it is desirable to minimise the gap.The paper proposes a concept of an elected dictatorship. It shows that, in a perfect information setting, the gap is empty if and only if the mechanism is an elected dictatorship. It also proves that in an imperfect information setting, the class of gap-free mechanisms is positioned strictly between two variations of the class of elected dictatorships."
2505.06378,"With the advancement of large language models and embodied Artificial Intelligence (AI) in the intelligent transportation scenarios, the combination of them in intelligent transportation spawns the Vehicular Embodied AI Network (VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local advanced AI applications are defined as vehicular embodied AI agents, enabling capabilities such as environment perception and multi-agent collaboration. Due to computation latency and resource constraints, the local AI applications and services running on vehicular embodied AI agents need to be migrated, and subsequently referred to as vehicular embodied AI agent twins, which drive the advancement of vehicular embodied AI networks to offload intensive tasks to Roadside Units (RSUs), mitigating latency problems while maintaining service quality. Recognizing workload imbalance among RSUs in traditional approaches, we model AV-RSU interactions as a Stackelberg game to optimize bandwidth resource allocation for efficient migration. A Tiny Multi-Agent Bidirectional LSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to approximate the Stackelberg equilibrium through decentralized coordination. Furthermore, a personalized neural network pruning algorithm based on Path eXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities by identifying task-critical parameters in trained models, reducing model complexity with less performance degradation. Experimental validation confirms the algorithm's effectiveness in balancing system load and minimizing delays, demonstrating significant improvements in vehicular embodied AI agent deployment."
2505.0639,"The strategic selection of resources by selfish agents has long been a key area of research, with Resource Selection Games and Congestion Games serving as prominent examples. In these traditional frameworks, agents choose from a set of resources, and their utility depends solely on the number of other agents utilizing the same respective resource, treating all agents as indistinguishable or anonymous. Only recently, the study of the Resource Selection Game with heterogeneous agents has begun, meaning agents have a type and the fraction of agents of their type at their resource is the basis of their decision-making. In this work, we initiate the study of the Resource Selection Game with heterogeneous agents in combination with single-peaked utility functions, as some research suggests that this may represent human decision-making in certain cases. We conduct a comprehensive analysis of the game's stability within this framework. We provide tight bounds that specify for which peak values equilibria exist across different dynamics on cycles and binary trees. On arbitrary graphs, in a setting where agents lack information about the selection of other agents, we provide tight bounds for the existence of equilibria, given that the utility function is linear on both sides of the peak. Agents possessing this information on arbitrary graphs creates the sole case where our bounds are not tight, instead, we narrow down the cases in which the game may admit equilibria and present how several conventional approaches fall short in proving stability."
2505.06717,"In the well-studied Stable Roommates problem, we seek a stable matching of agents into pairs, where no two agents prefer each other over their assigned partners. However, some instances of this problem are unsolvable, lacking any stable matching. A long-standing open question posed by Gusfield and Irving (1989) asks about the behavior of the probability function Pn, which measures the likelihood that a random instance with n agents is solvable.This paper provides a comprehensive analysis of the landscape surrounding this question, combining structural, probabilistic, and experimental perspectives. We review existing approaches from the past four decades, highlight connections to related problems, and present novel structural and experimental findings. Specifically, we estimate Pn for instances with preferences sampled from diverse statistical distributions, examining problem sizes up to 5,001 agents, and look for specific sub-structures that cause unsolvability. Our results reveal that while Pn tends to be low for most distributions, the number and lengths of ""unstable"" structures remain limited, suggesting that random instances are ""close"" to being solvable.Additionally, we present the first empirical study of the number of stable matchings and the number of stable partitions that random instances admit, using recently developed algorithms. Our findings show that the solution sets are typically small. This implies that many NP-hard problems related to computing optimal stable matchings and optimal stable partitions become tractable in practice, and motivates efficient alternative solution concepts for unsolvable instances, such as stable half-matchings and maximum stable matchings."
2505.07008,"Stochastic games have become a prevalent framework for studying long-term multi-agent interactions, especially in the context of multi-agent reinforcement learning. In this work, we comprehensively investigate the concept of constant-memory strategies in stochastic games. We first establish some results on best responses and Nash equilibria for behavioral constant-memory strategies, followed by a discussion on the computational hardness of best responding to mixed constant-memory strategies. Those theoretic insights are later verified on several sequential decision-making testbeds, including the $\textit{Iterated Prisoner's Dilemma}$, the $\textit{Iterated Traveler's Dilemma}$, and the $\textit{Pursuit}$ domain. This work aims to enhance the understanding of theoretical issues in single-agent planning under multi-agent systems, and uncover the connection between decision models in single-agent and multi-agent contexts. The code is available at $\texttt{this https URL.}$"
2505.07501,"We study rational synthesis problems for concurrent games with omega-regular objectives. Our model of rationality considers only pure strategy Nash equilibria that satisfy either a social welfare or Pareto optimality condition with respect to an omega-regular objective for each agent. This extends earlier work on equilibria in concurrent games, without consideration about their quality. Our results show that the existence of Nash equilibria satisfying social welfare conditions can be computed as efficiently as the constrained Nash equilibrium existence problem. On the other hand, the existence of Nash equilibria satisfying the Pareto optimality condition possibly involves a higher upper bound, except in the case of  Buchi and Muller games, for which all three problems are in the classes P and PSPACE-complete, respectively."
2505.07579,"We study \emph{rental games} -- a single-parameter dynamic mechanism design problem, in which a designer rents out an indivisible asset over $n$ days. Each day, an agent arrives with a private valuation per day of rental, drawn from that day's (known) distribution. The designer can either rent out the asset to the current agent for any number of remaining days, charging them a (possibly different) payment per day, or turn the agent away. Agents who arrive when the asset is not available are turned away. A defining feature of our dynamic model is that agents are \emph{stagewise-IR} (individually rational), meaning they reject any rental agreement that results in temporary negative utility, even if their final utility is positive. We ask whether and under which economic objectives it is useful for the designer to exploit the stagewise-IR nature of the agents.We show that an optimal rental mechanism can be modeled as a sequence of dynamic auctions with seller costs. However, the stagewise-IR behavior of the agents makes these auctions quite different from classical single-parameter auctions: Myerson's Lemma does not apply, and indeed we show that truthful mechanisms are not necessarily monotone, and payments do not necessarily follow Myerson's unique payment rule. We develop alternative characterizations of optimal mechanisms under several classes of economic objectives, including generalizations of welfare, revenue and consumer surplus. These characterizations allow us to use Myerson's unique payment rule in several cases, and for the other cases we develop optimal mechanisms from scratch. Our work shows that rental games raise interesting questions even in the single-parameter regime."
2505.07688,"Data heterogeneity across multiple sources is common in real-world machine learning (ML) settings. Although many methods focus on enabling a single model to handle diverse data, real-world markets often comprise multiple competing ML providers. In this paper, we propose a game-theoretic framework -- the Heterogeneous Data Game -- to analyze how such providers compete across heterogeneous data sources. We investigate the resulting pure Nash equilibria (PNE), showing that they can be non-existent, homogeneous (all providers converge on the same model), or heterogeneous (providers specialize in distinct data sources). Our analysis spans monopolistic, duopolistic, and more general markets, illustrating how factors such as the ""temperature"" of data-source choice models and the dominance of certain data sources shape equilibrium outcomes. We offer theoretical insights into both homogeneous and heterogeneous PNEs, guiding regulatory policies and practical strategies for competitive ML marketplaces."
2505.08342,"This paper investigates a two-stage game-theoretical model with multiple parallel rank-order contests. In this model, each contest designer sets up a contest and determines the prize structure within a fixed budget in the first stage. Contestants choose which contest to participate in and exert costly effort to compete against other participants in the second stage. First, we fully characterize the symmetric Bayesian Nash equilibrium in the subgame of contestants, accounting for both contest selection and effort exertion, under any given prize structures. Notably, we find that, regardless of whether contestants know the number of participants in their chosen contest, the equilibrium remains unchanged in expectation. Next, we analyze the designers' strategies under two types of objective functions based on effort and participation, respectively. For a broad range of effort-based objectives, we demonstrate that the winner-takes-all prize structure-optimal in the single-contest setting-remains a dominant strategy for all designers. For the participation objective, which maximizes the number of participants surpassing a skill threshold, we show that the optimal prize structure is always a simple contest. Furthermore, the equilibrium among designers is computationally tractable when they share a common threshold."
2505.09799,"We study binary-action pairwise-separable network games that encompass both coordinating and anti-coordinating behaviors. Our model is grounded in an underlying directed signed graph, where each link is associated with a weight that describes the strenght and nature of the interaction. The utility for each agent is an aggregation of pairwise terms determined by the weights of the signed graph in addition to an individual bias term. We consider a scenario that assumes the presence of a prominent cohesive subset of players, who are either connected exclusively by positive weights, or forms a structurally balanced subset that can be bipartitioned into two adversarial subcommunities with positive intra-community and negative inter-community edges. Given the properties of the game restricted to the remaining players, our results guarantee the existence of Nash equilibria characterized by a consensus or, respectively, a polarization within the first group, as well as their stability under best response transitions. Our results can be interpreted as robustness results, building on the supermodular properties of coordination games and on a novel use of the concept of graph cohesiveness."
2505.10005,"We consider a class of jump games in which agents of different types occupy the nodes of a graph aiming to maximize the variety of types in their neighborhood. In particular, each agent derives a utility equal to the number of types different from its own in its neighborhood. We show that the jump game induced by the strategic behavior of the agents (who aim to maximize their utility) may in general have improving response cycles, but is a potential game under any of the following four conditions: there are only two types of agents; or exactly one empty node; or the graph is of degree at most 2; or the graph is 3-regular and there are two empty nodes. Additionally, we show that on trees, cylinder graphs, and tori, there is always an equilibrium. Finally, we show tight bounds on the price of anarchy with respect to two different measures of diversity: the social welfare (the total utility of the agents) and the number of colorful edges (that connect agents of different types)."
2505.10082,"We provide a dual fitting technique on a semidefinite program yielding simple proofs of tight bounds for the robust price of anarchy of several congestion and scheduling games under the sum of weighted completion times objective. The same approach also allows to bound the approximation ratio of local search algorithms and the competitive ratio of online algorithms for the scheduling problem $R || \sum w_j C_j$. All of our results are obtained through a simple unified dual fitting argument on the same semidefinite programming relaxation, which can essentially be obtained through the first round of the Lasserre/Sum of Squares hierarchy.As our main application, we show that the known coordination ratio bounds of respectively $4, (3 + \sqrt{5})/2 \approx 2.618,$ and $32/15 \approx 2.133$ for the scheduling game $R || \sum w_j C_j$ under the coordination mechanisms Smith's Rule, Proportional Sharing and Rand (STOC 2011) can be extended to congestion games and obtained through this approach. For the natural restriction where the weight of each player is proportional to its processing time on every resource, we show that the last bound can be improved from 2.133 to 2. This improvement can also be made for general instances when considering the price of anarchy of the game, rather than the coordination ratio. As a further application of this technique, we show that it recovers the tight bound of $(3 + \sqrt{5})/2$ for the price of anarchy of weighted affine congestion games and the Kawaguchi-Kyan bound of $(1+ \sqrt{2})/2$ for the pure price of anarchy of $P || \sum w_j C_j$. Moreover, this approach can analyze a simple local search algorithm for $R || \sum w_j C_j$, the best currently known combinatorial approximation algorithm for this problem achieving an approximation ratio of $(5 + \sqrt{5})/4 + \varepsilon$ and an online greedy algorithm which is $4$-competitive."
2505.10377,"We study the voting problem with two alternatives where voters' preferences depend on a not-directly-observable state variable. While equilibria in the one-round voting mechanisms lead to a good decision, they are usually hard to compute and follow. We consider the two-round voting mechanism where the first round serves as a polling stage and the winning alternative only depends on the outcome of the second round. We show that the two-round voting mechanism is a powerful tool for making collective decisions. Firstly, every (approximated) equilibrium in the two-round voting mechanisms (asymptotically) leads to the decision preferred by the majority as if the state of the world were revealed to the voters. Moreover, there exist natural equilibria in the two-round game following intuitive behaviors such as informative voting, sincere voting [Austen-Smith and Banks, 1996], and the surprisingly popular strategy [Prelec et al., 2017]. This sharply contrasts with the one-round voting mechanisms in the previous literature, where no simple equilibrium is known. Finally, we show that every equilibrium in the standard one-round majority vote mechanism gives an equilibrium in the two-round mechanisms that is not more complicated than the one-round equilibrium. Therefore, the two-round voting mechanism provides a natural equilibrium in every instance, including those where one-round voting fails to have a natural solution, and it can reach an informed majority decision whenever one-round voting can. Our experiments on generative AI voters also imply that two-round voting leads to the correct outcome more often than one-round voting under some circumstances."
2505.10378,"This paper examines the convergence behaviour of simultaneous best-response dynamics in random potential games. We provide a theoretical result showing that, for two-player games with sufficiently many actions, the dynamics converge quickly to a cycle of length two. This cycle lies within the intersection of the neighbourhoods of two distinct Nash equilibria. For three players or more, simulations show that the dynamics converge quickly to a Nash equilibrium with high probability. Furthermore, we show that all these results are robust, in the sense that they hold in non-potential games, provided the players' payoffs are sufficiently correlated. We also compare these dynamics to gradient-based learning methods in near-potential games with three players or more, and observe that simultaneous best-response dynamics converge to a Nash equilibrium of comparable payoff substantially faster."
2505.10388,"We investigate a voting scenario with two groups of agents whose preferences depend on a ground truth that cannot be directly observed. The majority's preferences align with the ground truth, while the minorities disagree. Focusing on strategic behavior, we analyze situations where agents can form coalitions up to a certain capacity and adopt the concept of ex-ante Bayesian $k$-strong equilibrium, in which no group of at most $k$ agents has an incentive to deviate. Our analysis provides a complete characterization of the region where equilibria exist and yield the majority-preferred outcome when the ground truth is common knowledge. This region is defined by two key parameters: the size of the majority group and the maximum coalition capacity. When agents cannot coordinate beyond a certain threshold determined by these parameters, a stable outcome supporting the informed majority emerges. The boundary of this region exhibits several distinct segments, notably including a surprising non-linear relationship between majority size and deviation capacity. Our results reveal the complexity of the strategic behaviors in this type of voting game, which in turn demonstrate the capability of the ex-ante Bayesian $k$-strong equilibrium to provide a more detailed analysis."
2505.10433,"The allocation of resources among multiple agents is a fundamental problem in both economics and computer science. In these settings, fairness plays a crucial role in ensuring social acceptability and practical implementation of resource allocation algorithms. Traditional fair division solutions have given rise to a variety of approximate fairness notions, often as a response to the challenges posed by non-existence or computational intractability of exact solutions. However, the inherent incompatibility among these notions raises a critical question: which concept of fairness is most suitable for practical applications? In this paper, we examine two broad frameworks -- threshold-based and comparison-based fairness notions -- and evaluate their perceived fairness through a comprehensive human subject study. Our findings uncover novel insights into the interplay between perception of fairness, theoretical guarantees, the role of externalities and subjective valuations, and underlying cognitive processes, shedding light on the theory and practice of fair division."
2505.11265,"Modern open and softwarized systems -- such as O-RAN telecom networks and cloud computing platforms -- host independently developed applications with distinct, and potentially conflicting, objectives. Coordinating the behavior of such applications to ensure stable system operation poses significant challenges, especially when each application's utility is accessible only via costly, black-box evaluations. In this paper, we consider a centralized optimization framework in which a system controller suggests joint configurations to multiple strategic players, representing different applications, with the goal of aligning their incentives toward a stable outcome. To model this interaction, we formulate a Stackelberg game in which the central optimizer lacks access to analytical utility functions and instead must learn them through sequential, multi-fidelity evaluations. To address this challenge, we propose MF-UCB-PNE, a novel multi-fidelity Bayesian optimization strategy that leverages a budget-constrained sampling process to approximate pure Nash equilibrium (PNE) solutions. MF-UCB-PNE systematically balances exploration across low-cost approximations with high-fidelity exploitation steps, enabling efficient convergence to incentive-compatible configurations. We provide theoretical and empirical insights into the trade-offs between query cost and equilibrium accuracy, demonstrating the effectiveness of MF-UCB-PNE in identifying effective equilibrium solutions under limited cost budgets."
2505.11431,"We consider repeated allocation of a shared resource via a non-monetary mechanism, wherein a single item must be allocated to one of multiple agents in each round. We assume that each agent has i.i.d. values for the item across rounds, and additive utilities. Past work on this problem has proposed mechanisms where agents can get one of two kinds of guarantees: $(i)$ (approximate) Bayes-Nash equilibria via linkage-based mechanisms which need extensive knowledge of the value distributions, and $(ii)$ simple distribution-agnostic mechanisms with robust utility guarantees for each individual agent, which are worse than the Nash outcome, but hold irrespective of how others behave (including possibly collusive behavior). Recent work has hinted at barriers to achieving both simultaneously. Our work however establishes this is not the case, by proposing the first mechanism in which each agent has a natural strategy that is both a Bayes-Nash equilibrium and also comes with strong robust guarantees for individual agent utilities.Our mechanism comes out of a surprising connection between the online shared resource allocation problem and implementation theory, and uses a surprising strengthening of Border's theorem. In particular, we show that establishing robust equilibria in this setting reduces to showing that a particular subset of the Border polytope is non-empty. We establish this via a novel joint Schur-convexity argument. This strengthening of Border's criterion for obtaining a stronger conclusion is of independent technical interest, as it may prove useful in other settings."
2505.11791,"To steer the behavior of selfish, resource-sharing agents in a socio-technical system towards the direction of higher efficiency, the system designer requires accurate models of both agent behaviors and the underlying system infrastructure. For instance, traffic controllers often use road latency models to design tolls whose deployment can effectively mitigate traffic congestion. However, misspecifications of system parameters may restrict a system designer's ability to influence collective agent behavior toward efficient outcomes. In this work, we study the impact of system misspecifications on toll design for atomic congestion games. We prove that tolls designed under sufficiently minor system misspecifications, when deployed, do not introduce new Nash equilibria in atomic congestion games compared to tolls designed in the noise-free setting, implying a form of local robustness. We then upper bound the degree to which the worst-case equilibrium system performance could decrease when tolls designed under a given level of system misspecification are deployed. We validate our theoretical results via Monte-Carlo simulations as well as realizations of our worst-case guarantees."
2505.1201,"Classical federated learning (FL) assumes that the clients have a limited amount of noisy data with which they voluntarily participate and contribute towards learning a global, more accurate model in a principled manner. The learning happens in a distributed fashion without sharing the data with the center. However, these methods do not consider the incentive of an agent for participating and contributing to the process, given that data collection and running a distributed algorithm is costly for the clients. The question of rationality of contribution has been asked recently in the literature and some results exist that consider this problem. This paper addresses the question of simultaneous parameter learning and incentivizing contribution in a truthful manner, which distinguishes it from the extant literature. Our first mechanism incentivizes each client to contribute to the FL process at a Nash equilibrium and simultaneously learn the model parameters. We also ensure that agents are incentivized to truthfully reveal information in the intermediate stages of the algorithm. However, this equilibrium outcome can be away from the optimal, where clients contribute with their full data and the algorithm learns the optimal parameters. We propose a second mechanism that enables the full data contribution along with optimal parameter learning. Large scale experiments with real (federated) datasets (CIFAR-10, FEMNIST, and Twitter) show that these algorithms converge quite fast in practice, yield good welfare guarantees and better model performance for all agents."
2505.12377,"We investigate multi-organizational scheduling problems, building upon the framework introduced by Pascual et al.[2009]. In this setting, multiple organizations each own a set of identical machines and sequential jobs with distinct processing times. The challenge lies in optimally assigning jobs across organizations' machines to minimize the overall makespan while ensuring no organization's performance deteriorates. To formalize this fairness constraint, we introduce individual rationality, a game-theoretic concept that guarantees each organization benefits from participation.Our analysis reveals that finding an individually rational schedule with minimum makespan is $\Theta_2^{\text{P}}$-hard, placing it in a complexity class strictly harder than both NP and coNP. We further extend the model by considering an alternative objective: minimizing the sum of job completion times, both within individual organizations and across the entire system. The corresponding decision variant proves to be NP-complete. Through comprehensive parameterized complexity analysis of both problems, we provide new insights into these computationally challenging multi-organizational scheduling scenarios."
2505.12386,"As GenAI platforms grow, their dependence on content from competing providers, combined with access to alternative data sources, creates new challenges for data-sharing decisions. In this paper, we provide a model of data sharing between a content creation firm and a GenAI platform that can also acquire content from third-party experts. The interaction is modeled as a Stackelberg game: the firm first decides how much of its proprietary dataset to share with GenAI, and GenAI subsequently determines how much additional data to acquire from external experts. Their utilities depend on user traffic, monetary transfers, and the cost of acquiring additional data from external experts. We characterize the unique subgame perfect equilibrium of the game and uncover a surprising phenomenon: The firm may be willing to pay GenAI to share the firm's own data, leading to a costly data-sharing equilibrium. We further characterize the set of Pareto improving data prices, and show that such improvements occur only when the firm pays to share data. Finally, we study how the price can be set to optimize different design objectives, such as promoting firm data sharing, expert data acquisition, or a balance of both. Our results shed light on the economic forces shaping data-sharing partnerships in the age of GenAI, and provide guidance for platforms, regulators and policymakers seeking to design effective data exchange mechanisms."
2505.12451,"We consider a spatial voting model where both candidates and voters are positioned in the $d$-dimensional Euclidean space, and each voter ranks candidates based on their proximity to the voter's ideal point. We focus on the scenario where the given information about the locations of the voters' ideal points is incomplete; for each dimension, only an interval of possible values is known. In this context, we investigate the computational complexity of determining the possible winners under positional scoring rules. Our results show that the possible winner problem in one dimension is solvable in polynomial time for all $k$-truncated voting rules with constant $k$. Moreover, for some scoring rules for which the possible winner problem is NP-complete, such as approval voting for any dimension or $k$-approval for $d \geq 2$ dimensions, we give an FPT algorithm parameterized by the number of candidates. Finally, we classify tractable and intractable settings of the weighted possible winner problem in one dimension, and resolve the computational complexity of the weighted case for all two-valued positional scoring rules when $d=1$."
2505.12609,"Understanding a dynamical system fundamentally relies on establishing an appropriate Hamiltonian function and elucidating its symmetries. By formulating agents' strategies and cumulative payoffs as canonically conjugate variables, we identify the Hamiltonian function that generates the dynamics of poly-matrix zero-sum games. We reveal the symmetries of our Hamiltonian and derive the associated conserved quantities, showing how the conservation of probability and the invariance of the Fenchel coupling are intrinsically encoded within the system. Furthermore, we propose the dissipation FTRL (DFTRL) dynamics by introducing a perturbation that dissipates the Fenchel coupling, proving convergence to the Nash equilibrium and linking DFTRL to last-iterate convergent algorithms. Our results highlight the potential of Hamiltonian dynamics in uncovering the structural properties of learning dynamics in games, and pave the way for broader applications of Hamiltonian dynamics in game theory and machine learning."
2505.12699,"Multiwinner Elections have emerged as a prominent area of research with numerous practical applications. We contribute to this area by designing parameterized approximation algorithms and also resolving an open question by Yang and Wang [AAMAS'18]. More formally, given a set of candidates, \mathcal{C}, a set of voters,\mathcal{V}, approving a subset of candidates (called approval set of a voter), and an integer $k$, we consider the problem of selecting a ``good'' committee using Thiele rules. This problem is computationally challenging for most Thiele rules with monotone submodular satisfaction functions, as there is no (1-\frac{1}{e}-\epsilon)\footnote{Here, $e$ denotes the base of the natural logarithm.}-approximation algorithm in f(k)(|\mathcal{C}| + |\mathcal{V}|)^{o(k)} time for any fixed $\epsilon > 0$ and any computable function $f$, and no {\sf PTAS} even when the length of approval set is two. Skowron [WINE'16] designed an approximation scheme running in FPT time parameterized by the combined parameter, size of the approval set and $k$. In this paper, we consider a parameter $d+k$ (no $d$ voters approve the same set of $d$ candidates), where $d$ is upper bounded by the size of the approval set (thus, can be much smaller).With respect to this parameter, we design parameterized approximation schemes, a lossy polynomial-time preprocessing method, and show that an extra committee member suffices to achieve the desired score (i.e., $1$-additive approximation). Additionally, we resolve an open question by Yang and Wang~[AAMAS'18] regarding the fixed-parameter tractability of the problem under the PAV rule with the total score as the parameter, demonstrating that it admits an FPT algorithm."
2505.12943,"We study the problem of design of strategyproof in expectation (SP) mechanisms for facility location on a cycle, with the objective of minimizing the sum of costs of $n$ agents. We show that there exists an SP mechanism that attains an approximation ratio of $7/4$ with respect to the sum of costs of the agents, thus improving the best known upper bound of $2-2/n$ in the cases of $n \geq 5$. The mechanism obtaining the bound randomizes between two mechanisms known in the literature: the Random Dictator (RD) and the Proportional Circle Distance (PCD) mechanism of Meir (arXiv:1902.08070). To prove the result, we propose a cycle-cutting technique that allows for estimating the problem on a cycle by a problem on a line."
2505.12976,"The Schulze method is a voting rule widely used in practice and enjoys many positive axiomatic properties. While it is computable in polynomial time, its straight-forward implementation does not scale well for large elections. In this paper, we develop a highly optimised algorithm for computing the Schulze method with Pregel, a framework for massively parallel computation of graph problems, and demonstrate its applicability for large preference data sets. In addition, our theoretic analysis shows that the Schulze method is indeed particularly well-suited for parallel computation, in stark contrast to the related ranked pairs method. More precisely we show that winner determination subject to the Schulze method is NL-complete, whereas this problem is P-complete for the ranked pairs method."
2505.13428,"We study the Student Project Allocation problem with lecturer preferences over Students (SPA-S), an extension of the well-known Stable Marriage and Hospital Residents problem. In this model, students have preferences over projects, each project is offered by a single lecturer, and lecturers have preferences over students. The goal is to compute a stable matching which is an assignment of students to projects (and thus to lecturers) such that no student or lecturer has an incentive to deviate from their current assignment. While motivated by the university setting, this problem arises in many allocation settings where limited resources are offered by agents with their own preferences, such as in wireless networks.We establish new structural results for the set of stable matchings in SPA-S by developing the theory of meta-rotations, a generalisation of the well-known notion of rotations from the Stable Marriage problem. Each meta-rotation corresponds to a minimal set of changes that transforms one stable matching into another within the lattice of stable matchings. The set of meta-rotations, ordered by their precedence relations, forms the meta-rotation poset. We prove that there is a one-to-one correspondence between the set of stable matchings and the closed subsets of the meta-rotation poset. By developing this structure, we provide a foundation for the design of efficient algorithms for enumerating and counting stable matchings, and for computing other optimal stable matchings, such as egalitarian or minimum-cost matchings, which have not been previously studied in SPA-S."
2505.1363,"In the well-studied metric distortion problem in social choice, we have voters and candidates located in a shared metric space, and the objective is to design a voting rule that selects a candidate with minimal total distance to the voters. However, the voting rule has limited information about the distances in the metric, such as each voter's ordinal rankings of the candidates in order of distances. The central question is whether we can design rules that, for any election and underlying metric space, select a candidate whose total cost deviates from the optimal by only a small factor, referred to as the distortion.A long line of work resolved the optimal distortion of deterministic rules, and recent work resolved the optimal distortion of randomized (weighted) tournament rules, which only use the aggregate preferences between pairs of candidates. In both cases, simple rules achieve the optimal distortion of $3$. Can we achieve the best of both worlds: a deterministic tournament rule matching the lower bound of $3$? Prior to our work, the best rules have distortion $2 + \sqrt{5} \approx 4.2361$.In this work, we establish a lower bound of $3.1128$ on the distortion of any deterministic tournament rule, even when there are only 5 candidates, and improve the upper bound with a novel rule guaranteeing distortion $3.9312$. We then generalize tournament rules to the class of $k$-tournament rules which obtain the aggregate preferences between $k$-tuples of candidates. We show that there is a family of deterministic $k$-tournament rules that achieves distortion approaching $3$ as $k$ grows. Finally, we show that even with $k = 3$, a randomized $k$-tournament rule can achieve distortion less than $3$, which had been a longstanding barrier even for the larger class of ranked voting rules."
2505.13642,"In this work, we consider the design of Non-Obviously Manipulable (NOM) mechanisms, mechanisms that bounded rational agents may fail to recognize as manipulable, for two relevant classes of succinctly representable Hedonic Games: Additively Separable and Fractional Hedonic Games. In these classes, agents have cardinal scores towards other agents, and their preferences over coalitions are determined by aggregating such scores. This aggregation results in a utility function for each agent, which enables the evaluation of outcomes via the utilitarian social welfare. We first prove that, when scores can be arbitrary, every optimal mechanism is NOM; moreover, when scores are limited in a continuous interval, there exists an optimal mechanism that is NOM. Given the hardness of computing optimal outcomes in these settings, we turn our attention to efficient and NOM mechanisms. To this aim, we first prove a characterization of NOM mechanisms that simplifies the class of mechanisms of interest. Then, we design a NOM mechanism returning approximations that asymptotically match the best-known approximation achievable in polynomial time. Finally, we focus on discrete scores, where the compatibility of NOM with optimality depends on the specific values. Therefore, we initiate a systematic analysis to identify which discrete values support this compatibility and which do not."
2505.1368,"Core-selecting combinatorial auctions are popular auction designs that constrain prices to eliminate the incentive for any group of bidders -- with the seller -- to renegotiate for a better deal. They help overcome the low-revenue issues of classical combinatorial auctions. We introduce a new class of core-selecting combinatorial auctions that leverage bidder information available to the auction designer. We model such information through constraints on the joint type space of the bidders -- these are constraints on bidders' private valuations that are known to hold by the auction designer before bids are elicited. First, we show that type space information can overcome the well-known impossibility of incentive-compatible core-selecting combinatorial auctions. We present a revised and generalized version of that impossibility result that depends on how much information is conveyed by the type spaces. We then devise a new family of core-selecting combinatorial auctions and show that they minimize the sum of bidders' incentives to deviate from truthful bidding. We develop new constraint generation techniques -- and build upon existing quadratic programming techniques -- to compute core prices, and conduct experiments to evaluate the incentive, revenue, fairness, and computational merits of our new auctions. Our new core-selecting auctions directly improve upon existing designs that have been used in many high-stakes auctions around the world. We envision that they will be a useful addition to any auction designer's toolkit."
2505.13687,"We derive the revenue-optimal efficient (welfare-maximizing) mechanism in a general multidimensional mechanism design setting when type spaces -- that is, the underlying domains from which agents' values come from -- can capture arbitrarily complex informational constraints about the agents. Type spaces can encode information about agents representing, for example, machine learning predictions of agent behavior, institutional knowledge about feasible market outcomes (such as item substitutability or complementarity in auctions), and correlations between multiple agents. Prior work has only dealt with connected type spaces, which are not expressive enough to capture many natural kinds of constraints such as disjunctive constraints. We provide two characterizations of the optimal mechanism based on allocations and connected components; both make use of an underlying network flow structure to the mechanism design. Our results significantly generalize and improve the prior state of the art in revenue-optimal efficient mechanism design. They also considerably expand the scope of what forms of agent information can be expressed and used to improve revenue."
2505.13751,"Censorship resistance is one of the core value proposition of blockchains. A recurring design pattern aimed at providing censorship resistance is enabling multiple proposers to contribute inputs into block construction. Notably, Fork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in Ethereum. However, the current proposal relies on altruistic behavior, without a Transaction Fee Mechanism (TFM). This study aims to address this gap by exploring how multiple proposers should be rewarded to incentivize censorship resistance. The main contribution of this work is the identification of TFMs that ensure censorship resistance under bribery attacks, while also satisfying the incentive compatibility properties of EIP-1559. We provide a concrete payment mechanism for FOCIL, along with generalizable contributions to the literature by analyzing 1) incentive compatibility of TFMs in the presence of a bribing adversary, 2) TFMs in protocols with multiple phases of transaction inclusion, and 3) TFMs of protocols in which parties are uncertain about the behavior and the possible bribe of others."
2505.13824,"We study the problem of fair online resource allocation via non-monetary mechanisms, where multiple agents repeatedly share a resource without monetary transfers. Previous work has shown that every agent can guarantee $1/2$ of their ideal utility (the highest achievable utility given their fair share of resources) robustly, i.e., under arbitrary behavior by the other agents. While this $1/2$-robustness guarantee has now been established under very different mechanisms, including pseudo-markets and dynamic max-min allocation, improving on it has appeared difficult.In this work, we obtain the first significant improvement on the robustness of online resource sharing. In more detail, we consider the widely-studied repeated first-price auction with artificial currencies. Our main contribution is to show that a simple randomized bidding strategy can guarantee each agent a $2 - \sqrt 2 \approx 0.59$ fraction of her ideal utility, irrespective of others' bids. Specifically, our strategy requires each agent with fair share $\alpha$ to use a uniformly distributed bid whenever her value is in the top $\alpha$-quantile of her value distribution. Our work almost closes the gap to the known $1 - 1/e \approx 0.63$ hardness for robust resource sharing; we also show that any static (i.e., budget independent) bidding policy cannot guarantee more than a $0.6$-fraction of the ideal utility, showing our technique is almost tight."
2505.13827,"The sequence form, owing to its compact and holistic strategy representation, has demonstrated significant efficiency in computing normal-form perfect equilibria for two-player extensive-form games with perfect recall. Nevertheless, the examination of $n$-player games remains underexplored. To tackle this challenge, we present a sequence-form characterization of normal-form perfect equilibria for $n$-player extensive-form games, achieved through a class of perturbed games formulated in sequence form. Based on this characterization, we develop a differentiable path-following method for computing normal-form perfect equilibria and prove its convergence. This method formulates an artificial logarithmic-barrier game in sequence form, introducing an additional variable to regulate the impact of logarithmic-barrier terms on the payoff functions, as well as the transition of the strategy space. We prove the existence of a smooth equilibrium path defined by the artificial game, starting from an arbitrary positive realization plan and converging to a normal-form perfect equilibrium of the original game as the additional variable approaches zero. Furthermore, we extend Harsanyi's linear and logarithmic tracing procedures to the sequence form and develop two alternative methods for computing normal-form perfect equilibria. Numerical experiments further substantiate the effectiveness and computational efficiency of our methods."
2505.14547,"Game-theoretic algorithms are commonly benchmarked on recreational games, classical constructs from economic theory such as congestion and dispersion games, or entirely random game instances. While the past two decades have seen the rise of security games -- grounded in real-world scenarios like patrolling and infrastructure protection -- their practical evaluation has been hindered by limited access to the datasets used to generate them. In particular, although the structural components of these games (e.g., patrol paths derived from maps) can be replicated, the critical data defining target values -- central to utility modeling -- remain inaccessible. In this paper, we introduce a flexible framework that leverages open-access datasets to generate realistic matrix and security game instances. These include animal movement data for modeling anti-poaching scenarios and demographic and infrastructure data for infrastructure protection. Our framework allows users to customize utility functions and game parameters, while also offering a suite of preconfigured instances. We provide theoretical results highlighting the degeneracy and limitations of benchmarking on random games, and empirically compare our generated games against random baselines across a variety of standard algorithms for computing Nash and Stackelberg equilibria, including linear programming, incremental strategy generation, and self-play with no-regret learners."
2505.14551,"We investigate how a blockchain can distill the collective belief of its nodes regarding the trustworthiness of a (sub)set of nodes into a {\em reputation system} that reflects the probability of correctly performing a task. To address this question, we introduce a framework that breaks it down into two sub-problems:1. (Information Extraction): How can the system distill trust information from a function of the nodes' true beliefs?2. (Incentive Design): How can we incentivize nodes to truthfully report such information?To tackle the first sub-problem, we adapt, in a non-trivial manner, the well-known PageRank algorithm to our problem. For the second, we define a new class of games, called Trustworthy Reputation games (TRep games), which aim to extract the collective beliefs on trust from the actions of rational participants. We then propose a concrete TRep game whose utility function leverages Personalized PageRank and can be instantiated through a straightforward blockchain rewards mechanism. Building on this, we show how the TRep game enables the design of a reputation system. Such systems can enhance the robustness, scalability, and efficiency of blockchain and DeFi solutions. For instance, we demonstrate how such a system can be used within a Proof-of-Reputation blockchain."
2505.14817,"Cooperative bargaining games are widely used to model resource allocation and conflict resolution. Traditional solutions assume the mediator can access agents utility function values and gradients. However, there is an increasing number of settings, such as human AI interactions, where utility values may be inaccessible or incomparable due to unknown, nonaffine transformations. To model such settings, we consider that the mediator has access only to agents most preferred directions, i.e., normalized utility gradients in the decision space. To this end, we propose a cooperative bargaining algorithm where a mediator has access to only the direction oracle of each agent. We prove that unlike popular approaches such as the Nash and Kalai Smorodinsky bargaining solutions, our approach is invariant to monotonic nonaffine transformations, and that under strong convexity and smoothness assumptions, this approach enjoys global asymptotic convergence to Pareto stationary solutions. Moreover, we show that the bargaining solutions found by our algorithm also satisfy the axioms of symmetry and (under slightly stronger conditions) independence of irrelevant alternatives, which are popular in the literature. Finally, we conduct experiments in two domains, multi agent formation assignment and mediated stock portfolio allocation, which validate these theoretic results. All code for our experiments can be found atthis https URL."
2505.14847,"Infinitely repeated games support equilibrium concepts beyond those present in one-shot games (e.g., cooperation in the prisoner's dilemma). Nonetheless, repeated games fail to capture our real-world intuition for settings with many anonymous agents interacting in pairs. Repeated games with restarts, introduced by Berker and Conitzer [IJCAI '24], address this concern by giving players the option to restart the game with someone new whenever their partner deviates from an agreed-upon sequence of actions. In their work, they studied symmetric games with symmetric strategies. We significantly extend these results, introducing and analyzing more general notions of equilibria in asymmetric games with restarts. We characterize which goal strategies players can be incentivized to play in equilibrium, and we consider the computational problem of finding such sequences of actions with minimal cost for the agents. We show that this problem is NP-hard in general. However, when the goal sequence maximizes social welfare, we give a pseudo-polynomial time algorithm."
2505.15454,"In this work, we introduce the concept of non-negative weighted regret, an extension of non-negative regret \cite{anagnostides2022last} in games. Investigating games with non-negative weighted regret helps us to understand games with conflicting interests, including harmonic games and important classes of zero-sumthis http URLshow that optimistic variants of classical no-regret learning algorithms, namely optimistic mirror descent (OMD) and optimistic follow the regularized leader (OFTRL), converge to an $\epsilon$-approximate Nash equilibrium at a rate of $O(1/\epsilon^2)$.Consequently, they guarantee pointwise convergence to a Nash equilibrium if there are only finitely many Nash equilibria in the game. These algorithms are robust in the sense the convergence holds even if the players deviate Our theoretical findings are supported by empirical evaluations of OMD and OFTRL on the game of matching pennies and harmonic game instances."
2505.16043,"Defending against sophisticated cyber threats demands strategic allocation of limited security resources across complex network infrastructures. When the defender has limited defensive resources, the complexity of coordinating honeypot placements across hundreds of nodes grows exponentially. In this paper, we present a multi-attacker Bayesian Stackelberg framework modeling concurrent adversaries attempting to breach a directed network of system components. Our approach uniquely characterizes each adversary through distinct target preferences, exploit capabilities, and associated costs, while enabling defenders to strategically deploy honeypots at critical network positions. By integrating a multi-follower Stackelberg formulation with dynamic Bayesian belief updates, our framework allows defenders to continuously refine their understanding of attacker intentions based on actions detected through Intrusion Detection Systems (IDS). Experimental results show that the proposed method prevents attack success within a few rounds and scales well up to networks of 500 nodes with more than 1,500 edges, maintaining tractable run times."
2505.16049,"In the contemporary digital landscape, cybersecurity has become a critical issue due to the increasing frequency and sophistication of cyber attacks. This study utilizes a non-zero-sum game theoretical framework to model the strategic interactions between cyber attackers and defenders, with the objective of identifying optimal strategies for both. By defining precise payoff functions that incorporate the probabilities and costs associated with various exploits, as well as the values of network nodes and the costs of deploying honeypots, we derive Nash equilibria that inform strategic decisions. The proposed model is validated through extensive simulations, demonstrating its effectiveness in enhancing network security. Our results indicate that high-probability, low-cost exploits like Phishing and Social Engineering are more likely to be used by attackers, necessitating prioritized defense mechanisms. Our findings also show that increasing the number of network nodes dilutes the attacker's efforts, thereby improving the defender's payoff. This study provides valuable insights into optimizing resource allocation for cybersecurity and highlights the scalability and practical applicability of the game-theoretic approach."
2505.16054,"We consider a combinatorial auction setting where buyers have fractionally subadditive (XOS) valuations over the items and the seller's objective is to maximize the social welfare. A prophet inequality in this setting bounds the competitive ratio of sequential allocation (often using item pricing) against the hindsight optimum. We study the dependence of the competitive ratio on the number of copies, $k$, of each item.We show that the multi-unit combinatorial setting is strictly harder than its single-item counterpart in that there is a gap between the competitive ratios achieved by static item pricings in the two settings. However, if the seller is allowed to change item prices dynamically, it becomes possible to asymptotically match the competitive ratio of a single-item static pricing. We also develop a new non-adaptive anonymous multi-unit combinatorial prophet inequality where the item prices are determined up front but increase as the item supply decreases. Setting the item prices in our prophet inequality requires minimal information about the buyers' value distributions -- merely (an estimate of) the expected social welfare accrued by each item in the hindsight optimal solution suffices. Our non-adaptive pricing achieves a competitive ratio that increases strictly as a function of the item supply $k$."
2505.16068,"Retroactive Public Goods Funding (RetroPGF) rewards blockchain projects based on proven impact rather than future promises. This paper reviews voting mechanisms for Optimism's RetroPGF, where ""badgeholders"" allocate rewards to valuable projects. We explore Optimism's previous schemes for RetroPGF voting, including quadratic, mean, and median voting. We present a proof-based formal analysis for vulnerabilities in these voting schemes, empirically validate these vulnerabilities using voting simulations, and offer assessments and practical recommendations for future iterations of Optimism's system based on our findings."
2505.16141,"Bayesian persuasion, a central model in information design, studies how a sender, who privately observes a state drawn from a prior distribution, strategically sends a signal to influence a receiver's action. A key assumption is that both sender and receiver share the precise knowledge of the prior. Although this prior can be estimated from past data, such assumptions break down in high-dimensional or infinite state spaces, where learning an accurate prior may require a prohibitive amount of data. In this paper, we study a learning-based variant of persuasion, which we term persuasive prediction. This setting mirrors Bayesian persuasion with large state spaces, but crucially does not assume a common prior: the sender observes covariates $X$, learns to predict a payoff-relevant outcome $Y$ from past data, and releases a prediction to influence a population of receivers.To model rational receiver behavior without a common prior, we adopt a learnable proxy: decision calibration, which requires the prediction to be unbiased conditioned on the receiver's best response to the prediction. This condition guarantees that myopically responding to the prediction yields no swap regret. Assuming the receivers best respond to decision-calibrated predictors, we design a computationally and statistically efficient algorithm that learns a decision-calibrated predictor within a randomized predictor class that optimizes the sender's utility. In the commonly studied single-receiver case, our method matches the utility of a Bayesian sender who has full knowledge of the underlying prior distribution. Finally, we extend our algorithmic result to a setting where receivers respond stochastically to predictions and the sender may randomize over an infinite predictor class."
2505.16358,"We introduce a game-theoretic framework examining strategic interactions between a platform and its content creators in the presence of AI-generated content. Our model's main novelty is in capturing creators' dual strategic decisions: The investment in content quality and their (possible) consent to share their content with the platform's GenAI, both of which significantly impact their utility. To incentivize creators, the platform strategically allocates a portion of its GenAI-driven revenue to creators who share their content. We focus on the class of full-sharing equilibrium profiles, in which all creators willingly share their content with the platform's GenAI system. Such equilibria are highly desirable both theoretically and practically. Our main technical contribution is formulating and efficiently solving a novel optimization problem that approximates the platform's optimal revenue subject to inducing a full-sharing equilibrium. A key aspect of our approach is identifying conditions under which full-sharing equilibria exist and a surprising connection to the Prisoner's Dilemma. Finally, our simulations demonstrate how revenue-allocation mechanisms affect creator utility and the platform's revenue."
2505.16966,"Transactions are an important aspect of human social life, and represent dynamic flow of information, intangible values, such as trust, as well as monetary and social capital. Although much research has been conducted on the nature of transactions in fields ranging from the social sciences to game theory, the systemic effects of different types of agents transacting in real-world social networks (often following a scale-free distribution) are not fully understood. A particular systemic measure that has not received adequate attention in the complex networks and game theory communities, is the Gini Coefficient, which is widely used in economics to quantify and understand wealth inequality. In part, the problem is a lack of experimentation using a replicable algorithm and publicly available data. Motivated by this problem, this article proposes a model and simulation algorithm, based on game theory, for quantifying the evolution of inequality in complex networks of strategic agents. Our results shed light on several complex drivers of inequality, even in simple, abstract settings, and exhibit consistency across networks with different origins and descriptions."
2505.17271,"Resource distribution is a fundamental problem in economic and policy design, particularly when demand and supply are not naturally aligned. Without regulation, wealthier individuals may monopolize this resource, leaving the needs of others unsatisfied. While centralized distribution can ensure fairer division, it can struggle to manage logistics efficiently, and adapt to changing conditions, often leading to shortages, surpluses, and bureaucratic inefficiencies. Building on previous research on market-based redistribution, we examine a repeated hybrid market that incorporates buying rights. These rights, distributed iteratively by a central authority (for instance, as digital tokens), are intended to enhance fairness in the system - a unit of right is required to acquire a unit of the resource, but the rights themselves can also be traded alongside the resource in the market. We analyze how this regulatory mechanism influences the distribution of the scarce resource in the hybrid market over time. Unlike past works that relied on empirical methods, we explore the exact analytical properties of a system in which traders optimize over multiple rounds. We identify its market equilibrium, which is a natural generalization of the free market equilibrium, and show that it is coalition-proof. To assess the fairness in the system, we use the concept of frustration, which measures the gap between the resources a buyer is entitled to through their buying rights and what they actually obtain through trading. Our main theoretical result shows that using buying rights reduces the frustration by at least half compared to the free market. Empirical evaluations further support our findings, suggesting the system performs well even beyond the theoretically studied assumptions."
2505.17885,"We initiate the study of transaction fee mechanism design for blockchain protocols in which multiple block producers contribute to the production of each block. Our contributions include:- We propose an extensive-form (multi-stage) game model to reason about the game theory of multi-proposer transaction fee mechanisms.- We define the strongly BPIC property to capture the idea that all block producers should be motivated to behave as intended: for every user bid profile, following the intended allocation rule is a Nash equilibrium for block producers that Pareto dominates all other Nash equilibria.- We propose the first-price auction with equal sharing (FPA-EQ) mechanism as an attractive solution to the multi-proposer transaction fee mechanism design problem. We prove that the mechanism is strongly BPIC and guarantees at least a 63.2% fraction of the maximum-possible expected welfare at equilibrium.- We prove that the compromises made by the FPA-EQ mechanism are qualitatively necessary: no strongly BPIC mechanism with non-trivial welfare guarantees can be DSIC, and no strongly BPIC mechanism can guarantee optimal welfare at equilibrium."
2505.18061,"Posted price mechanisms are prevalent in allocating goods within online marketplaces due to their simplicity and practical efficiency. We explore a fundamental scenario where buyers' valuations are independent and identically distributed, focusing specifically on the allocation of a single unit. Inspired by the rapid growth and scalability of modern online marketplaces, we investigate optimal performance guarantees under the assumption of a significantly large market. We show a large market benefit when using fixed prices, improving the known guarantee of $1-1/e\approx 0.632$ to $0.712$. We then study the case of selling $k$ identical units, and we prove that the optimal fixed price guarantee approaches $1-1/\sqrt{2k \pi}$, which implies that the large market advantage vanishes as $k$ grows. We use real-world auction data to test our fixed price policies in the large market regime. Next, under the large market assumption, we show that the competition complexity for the optimal posted price mechanism is constant, and we identify precise scaling factors for the number of bidders that enable it to match benchmark performance. Remarkably, our findings break previously established worst-case impossibility results, underscoring the practical robustness and efficiency of posted pricing in large-scale marketplaces."
2505.18114,"In the facility location problem, the task is to place one or more facilities so as to minimize the sum of the agent costs for accessing their nearest facility. Heretofore, in the strategic version, agent locations have been assumed to be private, while their cost measures have been public and identical.For the most part, the cost measure has been the distance to the nearest facility.However, in multiple natural settings, such as placing a firehouse or a school, this modeling does not appear to be a good fit. For it seems natural that the agent locations would be known, but their costs might be private information. In addition, for these types of settings, agents may well want the nearest facility to be at the right distance: near, but not too near. This is captured by the doubly-peaked cost introduced by Filos-Ratsikas et al. (AAMAS 2017).In this paper, we re-examine the facility location problem from this perspective: known agent locations and private preferred distances to the nearest facility.We then give lower and upper bounds on achievable approximations, focusing on the problem in 1D, and in 2D with an $L_1$ distance measure."
2505.18287,"In a recently introduced model of successive committee elections (Bredereck et al., AAAI-20) for a given set of ordinal or approval preferences one aims to find a sequence of a given length of ""best"" same-size committees such that each candidate is a member of a limited number of consecutive committees. However, the practical usability of this model remains limited, as the described task turns out to be NP-hard for most selection criteria already for seeking committees of size three. Non-trivial or somewhat efficient algorithms for these cases are lacking too. Motivated by a desire to unlock the full potential of the described temporal model of committee elections, we devise (parameterized) algorithms that effectively solve the mentioned hard cases in realistic scenarios of a moderate number of candidates or of a limited time horizon."
2505.19134,"Human-annotated data plays a vital role in training large language models (LLMs), such as supervised fine-tuning and human preference alignment. However, it is not guaranteed that paid human annotators produce high-quality data. In this paper, we study how to incentivize human annotators to do so. We start from a principal-agent model to model the dynamics between the company (the principal) and the annotator (the agent), where the principal can only monitor the annotation quality by examining $n$ samples. We investigate the maximum likelihood estimators (MLE) and the corresponding hypothesis testing to incentivize annotators: the agent is given a bonus if the MLE passes the test. By analyzing the variance of the outcome, we show that the strategic behavior of the agent makes the hypothesis testing very different from traditional ones: Unlike the exponential rate proved by the large deviation theory, the principal-agent model's hypothesis testing rate is of $\Theta(1/\sqrt{n \log n})$. Our theory implies two criteria for the \emph{golden questions} to monitor the performance of the annotators: they should be of (1) high certainty and (2) similar format to normal ones. In that light, we select a set of golden questions in human preference data. By doing incentive-compatible experiments, we find out that the annotators' behavior is better revealed by those golden questions, compared to traditional survey techniques such as instructed manipulation checks."
2505.19298,"As markets have digitized, the number of tradable products has skyrocketed. Algorithmically constructed portfolios of these assets now dominate public and private markets, resulting in a combinatorial explosion of tradable assets. In this paper, we provide a simple means to compute market clearing prices for semi-fungible assets which have a partial ordering between them. Such assets are increasingly found in traditional markets (bonds, commodities, ETFs), private markets (private credit, compute markets), and in decentralized finance. We formulate the market clearing problem as an optimization problem over a directed acyclic graph that represents participant preferences. Subsequently, we use convex duality to efficiently estimate market clearing prices, which correspond to particular dual variables. We then describe dominant strategy incentive compatible payment and allocation rules for clearing these markets. We conclude with examples of how this framework can construct prices for a variety of algorithmically constructed, semi-fungible portfolios of practical importance."
2505.19338,"In the evolving digital landscape, it is crucial to study the dynamics of cyberattacks and defences. This study uses an Evolutionary Game Theory (EGT) framework to investigate the evolutionary dynamics of attacks and defences in cyberspace. We develop a two-population asymmetric game between attacker and defender to capture the essential factors of costs, potential benefits, and the probability of successful defences. Through mathematical analysis and numerical simulations, we find that systems with high defence intensities show stability with minimal attack frequencies, whereas low-defence environments show instability, and are vulnerable to attacks. Furthermore, we find five equilibria, where the strategy pair always defend and attack emerged as the most likely stable state as cyber domain is characterised by a continuous battle between defenders and attackers. Our theoretical findings align with real-world data from past cyber incidents, demonstrating the interdisciplinary impact, such as fraud detection, risk management and cybersecurity decision-making. Overall, our analysis suggests that adaptive cybersecurity strategies based on EGT can improve resource allocation, enhance system resilience, and reduce the overall risk of cyberattacks. By incorporating real-world data, this study demonstrates the applicability of EGT in addressing the evolving nature of cyber threats and the need for secure digital ecosystems through strategic planning and proactive defence measures."
2505.19453,"Two sellers compete to sell identical products to a single buyer. Each seller chooses an arbitrary mechanism, possibly involving lotteries, to sell their product. The utility-maximizing buyer can choose to participate in one or both mechanisms, resolving them in either order. Given a common prior over buyer values, how should the sellers design their mechanisms to maximize their respective revenues?We first consider a Stackelberg setting where one seller (Alice) commits to her mechanism and the other seller (Bob) best-responds. We show how to construct a simple and approximately-optimal single-lottery mechanism for Alice that guarantees her a quarter of the optimal monopolist's revenue, for any regular distribution. Along the way we prove a structural result: for any single-lottery mechanism of Alice, there will always be a best response mechanism for Bob consisting of a single take-it-or-leave-it price. We also show that no mechanism (single-lottery or otherwise) can guarantee Alice more than a 1/e fraction of the monopolist revenue. Finally, we show that our approximation result does not extend to Nash equilibrium: there exist instances in which a monopolist could extract full surplus, but neither competing seller obtains positive revenue at any equilibrium choice of mechanisms."
2505.19537,"Since Polyak's pioneering work, heavy ball (HB) momentum has been widely studied in minimization. However, its role in min-max games remains largely unexplored. As a key component of practical min-max algorithms like Adam, this gap limits their effectiveness. In this paper, we present a continuous-time analysis for HB with simultaneous and alternating update schemes in min-max games. Locally, we prove smaller momentum enhances algorithmic stability by enabling local convergence across a wider range of step sizes, with alternating updates generally converging faster. Globally, we study the implicit regularization of HB, and find smaller momentum guides algorithms trajectories towards shallower slope regions of the loss landscapes, with alternating updates amplifying this effect. Surprisingly, all these phenomena differ from those observed in minimization, where larger momentum yields similar effects. Our results reveal fundamental differences between HB in min-max games and minimization, and numerical experiments further validate our theoretical results."
2505.19556,"This paper presents a comprehensive framework for transaction posting and pricing in Layer 2 (L2) blockchain systems, focusing on challenges stemming from fluctuating Layer 1 (L1) gas fees and the congestion issues within L2 networks. Existing methods have focused on the problem of optimal posting strategies to L1 in isolation, without simultaneously considering the L2 fee mechanism. In contrast, our work offers a unified approach that addresses the complex interplay between transaction queue dynamics, L1 cost variability, and user responses to L2 fees. We contribute by (1) formulating a dynamic model that integrates both posting and pricing strategies, capturing the interplay between L1 gas price fluctuations and L2 queue management, (2) deriving an optimal threshold-based posting policy that guides L2 sequencers in managing transactions based on queue length and current L1 conditions, and (3) establishing theoretical foundations for a dynamic L2 fee mechanism that balances cost recovery with congestion control. We validate our framework through simulations."
2505.19961,"We consider fair allocations of indivisible goods to agents with general monotone valuations. We observe that it is useful to introduce a new share-based fairness notion, the {\em residual maximin share} (RMMS). This share is {\em feasible} and {\em self maximizing}. Its value is at least as large as the MXS for monotone valuations, and at least as large as $\frac{2}{3}$-MMS for additive valuations. Known techniques easily imply the existence of partial allocations that are both RMMS and EFX, and complete allocations that are both RMMS and EFL. This unifies and somewhat improves upon several different results from previous papers."
2505.20627,"Nash Learning from Human Feedback is a game-theoretic framework for aligning large language models (LLMs) with human preferences by modeling learning as a two-player zero-sum game. However, using raw preference as the payoff in the game highly limits the potential of the game-theoretic LLM alignment framework. In this paper, we systematically study using what choices of payoff based on the pairwise human preferences can yield desirable alignment properties. We establish necessary and sufficient conditions for Condorcet consistency, diversity through mixed strategies, and Smith consistency. These results provide a theoretical foundation for the robustness of game-theoretic LLM alignment. Further, we show the impossibility of preference matching -- i.e., no smooth and learnable mappings of pairwise preferences can guarantee a unique Nash equilibrium that matches a target policy, even under standard assumptions like the Bradley-Terry-Luce model. This result highlights the fundamental limitation of game-theoretic LLM alignment."
2505.21122,"We perform a comprehensive analysis of extensions of the Shapley value to groups. We propose a new, natural extension called the Union Shapley Value, which assesses a group's contribution by examining the impact of its removal from the game. This intuition is formalized through two axiomatic characterizations, closely related to existing axiomatizations of the Shapley value. Furthermore, we characterize the class of group semivalues and identify a dual approach that measures synergy instead of the value of a coalition. Our analysis reveals a novel connection between several group values previously proposed in the literature."
2505.21244,"Cyber deception is an emerging proactive defense strategy to counter increasingly sophisticated attacks such as Advanced Persistent Threats (APTs) by misleading and distracting attackers from critical assets. However, since deception techniques incur costs and may lose effectiveness over time, defenders must strategically time and select them to adapt to the dynamic system and the attacker's responses. In this study, we propose a Stackelberg game-based framework to design strategic timing for cyber deception: the lower tactical layer (follower) captures the evolving attacker-defender dynamics under a given deception through a one-sided information Markov game, while the upper strategic layer (leader) employs a stopping-time decision process to optimize the timing and selection of deception techniques. We also introduce a computational algorithm that integrates dynamic programming and belief-state updates to account for the attacker's adaptive behavior and limited deception resources. Numerical experiments validate the framework, showing that strategically timed deceptions can enhance the defender's expected utility and reduce the risk of asset compromise compared to baseline strategies."
2505.21286,"Agentic AI, often powered by large language models (LLMs), is becoming increasingly popular and adopted to support autonomous reasoning, decision-making, and task execution across various domains. While agentic AI holds great promise, its deployment as services for easy access raises critical challenges in pricing, due to high infrastructure and computation costs, multi-dimensional and task-dependent Quality of Service (QoS), and growing concerns around liability in high-stakes applications. In this work, we propose PACT, a Pricing framework for cloud-based Agentic AI services through a Contract-Theoretic approach, which models QoS along both objective (e.g., response time) and subjective (e.g., user satisfaction) dimensions. PACT accounts for computational, infrastructure, and potential liability costs for the service provider, while ensuring incentive compatibility and individual rationality for the user under information asymmetry. Through contract-based selection, users receive tailored service offerings aligned with their needs. Numerical evaluations demonstrate that PACT improves QoS alignment between users and providers and offers a scalable, liable approach to pricing agentic AI services in the future."
2505.21627,"State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we develop an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion. Crucially, we demonstrate that the cost of running the algorithm is lower than the additional revenue from overcharging users, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, we show that, to eliminate the financial incentive to strategize, a pricing mechanism must price tokens linearly on their character count. While this makes a provider's profit margin vary across tokens, we introduce a simple prescription under which the provider who adopts such an incentive-compatible pricing mechanism can maintain the average profit margin they had under the pay-per-token pricing mechanism. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform."
2505.21685,"Proof-of-Work (PoW) consensus is traditionally analyzed under the assumption that all miners incur similar costs per unit of computational effort. In reality, costs vary due to factors such as regional electricity cost differences and access to specialized hardware. These variations in mining costs become even more pronounced in the emerging paradigm of \emph{Proof-of-Useful-Work} (PoUW), where miners can earn additional \emph{external} rewards by performing beneficial computations, such as Artificial Intelligence (AI) training and inference workloads.Continuing the work of Fiat et al., who investigate equilibrium dynamics of PoW consensus under heterogeneous cost structures due to varying energy costs, we expand their model to also consider external rewards. We develop a theoretical framework to model miner behavior in such conditions and analyze the resulting equilibrium. Our findings suggest that in some cases, miners with access to external incentives will optimize profitability by concentrating their useful tasks in a single block. We also explore the implications of external rewards for decentralization, modeling it as the Shannon entropy of computational effort distribution among participants.Empirical evidence supports many of our assumptions, indicating that AI training and inference workloads, when reused for consensus, can retain security comparable to Bitcoin while dramatically reducing computational costs and environmental waste."
2505.22174,"We study an online fair division setting, where goods arrive one at a time and there is a fixed set of $n$ agents, each of whom has an additive valuation function over the goods. Once a good appears, the value each agent has for it is revealed and it must be allocated immediately and irrevocably to one of the agents. It is known that without any assumptions about the values being severely restricted or coming from a distribution, very strong impossibility results hold in this setting. To bypass the latter, we turn our attention to instances where the valuation functions are restricted. In particular, we study personalized $2$-value instances, where there are only two possible values each agent may have for each good, possibly different across agents, and we show how to obtain worst case guarantees with respect to well-known fairness notions, such as maximin share fairness and envy-freeness up to one (or two) good(s). We suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at every time step and show that this is the best possible any deterministic algorithm can achieve if one cares about every single time step; nevertheless, eventually the allocation constructed by our algorithm becomes a $1/4$-MMS allocation. To achieve this, the algorithm implicitly maintains a fragile system of priority levels for all agents. Further, we show that, by allowing some limited access to future information, it is possible to have stronger results with less involved approaches. By knowing the values of goods for $n-1$ time steps into the future, we design a matching-based algorithm that achieves an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$ allocation. Finally, we show that our results allow us to get the first nontrivial guarantees for additive instances in which the ratio of the maximum over the minimum value an agent has for a good is bounded."
2505.22513,"We study proportional representation in the framework of temporal voting with approval ballots. Prior work adapted basic proportional representation concepts -- justified representation (JR), proportional JR (PJR), and extended JR (EJR) -- from the multiwinner setting to the temporal setting. Our work introduces and examines ways of going beyond EJR. Specifically, we consider stronger variants of JR, PJR, and EJR, and introduce temporal adaptations of more demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR (FPJR), and the Core. For each of these concepts, we investigate its existence and study its relationship to existing notions, thereby establishing a rich hierarchy of proportionality concepts. Notably, we show that two of our proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable in every temporal election."
2505.22939,"A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions."
2505.22979,"An important challenge in non-cooperative game theory is coordinating on a single (approximate) equilibrium from many possibilities - a challenge that becomes even more complex when players hold private information. Recommender mechanisms tackle this problem by recommending strategies to players based on their reported type profiles. A key consideration in such mechanisms is to ensure that players are incentivized to participate, report their private information truthfully, and follow the recommendations. While previous work has focused on designing recommender mechanisms for one-shot and extensive-form games, these approaches cannot be effectively applied to stochastic games, particularly if we constrain recommendations to be Markov stationary policies. To bridge this gap, we introduce a novel bi-level reinforcement learning approach for automatically designing recommender mechanisms in Bayesian stochastic games. Our method produces a mechanism represented by a parametric function (such as a neural network), and is therefore highly efficient at execution time. Experimental results on two repeated and two stochastic games demonstrate that our approach achieves social welfare levels competitive with cooperative multi-agent reinforcement learning baselines, while also providing significantly improved incentive properties."
2505.22999,"In numerous online selection problems, decision-makers (DMs) must allocate on the fly limited resources to customers with uncertain values. The DM faces the tension between allocating resources to currently observed values and saving them for potentially better, unobserved values in the future. Addressing this tension becomes more demanding if an uncertain disruption occurs while serving customers. Without any disruption, the DM gets access to the capacity information to serve customers throughout the time horizon. However, with uncertain disruption, the DM must act more cautiously due to risk of running out of capacity abruptly or misusing the resources. Motivated by this tension, we introduce the Online Selection with Uncertain Disruption (OS-UD) problem. In OS-UD, a DM sequentially observes n non-negative values drawn from a common distribution and must commit to select or reject each value in real time, without revisiting past values. The disruption is modeled as a Bernoulli random variable with probability p each time DM selects a value. We aim to design an online algorithm that maximizes the expected sum of selected values before a disruption occurs, if any. We evaluate online algorithms using the competitive ratio. Using a quantile-based approach, we devise a non-adaptive single-threshold algorithm that attains a competitive ratio of at least 1-1/e, and an adaptive threshold algorithm characterized by a sequence of non-increasing thresholds that attains an asymptotic competitive ratio of at least 0.745. Both of these results are worst-case optimal within their corresponding class of algorithms."
2505.23124,"We initiate the study of a repeated principal-agent problem over a finite horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of agents arriving in an adversarial order. At each round, the principal strategically chooses one of the $N$ arms to incentivize for an arriving agent of unknown type. The agent then chooses an arm based on its own utility and the provided incentive, and the principal receives a corresponding reward. The objective is to minimize regret against the best incentive in hindsight. Without prior knowledge of agent behavior, we show that the problem becomes intractable, leading to linear regret. We analyze two key settings where sublinear regret is achievable. In the first setting, the principal knows the arm each agent type would select greedily for any given incentive. Under this setting, we propose an algorithm that achieves a regret bound of $O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to a $\log K$ factor. In the second setting, an agent's response varies smoothly with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under this setting, we show that there is an algorithm with a regret bound of $\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to logarithmic factors. Finally, we extend our algorithmic results for both settings by allowing the principal to incentivize multiple arms simultaneously in each round."
2505.23251,"We study the fair allocation problem of indivisible items with subsidy. In this paper, we focus on the notion of fairness - equitability (EQ), which requires that items be allocated such that all agents value the bundle they receive equally. First, we study the upper bounds of the minimum required subsidy to achieve EQ in different item settings and provide the corresponding lower bounds. Second, we consider the bounded subsidy for achieving EQ and another popular notion of fairness - envy-freeness (EF), and give a characterization of allocations that can achieve both EQ and EF. Finally, we analyze the bounds of subsidy of allocations achieving fairness and efficiency (utilitarian social welfare or Nash welfare) and design several polynomial-time algorithms to compute the desired allocation."
2505.24321,"In an online fair allocation problem, a sequence of indivisible items arrives online and needs to be allocated to offline agents immediately and irrevocably. In our paper, we study the online allocation of either goods or chores. We employ popular fairness notions, including envy-freeness up to one item (EF1) and maximin share fairness (MMS) to capture fairness, and utilitarian social welfare (USW) to measure efficiency. For both settings of items, we present a series of positive results regarding the existence of fair and efficient allocations with widely studied classes of additive binary and personalized bi-valued valuation/cost functions. Furthermore, we complement our results by constructing counterexamples to establish our results as among the best guarantees possible."
2505.24482,"Blockchain protocols incentivize participation through monetary rewards, assuming rational actors behave honestly to maximize their gains. However, attackers may attempt to harm others even at personal cost. These denial of profit attacks aim to reduce the rewards of honest participants, potentially forcing them out of the system. While existing work has largely focused on the profitability of attacks, they often neglect the potential harm inflicted on the victim, which can be significant even when the attacker gains little or nothing.This paper introduces a framework to quantify denial of profit attacks by measuring both attacker cost and victim loss. We model these attacks as a game and introduce relevant metrics to quantify these attacks. We then focus on committee-based blockchains and model vote collection as a game. We show that in the vote collection game, disincentivizing one denial of profit attack will make another attack more appealing, and therefore, attacks have to be balanced. We apply our framework to analyze real-world reward mechanisms in Ethereum and Cosmos. Our framework reveals imbalances in Cosmos that can make correct behavior suboptimal in practice. While Ethereum provides stronger protections, our framework shows that it is also not complete, and we propose alternative parameter settings to improve the balance between attacks. Our findings highlight the need for better-balanced reward designs to defend against denial of profit attacks."
2505.24503,"We study the problem of fairly allocating indivisible goods to agents in an online setting, where goods arrive sequentially and must be allocated irrevocably to agents. Focusing on the popular fairness notions of envy-freeness, proportionality, and maximin share fairness (and their approximate variants), we ask how the availability of information on future goods influences the existence and approximability of fair allocations. In the absence of any such information, we establish strong impossibility results, demonstrating the inherent difficulty of achieving even approximate fairness guarantees. In contrast, we demonstrate that knowledge of additional information -- such as aggregate of each agent's total valuations (equivalently, normalized valuations) or the multiset of future goods values (frequency predictions) -- would enable the design of fairer online algorithms. Given normalization information, we propose an algorithm that achieves stronger fairness guarantees than previously known results. Given frequency predictions, we introduce a meta-algorithm that leverages frequency predictions to match the best-known offline guarantees for a broad class of ''share-based'' fairness notions. Our complementary impossibility results in each setting underscore both the limitations imposed by uncertainty about future goods and the potential of leveraging structured information to achieve fairer outcomes in online fair division."
2505.24624,"Augmenting the input of algorithms with predictions is an algorithm design paradigm that suggests leveraging a (possibly erroneous) prediction to improve worst-case performance guarantees when the prediction is perfect (consistency), while also providing a performance guarantee when the prediction fails (robustness). Recently, Xu and Lu [2022] and Agrawal et al. [2024] proposed to consider settings with strategic agents under this framework. In this paper, we initiate the study of budget-feasible mechanism design with predictions. These mechanisms model a procurement auction scenario in which an auctioneer (buyer) with a strict budget constraint seeks to purchase goods or services from a set of strategic agents, so as to maximize her own valuation function. We focus on the online version of the problem where the arrival order of agents is random. We design mechanisms that are truthful, budget-feasible, and achieve a significantly improved competitive ratio for both monotone and non-monotone submodular valuation functions compared to their state-of-the-art counterparts without predictions. Our results assume access to a prediction for the value of the optimal solution to the offline problem. We complement our positive results by showing that for the offline version of the problem, access to predictions is mostly ineffective in improving approximation guarantees."
2506.0018,"The independent chip model (ICM) forms a cornerstone of all modern poker tournament strategy. However, despite its prominence, the ICM's performance in the real world has not been sufficiently scrutinized, especially at a large scale. In this paper, we introduce our new dataset of poker tournaments, consisting of results of over ten thousand events. Then, using this dataset, we perform two experiments as part of a large-scale empirical validation of the ICM. First, we verify that the ICM performs more accurately than a baseline we propose. Second, we obtain empirical evidence of the ICM underestimating the performances of players with larger stacks while overestimating those who are short-stacked. Our contributions may be useful to future researchers developing new algorithms for estimating a player's value in poker tournaments."
2506.00282,"In online auctions, fraudulent behaviors such as shill bidding pose significant risks. This paper presents a conceptual framework that applies dynamic, behavior-based penalties to deter auction fraud using blockchain smart contracts. Unlike traditional post-auction detection methods, this approach prevents manipulation in real-time by introducing an economic disincentive system where penalty severity scales with suspicious bidding patterns. The framework employs the proposed Bid Shill Score (BSS) to evaluate nine distinct bidding behaviors, dynamically adjusting the penalty fees to make fraudulent activity financially unaffordable while providing fair competition.The system is implemented within a decentralized English auction on the Ethereum blockchain, demonstrating how smart contracts enforce transparent auction rules without trusted intermediaries. Simulations confirm the effectiveness of the proposed model: the dynamic penalty mechanism reduces the profitability of shill bidding while keeping penalties low for honest bidders. Performance evaluation shows that the system introduces only moderate gas and latency overhead, keeping transaction costs and response times within practical bounds for real-world use. The approach provides a practical method for behaviour-based fraud prevention in decentralised systems where trust cannot be assumed."
2506.00554,"The Deferred Acceptance (DA) algorithm is an elegant procedure for finding a stable matching in two-sided matching markets. It ensures that no pair of agents prefers each other to their matched partners. In this work, we initiate the study of two-sided manipulations in matching markets as non-cooperative games. We introduce the accomplice manipulation game, where a man misreports to help a specific woman obtain a better partner, whenever possible. We provide a polynomial time algorithm for finding a pure strategy Nash equilibrium (NE) and show that our algorithm always yields a stable matching - although not every Nash equilibrium corresponds to a stable matching. Additionally, we show how our analytical techniques for the accomplice manipulation game can be applied to other manipulation games in matching markets, such as one-for-many and the standard self-manipulation games. We complement our theoretical findings with empirical evaluations of different properties of the resulting NE, such as the welfare of the agents."
2506.00627,"We study how partial information about scoring rules affects fairness in strategic learning settings. In strategic learning, a learner deploys a scoring rule, and agents respond strategically by modifying their features -- at some cost -- to improve their outcomes. However, in our work, agents do not observe the scoring rule directly; instead, they receive a noisy signal of said rule. We consider two different agent models: (i) naive agents, who take the noisy signal at face value, and (ii) Bayesian agents, who update a prior belief based on the signal.Our goal is to understand how disparities in outcomes arise between groups that differ in their costs of feature modification, and how these disparities vary with the level of transparency of the learner's rule. For naive agents, we show that utility disparities can grow unboundedly with noise, and that the group with lower costs can, perhaps counter-intuitively, be disproportionately harmed under limited transparency. In contrast, for Bayesian agents, disparities remain bounded. We provide a full characterization of disparities across groups as a function of the level of transparency and show that they can vary non-monotonically with noise; in particular, disparities are often minimized at intermediate levels of transparency. Finally, we extend our analysis to settings where groups differ not only in cost, but also in prior beliefs, and study how this asymmetry influences fairness."
2506.01178,"We study resource allocation in two-sided markets from a fundamental perspective and introduce a general modeling and algorithmic framework to effectively incorporate the complex and multidimensional aspects of fairness. Our main technical contribution is to show the existence of a range of near-feasible resource allocations parameterized in different model primitives to give flexibility when balancing the different policymaking requirements, allowing policy designers to fix these values according to the specific application. To construct our near-feasible allocations, we start from a fractional resource allocation and perform an iterative rounding procedure to get an integer allocation. We show a simple yet flexible and strong sufficient condition for the target feasibility deviations to guarantee that the rounding procedure succeeds, exhibiting the underlying trade-offs between market capacities, agents' demand, and fairness. To showcase our framework's modeling and algorithmic capabilities, we consider three prominent market design problems: school allocation, stable matching with couples, and political apportionment. In each of them, we obtain strengthened guarantees on the existence of near-feasible allocations capturing the corresponding fairness notions, such as proportionality, envy-freeness, and stability."
2506.01242,"Since the advent of AI, games have served as progress benchmarks. Meanwhile, imperfect-information variants of chess have existed for over a century, present extreme challenges, and have been the focus of significant AI research. Beyond calculation needed in regular chess, they require reasoning about information gathering, the opponent's knowledge, signaling, etc. The most popular variant, Fog of War (FoW) chess (aka. dark chess) is a recognized challenge problem in AI after superhuman performance was reached in no-limit Texas hold'em poker. We present Obscuro, the first superhuman AI for FoW chess. It introduces advances to search in imperfect-information games, enabling strong, scalable reasoning. Experiments against the prior state-of-the-art AI and human players -- including the world's best -- show that Obscuro is significantly stronger. FoW chess is the largest (by amount of imperfect information) turn-based game in which superhuman performance has been achieved and the largest game in which imperfect-information search has been successfully applied."
2506.01285,"Vertical Federated Learning (VFL)-based Traffic State Estimation (TSE) offers a promising approach for integrating vertically distributed traffic data from municipal authorities (MA) and mobility providers (MP) while safeguarding privacy. However, given the variations in MPs' data collection capabilities and the potential for MPs to underperform in data provision, we propose a reliable VFL-based TSE framework that ensures model reliability during training and operation. The proposed framework comprises two components: data provider selection and incentive mechanism design. Data provider selection is conducted in three stages to identify the most qualified MPs for VFL model training with the MA. First, the MA partitions the transportation network into road segments. Then, a mutual information (MI) model is trained for each segment to capture the relationship between data and labels. Finally, using a sampling strategy and the MI model, the MA assesses each MP's competence in data provision and selects the most qualified MP for each segment. For the incentive mechanism design, given the MA can leverage the MI mode to inspect the data quality of MP, we formulate the interaction between MA and MP as a supervision game model. Upon this, we devise a penalty-based incentive mechanism to inhibit the lazy probability of MP, thereby guaranteeing the utility of MA. Numerical simulation on real-world datasets showcased that our proposed framework augments the traffic flow and density prediction accuracy by 11.23\% and 23.15\% and elevates the utility of MA by 130$\sim$400\$ compared to the benchmark."
2506.01343,"We address an open problem on the computability of correlated equilibria in a variant of polymatrix where each player's utility is the maximum of their edge payoffs. We demonstrate that this max-variant game has the polynomial expectation property, and the results of Papadimitriou and Roughgarden can thus be applied. We propose ideas for extending these findings to other variants of polymatrix games, as well as briefly address the broader question of necessity for the polynomial expectation property when computing correlated equilibria."
2506.01685,"In the incentivized exploration model, a principal aims to explore and learn over time by interacting with a sequence of self-interested agents. It has been recently understood that the main challenge in designing incentive-compatible algorithms for this problem is to gather a moderate amount of initial data, after which one can obtain near-optimal regret via posterior sampling. With high-dimensional contexts, however, this \emph{initial exploration} phase requires exponential sample complexity in some cases, which prevents efficient learning unless initial data can be acquired exogenously. We show that these barriers to exploration disappear under mild geometric conditions on the set of available actions, in which case incentive-compatibility does not preclude regret-optimality. Namely, we consider the linear bandit model with actions in the Euclidean unit ball, and give an incentive-compatible exploration algorithm with sample complexity that scales polynomially with the dimension and other parameters."
2506.01899,"Correlated equilibria -- and their generalization $\Phi$-equilibria -- are a fundamental object of study in game theory, offering a more tractable alternative to Nash equilibria in multi-player settings. While computational aspects of equilibrium computation are well-understood in some settings, fundamental questions are still open in generalized games, that is, games in which the set of strategies allowed to each player depends on the other players' strategies. These classes of games model fundamental settings in economics and have been a cornerstone of economics research since the seminal paper of Arrow and Debreu [1954]. Recently, there has been growing interest, both in economics and in computer science, in studying correlated equilibria in generalized games. It is known that finding a social welfare maximizing correlated equilibrium in generalized games is NP-hard. However, the existence of efficient algorithms to find any equilibrium remains an important open question. In this paper, we answer this question negatively, showing that this problem is PPAD-complete."
2506.01927,"Game-theoretic agents must make plans that optimally gather information about their opponents. These problems are modeled by partially observable stochastic games (POSGs), but planning in fully continuous POSGs is intractable without heavy offline computation or assumptions on the order of belief maintained by each player. We formulate a finite history/horizon refinement of POSGs which admits competitive information gathering behavior in trajectory space, and through a series of approximations, we present an online method for computing rational trajectory plans in these games which leverages particle-based estimations of the joint state space and performs stochastic gradient play. We also provide the necessary adjustments required to deploy this method on individual agents. The method is tested in continuous pursuit-evasion and warehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more complex environments with visual and physical obstacles), demonstrating evidence of active information gathering and outperforming passive competitors."
2506.01936,"Strategic classification addresses a learning problem where a decision-maker implements a classifier over agents who may manipulate their features in order to receive favorable predictions. In the standard model of online strategic classification, in each round, the decision-maker implements and publicly reveals a classifier, after which agents perfectly best respond based on this knowledge. However, in practice, whether to disclose the classifier is often debated -- some decision-makers believe that hiding the classifier can prevent misclassification errors caused by manipulation.In this paper, we formally examine how limiting the agents' access to the current classifier affects the decision-maker's performance. Specifically, we consider an extended online strategic classification setting where agents lack direct knowledge about the current classifier and instead manipulate based on a weighted average of historically implemented classifiers. Our main result shows that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or $k_{\text{in}}$ times more mistakes compared to the full-knowledge setting, where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph (representing how many distinct feature vectors can be manipulated to appear as a single one), and $\gamma$ is the discount factor indicating agents' memory of past classifiers. Our results demonstrate how withholding access to the classifier can backfire and degrade the decision-maker's performance in online strategic classification."
2506.02193,"In many parts of the world - particularly in developing countries - the demand for electricity exceeds the available supply. In such cases, it is impossible to provide electricity to all households simultaneously. This raises a fundamental question: how should electricity be allocated fairly? In this paper, we explore this question through the lens of egalitarianism - a principle that emphasizes equality by prioritizing the welfare of the worst-off households. One natural rule that aligns with this principle is to maximize the egalitarian welfare - the smallest utility across all households. We show that computing such an allocation is NP-hard, even under strong simplifying assumptions. Leximin is a stronger fairness notion that generalizes the egalitarian welfare: it also requires to maximize the smallest utility, but then, subject to that, the second-smallest, then the third, and so on. The hardness results extends directly to leximin as well. Despite this, we present a Fully Polynomial-Time Approximation Scheme (FPTAS) for leximin in the special case where the network connectivity graph is a tree. This means that we can efficiently approximate leximin - and, in particular, the egalitarian welfare - to any desired level of accuracy."
2506.02259,"Eliciting reliable human feedback is essential for many machine learning tasks, such as learning from noisy labels and aligning AI systems with human preferences. Peer prediction mechanisms incentivize truthful reporting without ground truth verification by scoring agents based on correlations with peers. Traditional mechanisms, which ensure that truth-telling maximizes the expected scores in equilibrium, can elicit honest information while assuming agents' utilities are linear functions of their scores. However, in practice, non-linear payment rules are usually preferred, or agents' utilities are inherently non-linear.We propose stochastically dominant truthfulness (SD-truthfulness) as a stronger guarantee: the score distribution of truth-telling stochastically dominates all other strategies, incentivizing truthful reporting for a wide range of monotone utility functions. Our first observation is that no existing peer prediction mechanism naturally satisfies this criterion without strong assumptions. A simple solution -- rounding scores into binary lotteries -- can enforce SD-truthfulness, but often degrades sensitivity, a key property related to fairness and statistical efficiency. We demonstrate how a more careful application of rounding can better preserve sensitivity. Furthermore, we introduce a new enforced agreement (EA) mechanism that is theoretically guaranteed to be SD-truthful in binary-signal settings under mild assumptions, and empirically achieves the highest sensitivity among all known SD-truthful mechanisms."
2506.02284,"We study the problem of learning the optimal item pricing for a unit-demand buyer with independent item values, and the learner has query access to the buyer's value distributions. We consider two common query models in the literature: the sample access model where the learner can obtain a sample of each item value, and the pricing query model where the learner can set a price for an item and obtain a binary signal on whether the sampled value of the item is greater than our proposed price. In this work, we give nearly tight sample complexity and pricing query complexity of the unit-demand pricing problem."
2506.02435,"With the advancement of machine learning, an increasing number of studies are employing automated mechanism design (AMD) methods for optimal auction design. However, all previous AMD architectures designed to generate optimal mechanisms that satisfy near dominant strategy incentive compatibility (DSIC) fail to achieve deterministic allocation, and some also lack anonymity, thereby impacting the efficiency and fairness of advertising allocation. This has resulted in a notable discrepancy between the previous AMD architectures for generating near-DSIC optimal mechanisms and the demands of real-world advertising scenarios. In this paper, we prove that in all online advertising scenarios, previous non-deterministic allocation methods lead to the non-existence of feasible solutions, resulting in a gap between the rounded solution and the optimal solution. Furthermore, we propose JTransNet, a transformer-based neural network architecture, designed for optimal deterministic-allocation and anonymous joint auction design. Although the deterministic allocation module in JTransNet is designed for the latest joint auction scenarios, it can be applied to other non-deterministic AMD architectures with minor modifications. Additionally, our offline and online data experiments demonstrate that, in joint auction scenarios, JTransNet significantly outperforms baseline methods in terms of platform revenue, resulting in a substantial increase in platform earnings."
2506.0252,"Generalized Nash equilibrium problems with mixed-integer variables form an important class of games in which each player solves a mixed-integer optimization problem with respect to her own variables and the strategy space of each player depends on the strategies chosen by the rival players. In this work, we introduce a branch-and-cut algorithm to compute exact pure Nash equilibria for different classes of such mixed-integer games. The main idea is to reformulate the equilibrium problem as a suitable bilevel problem based on the Nikaido--Isoda function of the game. The proposed branch-and-cut method is applicable to generalized Nash equilibrium problems under quite mild assumptions. Depending on the specific setting, we use tailored equilibrium or intersection cuts. The latter are well-known in mixed-integer linear optimization and we adapt them to the game setting. We prove finite termination and correctness of the algorithm and present some first numerical results for two different types of knapsack games and another game based on capacitated flow problems."
2506.02603,"This paper provides an efficient computational scheme to handle general security games from an adversarial risk analysis perspective. Two cases in relation to single-stage and multi-stage simultaneous defend-attack games motivate our approach to general setups which uses bi-agent influence diagrams as underlying problem structure and augmented probability simulation as core computational methodology. Theoretical convergence and numerical, modeling, and implementation issues are thoroughly discussed. A disinformation war case study illustrates the relevance of the proposed approach."
2506.02655,"This paper investigates the role of mediators in Bayesian games by examining their impact on social welfare through the price of anarchy (PoA) and price of stability (PoS). Mediators can communicate with players to guide them toward equilibria of varying quality, and different communication protocols lead to a variety of equilibrium concepts collectively known as Bayes (coarse) correlated equilibria. To analyze these equilibrium concepts, we consider a general class of Bayesian games with submodular social welfare, which naturally extends valid utility games and their variant, basic utility games. These frameworks, introduced by Vetta (2002), have been developed to analyze the social welfare guarantees of equilibria in games such as competitive facility location, influence maximization, and other resource allocation problems.We provide upper and lower bounds on the PoA and PoS for a broad class of Bayes (coarse) correlated equilibria. Central to our analysis is the strategy representability gap, which measures the multiplicative gap between the optimal social welfare achievable with and without knowledge of other players' types. For monotone submodular social welfare functions, we show that this gap is $1-1/\mathrm{e}$ for independent priors and $\Theta(1/\sqrt{n})$ for correlated priors, where $n$ is the number of players. These bounds directly lead to upper and lower bounds on the PoA and PoS for various equilibrium concepts, while we also derive improved bounds for specific concepts by developing smoothness arguments. Notably, we identify a fundamental gap in the PoA and PoS across different classes of Bayes correlated equilibria, highlighting essential distinctions among these concepts."
2506.02852,"Proportional response is a well-established distributed algorithm which has been shown to converge to competitive equilibria in both Fisher and Arrow-Debreu markets, for various sub-families of homogeneous utilities, including linear and constant elasticity of substitution utilities. We propose a natural generalization of proportional response for gross substitutes utilities, and prove that it converges to competitive equilibria in Fisher markets. This is the first convergence result of a proportional response style dynamics in Fisher markets for utilities beyond the homogeneous utilities covered by the Eisenberg-Gale convex program. We show an empirical convergence rate of $O(1/T)$ for the prices. Furthermore, we show that the allocations of a lazy version of the generalized proportional response dynamics converge to competitive equilibria in Arrow-Debreu markets."
2506.03001,"Decentralized exchanges (DEXs) are crucial to decentralized finance (DeFi) as they enable trading without intermediaries. However, they face challenges like impermanent loss (IL), where liquidity providers (LPs) see their assets' value change unfavorably within a liquidity pool compared to outside it. To tackle these issues, we propose dynamic fee mechanisms over traditional fixed-fee structures used in automated market makers (AMM). Our solution includes asymmetric fees via block-adaptive, deal-adaptive, and the ""ideal but unattainable"" oracle-based fee algorithm, utilizing all data available to arbitrageurs to mitigate IL. We developed a simulation-based framework to compare these fee algorithms systematically. This framework replicates trading on a DEX, considering both informed and uninformed users and a psychological relative loss factor. Results show that adaptive algorithms outperform fixed-fee baselines in reducing IL while maintaining trading activity among uninformed users. Additionally, insights from oracle-based performance underscore the potential of dynamic fee strategies to lower IL, boost LP profitability, and enhance overall market efficiency."
2506.03102,"As AI technologies improve, people are increasingly willing to delegate tasks to AI agents. In many cases, the human decision-maker chooses whether to delegate to an AI agent based on properties of the specific instance of the decision-making problem they are facing. Since humans typically lack full awareness of all the factors relevant to this choice for a given decision-making instance, they perform a kind of categorization by treating indistinguishable instances -- those that have the same observable features -- as the same. In this paper, we define the problem of designing the optimal algorithmic delegate in the presence of categories. This is an important dimension in the design of algorithms to work with humans, since we show that the optimal delegate can be an arbitrarily better teammate than the optimal standalone algorithmic agent. The solution to this optimal delegation problem is not obvious: we discover that this problem is fundamentally combinatorial, and illustrate the complex relationship between the optimal design and the properties of the decision-making task even in simple settings. Indeed, we show that finding the optimal delegate is computationally hard in general. However, we are able to find efficient algorithms for producing the optimal delegate in several broad cases of the problem, including when the optimal action may be decomposed into functions of features observed by the human and the algorithm. Finally, we run computational experiments to simulate a designer updating an algorithmic delegate over time to be optimized for when it is actually adopted by users, and show that while this process does not recover the optimal delegate in general, the resulting delegate often performs quite well."
2506.03309,"We consider an extension to the classic position auctions in which sponsored creatives can be added within AI generated content rather than shown in predefined slots. New challenges arise from the natural requirement that sponsored creatives should smoothly fit into the context. With the help of advanced LLM technologies, it becomes viable to accurately estimate the benefits of adding each individual sponsored creatives into each potential positions within the AI generated content by properly taking the context into account. Therefore, we assume one click-through rate estimation for each position-creative pair, rather than one uniform estimation for each sponsored creative across all positions in classic settings. As a result, the underlying optimization becomes a general matching problem, thus the substitution effects should be treated more carefully compared to standard position auction settings, where the slots are independent with each other.In this work, we formalize a concrete mathematical model of the extended position auction problem and study the welfare-maximization and revenue-maximization mechanism design problem. Formally, we consider two different user behavior models and solve the mechanism design problems therein respectively. For the Multinomial Logit (MNL) model, which is order-insensitive, we can efficiently implement the optimal mechanisms. For the cascade model, which is order-sensitive, we provide approximately optimal solutions."
2506.03464,"The convergence of online learning algorithms in games under self-play is a fundamental question in game theory and machine learning. Among various notions of convergence, last-iterate convergence is particularly desirable, as it reflects the actual decisions made by the learners and captures the day-to-day behavior of the learning dynamics. While many algorithms are known to converge in the average-iterate, achieving last-iterate convergence typically requires considerably more effort in both the design and the analysis of the algorithm. Somewhat surprisingly, we show in this paper that for a large family of games, there exists a simple black-box reduction that transforms the average iterates of an uncoupled learning dynamics into the last iterates of a new uncoupled learning dynamics, thus also providing a reduction from last-iterate convergence to average-iterate convergence. Our reduction applies to games where each player's utility is linear in both their own strategy and the joint strategy of all opponents. This family includes two-player bimatrix games and generalizations such as multi-player polymatrix games. By applying our reduction to the Optimistic Multiplicative Weights Update algorithm, we obtain new state-of-the-art last-iterate convergence rates for uncoupled learning dynamics in two-player zero-sum normal-form games: (1) an $O(\frac{\log d}{T})$ last-iterate convergence rate under gradient feedback, representing an exponential improvement in the dependence on the dimension $d$ (i.e., the maximum number of actions available to either player); and (2) an $\widetilde{O}(d^{\frac{1}{5}} T^{-\frac{1}{5}})$ last-iterate convergence rate under bandit feedback, improving upon the previous best rates of $\widetilde{O}(\sqrt{d} T^{-\frac{1}{8}})$ and $\widetilde{O}(\sqrt{d} T^{-\frac{1}{6}})$."
2506.04092,"Kidney Exchange Programmes (KEPs) facilitate the exchange of kidneys, and larger pools of recipient-donor pairs tend to yield proportionally more transplants, leading to the proposal of international KEPs (IKEPs). However, as studied by \citet{mincu2021ip}, practical limitations must be considered in IKEPs to ensure that countries remain willing to participate. Thus, we study IKEPs with country-specific parameters, represented by a tuple $\Gamma$, restricting the selected transplants to be feasible for the countries to conduct, e.g., imposing an upper limit on the number of consecutive exchanges within a country's borders. We provide a complete complexity dichotomy for the problem of finding a feasible (according to the constraints given by $\Gamma$) cycle packing with the maximum number of transplants, for every possible $\Gamma$. We also study the potential for countries to misreport their parameters to increase their allocation. As manipulation can harm the total number of transplants, we propose a novel individually rational and incentive compatible mechanism $\mathcal{M}_{\text{order}}$. We first give a theoretical approximation ratio for $\mathcal{M}_{\text{order}}$ in terms of the number of transplants, and show that the approximation ratio of $\mathcal{M}_{\text{order}}$ is asymptotically optimal. We then use simulations which suggest that, in practice, the performance of $\mathcal{M}_{\text{order}}$ is significantly better than this worst-case ratio."
2506.04525,"Users of social media platforms based on recommendation systems (RecSys) (e.g. TikTok, X, YouTube) strategically interact with platform content to influence future recommendations. On some such platforms, users have been documented to form large-scale grassroots movements encouraging others to purposefully interact with algorithmically suppressed content in order to ""boost"" its recommendation; we term this behavior user altruism. To capture this behavior, we study a game between users and a RecSys, where users provide the RecSys (potentially manipulated) preferences over the contents available to them, and the RecSys -- limited by data and computation constraints -- creates a low-rank approximation preference matrix, and ultimately provides each user her (approximately) most-preferred item. We compare the users' social welfare under truthful preference reporting and under a class of strategies capturing user altruism. In our theoretical analysis, we provide sufficient conditions to ensure strict increases in user social welfare under user altruism, and provide an algorithm to find an effective altruistic strategy. Interestingly, we show that for commonly assumed recommender utility functions, effectively altruistic strategies also improve the utility of the RecSys! We show that our results are robust to several model misspecifications, thus strengthening our conclusions. Our theoretical analysis is complemented by empirical results of effective altruistic strategies on the GoodReads dataset, and an online survey on how real-world users behave altruistically in RecSys. Overall, our findings serve as a proof-of-concept of the reasons why traditional RecSys may incentivize users to form collectives and/or follow altruistic strategies when interacting with them."
2506.04602,"The burgeoning growth of the esports and multiplayer online gaming community has highlighted the critical importance of evaluating the Most Valuable Player (MVP). The establishment of an explainable and practical MVP evaluation method is very challenging. In our study, we specifically focus on play-by-play data, which records related events during the game, such as assists and points. We aim to address the challenges by introducing a new MVP evaluation framework, denoted as \oursys, which leverages Shapley values. This approach encompasses feature processing, win-loss model training, Shapley value allocation, and MVP ranking determination based on players' contributions. Additionally, we optimize our algorithm to align with expert voting results from the perspective of causality. Finally, we substantiated the efficacy of our method through validation using the NBA dataset and the Dunk City Dynasty dataset and implemented online deployment in the industry."
2506.04657,"In this paper, we analyze the misère versions of two impartial combinatorial games: k-Bounded Greedy Nim and Greedy Nim. We present a complete solution to both games by showing necessary and sufficient conditions for a position to be P-positions."
2506.04665,"In budget-feasible mechanism design, there is a set of items $U$, each owned by a distinct seller. The seller of item $e$ incurs a private cost $\overline{c}_e$ for supplying her item. A buyer wishes to procure a set of items from the sellers of maximum value, where the value of a set $S\subseteq U$ of items is given by a valuation function $v:2^U\to \mathbb{R}_+$. The buyer has a budget of $B \in \mathbb{R}_+$ for the total payments made to the sellers. We wish to design a mechanism that is truthful, that is, sellers are incentivized to report their true costs, budget-feasible, that is, the sum of the payments made to the sellers is at most the budget $B$, and that outputs a set whose value is large compared to $\text{OPT}:=\max\{v(S):\overline{c}(S)\le B,S\subseteq U\}$.Budget-feasible mechanism design has been extensively studied, with the literature focussing on (classes of) subadditive valuation functions, and various polytime, budget-feasible mechanisms, achieving constant-factor approximation, have been devised for the special cases of additive, submodular, and XOS valuations. However, for general subadditive valuations, the best-known approximation factor achievable by a polytime budget-feasible mechanism (given access to demand oracles) was only $O(\log n / \log \log n)$, where $n$ is the number of items.We improve this state-of-the-art significantly by designing a randomized budget-feasible mechanism for subadditive valuations that achieves a substantially-improved approximation factor of $O(\log\log n)$ and runs in polynomial time, given access to demand oracles."
2506.05251,"We consider the cooperative elements that arise in the design of public goods, such as transportation policies and infrastructure. These involve a variety of stakeholders: governments, businesses, advocates, and users. Their eventual deployment depends on the decision maker's ability to garner sufficient support from each of these groups; we formalize these strategic requirements from the perspective of cooperative game theory. Specifically, we introduce non-transferable utility, linear production (NTU LP) games, which combine the game-theoretic tensions inherent in public decision-making with the modeling flexibility of linear programming. We derive structural properties regarding the non-emptiness, representability and complexity of the core, a solution concept that models the viability of cooperation. In particular, we provide fairly general sufficient conditions under which the core of an NTU LP game is guaranteed to be non-empty, prove that determining membership in the core is co-NP-complete, and develop a cutting plane algorithm to optimize various social welfare objectives subject to core membership. Lastly, we apply these results in a data-driven case study on service plan optimization for the Chicago bus system. As our study illustrates, cooperation is necessary for the successful deployment of transportation service plans and similar public goods, but it may also have adverse or counterintuitive distributive implications."
2506.05322,"We consider the computational complexity of computing Bayes-Nash equilibria in first-price auctions, where the bidders' values for the item are drawn from a general (possibly correlated) joint distribution. We show that when the values and the bidding space are discrete, determining the existence of a pure Bayes-Nash equilibrium is NP-hard. This is the first hardness result in the literature of the problem that does not rely on assumptions of subjectivity of the priors, or convoluted tie-breaking rules. We then present two main approaches for achieving positive results, via bid sparsification and via bid densification. The former is more combinatorial and is based on enumeration techniques, whereas the latter makes use of the continuous theory of the problem developed in the economics literature. Using these approaches, we develop polynomial-time approximation algorithms for computing equilibria in symmetric settings or settings with a fixed number of bidders, for different (discrete or continuous) variants of the auction."
2506.05373,"Social media platforms are ecosystems in which many decisions are constantly made for the benefit of the creators in order to maximize engagement, which leads to a maximization of income. The decisions, ranging from collaboration to public conflict or ``beefing,'' are heavily influenced by social media algorithms, viewer preferences, and sponsor risk. This paper models this interaction as a Stackelberg game in which the algorithm is the leader, setting exposure and reward rules, and the content creators are the followers, who optimize their content to maximize engagement. It focuses on two influencer strategies of collaborating and beefing. Viewer preferences are modeled indirectly through the algorithm's utility function, which rewards engagement metrics like click-through rate and watch time. Our simplified game-theoretic model demonstrates how different algorithmic priorities can shift creator strategies and provides insight into the equilibrium dynamics of social media influence."
2506.05379,"Training large language models (LLMs) requires vast amounts of high-quality data from institutions that face legal, privacy, and strategic constraints. Existing data procurement methods often rely on unverifiable trust or ignore heterogeneous provider costs. We introduce a mechanism-design framework for truthful, trust-minimized data sharing that ensures dominant-strategy incentive compatibility (DSIC), individual rationality, and weak budget balance, while rewarding data based on both quality and learning utility. We formalize a model where providers privately know their data cost and quality, and value arises solely from the data's contribution to model performance. Based on this, we propose the Quality-Weighted Marginal-Incentive Auction (Q-MIA), which ranks providers using a virtual cost metric and uses Myerson-style payments to ensure DSIC and budget feasibility. To support settings with limited liquidity or long-term incentives, we introduce the Marginal Utility Token (MUT), which allocates future rights based on marginal contributions. We unify these in Mixed-MIA, a hybrid mechanism balancing upfront payments and deferred rewards. All mechanisms support verifiable, privacy-preserving implementation. Theoretically and empirically, they outperform volume-based and trust-based baselines, eliciting higher-quality data under budget constraints while remaining robust to misreporting and collusion. This establishes a principled foundation for sustainable and fair data markets for future LLMs."
2506.05613,"We study the problem of fair allocation of indivisible goods for subadditive agents. While constant-\textsf{MMS} bounds have been given for additive and fractionally subadditive agents, the best existential bound for the case of subadditive agents is $1/O(\log n \log \log n)$. In this work, we improve this bound to a $1/O((\log \log n)^2)$-\textsf{MMS} guarantee. To this end, we introduce new matching techniques and rounding methods for subadditive valuations that we believe are of independent interest and will find their applications in future work."
2506.05705,"We study a new class of contract design problems where a principal delegates the execution of multiple projects to a set of agents. The principal's expected reward from each project is a combinatorial function of the agents working on it. Each agent has limited capacity and can work on at most one project, and the agents are heterogeneous, with different costs and contributions for participating in different projects. The main challenge of the principal is to decide how to allocate the agents to projects when the number of projects grows in scale.We analyze this problem under different assumptions on the structure of the expected reward functions. As our main result, for XOS functions we show how to derive a constant approximation to the optimal multi-project contract in polynomial time, given access to value and demand oracles. Along the way (and of possible independent interest), we develop approximate demand queries for \emph{capped} subadditive functions, by reducing to demand queries for the original functions. Our work paves the way to combinatorial contract design in richer settings."
2506.05876,"Bayesian persuasion, an extension of cheap-talk communication, involves an informed sender committing to a signaling scheme to influence a receiver's actions. Compared to cheap talk, this sender's commitment enables the receiver to verify the incentive compatibility of signals beforehand, facilitating cooperation. While effective in one-shot scenarios, Bayesian persuasion faces computational complexity (NP-hardness) when extended to long-term interactions, where the receiver may adopt dynamic strategies conditional on past outcomes and future expectations. To address this complexity, we introduce the bargaining perspective, which allows: (1) a unified framework and well-structured solution concept for long-term persuasion, with desirable properties such as fairness and Pareto efficiency; (2) a clear distinction between two previously conflated advantages: the sender's informational advantage and first-proposer advantage. With only modest modifications to the standard setting, this perspective makes explicit the common knowledge of the game structure and grants the receiver comparable commitment capabilities, thereby reinterpreting classic one-sided persuasion as a balanced information bargaining framework. The framework is validated through a two-stage validation-and-inference paradigm: We first demonstrate that GPT-o3 and DeepSeek-R1, out of publicly available LLMs, reliably handle standard tasks; We then apply them to persuasion scenarios to test that the outcomes align with what our information-bargaining framework suggests. All code, results, and terminal logs are publicly available atthis http URL."
2506.06058,"With the integration of renewable sources in electricity distribution networks, the need to develop intelligent mechanisms for balancing the energy market has arisen. In the absence of such mechanisms, the energy market may face imbalances that can lead to power outages, financial losses or instability at the grid level. In this context, the grouping of microgrids into optimal coalitions that can absorb energy from the market during periods of surplus or supply energy to the market during periods of is a key aspect in the efficient management of distribution networks. In this article, we propose a method that identify an optimal microgrids coalition capable of addressing the dynamics of the energy market. The proposed method models the problem of identifying the optimal coalition as an optimization problem that it solves by combining a strategy inspired by cooperative game theory with a memetic algorithm. An individual is represented as a coalition of microgrids and the evolution of population of individuals over generations is assured by recombination and mutation. The fitness function is defined as the difference between the total value generated by the coalition and a penalty applied to the coalition when the energy traded by coalition exceeds the energy available/demanded on/by the energy market. The value generated by the coalition is calculated based on the profit obtained by the collation if it sells energy on the market during periods of deficit or the savings obtained by the coalition if it buys energy on the market during periods of surplus and the costs associated with the trading process. This value is divided equitably among the coalition members, according to the Shapley value, which considers the contribution of each one to the formation of collective value."
2506.06217,"Many centralized mechanisms for two-sided matching markets that enjoy strong theoretical properties assume that the planner solicits full information on the preferences of each participating agent. In particular, they expect that participants compile and communicate their complete preference lists over agents from the other side of the market. However, real-world markets are often very large and agents cannot always be expected to even produce a ranking of all options on the other side. It is therefore important to understand the impact of incomplete or truncated lists on the quality of the resultant matching.In this paper, we focus on the Serial Dictatorship mechanism in a model where each agent of the proposing side (students) has a random preference list of length $d$, sampled independently and uniformly at random from $n$ schools, each of which has one seat. Our main result shows that if the students primarily care about being matched to any school of their list (as opposed to ending up unmatched), then all students in position $i\leq n$ will prefer markets with longer lists, when $n$ is large enough. Schools on the other hand will always prefer longer lists in our model. We moreover investigate the impact of $d$ on the rank of the school that a student gets matched to.Our main result suggests that markets that are well-approximated by our hypothesis and where the demand of schools does not exceed supply should be designed with preference lists as long as reasonable, since longer lists would favor all agents."
2506.06223,"Significant progress has been recently achieved in developing efficient solutions for simple stochastic games (SSGs), focusing on reachability objectives. While reductions from stochastic parity games (SPGs) to SSGs have been presented in the literature through the use of multiple intermediate game models, a direct and simple reduction has been notably absent. This paper introduces a novel and direct polynomial-time reduction from quantitative SPGs to quantitative SSGs. By leveraging a gadget-based transformation that effectively removes the priority function, we construct an SSG that simulates the behavior of a given SPG. We formally establish the correctness of our direct reduction. Furthermore, we demonstrate that under binary encoding this reduction is polynomial, thereby directly corroborating the known $\textbf{NP}\,\mathbf{\cap}\,\textbf{coNP}$ complexity of SPGs and providing new understanding in the relationship between parity and reachability objectives in turn-based stochastic games."
2506.07162,"We study the problem of delegated choice with inspection cost (DCIC), which is a variant of the delegated choice problem by Kleinberg and Kleinberg (EC'18) as well as an extension of the Pandora's box problem with nonobligatory inspection (PNOI) by Doval (JET'18). In our model, an agent may strategically misreport the proposed element's utility, unlike the standard delegated choice problem which assumes that the agent truthfully reports the utility for the proposed alternative. Thus, the principal needs to inspect the proposed element possibly along with other alternatives to maximize its own utility, given an exogenous cost of inspecting each element. Further, the delegation itself incurs a fixed cost, thus the principal can decide whether to delegate or not and inspect by herself.We show that DCIC indeed is a generalization of PNOI where the side information from a strategic agent is available at certain cost, implying its NP-hardness by Fu, Li, and Liu (STOC'23). We first consider a costless delegation setting in which the cost of delegation is free. We prove that the maximal mechanism over the pure delegation with a single inspection and an PNOI policy without delegation achieves a $3$-approximation for DCIC with costless delegation, which is further proven to be tight. These results hold even when the cost comes from an arbitrary monotone set function, and can be improved to a $2$-approximation if the cost of inspection is the same for every element. We extend these techniques by presenting a constant factor approximate mechanism for the general setting for rich class of instances."
2506.07186,"We study the problem of computing optimal correlated equilibria (CEs) in infinite-horizon multi-player stochastic games, where correlation signals are provided over time. In this setting, optimal CEs require history-dependent policies; this poses new representational and algorithmic challenges as the number of possible histories grows exponentially with the number of time steps. We focus on computing $(\epsilon, \delta)$-optimal CEs -- solutions that achieve a value within $\epsilon$ of an optimal CE, while allowing the agents' incentive constraints to be violated by at most $\delta$. Our main result is an algorithm that computes an $(\epsilon,\delta)$-optimal CE in time polynomial in $1/(\epsilon\delta(1 - \gamma))^{n+1}$, where $\gamma$ is the discount factor, and $n$ is the number of agents. For (a slightly more general variant of) turn-based games, we further reduce the complexity to a polynomial in $n$. We also establish that the bi-criterion approximation is necessary by proving matching inapproximability bounds.Our technical core is a novel approach based on inducible value sets, which leverages a compact representation of history-dependent CEs through the values they induce to overcome the representational challenge. We develop the value-set iteration algorithm -- which operates by iteratively updating estimates of inducible value sets -- and characterize CEs as the greatest fixed point of the update map. Our algorithm provides a groundwork for computing optimal CEs in general multi-player stochastic settings."
2506.07316,"This paper examines the tactical interaction between drones and tanks in modern warfare through game theory, particularly focusing on Stackelberg equilibrium and backward induction. It describes a high-stakes conflict between two teams: one using advanced drones for attack, and the other defending using tanks. The paper conceptualizes this as a sequential game, illustrating the complex strategic dynamics similar to Stackelberg competition, where moves and countermoves are carefully analyzed and predicted."
2506.09288,"In recent years, a new line of work in fair allocation has focused on EFX allocations for \((p, q)\)-bounded valuations, where each good is relevant to at most \(p\) agents, and any pair of agents share at most \(q\) relevant goods. For the case \(p = 2\) and \(q = \infty\), such instances can be equivalently represented as multigraphs whose vertices are the agents and whose edges represent goods, each edge incident to exactly the one or two agents for whom the good is relevant. A recent result of \citet{amanatidis2024pushing} shows that for additive $(2,\infty)$ bounded valuations, a \((\nicefrac{2}{3})\)-EFX allocation always exists. In this paper, we improve this bound by proving the existence of a \((\nicefrac{1}{\sqrt{2}})\)-\(\efx\) allocation for additive \((2,\infty)\)-bounded valuations."
2506.09291,"We quantify the value of the monopoly's bargaining power in terms of competition complexity--that is, the number of additional bidders the monopoly must attract in simple auctions to match the expected revenue of the optimal mechanisms (c.f., Bulow and Klemperer, 1996, Eden et al., 2017)--within the setting of multi-item auctions. We show that for simple auctions that sell items separately, the competition complexity is $\Theta(\frac{n}{\alpha})$ in an environment with $n$ original bidders under the slightly stronger assumption of $\alpha$-strong regularity, in contrast to the standard regularity assumption in the literature, which requires $\Omega(n \cdot \ln \frac{m}{n})$ additional bidders (Feldman et al., 2018). This significantly reduces the value of learning the distribution to design the optimal mechanisms, especially in large markets with many items for sale. For simple auctions that sell items as a grand bundle, we establish a constant competition complexity bound in a single-bidder environment when the number of items is small or when the value distribution has a monotone hazard rate. Some of our competition complexity results also hold when we compete against the first best benchmark (i.e., optimal social welfare)."
2506.1046,"We consider strategy proof mechanisms for facility location which maximize equitability between agents. As is common in the literature, we measure equitability with the Gini index. We first prove a simple but fundamental impossibility result that no strategy proof mechanism can bound the approximation ratio of the optimal Gini index of utilities for one or more facilities. We propose instead computing approximation ratios of the complemented Gini index of utilities, and consider how well both deterministic and randomized mechanisms approximate this. In addition, as Nash welfare is often put forwards as an equitable compromise between egalitarian and utilitarian outcomes, we consider how well mechanisms approximate the Nash welfare."
2506.10843,"We study diversity in approval-based committee elections with incomplete or inaccurate information. We define diversity according to the Maximum Coverage problem, which is known to be \textsc{np}-complete, with a best attainable polynomial time approximation ratio of $1-1/\e$. In the incomplete information setting, voters vote only on a small portion of the candidates, and we prove that getting arbitrarily close to the optimal approximation ratio w.h.p. requires $\Omega(m^2)$ non-adaptive queries, where $m$ is the number of candidates. This motivates studying adaptive querying algorithms, that can adapt their querying strategy to information obtained from previous query outcomes. In that setting, we lower this bound to only $\Omega(m)$ queries. We propose a greedy algorithm to match this lower bound up to log-factors. We prove the same $\tilde\Theta(m)$ bound for the generalized problem of Max Cover over a matroid constraint, using a local search algorithm. Specifying a matroid of valid committees lets us implement extra structural requirements on the committee, like quota. In the inaccurate information setting, voters' responses are corrupted with a small probability. We prove $\tilde\Theta(nm)$ queries are required to attain a $(1-1/\e)$-approximation with high probability, where $n$ is the number of voters. While the proven bounds show that all our algorithms are viable asymptotically, they also show that some of them would still require large numbers of queries in instances of practical relevance. Using real data from Polis as well as synthetic data, we observe that our algorithms perform well also on smaller instances, both with incomplete and inaccurate information."
2506.1295,"We consider the classic cake-cutting problem of producing fair allocations for $n$ agents, in the Robertson-Webb query model. In this model, it is known that: (i) proportional allocations can be computed using $O(n \log n)$ queries, and this is optimal for deterministic protocols; (ii) envy-free allocations (a subset of proportional allocations) can be computed using $O\left( n^{n^{n^{n^{n^{n}}}}} \right)$ queries, and the best known lower bound is $\Omega(n^2)$; (iii) perfect allocations (a subset of envy-free allocations) cannot be computed using a bounded (in $n$) number of queries.In this work, we introduce two hierarchies of new fairness notions: Complement Harmonically Bounded (CHB) and Complement Linearly Bounded (CLB). Intuitively, these notions of fairness ask that, for every agent $i$, the collective value that a group of agents has (from the perspective of agent $i$) is limited. CHB-$k$ and CLB-$k$ coincide with proportionality for $k=1$. For all $k \leq n$, CHB-$k$ allocations are a superset of envy-free allocations (i.e., easier to find). On the other hand, for $k \in [2, \lceil n/2 \rceil - 1]$, CLB-$k$ allocations are incomparable to envy-free allocations. For $k \geq \lceil n/2 \rceil$, CLB-$k$ allocations are a subset of envy-free allocations (i.e., harder to find).We prove that CHB-$n$ allocations can be computed using $O(n^4)$ queries in the Robertson-Webb model. On the flip side, finding CHB-$2$ (and therefore all CHB-$k$ for $k \geq 2$) allocations requires $\Omega(n^2)$ queries, while CLB-$2$ (and therefore all CLB-$k$ for $k \geq 2$) allocations cannot be computed using a bounded (in $n$) number of queries."
2506.12961,"In this paper we develop a novel approach to relaxing Arrow's axioms for voting rules, addressing a long-standing critique in social choice theory. Classical axioms (often styled as fairness axioms or fairness criteria) are assessed in a binary manner, so that a voting rule fails the axiom if it fails in even one corner case. Many authors have proposed a probabilistic framework to soften the axiomatic approach. Instead of immediately passing to random preference profiles, we begin by measuring the degree to which an axiom is upheld or violated on a given profile. We focus on two foundational axioms-Independence of Irrelevant Alternatives (IIA) and Unanimity (U)-and extend them to take values in $[0,1]$. Our $\sigma_{IIA}$ measures the stability of a voting rule when candidates are removed from consideration, while $\sigma_{U}$ captures the degree to which the outcome respects majority preferences. Together, these metrics quantify how a voting rule navigates the fundamental trade-off highlighted by Arrow's Theorem. We show that $\sigma_{IIA}\equiv 1$ recovers classical IIA, and $\sigma_{U}>0$ recovers classical Unanimity, allowing a quantitative restatement of Arrow's Theorem. In the empirical part of the paper, we test these metrics on two kinds of data: a set of over 1000 ranked choice preference profiles from Scottish local elections, and a batch of synthetic preference profiles generated with a Bradley-Terry-type model. We use those to investigate four positional voting rules-Plurality, 2-Approval, 3-Approval, and the Borda rule-as well as the iterative rule known as Single Transferable Vote (STV). The Borda rule consistently receives the highest $\sigma_{IIA}$ and $\sigma_{U}$ scores across observed and synthetic elections. This compares interestingly with a recent result of Maskin showing that weakening IIA to include voter preference intensity uniquely selects Borda."
2506.13271,"Blockchain transactions consume diverse resources, foremost among them storage, but also computation, communication, and others. Efficiently charging for these resources is crucial for effective system resource allocation and long-term economic viability. The prevailing approach, one-dimensional pricing, sets a single price for a linear combination of resources. However, this often leads to under-utilization when resource capacities are limited. Multi-dimensional pricing, which independently prices each resource, offers an alternative but presents challenges in price discovery.This work focuses on the welfare achieved by these two schemes. We prove that multi-dimensional pricing is superior under stable blockchain conditions. Conversely, we show that one-dimensional pricing outperforms its multi-dimensional counterpart in transient states, exhibiting faster convergence and greater computational tractability. These results highlight a critical trade-off: while multi-dimensional pricing offers efficiency gains at equilibrium, its implementation incurs costs associated with system transitions. Our findings underscore the necessity for a deeper understanding of these transient effects before widespread adoption. Finally, we propose mechanisms that aim to mitigate some of these issues, paving the way for future research."
2506.13286,"In this paper, we investigate how randomness and uncertainty influence learning in games. Specifically, we examine a perturbed variant of the dynamics of ""follow-the-regularized-leader"" (FTRL), where the players' payoff observations and strategy updates are continually impacted by random shocks. Our findings reveal that, in a fairly precise sense, ""uncertainty favors extremes"": in any game, regardless of the noise level, every player's trajectory of play reaches an arbitrarily small neighborhood of a pure strategy in finite time (which we estimate). Moreover, even if the player does not ultimately settle at this strategy, they return arbitrarily close to some (possibly different) pure strategy infinitely often. This prompts the question of which sets of pure strategies emerge as robust predictions of learning under uncertainty. We show that (a) the only possible limits of the FTRL dynamics under uncertainty are pure Nash equilibria; and (b) a span of pure strategies is stable and attracting if and only if it is closed under better replies. Finally, we turn to games where the deterministic dynamics are recurrent - such as zero-sum games with interior equilibria - and we show that randomness disrupts this behavior, causing the stochastic dynamics to drift toward the boundary on average."
2506.14149,"We study the allocation of indivisible goods under conflicting constraints, represented by a graph. In this framework, vertices correspond to goods and edges correspond to conflicts between a pair of goods. Each agent is allocated an independent set in the graph. In a recent work of Kumar et al. (2024), it was shown that a maximal EF1 allocation exists for interval graphs and two agents with monotone valuations. We significantly extend this result by establishing that a maximal EF1 allocation exists for \emph{any graph} when the two agents have monotone valuations. To compute such an allocation, we present a polynomial-time algorithm for additive valuations, as well as a pseudo-polynomial time algorithm for monotone valuations. Moreover, we complement our findings by providing a counterexample demonstrating a maximal EF1 allocation may not exist for three agents with monotone valuations; further, we establish NP-hardness of determining the existence of such allocations for every fixed number $n \geq 3$ of agents. All of our results for goods also apply to the allocation of chores."
2506.14544,"This paper contributes to the study of positional determinacy of infinite duration games played on potentially infinite graphs. Recently, [Ohlmann, TheoretiCS 2023] established that positionality of prefix-independent objectives is preserved by finite lexicographic products. We propose two different notions of infinite lexicographic products indexed by arbitrary ordinals, and extend Ohlmann's result by proving that they also preserve positionality. In the context of one-player positionality, this extends positional determinacy results of [Grädel and Walukiewicz, Logical Methods in Computer Science 2006] to edge-labelled games and arbitrarily many priorities for both Max-Parity and Min-Parity. Moreover, we show that the Max-Parity objectives over countable ordinals are complete for the infinite levels of the difference hierarchy over $\Sigma^0_2$ and that Min-Parity is complete for the class $\Sigma^0_3$. We obtain therefore positional languages that are complete for all those levels, as well as new insights about closure under unions and neutral letters."
2506.15295,"Lending protocols are one of the main applications of Decentralized Finance (DeFi), enabling crypto-assets loan markets with a total value estimated in the tens of billions of dollars. Unlike traditional lending systems, these protocols operate without relying on trusted authorities or off-chain enforcement mechanisms. To achieve key economic goals such as stability of the loan market, they devise instead trustless on-chain mechanisms, such as rewarding liquidators who repay the loans of under-collateralized borrowers by awarding them part of the borrower's collateral. The complexity of these incentive mechanisms, combined with their entanglement in low-level implementation details, makes it challenging to precisely assess the structural and economic properties of lending protocols, as well as to analyze user strategies and attacks. Crucially, since participation is open to anyone, any weaknesses in the incentive mechanism may give rise to unintended emergent behaviours, or even enable adversarial strategies aimed at making profits to the detriment of legit users, or at undermining the stability of the protocol. In this work, we propose a formal model of lending protocols that captures the essential features of mainstream platforms, enabling us to identify and prove key properties related to their economic and strategic dynamics."
2506.15379,"Since its introduction, envy-freeness up to any good (EFX) has become a fundamental solution concept in fair division of indivisible goods. Its existence remains elusive -- even for four agents with additive utility functions, it is unknown whether an EFX allocation always exists. Unsurprisingly, restricted settings to delineate tractable and intractable cases have been explored. Christadolou, Fiat et al.[EC'23] introduced the notion of EFX-orientation, where the agents form the vertices of a graph and the items correspond to edges, and an agent values only the items that are incident to it. The goal is to allocate items to one of the adjacent agents while satisfying the EFX condition.Building on the work of Zeng and Mehta'24, which established a sharp complexity threshold based on the structure of the underlying graph -- polynomial-time solvability for bipartite graphs and NP-hardness for graphs with chromatic number at least three -- we further explore the algorithmic landscape of EFX-orientation using parameterized graph algorithms.Specifically, we show that bipartiteness is a surprisingly stringent condition for tractability: EFX orientation is NP-complete even when the valuations are symmetric, binary and the graph is at most two edge-removals away from being bipartite. Moreover, introducing a single non-binary value makes the problem NP-hard even when the graph is only one edge removal away from being bipartite. We further perform a parameterized analysis to examine structures of the underlying graph that enable tractability. In particular, we show that the problem is solvable in linear time on graphs whose treewidth is bounded by a constant and that the complexity of an instance is closely tied to the sizes of acyclic connected components on its one-valued edges."
2506.15887,"Fairness is desirable yet challenging to achieve within multi-agent systems, especially when agents differ in latent traits that affect their abilities. This hidden heterogeneity often leads to unequal distributions of wealth, even when agents operate under the same rules. Motivated by real-world examples, we propose a framework based on repeated principal-agent games, where a principal, who also can be seen as a player of the game, learns to offer adaptive contracts to agents. By leveraging a simple yet powerful contract structure, we show that a fairness-aware principal can learn homogeneous linear contracts that equalize outcomes across agents in a sequential social dilemma. Importantly, this fairness does not come at the cost of efficiency: our results demonstrate that it is possible to promote equity and stability in the system while preserving overall performance."
2506.1612,"We contribute the first provable guarantees of global convergence to Nash equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using independent policy gradient methods. Convex Markov games, recently defined by Gemp et al. (2024), extend Markov decision processes to multi-agent settings with preferences that are convex over occupancy measures, offering a broad framework for modeling generic strategic interactions. However, even the fundamental min-max case of cMGs presents significant challenges, including inherent nonconvexity, the absence of Bellman consistency, and the complexity of the infinite horizon.We follow a two-step approach. First, leveraging properties of hidden-convex--hidden-concave functions, we show that a simple nonconvex regularization transforms the min-max optimization problem into a nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this regularization can stabilize the iterates of independent policy gradient methods and ultimately lead them to converge to equilibria. Second, building on this reduction, we address the general constrained min-max problems under NC-pPL and two-sided pPL conditions, providing the first global convergence guarantees for stochastic nested and alternating gradient descent-ascent methods, which we believe may be of independent interest."
2506.17058,"In first-price auctions for display advertising, exchanges typically communicate the ""minimum-bid-to-win"" to bidders after the auction as feedback for their bidding algorithms. For a winner, this is the second-highest bid, while for losing bidders it is the highest bid. In this paper we investigate the generalization of this concept to general combinatorial auctions, motivated by the domain of video advertising. In a video pod auction, ad slots during an advertising break in a video stream are auctioned all at once, under several kinds of allocation constraints such as a constraint on total ad duration. We cast the problem in terms of computing bid updates (discounts and raises) that maintain the optimality of the current allocation. Our main result characterizes the set of joint bid updates with this property as the core of an associated bicooperative game. In the case of the assignment problem--a special case of video pod auctions--we provide a linear programming characterization of this bicooperative core. Our characterization leads to several candidates for a generalized minimum-bid-to-win. Drawing on video pod auction data from a real ad exchange, we perform an empirical analysis to understand the bidding dynamics they induce and their convergence properties."
2506.17115,"Control systems will play a pivotal role in addressing societal-scale challenges as they drive the development of sustainable future smart cities. At the heart of these challenges is the trustworthy, fair, and efficient allocation of scarce public resources, including renewable energy, transportation, data, computation, etc.. Historical evidence suggests that monetary control -- the prototypical mechanism for managing resource scarcity -- is not always well-accepted in socio-technical resource contexts. In this vision article, we advocate for karma economies as an emerging non-monetary mechanism for socio-technical control. Karma leverages the repetitive nature of many socio-technical resources to jointly attain trustworthy, fair, and efficient allocations; by budgeting resource consumption over time and letting resource users ``play against their future selves.'' To motivate karma, we review related concepts in economics through a control systems lens, and make a case for a) shifting the viewpoint of resource allocations from single-shot and static to repeated and dynamic games; and b) adopting long-run Nash welfare as the formalization of ``fairness and efficiency'' in socio-technical contexts. We show that in many dynamic resource settings, karma Nash equilibria maximize long-run Nash welfare. Moreover, we discuss implications for a future smart city built on multi-karma economies: by choosing whether to combine different socio-technical resources, e.g., electricity and transportation, in a single karma economy, or separate into resource-specific economies, karma provides new flexibility to design the scope of fairness and efficiency."
2506.18008,"We study the optimal contract problem in the framework of combinatorial contracts, introduced by Duetting et al. [FOCS'21], where a principal delegates the execution of a project to an agent, and the agent can choose any subset from a given set of costly actions. At the core of the model is a reward function - a monotone set function that maps each set of actions taken by the agent into an expected reward to the principal. To incentivize the agent, the principal offers a contract specifying the fraction of the reward to be paid, and the agent responds with their optimal action set. The goal is to compute the contract that maximizes the principal's expected utility.Previous work showed that when the reward function is gross substitutes (GS), the optimal contract can be computed in polynomial time, but the problem is NP-hard for the broader class of Submodular functions. This raised the question: is GS the true boundary of tractability for the optimal contract problem? We prove that tractability extends to the strictly broader class of Ultra functions. Interestingly, GS constitutes precisely the intersection of Ultra and Submodular functions, and our result reveals that it is Ultra - not Submodular - that drives tractability, overturning the prevailing belief that the submodularity component of GS is essential. We further extend tractability beyond additive costs, handling costs that are additive plus symmetric. Our results require new techniques, as prior approaches relied on the submodularity of GS. To the best of our knowledge, this is the first application of Ultra functions in a prominent economic setting."
2506.18545,"We investigate refinements of the mean-payoff criterion in two-player zero-sum perfect-information stochastic games. A strategy is Blackwell optimal if it is optimal in the discounted game for all discount factors sufficiently close to $1$. The notion of $d$-sensitive optimality interpolates between mean-payoff optimality (corresponding to the case $d=-1$) and Blackwell optimality ($d=+\infty$). The Blackwell threshold $\alpha_{\sf Bw} \in [0,1[$ is the discount factor above which all optimal strategies in the discounted game are guaranteed to be Blackwell optimal. The $d$-sensitive threshold $\alpha_{\sf d} \in [0,1[$ is defined analogously. Bounding $\alpha_{\sf Bw}$ and $\alpha_{\sf d}$ are fundamental problems in algorithmic game theory, since these thresholds control the complexity for computing Blackwell and $d$-sensitive optimal strategies, by reduction to discounted games which can be solved in $O\left((1-\alpha)^{-1}\right)$ iterations. We provide the first bounds on the $d$-sensitive threshold $\alpha_{\sf d}$ beyond the case $d=-1$, and we establish improved bounds for the Blackwell threshold $\alpha_{\sf Bw}$. This is achieved by leveraging separation bounds on algebraic numbers, relying on Lagrange bounds and more advanced techniques based on Mahler measures and multiplicity theorems."
2506.18571,"Autonomous and learning agents increasingly participate in markets - setting prices, placing bids, ordering inventory. Such agents are not just aiming to optimize in an uncertain environment; they are making decisions in a game-theoretical environment where the decision of one agent influences the profit of other agents. While game theory usually predicts outcomes of strategic interaction as an equilibrium, it does not capture how repeated interaction of learning agents arrives at a certain outcome. This article surveys developments in modeling agent behavior as dynamical systems, with a focus on projected gradient and no-regret learning algorithms. In general, learning in games can lead to all types of dynamics, including convergence to equilibrium, but also cycles and chaotic behavior. It is important to understand when we can expect efficient equilibrium in automated markets and when this is not the case. Thus, we analyze when and how learning agents converge to an equilibrium of a market game, drawing on tools from variational inequalities and Lyapunov stability theory. Special attention is given to the stability of projected dynamics and the convergence to equilibrium sets as limiting outcomes. Overall, the paper provides mathematical foundations for analyzing stability and convergence in agentic markets driven by autonomous, learning agents."
2506.18643,"We study approval-based committee voting from a novel perspective. While extant work largely centers around proportional representation of the voters, we shift our focus to the candidates while preserving proportionality. Intuitively, candidates supported by similar voter groups should receive comparable representation. Since deterministic voting rules cannot achieve this ideal, we develop randomized voting rules that satisfy ex-ante neutrality, monotonicity, and continuity, while maintaining strong ex-post proportionality guarantees.Continuity of the candidate selection probabilities proves to be the most demanding of our ex-ante desiderata. We provide it via voting rules that are algorithmically stable, a stronger notion of robustness which captures the continuity of the committee distribution under small changes. First, we introduce Softmax-GJCR, a randomized variant of the Greedy Justified Candidate Rule (GJCR) [Brill and Peters, 2023], which carefully leverages slack in GJCR to satisfy our ex-ante properties. This polynomial-time algorithm satisfies EJR+ ex post, assures ex-ante monotonicity and neutrality, and provides $O(k^3/n)$-stability (ignoring $\log$ factors). Building on our techniques for Softmax-GJCR, we further show that stronger stability guarantees can be attained by (i) allowing exponential running time, (ii) relaxing EJR+ to an approximate $\alpha$-EJR+, and (iii) relaxing EJR+ to JR.We finally demonstrate the utility of stable voting rules in other settings. In online dynamic committee voting, we show that stable voting rules imply dynamic voting rules with low expected recourse, and illustrate this reduction for Softmax-GJCR. Our voting rules also satisfy a stronger form of stability that coincides with differential privacy, suggesting their applicability in privacy-sensitive domains."
2506.18794,"When allocating indivisible items, there are various ways to use monetary transfers for eliminating envy. Particularly, one can apply a balanced vector of transfer payments, or charge each agent a positive amount, or -- contrarily -- give each agent a positive amount as a ``subsidy''. In each model, one can aim to minimize the amount of payments used; this aim translates into different optimization objectives in each setting. This note compares the various models, and the relations between upper and lower bounds for these objectives."
2506.19038,"We consider the problem of online dynamic mechanism design for sequential auctions in unknown environments, where the underlying market and, thus, the bidders' values vary over time as interactions between the seller and the bidders progress. We model the sequential auctions as an infinite-horizon average-reward Markov decision process (MDP). In each round, the seller determines an allocation and sets a payment for each bidder, while each bidder receives a private reward and submits a sealed bid to the seller. The state, which represents the underlying market, evolves according to an unknown transition kernel and the seller's allocation policy without episodic resets. We first extend the Vickrey-Clarke-Groves (VCG) mechanism to sequential auctions, thereby obtaining a dynamic counterpart that preserves the desired properties: efficiency, truthfulness, and individual rationality. We then focus on the online setting and develop a reinforcement learning algorithm for the seller to learn the underlying MDP and implement a mechanism that closely resembles the dynamic VCG mechanism. We show that the learned mechanism approximately satisfies efficiency, truthfulness, and individual rationality and achieves guaranteed performance in terms of various notions of regret."
2506.19083,"Many decision-making processes involve evaluating and then selecting items; examples include scientific peer review, job hiring, school admissions, and investment decisions. The eventual selection is performed by applying rules or deliberations to the raw evaluations, and then deterministically selecting the items deemed to be the best. These domains feature error-prone evaluations and uncertainty about future outcomes, which undermine the reliability of such deterministic selection rules. As a result, selection mechanisms involving explicit randomization that incorporate the uncertainty are gaining traction in practice. However, current randomization approaches are ad hoc, and as we prove, inappropriate for their purported objectives. In this paper, we propose a principled framework for randomized decision-making based on interval estimates of the quality of each item. We introduce MERIT (Maximin Efficient Randomized Interval Top-k), an optimization-based method that maximizes the worst-case expected number of top candidates selected, under uncertainty represented by overlapping intervals (e.g., confidence intervals or min-max intervals). MERIT provides an optimal resource allocation scheme under an interpretable notion of robustness. We develop a polynomial-time algorithm to solve the optimization problem and demonstrate empirically that the method scales to over 10,000 items. We prove that MERIT satisfies desirable axiomatic properties not guaranteed by existing approaches. Finally, we empirically compare algorithms on synthetic peer review data. Our experiments demonstrate that MERIT matches the performance of existing algorithms in expected utility under fully probabilistic review data models used in previous work, while outperforming previous methods with respect to our novel worst-case formulation."
2506.19345,"In real-world settings of the Deferred Acceptance stable matching algorithm, such as the American medical residency match (NRMP), school choice programs, and various national university entrance systems, candidates need to decide which programs to list. In many of these settings there is an initial phase of interviews or information gathering which affect the preferences on one or both sides. We ask: which interviews should candidates seek? We study this question in a model, introduced by Lee (2016) and modified by Allman and Ashlagi (2023), with preferences based on correlated cardinal utilities.We describe a distributed, low-communication strategy for the doctors and students, which lead to non-match rates of $e^{(-\widetilde{O}(\sqrt{k}))}$ in the residency setting and $e^{(-\widetilde{O}(k))}$ in the school-choice setting, where $k$ is the number of interviews per doctor in the first setting, and the number of proposals per student in the second setting; these bounds do not apply to the agents with the lowest public ratings, the bottommost agents, who may not fare as well. We also obtain bounds on the expected utilities each non-bottommost agent obtains.These results are parameterized by the capacity of the hospital programs and schools. Larger capacities improve the outcome for the hospitals and schools, but don't significantly affect the outcomes of the doctors or students. Finally, in the school choice setting we obtain an $\epsilon$-Nash type equilibrium for the students apart from the bottommost ones; importantly, the equilibrium holds regardless of the actions of the bottommost students. We also discuss to what extent this result extends to the residency setting. We complement our theoretical results with an experimental study that shows the asymptotic results hold for real-world values of $n$."
2506.2003,"This paper derives polynomial-time approximation schemes for several NP-hard stochastic optimization problems from the algorithmic mechanism design and operations research literatures. The problems we consider involve a principal or seller optimizing with respect to a subsequent choice by an agent or buyer. These include posted pricing for a unit-demand buyer with independent values (Chawla et al., 2007, Cai and Daskalakis, 2011), assortment optimization with independent utilities (Talluri and van Ryzin, 2004), and delegated choice (Khodabakhsh et al., 2024). Our results advance the state of the art for each of these problems. For unit-demand pricing with discrete distributions, our multiplicative PTAS improves on the additive PTAS of Cai and Daskalakis, and we additionally give a PTAS for the unbounded regular case, improving on the latter paper's QPTAS. For assortment optimization, no constant approximation was previously known. For delegated choice, we improve on both the $3$-approximation for the case with no outside option and the super-constant-approximation with an outside option.A key technical insight driving our results is an economically meaningful property we term utility alignment. Informally, a problem is utility aligned if, at optimality, the principal derives most of their utility from realizations where the agent's utility is also high. Utility alignment allows the algorithm designer to focus on maximizing performance on realizations with high agent utility, which is often an algorithmically simpler task. We prove utility alignment results for all the problems mentioned above, including strong results for unit-demand pricing and delegation, as well as a weaker but very broad guarantee that holds for many other problems under very mild conditions."
2506.20317,"We study the problem of (approximate) maximin share (MMS) allocation of indivisible items among a set of agents. We focus on the graphical valuation model, previously studied by Christodolou, Fiat, Koutsoupias, and Sgouritsa (""Fair allocation in graphs"", EC 2023), where the input is given by a graph where edges correspond to items, and vertices correspond to agents. An edge may have non-zero marginal value only for its incident vertices. We study additive, XOS and subadditive valuations and we present positive and negative results for (approximate) MMS fairness, and also for (approximate) pair-wise maximin share (PMMS) fairness."
2506.20908,"Online advertising systems have recently transitioned to autobidding, allowing advertisers to delegate bidding decisions to automated agents. Each advertiser directs their agent to optimize an objective function subject to return-on-investment (ROI) and budget constraints. Given their practical relevance, this shift has spurred a surge of research on the liquid welfare price of anarchy (POA) of fundamental auction formats under autobidding, most notably simultaneous first-price auctions (FPA). One of the main challenges is to understand the efficiency of FPA in the presence of heterogeneous agent types. We introduce {type-dependent smoothness framework that enables a unified analysis of the POA in such complex autobidding environments. In our approach, we derive type-dependent smoothness parameters which we carefully balance to obtain POA bounds. This balancing gives rise to a POA-revealing mathematical program, which we use to determine tight bounds on the POA of coarse correlated equilibria (CCE). Our framework is versatile enough to handle heterogeneous agent types and extends to the general class of fractionally subadditive valuations. Additionally, we develop a novel reduction technique that transforms budget-constrained agents into budget-unconstrained ones. Combining this reduction technique with our smoothness framework enables us to derive tight bounds on the POA of CCE in the general hybrid agent model with both ROI and budget constraints. Among other results, our bounds uncover an intriguing threshold phenomenon showing that the POA depends intricately on the smallest and largest agent types. We also extend our study to FPAs with reserve prices, which can be interpreted as predictions of agents' values, to further improve efficiency guarantees."
2506.21493,"We consider the problem of fair allocation of $m$ indivisible items to $n$ agents with monotone subadditive valuations. For integer $d \ge 2$, a $d$-multi-allocation is an allocation in which each item is allocated to at most $d$ different agents. We show that $d$-multi-allocations can be transformed into allocations, while not losing much more than a factor of $d$ in the value that each agent receives. One consequence of this result is that for allocation instances with equal entitlements and subadditive valuations, if $\rho$-MMS $d$-multi-allocations exist, then so do $\frac{\rho}{4d}$-MMS allocations. Combined with recent results of Seddighin and Seddighin [EC 2025], this implies the existence of $\Omega(\frac{1}{\log\log n})$-MMS allocations."
2506.21727,"This paper explores the fair allocation of indivisible items in a multidimensional setting, motivated by the need to address fairness in complex environments where agents assess bundles according to multiple criteria. Such multidimensional settings are not merely of theoretical interest but are central to many real-world applications. For example, cloud computing resources are evaluated based on multiple criteria such as CPU cores, memory, and network bandwidth. In such cases, traditional one dimensional fairness notions fail to capture fairness across multiple attributes. To address these challenges, we study two relaxed variants of envy-freeness: weak simultaneously envy-free up to c goods (weak sEFc) and strong simultaneously envy-free up to c goods (strong sEFc), which accommodate the multidimensionality of agents' preferences. Under the weak notion, for every pair of agents and for each dimension, any perceived envy can be eliminated by removing, if necessary, a different set of goods from the envied agent's allocation. In contrast, the strong version requires selecting a single set of goods whose removal from the envied bundle simultaneously eliminates envy in every dimension. We provide upper and lower bounds on the relaxation parameter c that guarantee the existence of weak or strong sEFc allocations, where these bounds are independent of the total number of items. In addition, we present algorithms for checking whether a weak or strong sEFc allocation exists. Moreover, we establish NP-hardness results for checking the existence of weak sEF1 and strong sEF1 allocations."
2506.22089,"We consider the problem of a game theorist analyzing a game that uses cryptographic protocols. Ideally, a theorist abstracts protocols as ideal, implementation-independent primitives, letting conclusions in the ""ideal world"" carry over to the ""real world."" This is crucial, since the game theorist cannot--and should not be expected to--handle full cryptographic complexity. In today's landscape, the rise of distributed ledgers makes a shared language between cryptography and game theory increasingly necessary.The security of cryptographic protocols hinges on two types of assumptions: state-of-the-world (e.g., ""factoring is hard"") and behavioral (e.g., ""honest majority""). We observe that for protocols relying on behavioral assumptions (e.g., ledgers), our goal is unattainable in full generality. For state-of-the-world assumptions, we show that standard solution concepts, e.g., ($\epsilon$-)Nash equilibria, are not robust to transfer from the ideal to the real world.We propose a new solution concept: the pseudo-Nash equilibrium. Informally, a profile $s=(s_1,\dots,s_n)$ is a pseudo-Nash equilibrium if, for any player $i$ and deviation $s'_i$ with higher expected utility, $i$'s utility from $s_i$ is (computationally) indistinguishable from that of $s'_i$. Pseudo-Nash is simpler and more accessible to game theorists than prior notions addressing the mismatch between (asymptotic) cryptography and game theory. We prove that Nash equilibria in games with ideal, unbreakable cryptography correspond to pseudo-Nash equilibria when ideal cryptography is instantiated with real protocols (under state-of-the-world assumptions). Our translation is conceptually simpler and more general: it avoids tuning or restricting utility functions in the ideal game to fit quirks of cryptographic implementations. Thus, pseudo-Nash lets us study game-theoretic and cryptographic aspects separately and seamlessly."
2506.22133,"A Condorcet winning set addresses the Condorcet paradox by selecting a few candidates--rather than a single winner--such that no unselected alternative is preferred to all of them by a majority of voters. This idea extends to $\alpha$-undominated sets, which ensure the same property for any $\alpha$-fraction of voters and are guaranteed to exist in constant size for any $\alpha$. However, the requirement that an outsider be preferred to every member of the set can be overly restrictive and difficult to justify in many applications. Motivated by this, we introduce a more flexible notion: $(t, \alpha)$-undominated sets. Here, each voter compares an outsider to their $t$-th most preferred member of the set, and the set is undominated if no outsider is preferred by more than an $\alpha$-fraction of voters. This framework subsumes prior definitions, recovering Condorcet winning sets when $(t = 1, \alpha = 1/2)$ and $\alpha$-undominated sets when $t = 1$, and introduces a new, tunable notion of collective acceptability for $t > 1$. We establish three main results:1. We prove that a $(t, \alpha)$-undominated set of size $O(t/\alpha)$ exists for all values of $t$ and $\alpha$.2. We show that as $t$ becomes large, the minimum size of such a set approaches $t/\alpha$, which is asymptotically optimal.3. In the special case $t = 1$, we improve the bound on the size of an $\alpha$-undominated set given by Charikar, Lassota, Ramakrishnan, Vetta, and Wang (STOC 2025). As a consequence, we show that a Condorcet winning set of five candidates exists, improving their bound of six."
2506.2256,"Recent studies on many-to-one matching markets have explored agents with flexible capacity and truthful preference reporting, focusing on mechanisms that jointly design capacities and select a matching. However, in real-world applications such as school choice and residency matching, preferences are revealed after capacity decisions are made, with matching occurring afterward; uncertainty about agents' preferences must be considered during capacity planning. Moreover, even under strategy-proof mechanisms, agents may strategically misreport preferences based on beliefs about admission chances. We introduce a two-stage stochastic matching problem with uncertain preferences, using school choice as a case study. In the first stage, the clearinghouse expands schools' capacities before observing students' reported preferences. Students either report their true preferences, producing exogenous uncertainty, or act strategically, submitting reported preferences based on their true preferences and admission chances (which depend on capacities), introducing endogenous uncertainty. In the second stage, the clearinghouse computes the student-optimal stable matching based on schools' priorities and students' reported preferences. In strategic cases, endogenous reported preferences are utility-maximizing transformations of capacity decisions and exogenous true preferences; we handle uncertainty using sample average approximation(SAA). We develop behavior-based mathematical formulations and, due to problem complexity, propose Lagrangian- and local-search-based behavior-specific heuristics for near-optimal solutions. Our SAA-based approaches outperform the average scenario approach on students' matching preferences and admission outcomes, emphasizing the impact of stochastic preferences on capacity decisions. Student behavior notably influences capacity design, stressing the need to consider misreports."
2506.22911,"This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive approach), a discretization-free algorithm to learn truthful and utility-maximizing mechanisms. Existing learning-based approaches often rely on discretization of outcome spaces to ensure truthfulness, which leads to inefficiency with increasing problem size. To address this limitation, we formalize the concept of pricing rules, defined as functions that map outcomes to prices. Based on this concept, we propose a novel menu mechanism, which can be equivalent to a truthful direct mechanism under specific conditions. The core idea of TEDI lies in its parameterization of pricing rules using Partial GroupMax Network, a new network architecture designed to universally approximate partial convex functions. To learn optimal pricing rules, we develop novel training techniques, including covariance trick and continuous sampling, to derive unbiased gradient estimators compatible with first-order optimization. Theoretical analysis establishes that TEDI guarantees truthfulness, full expressiveness, and dimension-insensitivity. Experimental evaluation in the studied auction setting demonstrates that TEDI achieves strong performance, competitive with or exceeding state-of-the-art methods.This work presents the first approaches to learn truthful mechanisms without outcome discretization, thereby enhancing algorithmic efficiency. The proposed concepts, network architecture, and learning techniques might offer potential value and provide new insights for automated mechanism design and differentiable economics."
2506.23134,"We construct and study the transition probability matrix of evolutionary games in which the number of players is finite (and relatively small) of such games. We use a simplified version of the population games studied by Sandholm. After laying out a general framework we concentrate on specific examples, involving the Iterated Prisoner's Dilemma, the Iterated Stag Hunt, and the Rock-Paper-Scissors game. Also we consider several revision protocols: Best Response, Pairwise Comparison, Pairwise Proportional Comparison etc. For each of these we explicitly construct the MC transition probability matrix and study its properties."
2506.23896,"Welfare maximization in bilateral trade has been extensively studied in recent years. Previous literature obtained incentive-compatible approximation mechanisms only for the private values case. In this paper, we study welfare maximization in bilateral trade with interdependent values. Designing mechanisms for interdependent settings is much more challenging because the values of the players depend on the private information of the others, requiring complex belief updates and strategic inference. We propose to classify information structures by quantifying the influence that a player's private signal has on their own valuation. We then paint a picture of where approximations are possible and impossible based on these information structures. Finally, we also study the possible approximation ratios for a natural family of information structures."
2506.2404,"We consider correlated equilibria in strategic games in an adversarial environment, where an adversary can compromise the public signal used by the players for choosing their strategies, while players aim at detecting a potential attack as soon as possible to avoid loss of utility. We model the interaction between the adversary and the players as a zero-sum game and we derive the maxmin strategies for both the defender and the attacker using the framework of quickest change detection. We define a class of adversarial strategies that achieve the optimal trade-off between attack impact and attack detectability and show that a generalized CUSUM scheme is asymptotically optimal for the detection of the attacks. Our numerical results on the Sioux-Falls benchmark traffic routing game show that the proposed detection scheme can effectively limit the utility loss by a potential adversary."
2507.00631,"Correctness is an emergent property of systems where exposing error is cheaper than committing it. In dynamic, low-trust environments, autonomous AI agents benefit from delegating work to sub-agents, yet correctness cannot be assured through upfront specification or centralized oversight. We propose a protocol that enforces correctness through collateralized claims in a recursive verification game. Tasks are published as intents, and solvers compete to fulfill them. Selected solvers carry out tasks under risk, with correctness checked post hoc by verifiers. Any challenger can challenge a result by staking against it to trigger the verification process. Incorrect agents are slashed and correct opposition is rewarded, with an escalation path that penalizes erroneous verifiers themselves. When incentives are aligned across solvers, challengers, and verifiers, falsification conditions make correctness the Nash equilibrium."
2507.01267,"The Shapley value is widely used for data valuation in data markets. However, explaining the Shapley value of an owner in a data coalition is an unexplored and challenging task. To tackle this, we formulate the problem of finding the counterfactual explanation of Shapley value in data coalitions. Essentially, given two data owners $A$ and $B$ such that $A$ has a higher Shapley value than $B$, a counterfactual explanation is a smallest subset of data entries in $A$ such that transferring the subset from $A$ to $B$ makes the Shapley value of $A$ less than that of $B$. We show that counterfactual explanations always exist, but finding an exact counterfactual explanation is NP-hard. Using Monte Carlo estimation to approximate counterfactual explanations directly according to the definition is still very costly, since we have to estimate the Shapley values of owners $A$ and $B$ after each possible subset shift. We develop a series of heuristic techniques to speed up computation by estimating differential Shapley values, computing the power of singular data entries, and shifting subsets greedily, culminating in the SV-Exp algorithm. Our experimental results on real datasets clearly demonstrate the efficiency of our method and the effectiveness of counterfactuals in interpreting the Shapley value of an owner."
2507.01413,"Large language models (LLMs) have demonstrated impressive capabilities as autonomous agents with rapidly expanding applications in various domains. As these agents increasingly engage in socioeconomic interactions, identifying their potential for undesirable behavior becomes essential. In this work, we examine scenarios where they can choose to collude, defined as secretive cooperation that harms another party. To systematically study this, we investigate the behavior of LLM agents acting as sellers in simulated continuous double auction markets. Through a series of controlled experiments, we analyze how parameters such as the ability to communicate, choice of model, and presence of environmental pressures affect the stability and emergence of seller collusion. We find that direct seller communication increases collusive tendencies, the propensity to collude varies across models, and environmental pressures, such as oversight and urgency from authority figures, influence collusive behavior. Our findings highlight important economic and ethical considerations for the deployment of LLM-based market agents."
2507.01453,"Censorship resilience is a fundamental assumption underlying the security of blockchain protocols. Additionally, the analysis of blockchain security from an economic and game theoretic perspective has been growing in popularity in recent years. In this work, we present a surprising rational censorship attack on blockchain censorship resilience when we adopt the analysis of blockchain security from a game theoretic lens and assume all users are rational. In our attack, a colluding group with sufficient voting power censors the remainder nodes such that the group alone can gain all the rewards from maintaining the blockchain. We show that if nodes are rational, coordinating this attack just requires a public read and write blackboard and we formally model the attack using a game theoretic framework. Furthermore, we note that to ensure the success of the attack, nodes need to know the total true voting power held by the colluding group. We prove that the strategy to join the rational censorship attack and also for nodes to honestly declare their power is a subgame perfect equilibrium in the corresponding extensive form game induced by our attack. Finally, we discuss the implications of the attack on blockchain users and protocol designers as well as some potential countermeasures."
2507.02464,"The CAP theorem asserts a trilemma between consistency, availability, and partition tolerance. This paper introduces a rigorous automata-theoretic and economically grounded framework that reframes the CAP trade-off as a constraint optimization problem. We model distributed systems as partition-aware state machines and embed economic incentive layers to stabilize consensus behavior across adversarially partitioned networks. By incorporating game-theoretic mechanisms into the global transition semantics, we define provable bounds on convergence, liveness, and correctness. Our results demonstrate that availability and consistency can be simultaneously preserved within bounded epsilon margins, effectively extending the classical CAP limits through formal economic control."
2507.02675,"We introduce Team Utility-Constrained Proximal Policy Optimization (TUC-PPO), a new deep reinforcement learning framework. It extends Proximal Policy Optimization (PPO) by integrating team welfare objectives specifically for spatial public goods games. Unlike conventional approaches where cooperation emerges indirectly from individual rewards, TUC-PPO instead optimizes a bi-level objective integrating policy gradients and team utility constraints. Consequently, all policy updates explicitly incorporate collective payoff thresholds. The framework preserves PPO's policy gradient core while incorporating constrained optimization through adaptive Lagrangian multipliers. Therefore, decentralized agents dynamically balance selfish and cooperative incentives. The comparative analysis demonstrates superior performance of this constrained deep reinforcement learning approach compared to unmodified PPO and evolutionary game theory baselines. It achieves faster convergence to cooperative equilibria and greater stability against invasion by defectors. The framework formally integrates team objectives into policy updates. This work advances multi-agent deep reinforcement learning for social dilemmas while providing new computational tools for evolutionary game theory research."
2507.02801,"In non-truthful auctions such as first-price and all-pay auctions, the independent strategic behaviors of bidders, with the corresponding Bayes-Nash equilibrium notion, are notoriously difficult to characterize and can cause undesirable outcomes. An alternative approach to achieve better outcomes in non-truthful auctions is to coordinate the bidders: let a mediator make incentive-compatible recommendations of correlated bidding strategies to the bidders, namely, implementing a Bayes correlated equilibrium (BCE). The implementation of BCE, however, requires knowledge of the distributions of bidders' private valuations, which is often unavailable. We initiate the study of the sample complexity of learning Bayes correlated equilibria in non-truthful auctions. We prove that the set of strategic-form BCEs in a large class of non-truthful auctions, including first-price and all-pay auctions, can be learned with a polynomial number $\tilde O(\frac{n}{\varepsilon^2})$ of samples of bidders' values. This moderate number of samples demonstrates the statistical feasibility of learning to coordinate bidders. Our technique is a reduction to the problem of estimating bidders' expected utility from samples, combined with an analysis of the pseudo-dimension of the class of all monotone bidding strategies."
2507.02931,"On e-commerce platforms, sellers typically bid for impressions from ad traffic to promote their products. However, for most sellers, the majority of their sales come from organic traffic. Consequently, the relationship between their ad spending and total sales remains uncertain, resulting in operational inefficiency. To address this issue, e-commerce platforms have recently introduced a novel platform-wide marketing service known as QuanZhanTui, which has reportedly enhanced marketing efficiency for sellers and driven substantial revenue growth for platforms. QuanZhanTui allows sellers to bid for impressions from the platform's entire traffic to boost their total sales without compromising the platform's user experience. In this paper, we investigate the mechanism design problem that arises from QuanZhanTui. The problem is formulated as a multi-objective optimization to balance sellers' welfare and platform's user experience. We first introduce the stock-constrained value maximizer model, which reflects sellers' dual requirements on marketing efficiency and platform-wide ROI. Then, we propose the Liquid Payment Auction (LPA), an auction designed to optimize the balanced objectives while accounting for sellers' requirements in the auto-bidding environment. It employs a simple payment rule based on sellers' liquid welfare, providing a clearer link between their investment and total sales. Under mild assumptions, we theoretically prove desirable properties of LPA, such as optimality and incentive compatibility. Extensive experiments demonstrate LPA's superior performance over conventional auctions in QuanZhanTui."
2507.0315,"Bargaining games, where agents attempt to agree on how to split utility, are an important class of games used to study economic behavior, which motivates a study of online learning algorithms in these games. In this work, we tackle when no-regret learning algorithms converge to Nash equilibria in bargaining games. Recent results have shown that online algorithms related to Follow the Regularized Leader (FTRL) converge to Nash equilibria (NE) in the last iterate in a wide variety of games, including zero-sum games. However, bargaining games do not have the properties used previously to established convergence guarantees, even in the simplest case of the ultimatum game, which features a single take-it-or-leave-it offer. Nonetheless, we establish that FTRL (without the modifications necessary for zero-sum games) achieves last-iterate convergence to an approximate NE in the ultimatum game along with a bound on convergence time under mild assumptions. Further, we provide experimental results to demonstrate that convergence to NE, including NE with asymmetric payoffs, occurs under a broad range of initial conditions, both in the ultimatum game and in bargaining games with multiple rounds. This work demonstrates how complex economic behavior (e.g. learning to use threats and the existence of many possible equilibrium outcomes) can result from using a simple learning algorithm, and that FTRL can converge to equilibria in a more diverse set of games than previously known."
2507.03252,"Building on the linear programming approach to competitive equilibrium pricing, we develop a general method for constructing iterative auctions that achieve Vickrey-Clarke-Groves (VCG) outcomes. We show how to transform a linear program characterizing competitive equilibrium prices into one that characterizes universal competitive equilibrium (UCE) prices, which elicit precisely the information needed to compute VCG payments. By applying a primal-dual algorithm to these transformed programs, we derive iterative Vickrey auctions that maintain a single price path, eliminating the overhead and incentive problems associated with multiple price paths used solely for payment calculations. We demonstrate the versatility of our method by developing a novel iterative Vickrey auction for the multi-unit setting and an iterative variant of the Product-Mix auction. The resulting auctions combine the transparency of iterative price discovery with the efficiency and incentive properties of the VCG mechanism."
2507.03359,"The Probabilistic Serial (PS) mechanism -- also known as the simultaneous eating algorithm -- is a canonical solution for the assignment problem under ordinal preferences. It guarantees envy-freeness and ordinal efficiency in the resulting random assignment. However, under cardinal preferences, its efficiency may degrade significantly: it is known that PS may yield allocations that are $\Omega(\ln{n})$-worse than Pareto optimal, but whether this bound is tight remained an open question.Our first result resolves this question by showing that the PS mechanism guarantees $(\ln(n)+2)$-approximate Pareto efficiency, even in the more general submodular setting introduced by Fujishige, Sano, and Zhan (ACM TEAC 2018). This is established by showing that, although the PS mechanism may incur a loss of up to $O(\sqrt{n})$ in utilitarian social welfare, it still achieves a $(\ln{n}+2)$-approximation to the maximum Nash welfare. In addition, we present a polynomial-time algorithm that computes an allocation which is envy-free and $e^{1/e}$-approximately Pareto-efficient, answering an open question posed by Tröbst and Vazirani (EC 2024).The PS mechanism also applies to the allocation of chores instead of goods. We prove that it guarantees an $n$-approximately Pareto-efficient allocation in this setting, and that this bound is asymptotically tight. This result provides the first known approximation guarantee for computing a fair and efficient allocation in the assignment problem with chores under cardinal preferences."
2507.03502,"Markov games with coupling constraints provide a natural framework to study constrained decision-making involving self-interested agents, where the feasibility of an individual agent's strategy depends on the joint strategies of the others. Such games arise in numerous real-world applications involving safety requirements and budget caps, for example, in environmental management, electricity markets, and transportation systems. While correlated equilibria have emerged as an important solution concept in unconstrained settings due to their computational tractability and amenability to learning, their constrained counterparts remain less explored. In this paper, we study constrained correlated equilibria-feasible policies where any unilateral modifications are either unprofitable or infeasible. We first characterize the constrained correlated equilibrium showing that different sets of modifications result in an equivalent notion, a result which may enable efficient learning algorithms. We then address existence conditions. In particular, we show that a strong Slater-type condition is necessary in games with playerwise coupling constraints, but can be significantly weakened when all players share common coupling constraints. Under this relaxed condition, we prove the existence of a constrained correlated equilibrium."
2507.03946,"We study fair division of indivisible mixed manna (items whose values may be positive, negative, or zero) among agents with additive valuations. Here, we establish that fairness -- in terms of a relaxation of envy-freeness -- and Pareto efficiency can always be achieved together. Specifically, our fairness guarantees are in terms of envy-freeness up to $k$ reallocations (EFR-$k$): An allocation $A$ of the indivisible items is said to be EFR-$k$ if there exists a subset $R$ of at most $k$ items such that, for each agent $i$, we can reassign items from within $R$ (in $A$) and obtain an allocation, $A^i$, which is envy-free for $i$. We establish that, when allocating mixed manna among $n$ agents with additive valuations, an EFR-$(n-1)$ and Pareto optimal (PO) allocation $A$ always exists. Further, the individual envy-free allocations $A^i$, induced by reassignments, are also PO. In addition, we prove that such fair and efficient allocations are efficiently computable when the number of agents, $n$, is fixed.We also obtain positive results focusing on EFR by itself (and without the PO desideratum). Specifically, we show that an EFR-$(n-1)$ allocation of mixed manna can be computed in polynomial time. In addition, we prove that when all the items are goods, an EFR-${\lfloor n/2 \rfloor}$ allocation exists and can be computed efficiently. Here, the $(n-1)$ bound is tight for chores and $\lfloor n/2 \rfloor$ is tight for goods.Our results advance the understanding of fair and efficient allocation of indivisible mixed manna and rely on a novel application of the Knaster-Kuratowski-Mazurkiewicz (KKM) Theorem in discrete fair division. We utilize weighted welfare maximization, with perturbed valuations, to achieve Pareto efficiency, and overall, our techniques are notably different from existing market-based approaches."
2507.0403,"This paper studies mechanism design for revenue maximization in a distribution-reporting setting, where the auctioneer does not know the buyers' true value distributions. Instead, each buyer reports and commits to a bid distribution in the ex-ante stage, which the auctioneer uses as input to the mechanism. Buyers strategically decide the reported distributions to maximize ex-ante utility, potentially deviating from their value distributions. As shown in previous work, classical prior-dependent mechanisms such as the Myerson auction fail to elicit truthful value distributions at the ex-ante stage, despite satisfying Bayesian incentive compatibility at the interim stage. We study the design of ex-ante incentive compatible mechanisms, and aim to maximize revenue in a prior-independent approximation framework. We introduce a family of threshold-augmented mechanisms, which ensures ex-ante incentive compatibility while boosting revenue through ex-ante thresholds. Based on these mechanisms, we construct the Peer-Max Mechanism, which achieves an either-or approximation guarantee for general non-identical distributions. Specifically, for any value distributions, its expected revenue either achieves a constant fraction of the optimal social welfare, or surpasses the second-price revenue by a constant fraction, where the constants depend on the number of buyers and a tunable parameter. We also provide an upper bound on the revenue achievable by any ex-ante incentive compatible mechanism, matching our lower bound up to a constant factor. Finally, we extend our approach to a setting where multiple units of identical items are sold to buyers with multi-unit demands."
2507.04148,"We consider a mechanism design setting with a single item and a single buyer who is uncertain about the value of the item. Both the buyer and the seller have a common model for the buyer's value, but the buyer discovers her true value only upon receiving the item. Mechanisms in this setting can be interpreted as randomized refund mechanisms, which allocate the item at some price and then offer a (partial and/or randomized) refund to the buyer in exchange for the item if the buyer is unsatisfied with her purchase. Motivated by their practical importance, we study the design of optimal deterministic mechanisms in this setting. We characterize optimal mechanisms as virtual value maximizers for both continuous and discrete type settings. We then use this characterization, along with bounds on the menu size complexity, to develop efficient algorithms for finding optimal and near-optimal deterministic mechanisms."
2507.04156,"We study adaptive two-sided assortment optimization for revenue maximization in choice-based matching platforms. The platform has two sides of agents, an initiating side, and a responding side. The decision-maker sequentially selects agents from the initiating side, shows each an assortment of agents from the responding side, and observes their choices. After processing all initiating agents, the responding agents are shown assortments and make their selections. A match occurs when two agents mutually select each other, generating pair-dependent revenue. Choices follow Multinomial Logit (MNL) models. This setting generalizes prior work focused on maximizing the number of matches under submodular demand assumptions, which do not hold in our revenue-maximization context. Our main contribution is the design of polynomial-time approximation algorithms with constant-factor guarantees. In particular, for general pairwise revenues, we develop a randomized algorithm that achieves a $(\frac{1}{2} - \epsilon)$-approximation in expectation for any $\epsilon > 0$. The algorithm is static and provides guarantees under various agent arrival settings, including fixed order, simultaneous processing, and adaptive selection. When revenues are uniform across all pairs involving any given responding-side agent, the guarantee improves to $(1 - \frac{1}{e} - \epsilon)$. In structural settings where responding-side agents share a common revenue-based ranking, we design a simpler adaptive deterministic algorithm achieving a $\frac{1}{2}$-approximation. Our approach leverages novel linear programming relaxations, correlation gap arguments, and structural properties of the revenue functions."
2507.04485,"We study deterministic mechanisms for the two-facility location problem. Given the reported locations of n agents on the real line, such a mechanism specifies where to build the two facilities. The single-facility variant of this problem admits a simple strategyproof mechanism that minimizes social cost. For two facilities, however, it is known that any strategyproof mechanism is $\Omega(n)$-approximate. We seek to circumvent this strong lower bound by relaxing the problem requirements. Following other work in the facility location literature, we consider a relaxed form of strategyproofness in which no agent can lie and improve their outcome by more than a constant factor. Because the aforementioned $\Omega(n)$ lower bound generalizes easily to constant-strategyproof mechanisms, we introduce a second relaxation: Allowing the facilities (but not the agents) to be located in the plane. Our first main result is a natural mechanism for this relaxation that is constant-approximate and constant-strategyproof. A characteristic of this mechanism is that a small change in the input profile can produce a large change in the solution. Motivated by this observation, and also by results in the facility reallocation literature, our second main result is a constant-approximate, constant-strategyproof, and Lipschitz continuous mechanism."
2507.04592,"We consider a revenue-optimizing auctioneer in single-dimensional environments with matroid feasibility constraints. Akbarpour and Li (2020) argue that any revenue-optimal, truthful, and credible mechanism requires unbounded communication. Recent works (Ferreira and Weinberg, 2020; Essaidi et al., 2022; Chitra et al., 2024) circumvent their impossibility for the single-item setting through the use of cryptographic commitments and blockchains. We extend their results to matroid feasibility constraints.At a high level, the two-round Deferred-Revelation Auction (DRA) discussed by Ferreira and Weinberg (2020) and Chitra et al., (2024) requires each bidder to submit a deposit, which is slashed upon presenting verifiable evidence indicating a deviation from the behaviour prescribed by the mechanism. We prove that the DRA satisfies truthfulness, credibility and revenue-optimality for all matroid environments when bidders' values are drawn from $\alpha$-strongly regular distributions for $\alpha > 0$. Further, we argue that the DRA is not credible for any feasibility constraint beyond matroids and for any smaller deposits than suggested by previous literature even in single-item environments.Finally, we modify the Ascending Deferred-Revelation Auction (ADRA) for single-item settings proposed by Essaidi et al., (2022) for arbitrary bidder value distributions. We implement a deferred-revelation variant of the deferred-acceptance auction for matroids due to Bikhchandani et al., (2011), which requires the same bounded communication as the ADRA."
2507.04717,"Number games play a central role in alternating normal play combinatorial game theory due to their real-number-like properties (Conway 1976). Here we undertake a critical re-examination: we begin with integer and dyadic games and identify subtle inconsistencies and oversights in the established literature (e.g. Siegel 2013), most notably, the lack of distinction between a game being a number and a game being equal to a number. After addressing this, we move to the general theory of number games. We analyze Conway's original definition and a later refinement by Siegel, and highlight conceptual gaps that have largely gone unnoticed. Through a careful dissection of these issues, we propose a more coherent and robust formulation. Specifically, we develop a refined characterization of numbers, via several subclasses, dyadics, canonical forms, their group theoretic closure and zugzwangs, that altogether better capture the essence of number games. This reconciliation not only clarifies existing ambiguities but also uncovers several open problems."
2507.05171,"We formulate a vector cost alternative to the scalarization method for weighting and combining multi-objective costs. The algorithm produces solutions to bimatrix games that are simultaneously pure, unique Nash equilibria and Pareto optimal with guarantees for avoiding worst case outcomes. We achieve this by enforcing exact potential game constraints to guide cost adjustments towards equilibrium, while minimizing the deviation from the original cost structure. The magnitude of this adjustment serves as a metric for differentiating between Pareto optimal solutions. We implement this approach in a racing competition between agents with heterogeneous cost structures, resulting in fewer collision incidents with a minimal decrease in performance. Code is available atthis https URL."
2507.05606,"Assortment optimization is a critical tool for online retailers aiming to maximize revenue. However, optimizing purely for revenue can lead to imbalanced sales across products, potentially causing supplier disengagement and reduced product diversity. To address these fairness concerns, we introduce a market share balancing constraint that limits the disparity in expected sales between any two offered products to a factor of a given parameter $\alpha$. We study both static and dynamic assortment optimization under the multinomial logit (MNL) model with this fairness constraint. In the static setting, the seller selects a distribution over assortments that satisfies the market share balancing constraint while maximizing expected revenue. We show that this problem can be solved in polynomial time, and we characterize the structure of the optimal solution: a product is included if and only if its revenue and preference weight exceed certain thresholds. We further extend our analysis to settings with additional feasibility constraints on the assortment and demonstrate that, given a $\beta$-approximation oracle for the constrained problem, we can construct a $\beta$-approximation algorithm under the fairness constraint. In the dynamic setting, each product has a finite initial inventory, and the seller implements a dynamic policy to maximize total expected revenue while respecting both inventory limits and the market share balancing constraint in expectation. We design a policy that is asymptotically optimal, with its approximation ratio converging to one as inventories grow large."
2507.05898,"Minimal balanced collections are a generalization of partitions of a finite set of n elements and have important applications in cooperative game theory and discrete mathematics. However, their number is not known beyond n = 4. In this paper we investigate the problem of generating minimal balanced collections and implement the Peleg algorithm, permitting to generate all minimal balanced collections till n = 7. Secondly, we provide practical algorithms to check many properties of coalitions and games, based on minimal balanced collections, in a way which is faster than linear programming-based methods. In particular, we construct an algorithm to check if the core of a cooperative game is a stable set in the sense of von Neumann and Morgenstern. The algorithm implements a theorem according to which the core is a stable set if and only if a certain nested balancedness condition is valid. The second level of this condition requires generalizing the notion of balanced collection to balanced sets."
2507.06035,"The design of energy markets is a subject of ongoing debate, particularly concerning the choice between the widely adopted Pay-as-Clear (PC) pricing mechanism and the alternative Pay-as-Bid (PB). These mechanisms determine how energy producers are compensated: under PC, all selected producers are paid the market-clearing price (i.e., the highest accepted bid), while under PB, each selected producer is paid their own submitted bid. The overarching objective is to meet the total demand for energy at minimal cost in the presence of strategic behavior. We present two key theoretical results. First, no mechanism can uniformly dominate PC or PB. This means that for any mechanism $\mathcal{M}$, there exists a market configuration and a mixed-strategy Nash equilibrium of PC (respectively for PB) that yields strictly lower total energy costs than under $\mathcal{M}$. Second, in terms of worst-case equilibrium outcomes, PB consistently outperforms PC: across all market instances, the highest possible equilibrium price under PB is strictly lower than that under PC. This suggests a structural robustness of PB to strategic manipulation. These theoretical insights are further supported by extensive simulations based on no-regret learning dynamics, which consistently yield lower average market prices in several energy market settings."
2507.07418,"Online advertising is a vital revenue source for major internet platforms. Recently, joint advertising, which assigns a bundle of two advertisers in an ad slot instead of allocating a single advertiser, has emerged as an effective method for enhancing allocation efficiency and revenue. However, existing mechanisms for joint advertising fail to realize the optimality, as they tend to focus on individual advertisers and overlook bundle structures. This paper identifies an optimal mechanism for joint advertising in a single-slot setting. For multi-slot joint advertising, we propose \textbf{BundleNet}, a novel bundle-based neural network approach specifically designed for joint advertising. Our extensive experiments demonstrate that the mechanisms generated by \textbf{BundleNet} approximate the theoretical analysis results in the single-slot setting and achieve state-of-the-art performance in the multi-slot setting. This significantly increases platform revenue while ensuring approximate dominant strategy incentive compatibility and individual rationality."
2507.07688,"Mobile Crowd Sensing (MCS) is the mechanism wherein people can contribute in data collection process using their own mobile devices which have sensing capabilities. Incentives are rewards that individuals get in exchange for data they submit. Reverse Auction Bidding (RAB) is a framework that allows users to place bids for selling the data they collected. Task providers can select users to buy data from by looking at bids. Using the RAB framework, MCS system can be optimized for better user utility, task provider utility and platform utility. In this paper, we propose a novel approach called Reverse Auction with Assumed Bid Cost (RA-ABC) which allows users to place a bid in the system before collecting data. We opine that performing the tasks only after winning helps in reducing resource consumption instead of performing the tasks before bidding. User Return on Investment (ROI) is calculated with which they decide to further participate or not by either increasing or decreasing their bids. We also propose an extension of RA-ABC with dynamic recruitment (RA-ABCDR) in which we allow new users to join the system at any time during bidding rounds. Simulation results demonstrate that RA-ABC and RA-ABCDR outperform the widely used Tullock Optimal Prize Function, with RA-ABCDR achieving up to 54.6\% higher user retention and reducing auction cost by 22.2\%, thereby ensuring more efficient and sustainable system performance. Extensive simulations confirm that dynamic user recruitment significantly enhances performance across stability, fairness, and cost-efficiency metrics."
2507.07711,"Online advertisements are a primary revenue source for e-commerce platforms. Traditional advertising models are store-centric, selecting winning stores through auction mechanisms. Recently, a new approach known as joint advertising has emerged, which presents sponsored bundles combining one store and one brand in ad slots. Unlike traditional models, joint advertising allows platforms to collect payments from both brands and stores. However, each of these two advertising models appeals to distinct user groups, leading to low click-through rates when users encounter an undesirable advertising model. To address this limitation and enhance generality, we propose a novel advertising model called ''Hybrid Advertising''. In this model, each ad slot can be allocated to either an independent store or a bundle. To find the optimal auction mechanisms in hybrid advertising, while ensuring nearly dominant strategy incentive compatibility and individual rationality, we introduce the Hybrid Regret Network (HRegNet), a neural network architecture designed for this purpose. Extensive experiments on both synthetic and real-world data demonstrate that the mechanisms generated by HRegNet significantly improve platform revenue compared to established baseline methods."
2507.07915,"We study non-atomic congestion games on parallel-link networks with affine cost functions. We investigate the power of machine-learned predictions in the design of coordination mechanisms aimed at minimizing the impact of selfishness. Our main results demonstrate that enhancing coordination mechanisms with a simple advice on the input rate can optimize the social cost whenever the advice is accurate (consistency), while only incurring minimal losses even when the predictions are arbitrarily inaccurate (bounded robustness). Moreover, we provide a full characterization of the consistent mechanisms that holds for all monotone cost functions, and show that our suggested mechanism is optimal with respect to the robustness. We further explore the notion of smoothness within this context: we extend our mechanism to achieve error-tolerance, i.e. we provide an approximation guarantee that degrades smoothly as a function of the prediction error, up to a predetermined threshold, while achieving a bounded robustness."
2507.08846,"Although resource allocation is a well studied problem in computer science, until the prevalence of distributed systems, such as computing clouds and data centres, the question had been addressed predominantly for single resource type scenarios. At the beginning of the last decade, with the introuction of Dominant Resource Fairness, the studies of the resource allocation problem has finally extended to the multiple resource type scenarios. Dominant Resource Fairness is a solution, addressing the problem of fair allocation of multiple resource types, among users with heterogeneous demands. Based on Max-min Fairness, which is a well established algorithm in the literature for allocating resources in the single resource type scenarios, Dominant Resource Fairness generalises the scheme to the multiple resource case. It has a number of desirable properties that makes it preferable over alternatives, such as Sharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness, and as such, it is widely adopted in distributed systems. In the present study, we revisit the original study, and analyse the structure of the algorithm in closer view, to come up with an alternative algorithm, which approximates the Dominant Resource Fairness allocation in fewer steps. We name the new algorithm Precomputed Dominant Resource Fairness, after its main working principle."
2507.08868,"Today, static cloud markets where consumers purchase services directly from providers are dominating. Thus, consumers neither negotiate the price nor the characteristics of the service. In recent years, providers have adopted more dynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to the reservation marketspace and the on-demand marketspace, Amazon offers a spot marketspace where consumers can bid for virtual machines. This spot marketspace was extended with spot blocks, and recently Amazon reworked the bidding options. In addition, other cloud providers, such as Virtustream, adopt dynamic trading mechanisms. The scientific community envisions autonomous multi-round negotiations for realizing future cloud marketspaces. Consequently, consumers and providers exchange offers and counteroffers to reach an agreement. This helps providers increase the utilization of their datacenters, while consumers can purchase highly customized cloud services.In the paper at hand, we present a survey on multi-round bilateral negotiation strategies for trading cloud resources. Thus, we analyzed peer-reviewed articles in order to identify trends, gaps, similarities, and the scope of such negotiation strategies. In addition, we surveyed the formalism that the scientific community uses to describe such strategies. Based on these findings, we derived recommendations for creating and documenting bilateral multi-round negotiation strategies to foster their implementation in the industry."
2507.09083,"This paper investigates the behavior of simulated AI agents (large language models, or LLMs) in auctions, introducing a novel synthetic data-generating process to help facilitate the study and design of auctions. We find that LLMs -- when endowed with chain of thought reasoning capacity -- agree with the experimental literature in auctions across a variety of classic auction formats. In particular, we find that LLM bidders produce results consistent with risk-averse human bidders; that they perform closer to theoretical predictions in obviously strategy-proof auctions; and, that they succumb to the winner's curse in common value settings. On prompting, we find that LLMs are not very sensitive to naive changes in prompts (e.g., language, currency) but can improve dramatically towards theoretical predictions with the right mental model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for less than $\$$400 with GPT-4 models (three orders of magnitude cheaper than modern auction experiments) and develop a framework flexible enough to run auction experiments with any LLM model and a wide range of auction design specifications, facilitating further experimental study by decreasing costs and serving as a proof-of-concept for the use of LLM proxies."
2507.09422,"We present for every $n\ge4$ an $n$-player game in normal form with payoffs in $\{0,1,2\}$ that has a unique, fully mixed, Nash equilibrium in which all the probability weights are irradical (i.e., algebraic but not closed form expressible even with $m$-th roots for any integer $m$)."
2507.09473,"Motivated by applications such as cloud platforms allocating GPUs to users or governments deploying mobile health units across competing regions, we study the dynamic allocation of a reusable resource to strategic agents with private valuations. Our objective is to simultaneously (i) maximize social welfare, (ii) satisfy multi-dimensional long-term cost constraints, and (iii) incentivize truthful reporting. We begin by numerically evaluating primal-dual methods widely used in constrained online optimization and find them to be highly fragile in strategic settings -- agents can easily manipulate their reports to distort future dual updates for future gain.To address this vulnerability, we develop an incentive-aware framework that makes primal-dual methods robust to strategic behavior. Our design combines epoch-based lazy updates -- where dual variables remain fixed within each epoch -- with randomized exploration rounds that extract approximately truthful signals for learning. Leveraging carefully designed online learning subroutines that can be of independent interest for dual updates, our mechanism achieves $\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost constraints, and ensures incentive alignment. This matches the performance of non-strategic allocation approaches while being robust to strategic agents."
2507.09544,"We study the problem of allocating indivisible chores among agents with additive cost functions in a fair and efficient manner. A major open question in this area is whether there always exists an allocation that is envy-free up to one chore (EF1) and Pareto optimal (PO). Our main contribution is to provide a positive answer to this question by proving the existence of such an allocation for indivisible chores under additive cost functions. This is achieved by a novel combination of a fixed point argument and a discrete algorithm, providing a significant methodological advance in this area.Our additional key contributions are as follows. We show that there always exists an allocation that is EF1 and fractional Pareto optimal (fPO), where fPO is a stronger efficiency concept than PO. We also show that an EF1 and PO allocation can be computed in polynomial time when the number of agents is constant. Finally, we extend all of these results to the more general setting of weighted EF1 (wEF1), which accounts for the entitlements of agents."
2507.09902,"Fictitious play (FP) is a natural learning dynamic in two-player zero-sum games. Samuel Karlin conjectured in 1959 that FP converges at a rate of $O(t^{-1/2})$ to Nash equilibrium, where $t$ is the number of steps played. However, Daskalakis and Pan disproved the stronger form of this conjecture in 2014, where \emph{adversarial} tie-breaking is allowed.This paper disproves Karlin's conjecture in its weaker form. In particular, there exists a 10-by-10 zero-sum matrix game, in which FP converges at a rate of $\Omega(t^{-1/3})$, and no ties occur except for the first step."
2507.09928,"We introduce a new solution concept for bounded rational agents in finite normal-form general-sum games called Generalized Quantal Response Equilibrium (GQRE) which generalizes Quantal Response Equilibrium~\citep{mckelvey1995quantal}. In our setup, each player maximizes a smooth, regularized expected utility of the mixed profiles used, reflecting bounded rationality that subsumes stochastic choice. After establishing existence under mild conditions, we present computationally efficient no-regret independent learning via smoothened versions of the Frank-Wolfe algorithm. Our algorithm uses noisy but correlated gradient estimates generated via a simulation oracle that reports on repeated plays of the game. We analyze convergence properties of our algorithm under assumptions that ensure uniqueness of equilibrium, using a class of gap functions that generalize the Nash gap. We end by demonstrating the effectiveness of our method on a set of complex general-sum games such as high-rank two-player games, large action two-player games, and known examples of difficult multi-player games."
2507.09972,"This paper outlines an incentive-driven and decentralized approach to verifying the veracity of digital content at scale. Widespread misinformation, an explosion in AI-generated content and reduced reliance on traditional news sources demands a new approach for content authenticity and truth-seeking that is fit for a modern, digital world. By using smart contracts and digital identity to incorporate 'trust' into the reward function for published content, not just engagement, we believe that it could be possible to foster a self-propelling paradigm shift to combat misinformation through a community-based governance model. The approach described in this paper requires that content creators stake financial collateral on factual claims for an impartial jury to vet with a financial reward for contribution. We hypothesize that with the right financial and social incentive model users will be motivated to participate in crowdsourced fact-checking and content creators will place more care in their attestations. This is an exploratory paper and there are a number of open issues and questions that warrant further analysis and exploration."
2507.10149,"We propose a mathematically rigorous framework for identifying and completing Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators. Unlike existing auction based systems such as CoWSwap, our approach introduces an asset matrix formulation that not only verifies feasibility using oracle prices and formal conservation laws but also completes partial CoW cycles of swap orders that are discovered using graph traversal and are settled using imbalance correction. We define bridging orders and show that the resulting execution is slippage free and capital preserving for LPs. Applied to real world Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW cycles and supports the insertion of synthetic orders for atomic cycle closure. This work can be thought of as the detailing of a potential delta-neutral strategy by liquidity providing market makers: a structured CoW cycle execution."
2507.1055,"The Value Problem for weighted timed games (WTGs) consists in determining, given a two-player weighted timed game with a reachability objective and a rational threshold, whether or not the value of the game exceeds the threshold. This problem was shown to be undecidable some ten years ago for WTGs making use of at least three clocks, and is known to be decidable for single-clock WTGs. In this paper, we establish undecidability for two-clock WTGs making use of non-negative weights, even in a time-bounded setting, closing the last remaining major gap in our algorithmic understanding of WTGs."
2507.10567,"We study protocols for verifying approximate optimality of strategies in multi-armed bandits and normal-form games. As the number of actions available to each player is often large, we seek protocols where the number of queries to the utility oracle is sublinear in the number of actions. We prove that such verification is possible for sufficiently smooth strategies that do not put too much probability mass on any specific action. We provide protocols for verifying that a smooth policy for a multi-armed bandit is $\varepsilon$-optimal. Our verification protocols require provably fewer arm queries than learning. Furthermore, we establish a nearly-tight lower bound on the query complexity of verification in our settings. As an application, we show how to use verification for bandits to achieve verification in normal-form games. This gives a protocol for verifying whether a given strategy profile is an approximate strong smooth Nash equilibrium, with a query complexity that is sublinear in the number of actions."
2507.10872,"We model a delivery platform facilitating transactions among three sides: buyers, stores, and couriers. In addition to buyers paying store-specific purchase prices and couriers receiving store--buyer-specific delivery compensation from the platform, each buyer has the option to directly tip for delivery from a specific store. An equilibrium consists of prices, compensations, tips, and transactions that clear the market, such that buyers receive deliveries from preferred stores considering the prices and tips they pay, and couriers deliver preferred orders considering the compensations and tips they receive.We illustrate the role of tips in pricing: Without tips, an equilibrium is only guaranteed to exist when there are at least as many couriers as buyers or stores. In contrast, with tips an equilibrium always exists. From an efficiency perspective, the optimal with-tip equilibrium welfare is always weakly larger than the optimal without-tip equilibrium welfare. However, we show that even with tips, efficient equilibria may not exist, and calculating the optimal equilibrium welfare is NP-hard. To address these challenges, we identify natural conditions on market structure that ensure the existence of efficient with-tip equilibria and allow these efficient equilibria to be computed in polynomial time."
2507.11214,"We introduce and study the problem of designing optimal contracts under fairness constraints on the task assignments and compensations. We adopt the notion of envy-free (EF) and its relaxations, $\epsilon$-EF and envy-free up to one item (EF1), in contract design settings. Unlike fair allocations, EF contracts are guaranteed to exist. However, computing any constant-factor approximation to the optimal EF contract is NP-hard in general, even using $\epsilon$-EF contracts. For this reason, we consider settings in which the number of agents or tasks is constant. Notably, while even with three agents, finding an EF contract better than $2/5$ approximation of the optimal is NP-hard, we are able to design an FPTAS when the number of agents is constant, under relaxed notions of $\epsilon$-EF and EF1. Moreover, we present a polynomial-time algorithm for computing the optimal EF contract when the number of tasks is constant. Finally, we analyze the price of fairness in contract design. We show that the price of fairness for exact EF contracts can be unbounded, even with a single task and two agents. In contrast, for EF1 contracts, the price of fairness is bounded between $\Omega(\sqrt{n})$ and $O(n^2)$, where $n$ is the number of agents."
2507.11366,"We study online optimization methods for zero-sum games, a fundamental problem in adversarial learning in machine learning, economics, and many other domains. Traditional methods approximate Nash equilibria (NE) using either regret-based methods (time-average convergence) or contraction-map-based methods (last-iterate convergence). We propose a new method based on Hamiltonian dynamics in physics and prove that it can characterize the set of NE in a finite (linear) number of iterations of alternating gradient descent in the unbounded setting, modulo degeneracy, a first in online optimization. Unlike standard methods for computing NE, our proposed approach can be parallelized and works with arbitrary learning rates, both firsts in algorithmic game theory. Experimentally, we support our results by showing our approach drastically outperforms standard methods."
2507.11419,"Bilateral trade is a central problem in algorithmic economics, and recent work has explored how to design trading mechanisms using no-regret learning algorithms. However, no-regret learning is impossible when budget balance has to be enforced at each time step. Bernasconi et al. [Ber+24] show how this impossibility can be circumvented by relaxing the budget balance constraint to hold only globally over all time steps. In particular, they design an algorithm achieving regret of the order of $\tilde O(T^{3/4})$ and provide a lower bound of $\Omega(T^{5/7})$.In this work, we interpolate between these two extremes by studying how the optimal regret rate varies with the allowed violation of the global budget balance constraint. Specifically, we design an algorithm that, by violating the constraint by at most $T^{\beta}$ for any given $\beta \in [\frac{3}{4}, \frac{6}{7}]$, attains regret $\tilde O(T^{1 - \beta/3})$. We complement this result with a matching lower bound, thus fully characterizing the trade-off between regret and budget violation. Our results show that both the $\tilde O(T^{3/4})$ upper bound in the global budget balance case and the $\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation obtained by Bernasconi et al. [Ber+24] are tight."
2507.11509,"A major open question in algorithmic game theory is whether normal-form correlated equilibria (NFCE) can be computed efficiently in succinct games such as extensive-form games [DFF+25,6PR24,FP23,HvS08,VSF08,PR08]. Motivated by this question, we study the associated Threshold problem: deciding whether there exists a correlated equilibrium whose value exceeds a given threshold. We prove that this problem is PSPACE-hard for NFCE in multiplayer extensive-form games with perfect recall, even for fixed thresholds. To contextualize this result, we also establish the complexity of the Threshold problem for Nash equilibria in this setting, showing it is ER-complete. These results uncover a surprising complexity reversal: while optimal correlated equilibria are computationally simpler than optimal Nash in normal-form games, the opposite holds in extensive-form games, where computing optimal correlated equilibria is provably harder. Building on this line of inquiry, we also address a related question by [VSF08], who introduced the notions of extensive-form correlated equilibrium (EFCE) and agent-form correlated equilibrium (AFCE). They asked how difficult the Threshold problem is for AFCE; we answer this question by proving that it is NP-hard, even in two-player games without chance nodes. Complementing our hardness results, we establish tight complexity classifications for the Threshold problem across several correlated equilibrium concepts - including EFCE, AFCE, normal-form coarse, extensive-form coarse, and agent-form coarse correlated equilibria. For each of these solution concepts in multiplayer stochastic extensive-form games with perfect recall, we prove NP-completeness by providing matching NP upper bounds to the previously known hardness results. Together, our results provide the most complete landscape to date for the complexity of optimal equilibrium computation in extensive-form games."
2507.11808,"This study introduces the \emph{edge-based Shapley value}, a novel allocation rule within cooperative game theory, specifically tailored for networked systems, where value is generated through interactions represented by edges. Traditional allocation rules, such as the Shapley and Myerson values, evaluate player contributions based on node-level characteristics, or connected components. However, these approaches often fail to adequately capture the functional role of edges, which are crucial in systems such as supply chains and digital platforms, where interactions, rather than individual agents, are the primary drivers of value. Our edge-based Shapley value shifts the characteristic function from node sets to edge sets, thereby enabling a more granular and context-sensitive evaluation of the contributions. We establish its theoretical foundations, demonstrate its relationship to classical allocation rules, and show that it retains key properties such as fairness and symmetry. To illustrate its applicability, we present two use cases: content platform networks and supply chain logistics (SCL). In both cases, our method produces intuitive and structurally consistent allocations, particularly in scenarios with overlapping routes, exclusive contracts or cost-sensitive paths. This framework offers a new perspective on value attribution in cooperative settings with complex interaction structures and provides practical tools for analyzing real-world economic and logistical networks."
2507.11883,"In this work, we examine a sequential setting of a cooperative game in which players arrive dynamically to form coalitions and complete tasks either together or individually, depending on the value created. Upon arrival, a new player as a decision maker faces two options: forming a new coalition or joining an existing one. We assume that players are greedy, i.e., they aim to maximize their rewards based on the information available at their arrival. The objective is to design an online value distribution policy that incentivizes players to form a coalition structure that maximizes social welfare. We focus on monotone and bounded cooperative games. Our main result establishes an upper bound of $\frac{3\mathsf{min}}{\mathsf{max}}$ on the competitive ratio for any irrevocable policy (i.e., one without redistribution), and proposes a policy that achieves a near-optimal competitive ratio of $\min\left\{\frac{1}{2}, \frac{3\mathsf{min}}{\mathsf{max}}\right\}$, where $\mathsf{min}$ and $\mathsf{max}$ denote the smallest and largest marginal contribution of any sub-coalition of players respectively. Finally, we also consider non-irrevocable policies, with alternative bounds only when the number of players is limited."
2507.12054,"This paper explores the economic interactions within modern crowdsourcing markets. In these markets, employers issue requests for tasks, platforms facilitate the recruitment of crowd workers, and workers complete tasks for monetary rewards. Recognizing that these roles serve distinct functions within the ecosystem, we introduce a three-party model that distinguishes among the principal (the requester), the intermediary (the platform), and the pool of agents (the workers). The principal, unable to directly engage with agents, relies on the intermediary to recruit and incentivize them. This interaction unfolds in two stages: first, the principal designs a profit-sharing contract with the intermediary; second, the intermediary implements a mechanism to select an agent to complete the delegated task.We analyze the proposed model as an extensive-form Stackelberg game. Our contributions are fourfold: (1) We fully characterize the subgame perfect equilibrium. In particular, we reduce the principal's contract design problem to a novel auction-theoretic formulation we term virtual value pricing, and reveals that linear contracts are optimal even when the task have multiple outcomes and agents' cost distributions are asymmetric. (2) To quantify the principal's utility loss from delegation and information asymmetry, we introduce the price of double marginalization (PoDM) and the classical price of anarchy (PoA), and derive tight or nearly tight bounds on both ratios under regular and monotone hazard rate (MHR) distributions. (3) We further examine these two ratios in a natural setting where the intermediary is restricted to anonymous pricing mechanisms, and show that similar qualitative insights continue to hold. (4) Finally, we extend our results on both ratios to a robust framework that accommodates scenarios in which the principal lacks precise information about the market size."
2507.12733,"Real-world pricing mechanisms are typically optimized using training data, a setting corresponding to the $\textit{pricing query complexity}$ problem in Mechanism Design. The previous work (LSTW23, SODA) studies the $\textit{single-distribution}$ case, with tight bounds of $\widetilde{\Theta}(\varepsilon^{-3})$ for a $\textit{general}$ distribution and $\widetilde{\Theta}(\varepsilon^{-2})$ for either a $\textit{regular}$ or $\textit{monotone-hazard-rate (MHR)}$ distribution.This can be directly interpreted as ''the query complexity of the $\textsf{Uniform Pricing}$ mechanism, in the $\textit{single-distribution}$ case''. Yet in the $\textit{multi-distribution}$ case, can the regularity and MHR conditions still lead to improvements over the tight bound $\widetilde{\Theta}(\varepsilon^{-3})$ for general distributions? We answer this question in the negative, by establishing a (near-)matching lower bound $\Omega(\varepsilon^{-3})$ for either $\textit{two regular distributions}$ or $\textit{three MHR distributions}$.We also address the $\textit{regret minimization}$ problem and, in comparison with the folklore upper bound $\widetilde{O}(T^{2 / 3})$ for general distributions (see, e.g., SW24, EC), establish a (near-)matching lower bound $\Omega(T^{2 / 3})$ for either $\textit{two regular distributions}$ or $\textit{three MHR distributions}$, via a black-box reduction. Again, this is in stark contrast to the tight bound $\widetilde{\Theta}(T^{1 / 2})$ for a single regular or MHR distribution."
2507.12984,"We consider the problem of online assignment of indivisible chores under \MMS\ criteria. The previous work proves that any deterministic online algorithm for chore division has a competitive ratio of at least 2. In this work, we improve this bound by showing that no deterministic online algorithm can obtain a competitive ratio better than $n$ for $n$ agents."
2507.13853,"This paper introduces a novel class of multi-stage resource allocation games that model real-world scenarios in which profitability depends on the balance between supply and demand, and where higher resource investment leads to greater returns. Our proposed framework, which incorporates the notion of profit loss due to insufficient player participation, gives rise to a Tullock-like functional form of the stage payoff structure when weighted fair proportional resource allocation is applied. We explore both centralized and Nash equilibrium strategies, establish sufficient conditions for their existence and uniqueness, and provide an iterative, semi-decentralized method to compute the Nash equilibrium in games with arbitrarily many players. Additionally, we demonstrate that the framework generalizes instances of several existing models, including Receding Horizon and Blotto games, and present a semi-analytical method for computing the unique Nash equilibrium within the Blotto setup. Our findings are validated through a numerical case study in smart mobility, highlighting the practical relevance and applicability of the proposed model."
2507.14039,"We study the problem of fair division of indivisible chores among $n$ agents in an online setting, where items arrive sequentially and must be allocated irrevocably upon arrival. The goal is to produce an $\alpha$-MMS allocation at the end. Several recent works have investigated this model, but have only succeeded in obtaining non-trivial algorithms under restrictive assumptions, such as the two-agent bi-valued special case (Wang and Wei, 2025), or by assuming knowledge of the total disutility of each agent (Zhou, Bai, and Wu, 2023). For the general case, the trivial $n$-MMS guarantee remains the best known, while the strongest lower bound is still only $2$.We close this gap on the negative side by proving that for any fixed $n$ and $\varepsilon$, no algorithm can guarantee an $(n - \varepsilon)$-MMS allocation. Notably, this lower bound holds precisely for every $n$, without hiding constants in big-$O$ notation, thereby exactly matching the trivial upper bound.Despite this strong impossibility result, we also present positive results. We provide an online algorithm that applies in the general case, guaranteeing a $\min\{n, O(k), O(\log D)\}$-MMS allocation, where $k$ is the maximum number of distinct disutilities across all agents and $D$ is the maximum ratio between the largest and smallest disutilities for any agent. This bound is reasonable across a broad range of scenarios and, for example, implies that we can achieve an $O(1)$-MMS allocation whenever $k$ is constant. Moreover, to optimize the constant in the important personalized bi-valued case, we show that if each agent has at most two distinct disutilities, our algorithm guarantees a $(2 + \sqrt{3}) \approx 3.7$-MMS allocation."
2507.14193,"Regulatory frameworks, such as the EU AI Act, encourage openness of general-purpose AI models by offering legal exemptions for ""open-source"" models. Despite this legislative attention on openness, the definition of open-source foundation models remains ambiguous. This paper models the strategic interactions among the creator of a general-purpose model (the generalist) and the entity that fine-tunes the general-purpose model to a specialized domain or task (the specialist), in response to regulatory requirements on model openness. We present a stylized model of the regulator's choice of an open-source definition to evaluate which AI openness standards will establish appropriate economic incentives for developers. Our results characterize market equilibria -- specifically, upstream model release decisions and downstream fine-tuning efforts -- under various openness regulations and present a range of effective regulatory penalties and open-source thresholds. Overall, we find the model's baseline performance determines when increasing the regulatory penalty vs. the open-source threshold will significantly alter the generalist's release strategy. Our model provides a theoretical foundation for AI governance decisions around openness and enables evaluation and refinement of practical open-source policies."
2507.14472,"Strategyproofness in network auctions requires that bidders not only report their valuations truthfully, but also do their best to invite neighbours from the social network. In contrast to canonical auctions, where the value-monotone allocation in Myerson's Lemma is a cornerstone, a general principle of allocation rules for strategyproof network auctions is still missing. We show that, due to the absence of such a principle, even extensions to multi-unit network auctions with single-unit demand present unexpected difficulties, and all pioneering researches fail to be strategyproof. For the first time in this field, we identify two categories of monotone allocation rules on networks: Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity (IP-MON). They encompass all existing allocation rules of network auctions as specific instances. For any given ID-MON or IP-MON allocation rule, we characterize the existence and sufficient conditions for the strategyproof payment rules, and show that among all such payment rules, the revenue-maximizing one exists and is computationally feasible. With these results, the obstacle of combinatorial network auction with single-minded bidders is now resolved."
2507.14957,"We study the fair division of indivisible items and provide new insights into the EFX problem, which is widely regarded as the central open question in fair division, and the PMMS problem, a strictly stronger variant of EFX. Our first result constructs a three-agent instance with two monotone valuations and one additive valuation in which no PMMS allocation exists. Since EFX allocations are known to exist under these assumptions, this establishes a formal separation between EFX and PMMS.We prove existence of fair allocations for three important special cases. We show that EFX allocations exist for personalized bivalued valuations, where for each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value $v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also prove that PMMS allocations exist for binary-valued MMS-feasible valuations, where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result holds even without assuming monotonicity of valuations and thus applies to the fair division of chores and mixed manna. Finally, we study a class of valuations called pair-demand valuations, which extend the well-studied unit-demand valuations to the case where each agent derives value from at most two items, and we show that PMMS allocations exist in this setting. Our proofs are constructive, and we provide polynomial-time algorithms for all three existence results."
2507.15325,"In many game-theoretic settings, agents are challenged with taking decisions against the uncertain behavior exhibited by others. Often, this uncertainty arises from multiple sources, e.g., incomplete information, limited computation, bounded rationality. While it may be possible to guide the agents' decisions by modeling each source, their joint presence makes this task particularly daunting. Toward this goal, it is natural for agents to seek protection against deviations around the emergent behavior itself, which is ultimately impacted by all the above sources of uncertainty. To do so, we propose that each agent takes decisions in face of the worst-case behavior contained in an ambiguity set of tunable size, centered at the emergent behavior so implicitly defined. This gives rise to a novel equilibrium notion, which we call strategically robust equilibrium. Building on its definition, we show that, when judiciously operationalized via optimal transport, strategically robust equilibria (i) are guaranteed to exist under the same assumptions required for Nash equilibria; (ii) interpolate between Nash and security strategies; (iii) come at no additional computational cost compared to Nash equilibria. Through a variety of experiments, including bi-matrix games, congestion games, and Cournot competition, we show that strategic robustness protects against uncertainty in the opponents' behavior and, surprisingly, often results in higher equilibrium payoffs - an effect we refer to as coordination via robustification."
2507.15735,"In the setup of selling one or more goods, various papers have shown, in various forms and for various purposes, that a small change in the distribution of a buyer's valuations may cause only a small change in the possible revenue that can be extracted. We prove a simple, clean, convenient, and general statement to this effect: let X and Y be random valuations on k additive goods, and let W(X,Y) be the Wasserstein (or ""earth mover's"") distance between them; then sqrt(Rev(X))-sqrt(Rev(Y)) <= sqrt(W(X,Y)). This further implies that a simple explicit modification of any optimal mechanism for X, namely, ""uniform discounting"", is guaranteed to be almost optimal for any Y that is close to X in the Wasserstein distance."
2507.15737,"Matching games is a one-to-one two sided market model introduced by Garrido-Lucero and Laraki, in which coupled agents' utilities are endogenously determined as the outcome of a strategic game. They refine the classical pairwise stability by requiring robustness to renegotiation and provide general conditions under which pairwise stable and renegotiation-proof outcomes exist as the limit of a deferred acceptance with competitions algorithm together with a renegotiation process. In this article, we extend their model to a general setting encompassing most of one-to-many matching markets and roommates models and specify two frameworks under which core stable and renegotiation-proof outcomes exist and can be efficiently computed."
2507.16209,"Fair allocation of indivisible goods is a fundamental problem at the interface of economics and computer science. Traditional approaches focus either on randomized allocations that are fair in expectation or deterministic allocations that are approximately fair. Recent work reconciles both these approaches via best-of-both-worlds guarantees, wherein one seeks randomized allocations that are fair in expectation (ex-ante fair) while being supported on approximately fair allocations (ex-post fair). Prior work has shown that under additive valuations, there always exists a randomized allocation that is ex-ante stochastic-dominance envy-free (sd-EF) and ex-post envy-free up to one good (EF1).Our work is motivated by the goal of achieving stronger ex-post fairness guarantees such as envy-freeness up to any good (EFX) along with meaningful ex-ante guarantees. We make the following contributions:1) We first consider lexicographic preferences, a subdomain of additive valuations where ex-post EFX allocations always exist and can be computed efficiently. On the negative side, we show that ex-ante sd-EF is fundamentally incompatible with ex-post EFX, prompting a relaxation of the ex-ante benchmark. We then present a poly. time algorithm that achieves ex-post EFX and PO together with ex-ante 9/10-EF. Our algorithm uses dependent rounding and leverages structural properties of EFX and PO allocations.2)For monotone valuations, we study EFX-with-charity: a relaxation of EFX where some goods remain unallocated, with no agent envying the unallocated pool. We show that ex-post EFX-with-charity can be achieved alongside ex-ante 0.5-EF.3)Finally, for subadditive valuations, we strengthen our previous ex-post guarantee to EFX-with-bounded-charity, where at most n-1 goods (n= no. of agents) remain unallocated, at the price of weakening the ex-ante guarantee to 0.5-proportionality."
2507.17183,"Understanding and predicting the behavior of large-scale multi-agents in games remains a fundamental challenge in multi-agent systems. This paper examines the role of heterogeneity in equilibrium formation by analyzing how smooth regret-matching drives a large number of heterogeneous agents with diverse initial policies toward unified behavior. By modeling the system state as a probability distribution of regrets and analyzing its evolution through the continuity equation, we uncover a key phenomenon in diverse multi-agent settings: the variance of the regret distribution diminishes over time, leading to the disappearance of heterogeneity and the emergence of consensus among agents. This universal result enables us to prove convergence to quantal response equilibria in both competitive and cooperative multi-agent settings. Our work advances the theoretical understanding of multi-agent learning and offers a novel perspective on equilibrium selection in diverse game-theoretic scenarios."
2507.17187,"In digital advertising, online platforms allocate ad impressions through real-time auctions, where advertisers typically rely on autobidding agents to optimize bids on their behalf. Unlike traditional auctions for physical goods, the value of an ad impression is uncertain and depends on the unknown click-through rate (CTR). While platforms can estimate CTRs more accurately using proprietary machine learning algorithms, these estimates/algorithms remain opaque to advertisers. This information asymmetry naturally raises the following questions: how can platforms disclose information in a way that is both credible and revenue-optimal? We address these questions through calibrated signaling, where each prior-free bidder receives a private signal that truthfully reflects the conditional expected CTR of the ad impression. Such signals are trustworthy and allow bidders to form unbiased value estimates, even without access to the platform's internal algorithms.We study the design of platform-optimal calibrated signaling in the context of second-price auction. Our first main result fully characterizes the structure of the optimal calibrated signaling, which can also be computed efficiently. We show that this signaling can extract the full surplus -- or even exceed it -- depending on a specific market condition. Our second main result is an FPTAS for computing an approximately optimal calibrated signaling that satisfies an IR condition. Our main technical contributions are: a reformulation of the platform's problem as a two-stage optimization problem that involves optimal transport subject to calibration feasibility constraints on the bidders' marginal bid distributions; and a novel correlation plan that constructs the optimal distribution over second-highest bids."
2507.17981,"In the context of single-winner ranked-choice elections between $m$ candidates, we explore the tradeoff between two competing goals in every democratic system: the majority principle (maximizing the social welfare) and the minority principle (safeguarding minority groups from overly bad outcomes).To measure the social welfare, we use the well-established framework of metric distortion subject to various objectives: utilitarian (i.e., total cost), $\alpha$-percentile (e.g., median cost for $\alpha = 1/2$), and egalitarian (i.e., max cost). To measure the protection of minorities, we introduce the $\ell$-mutual minority criterion, which requires that if a sufficiently large (parametrized by $\ell$) coalition $T$ of voters ranks all candidates in $S$ lower than all other candidates, then none of the candidates in $S$ should win. The highest $\ell$ for which the criterion is satisfied provides a well-defined measure of mutual minority protection (ranging from 1 to $m$).Our main contribution is the analysis of a recently proposed class of voting rules called $k$-Approval Veto, offering a comprehensive range of trade-offs between the two principles. This class spans between Plurality Veto (for $k=1$) - a simple voting rule achieving optimal metric distortion - and Vote By Veto (for $k=m$) which picks a candidate from the proportional veto core. We show that $k$-Approval Veto has minority protection at least $k$, and thus, it accommodates any desired level of minority protection. However, this comes at the price of lower social welfare. For the utilitarian objective, the metric distortion increases linearly in $k$. For the $\alpha$-percentile objective, the metric distortion is the optimal value of 5 for $\alpha \ge k/(k+1)$ and unbounded for $\alpha < k/(k+1)$. For the egalitarian objective, the metric distortion is the optimal value of 3 for all values of $k$."
2507.18251,"We study the fair division problem of allocating $m$ indivisible goods to $n$ agents with additive personalized bi-valued utilities. Specifically, each agent $i$ assigns one of two positive values $a_i > b_i > 0$ to each good, indicating that agent $i$'s valuation of any good is either $a_i$ or $b_i$. For convenience, we denote the value ratio of agent $i$ as $r_i = a_i / b_i$.We give a characterization to all the Pareto-optimal allocations. Our characterization implies a polynomial-time algorithm to decide if a given allocation is Pareto-optimal in the case each $r_i$ is an integer. For the general case (where $r_i$ may be fractional), we show that this decision problem is coNP-complete. Our result complements the existing results: this decision problem is coNP-complete for tri-valued utilities (where each agent's value for each good belongs to $\{a,b,c\}$ for some prescribed $a>b>c\geq0$), and this decision problem belongs to P for bi-valued utilities (where $r_i$ in our model is the same for each agent).We further show that an EFX allocation always exists and can be computed in polynomial time under the personalized bi-valued utilities setting, which extends the previous result on bi-valued utilities. We propose the open problem of whether an EFX and Pareto-optimal allocation always exists (and can be computed in polynomial time)."
2507.19461,"We study the fair division of indivisible chores among agents with additive disutility functions. We investigate the existence of allocations satisfying the popular fairness notion of envy-freeness up to any chore (EFX), and its multiplicative approximations. The existence of $4$-EFX allocations was recently established by Garg, Murhekar, and Qin (2025). We improve this guarantee by proving the existence of $2$-EFX allocations for all instances with additive disutilities. This approximation was previously known only for restricted instances such as bivalued disutilities (Lin, Wu, and Zhou (2025)) or three agents (Afshinmehr, Ansaripour, Danaei, and Mehlhorn (2024)).We obtain our result by providing a general framework for achieving approximate-EFX allocations. The approach begins with a suitable initial allocation and performs a sequence of local swaps between the bundles of envious and envied agents. For our main result, we begin with an initial allocation that satisfies envy-freeness up to one chore (EF1) and Pareto-optimality (PO); the existence of such an allocation was recently established in a major breakthrough by Mahara (2025). We further demonstrate the strength and generality of our framework by giving simple and unified proofs of existing results, namely (i) $2$-EFX for bivalued instances, (ii) 2-EFX for three agents, (iii) EFX when the number of chores is at most twice the number of agents, and (iv) $4$-EFX for all instances. We expect this framework to have broader applications in approximate-EFX due to its simplicity and generality."
2507.1972,"Traditional combinatorial spectrum auctions mainly rely on fixed bidding and matching processes, which limit participants' ability to adapt their strategies and often result in suboptimal social welfare in dynamic spectrum sharing environments. To address these limitations, we propose a novel approximately truthful combinatorial forward auction scheme with a flexible bidding mechanism aimed at enhancing resource efficiency and maximizing social welfare. In the proposed scheme, each buyer submits a combinatorial bid consisting of the base spectrum demand and adjustable demand ranges, enabling the auctioneer to dynamically optimize spectrum allocation in response to market conditions. To standardize the valuation across heterogeneous frequency bands, we introduce a Spectrum Equivalent Mapping (SEM) coefficient. A greedy matching algorithm is employed to determine winning bids by sorting buyers based on their equivalent unit bid prices and allocating resources within supply constraints. Simulation results demonstrate that the proposed flexible bidding mechanism significantly outperforms existing benchmark methods, achieving notably higher social welfare in dynamic spectrum sharing scenarios."
2507.20038,"Consider costly tasks that add up to the success of a project, and must be fitted by an agent into a given time-frame. This is an instance of the classic budgeted maximization problem, which admits an approximation scheme (FPTAS). Now assume the agent is performing these tasks on behalf of a principal, who is the one to reap the rewards if the project succeeds. The principal must design a contract to incentivize the agent. Is there still an approximation scheme? In this work, our ultimate goal is an algorithm-to-contract transformation, which transforms algorithms for combinatorial problems (like budgeted maximization) to tackle incentive constraints that arise in contract design. Our approach diverges from previous works on combinatorial contract design by avoiding an assumption of black-box access to a demand oracle.We first show how to ""lift"" the FPTAS for budgeted maximization to obtain the best-possible multiplicative and additive FPTAS for the contract design problem. We establish this through our ""local-global"" framework, in which the ""local"" step is to (approximately) solve a two-sided strengthened variant of the demand problem. The ""global"" step then utilizes the local one to find the approximately optimal contract. We apply our framework to a host of combinatorial constraints including multi-dimensional budgets, budgeted matroid, and budgeted matching constraints. In all cases we achieve an approximation essentially matching the best approximation for the purely algorithmic problem. We also develop a method to tackle multi-agent contract settings, where the team of working agents must abide to combinatorial feasibility constraints."
2507.20899,"We study the fair allocation of indivisible goods under cardinality constraints, where each agent must receive a bundle of fixed size. This models practical scenarios, such as assigning shifts or forming equally sized teams. Recently, variants of envy-freeness up to one/any item (EF1, EFX) were introduced for this setting, based on flips or exchanges of items. Namely, one can define envy-freeness up to one/any flip (EFF1, EFFX), meaning that an agent $i$ does not envy another agent $j$ after performing one or any one-item flip between their bundles that improves the value of $i$.We explore algorithmic aspects of this notion, and our contribution is twofold: we present both algorithmic and impossibility results, highlighting a stark contrast between the classic EFX concept and its flip-based analogue. First, we explore standard techniques used in the literature and show that they fail to guarantee EFFX approximations. On the positive side, we show that we can achieve a constant factor approximation guarantee when agents share a common ranking over item values, based on the well-known envy cycle elimination technique. This idea also leads to a generalized algorithm with approximation guarantees when agents agree on the top $n$ items and their valuation functions are bounded. Finally, we show that an algorithm that maximizes the Nash welfare guarantees a 1/2-EFF1 allocation, and that this bound is tight."
2507.20985,"Visualization dashboards are increasingly used in strategic settings like auctions to enhance decision-making and reduce strategic confusion. This paper presents behavioral experiments evaluating how different dashboard designs affect bid optimization in reverse first-price auctions. Additionally, we assess how dashboard designs impact the auction designer's ability to accurately infer bidders' preferences within the dashboard mechanism framework. We compare visualizations of the bid allocation rule, commonly deployed in practice, to alternatives that display expected utility. We find that utility-based visualizations significantly improve bidding by reducing cognitive demands on bidders. However, even with improved dashboards, bidders systematically under-shade their bids, driven by an implicit preference for certain wins in uncertain settings. As a result, dashboard-based mechanisms that assume fully rational or risk-neutral bidder responses to dashboards can produce significant estimation errors when inferring private preferences, which may lead to suboptimal allocations in practice. Explicitly modeling agents' behavioral responses to dashboards substantially improves inference accuracy, highlighting the need to align visualization design and econometric inference assumptions in practice."
2507.21795,"Commitments play a crucial role in game theory, shaping strategic interactions by either altering a player's own payoffs or influencing the incentives of others through outcome-contingent payments. While most research has focused on using commitments to achieve efficient equilibria, their potential applications beyond this goal remain largely unexplored. In this study, we introduce a non-coercive extortion mechanism that leverages commitments to outcome-contingent payments, demonstrating how a player or external agent can extract profit by offering rewards rather than threatening punishment. At the core of the mechanism is the introduction of sequentiality into a simultaneous-move game, fundamentally reshaping the strategic interaction. We derive the conditions under which extortion is successful, identify the class of games susceptible to this scheme, and determine both the maximum extractable profit and the minimum required payment. To illustrate the extortion mechanism, we apply it to 2x2 games, highlighting how even simple strategic settings can be vulnerable to this form of manipulation. Our results reveal strategic vulnerabilities in competitive settings, with significant implications for economic markets, diplomatic relations, and multi-agent systems operating in blockchain environments. This work broadens our understanding of commitments in game theory and raises critical questions about how to safeguard strategic interactions from exploitation through non-coercive extortion."
2507.22819,"The Colonel Blotto game, formulated by Emile Borel, involves players allocating limited resources to multiple battlefields simultaneously, with the winner being the one who allocates more resources to each battlefield. Computation of the Nash equilibrium, including of two person, zero sum, mixed strategy Colonel Blotto games have encountered issues of scalability and complexity owing to their PPAD completeness. This paper proposes an algorithm that computes the same value as the Nash equilibrium but cannot be characterized by the Fixed point Theorems of Tarski, Kakutani and Brouwer. The reduced complexity of the proposed algorithm is based on dispensing with the need for computing both players Nash strategies in Colonel Blotto games. The same algorithm can, therefore, be extended to all two person, zero sum games to compute the value of the Nash equilibrium. The theoretical superiority of the proposed algorithm over both LP solvers and another method that computes the same value of the game as its Nash equilibrium by a random assignment of probabilities to the active strategy set of the defending player, is also proposed."
2507.23149,"We introduce a new hypothesis testing-based learning dynamics in which players update their strategies by combining hypothesis testing with utility-driven exploration. In this dynamics, each player forms beliefs about opponents' strategies and episodically tests these beliefs using empirical observations. Beliefs are resampled either when the hypothesis test is rejected or through exploration, where the probability of exploration decreases with the player's (transformed) utility. In general finite normal-form games, we show that the learning process converges to a set of approximate Nash equilibria and, more importantly, to a refinement that selects equilibria maximizing the minimum (transformed) utility across all players. Our result establishes convergence to equilibrium in general finite games and reveals a novel mechanism for equilibrium selection induced by the structure of the learning dynamics."
2507.235,"We study online combinatorial allocation problems in the secretary setting, under interdependent values. In the interdependent model, introduced by Milgrom and Weber (1982), each agent possesses a private signal that captures her information about an item for sale, and the value of every agent depends on the signals held by all agents. Mauras, Mohan, and Reiffenhäuser (2024) were the first to study interdependent values in online settings, providing constant-approximation guarantees for secretary settings, where agents arrive online along with their signals and values, and the goal is to select the agent with the highest value.In this work, we extend this framework to {\em combinatorial} secretary problems, where agents have interdependent valuations over {\em bundles} of items, introducing additional challenges due to both combinatorial structure and interdependence. We provide $2e$-competitive algorithms for a broad class of valuation functions, including submodular and XOS functions, matching the approximation guarantees in the single-choice secretary setting. Furthermore, our results cover the same range of valuation classes for which constant-factor algorithms exist in classical (non-interdependent) secretary settings, while incurring only an additional factor of $2$ due to interdependence. Finally, we extend our study to strategic settings, and provide a $4e$-competitive truthful mechanism for online bipartite matching with interdependent valuations, again meeting the frontier of what is known, even without interdependence."
2508.0013,"Approval-based committee selection is a model of significant interest in social choice theory. In this model, we have a set of voters $\mathcal{V}$, a set of candidates $\mathcal{C}$, and each voter has a set $A_v \subset \mathcal{C}$ of approved candidates. For any committee size $K$, the goal is to choose $K$ candidates to represent the voters' preferences. We study a criterion known as \emph{approximate stability}, where a committee is $\lambda$-approximately-stable if there is no other committee $T$ preferred by at least $\frac{\lambda|T|}{k} |\mathcal{V}| $ voters. We prove that a $3.65$-approximately stable committee always exists and can be computed algorithmically in this setting. Our approach is based on finding a Lindahl equilibrium and sampling from a strongly Rayleigh distribution associated with it."
2508.00349,"Popular matchings provide a model of matching under preferences in which a solution corresponds to a Condorcet winner in voting systems. In a bipartite graph in which the vertices have preferences over their neighbours, a matching is defined to be popular if it does not lose in a majority vote against any matching. In this paper, we study the following three primary problems: only the vertices on one side have preferences; a generalization of this problem allowing ties in the preferences; and the vertices on both sides have preferences. A principal issue in the algorithmic aspects of popular matchings is how to determine the popularity of a matching, because it requires exponential time if the definition is simply applied. In the literature, we have the following two types of characterizations: a graph-structural characterization; and an optimization-based characterization described by maximum-weight matchings. The graph-structural characterizations are specifically designed for each problem and provide a combinatorial structure of the popular matchings. The optimization-based characterizations work in the same manner for all problems, while they do not reveal the structure of the popular matchings. A main contribution of this paper is to provide a direct connection of the above two types of characterizations for all of the three problems. Specifically, we prove that each characterization can be derived from the other, without relying on the fact that they characterize popular matchings. Our proofs offer a comprehensive understanding of the equivalence of the two types of characterizations, and suggest a new interpretation of the graph-structural characterization in terms of the dual optimal solution for the maximum-weight matching problem."
2508.00811,"The study of proportionality in multiwinner voting with approval ballots has received much attention in recent years. Typically, proportionality is captured by variants of the Justified Representation axiom, which say that cohesive groups of at least $\ell\cdot\frac{n}{k}$ voters (where $n$ is the total number of voters and $k$ is the desired number of winners) deserve $\ell$ representatives. The quantity $\frac{n}{k}$ is known as the Hare quota in the social choice literature. Another -- more demanding -- choice of quota is the Droop quota, defined as $\lfloor\frac{n}{k+1}\rfloor+1$. This quota is often used in multiwinner voting with ranked ballots: in algorithms such as Single Transferable Voting, and in proportionality axioms, such as Droop's Proportionality Criterion. A few authors have considered it in the context of approval ballots, but the existing analysis is far from comprehensive. The contribution of our work is a systematic study of JR-style axioms (and voting rules that satisfy them) defined using the Droop quota instead of the Hare quota. For each of the standard JR axioms (namely, JR, PJR, EJR, FPJR, FJR, PJR+ and EJR+), we identify a voting rule that satisfies the Droop version of this axiom. In some cases, it suffices to consider known rules (modifying the corresponding Hare proof, sometimes quite substantially), and in other cases it is necessary to modify the rules from prior work. Each axiom is more difficult to satisfy when defined using the Droop quota, so our results expand the frontier of satisfiable proportionality axioms. We complement our theoretical results with an experimental study, showing that for many probabilistic models of voter approvals, Droop JR/EJR+ are considerably more demanding than standard (Hare) JR/EJR+."
2508.03253,"We study the online fair division problem, where indivisible goods arrive sequentially and must be allocated immediately and irrevocably to agents. Prior work has established strong impossibility results for approximating classic fairness notions, such as envy-freeness and maximin share fairness, in this setting. In contrast, we focus on proportionality up to one good (PROP1), a natural relaxation of proportionality whose approximability remains unresolved. We begin by showing that three natural greedy algorithms fail to guarantee any positive approximation to PROP1 in general, against an adaptive adversary. This is surprising because greedy algorithms are commonly used in fair division and a natural greedy algorithm is known to be able to achieve PROP1 under additional information assumptions. This hardness result motivates the study of non-adaptive adversaries and the use of side-information, in the spirit of learning-augmented algorithms. For non-adaptive adversaries, we show that the simple uniformly random allocation can achieve a meaningful PROP1 approximation with high probability. Meanwhile, we present an algorithm that obtain robust approximation ratios against PROP1 when given predictions of the maximum item value (MIV). Interestingly, we also show that stronger fairness notions such as EF1, MMS, and PROPX remain inapproximable even with perfect MIV predictions."
2508.03818,"We study mechanisms for the facility location problem augmented with predictions of the optimal facility location. We demonstrate that an egalitarian viewpoint which considers both the maximum distance of any agent from the facility and the minimum utility of any agent provides important new insights compared to a viewpoint that just considers the maximum distance. As in previous studies, we consider performance in terms of consistency (worst case when predictions are accurate) and robustness (worst case irrespective of the accuracy of predictions). By considering how mechanisms with predictions can perform poorly, we design new mechanisms that are more robust. Indeed, by adjusting parameters, we demonstrate how to trade robustness for consistency. We go beyond the single facility problem by designing novel strategy proof mechanisms for locating two facilities with bounded consistency and robustness that use two predictions for where to locate the two facilities."
2508.03824,"Effectively interpreting strategic interactions among multiple agents requires us to infer each agent's objective from limited information. Existing inverse game-theoretic approaches frame this challenge in terms of a ""level-1"" inference problem, in which we take the perspective of a third-party observer and assume that individual agents share complete knowledge of one another's objectives. However, this assumption breaks down in decentralized, real-world scenarios like urban driving and bargaining, in which agents may act based on conflicting views of one another's objectives. We demonstrate the necessity of inferring agents' different estimates of each other's objectives through empirical examples, and by theoretically characterizing the prediction error of level-1 inference on fictitious gameplay data from linear-quadratic games. To address this fundamental issue, we propose a framework for level-2 inference to address the question: ""What does each agent believe about other agents' objectives?"" We prove that the level-2 inference problem is non-convex even in benign settings like linear-quadratic games, and we develop an efficient gradient-based approach for identifying local solutions. Experiments on a synthetic urban driving example show that our approach uncovers nuanced misalignments that level-1 methods miss."
2508.04668,"Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings that are common in the digital age. One key challenge of such environments is the ability of actors to create fake identities under fictitious false names, also known as ``Sybils.'' While some actors may do so to preserve their privacy, we show that this can inadvertently hamper inequality measurements. As we prove, it is impossible for measures satisfying the literature's canonical set of desired properties to assess the inequality of an economy that may harbor Sybils. We characterize the class of all Sybil-proof measures, and prove that they must satisfy relaxed version of the aforementioned properties. Furthermore, we show that the structure imposed restricts the ability to assess inequality at a fine-grained level. By applying our results, we prove that large classes of popular measures are not Sybil-proof, with the famous Gini coefficient being but one example out of many. Finally, we examine the dynamics leading to the creation of Sybils in digital and traditional settings."
2508.04779,"We study an online fair division problem where a fixed number of goods arrive sequentially and must be allocated to a given set of agents. Once a good arrives, its true value for each agent is revealed, and it has to be immediately and irrevocably allocated to some agent. The ultimate goal is to ensure envy-freeness up to any good (EFX) after all goods have been allocated. Unfortunately, as we show, approximate EFX allocations are unattainable in general, even under restrictive assumptions on the valuation functions.To address this, we follow a recent and fruitful trend of augmenting algorithms with predictions. Specifically, we assume access to a prediction vector estimating the agents' true valuations -- e.g., generated by a machine learning model trained on past data. Predictions may be unreliable, and we measure their error using the total variation distance from the true valuations, that is, the percentage of predicted value-mass that disagrees with the true values.Focusing on the natural class of additive valuations, we prove impossibility results even on approximate EFX allocations for algorithms that either ignore predictions or rely solely on them. We then turn to algorithms that use both the predictions and the true values and show strong lower bounds on the prediction accuracy that is required by any algorithm to compute an approximate EFX. These negative results persist even under identical valuations, contrary to the offline setting where exact EFX allocations always exist without the necessity of predictions. We then present an algorithm for two agents with identical valuations that uses effectively the predictions and the true values. The algorithm approximates EFX, with its guarantees improving as the accuracy of the predictions increases."
2508.05109,"Wireless networks are evolving from radio resource providers to complex systems that also involve computing, with the latter being distributed across edge and cloud facilities. Also, their optimization is shifting more and more from a performance to a value-oriented paradigm. The two aspects shall be balanced continuously, to maximize the utilities of Services Providers (SPs), users quality of experience and fairness, while meeting global constraints in terms of energy consumption and carbon footprint among others, with all these heterogeneous resources contributing. In this paper, we tackle the problem of communication and compute resource allocation under energy constraints, with multiple SPs competing to get their preferred resource bundle by spending a a fictitious currency budget. By modeling the network as a Fisher market, we propose a low complexity solution able to achieve high utilities and guarantee energy constraints, while also promoting fairness among SPs, as compared to a social optimal solution. The market equilibrium is proved mathematically, and numerical results show the multi-dimensional trade-off between utility and energy at different locations, with communication and computation-intensive services."
2508.05582,"This article presents a new three-player version of the bridge playing card game for the purpose of ending fixed partnerships so that the play can be more dynamic and flexible. By dynamically redefining team makeup in real time, this game design increases unpredictability and forces players to repeatedly update strategy. A novel scoring system is introduced to reduce biases present in conventional rule-based games by favoring fairness via reward systems that enforce tactical decision making and risk assessment. Being subject to regular bridge rules, this version tests players to collaborate without fixed friendships, requiring fluid adjustment and adaptive bidding behavior in real time. Strategic issues involve aggressive and defensive bidding, adaptable playing styles, and loss-seeking strategies specific to the three-player structure. The article discusses probabilistic issues of bidding, trump and no-trump declarative effects, and algorithmic methods to trick-taking. Simulation outcomes illustrate the efficiency of diverse strategies. The game's architecture is ideal for competitions and possibly influential in broadening entry pools for tournament card games."
2508.05844,"Motivated by applications in crowdsourcing, where a fixed sum of money is split among $K$ workers, and autobidding, where a fixed budget is used to bid in $K$ simultaneous auctions, we define a stochastic bandit model where arms belong to the $K$-dimensional probability simplex and represent the fraction of budget allocated to each task/auction. The reward in each round is the sum of $K$ stochastic rewards, where each of these rewards is unlocked with a probability that varies with the fraction of the budget allocated to that task/auction. We design an algorithm whose expected regret after $T$ steps is of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound. Improved bounds of order $K (\log T)^2$ are shown when the function mapping budget to probability of unlocking the reward (i.e., terminating the task or winning the auction) satisfies additional diminishing-returns conditions."
2508.06031,"Mobile edge computing (MEC) is a promising technology that enhances the efficiency of mobile blockchain networks, by enabling miners, often acted by mobile users (MUs) with limited computing resources, to offload resource-intensive mining tasks to nearby edge computing servers. Collaborative block mining can further boost mining efficiency by allowing multiple miners to form coalitions, pooling their computing resources and transaction data together to mine new blocks collaboratively. Therefore, an MEC-assisted collaborative blockchain network can leverage the strengths of both technologies, offering improved efficiency, security, and scalability for blockchain systems. While existing research in this area has mainly focused on the single-coalition collaboration mode, where each miner can only join one coalition, this work explores a more comprehensive multi-coalition collaboration mode, which allows each miner to join multiple coalitions. To analyze the behavior of miners and the edge computing service provider (ECP) in this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the ECP, as the leader, determines the prices of computing resources for all MUs. In Stage II, each MU decides the coalitions to join, resulting in an overlapping coalition formation (OCF) game; Subsequently, each coalition decides how many edge computing resources to purchase from the ECP, leading to an edge resource competition (ERC) game. We derive the closed-form Nash equilibrium for the ERC game, based on which we further propose an OCF-based alternating algorithm to achieve a stable coalition structure for the OCF game and develop a near-optimal pricing strategy for the ECP's resource pricing problem."
2508.0632,"The recent rise of renewable energy produced by many decentralized sources yields interesting market design challenges for electrical grids. Balancing supply and demand in such networks is both a temporal and spatial challenge due to capacity constraints. The recent surge in the number of household-owned batteries, especially in regions with rooftop solar adoption, offers mitigation potential but often acts misaligned with grid-level objectives. In fact, the decision to charge or discharge a household-owned battery is a strategic choice by each battery owner governed by selfish incentives. This calls for an analysis from a game-theoretic point of view.We initiate this timely research direction by considering a game-theoretic setting where selfish agents strategically charge or discharge their batteries to increase their profit. In particular, we study a Stackelberg-like market model where a third party introduces price incentives, aiming to optimize renewable energy utilization while preserving grid feasibility. For this, we study the existence and the quality of equilibria under various pricing strategies. We find that the existence of equilibria crucially depends on the chosen pricing and that the obtained social welfare varies widely. This calls for more sophisticated market models and pricing mechanisms and opens up a rich field for future research in Algorithmic Game Theory on incentives in renewable energy networks."
2508.06469,We provide a geometric proof that the random proposer mechanism is a $4$-approximation to the first-best gains from trade in bilateral exchange. We then refine this geometric analysis to recover the state-of-the-art approximation ratio of $3.15$.
2508.0655,"Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively adjusting the bid to avoid advertisers overspending. Existing mainstream two-stage methods, which first model bid landscapes and then optimize surplus using operations research techniques, are constrained by unimodal assumptions that fail to adapt for non-convex surplus curves and are vulnerable to cascading errors in sequential workflows. Additionally, existing discretization models of continuous values ignore the dependence between discrete intervals, reducing the model's error correction ability, while sample selection bias in bidding scenarios presents further challenges for prediction. To address these issues, this paper introduces Generative Bid Shading~(GBS), which comprises two primary components: (1) an end-to-end generative model that utilizes an autoregressive approach to generate shading ratios by stepwise residuals, capturing complex value dependencies without relying on predefined priors; and (2) a reward preference alignment system, which incorporates a channel-aware hierarchical dynamic network~(CHNet) as the reward model to extract fine-grained features, along with modules for surplus optimization and exploration utility reward alignment, ultimately optimizing both short-term and long-term surplus using group relative policy optimization~(GRPO). Extensive experiments on both offline and online A/B tests validate GBS's effectiveness. Moreover, GBS has been deployed on the Meituan DSP platform, serving billions of bid requests daily."
2508.06562,"The problem of delegated choice has been of long interest in economics and recently on computer science. We overview a list of papers on delegated choice problem, from classic works to recent papers with algorithmic perspectives."
2508.06619,"In a network game, players interact over a network and the utility of each player depends on his own action and on an aggregate of his neighbours' actions. Many real world networks of interest are asymmetric and involve a large number of heterogeneous players. This paper analyzes static network games using the framework of $\alpha$-potential games. Under mild assumptions on the action sets (compact intervals) and the utility functions (twice continuously differentiable) of the players, we derive an expression for an inexact potential function of the game, called the $\alpha$-potential function. Using such a function, we show that modified versions of the sequential best-response algorithm and the simultaneous gradient play algorithm achieve convergence of players' actions to a $2\alpha$-Nash equilibrium. For linear-quadratic network games, we show that $\alpha$ depends on the maximum asymmetry in the network and is well-behaved for a wide range of networks of practical interest. Further, we derive bounds on the social welfare of the $\alpha$-Nash equilibrium corresponding to the maximum of the $\alpha$-potential function, under suitable assumptions. We numerically illustrate the convergence of the proposed algorithms and properties of the learned $2\alpha$-Nash equilibria."
2508.06661,"Markov games and robust MDPs are closely related models that involve computing a pair of saddle point policies. As part of the long-standing effort to develop efficient algorithms for these models, the Filar-Tolwinski (FT) algorithm has shown considerable promise. As our first contribution, we demonstrate that FT may fail to converge to a saddle point and may loop indefinitely, even in small games. This observation contradicts the proof of FT's convergence to a saddle point in the original paper. As our second contribution, we propose Residual Conditioned Policy Iteration (RCPI). RCPI builds on FT, but is guaranteed to converge to a saddle point. Our numerical results show that RCPI outperforms other convergent algorithms by several orders of magnitude."
2508.06702,"Commitment is a well-established mechanism for fostering cooperation in human society and multi-agent systems. However, existing research has predominantly focused on the commitment that neglects the freedom of players to abstain from an interaction, limiting their applicability to many real-world scenarios where participation is often voluntary. In this paper, we present a two-stage game model to investigate the evolution of commitment-based behaviours and cooperation within the framework of the optional Prisoner's Dilemma game. In the pre-game stage, players decide whether to accept a mutual commitment. Once in the game, they choose among cooperation, defection, or exiting, depending on the formation of a pre-game commitment. We find that optional participation boosts commitment acceptance but fails to foster cooperation, leading instead to widespread exit behaviour. To address this, we then introduce and compare two institutional incentive approaches: i) a strict one (STRICT-COM) that rewards only committed players who cooperate in the game, and ii) a flexible one (FLEXIBLE-COM) that rewards any committed players who do not defect in the game. The results reveal that, while the strict approach is demonstrably better for promoting cooperation as the flexible rule creates a loophole for an opportunistic exit after committing, the flexible rule offers an efficient alternative for enhancing social welfare when such opportunistic behaviour results in a high gain. This study highlights the limitations of relying solely on voluntary participation and commitment to resolving social dilemmas, emphasising the importance of well-designed institutional incentives to promote cooperation and social welfare effectively."
2508.07145,"The inefficiency of selfish routing in congested networks is a classical problem in algorithmic game theory, often captured by the Price of Anarchy (i.e., the ratio between the social cost of decentralized decisions and that of a centrally optimized solution.) With the advent of autonomous vehicles, capable of receiving and executing centrally assigned routes, it is natural to ask whether their deployment can eliminate this inefficiency. At first glance, a central authority could simply compute an optimal traffic assignment and instruct each vehicle to follow its assigned path. However, this vision overlooks critical challenges: routes must be individually rational (no vehicle has an incentive to deviate), and in practice, multiple planning agents (e.g., different companies) may coexist and compete. Surprisingly, we show that such competition is not merely an obstacle but a necessary ingredient for achieving optimal outcomes. In this work, we design a routing mechanism that embraces competition and converges to an optimal assignment, starting from the classical Pigou network as a foundational case."
2508.07147,"We examine normal-form games in which players may \emph{pre-commit} to outcome-contingent transfers before choosing their actions. In the one-shot version of this model, Jackson and Wilkie showed that side contracting can backfire: even a game with a Pareto-optimal Nash equilibrium can devolve into inefficient equilibria once unbounded, simultaneous commitments are allowed. The root cause is a prisoner's dilemma effect, where each player can exploit her commitment power to reshape the equilibrium in her favor, harming overall welfare.To circumvent this problem we introduce a \emph{staged-commitment} protocol. Players may pledge transfers only in small, capped increments over multiple rounds, and the phase continues only with unanimous consent. We prove that, starting from any finite game $\Gamma$ with a non-degenerate Nash equilibrium $\vec{\sigma}$, this protocol implements every welfare-maximizing payoff profile that \emph{strictly} Pareto-improves $\vec{\sigma}$. Thus, gradual and bounded commitments restore the full efficiency potential of side payments while avoiding the inefficiencies identified by Jackson and Wilkie."
2508.07699,"The Nash Equilibrium (NE) assumes rational play in imperfect-information Extensive-Form Games (EFGs) but fails to ensure optimal strategies for off-equilibrium branches of the game tree, potentially leading to suboptimal outcomes in practical settings. To address this, the Extensive-Form Perfect Equilibrium (EFPE), a refinement of NE, introduces controlled perturbations to model potential player errors. However, existing EFPE-finding algorithms, which typically rely on average strategy convergence and fixed perturbations, face significant limitations: computing average strategies incurs high computational costs and approximation errors, while fixed perturbations create a trade-off between NE approximation accuracy and the convergence rate of NE refinements.To tackle these challenges, we propose an efficient adaptive regret minimization algorithm for computing approximate EFPE, achieving last-iterate convergence in two-player zero-sum EFGs. Our approach introduces Reward Transformation Counterfactual Regret Minimization (RTCFR) to solve perturbed games and defines a novel metric, the Information Set Nash Equilibrium (ISNE), to dynamically adjust perturbations. Theoretical analysis confirms convergence to EFPE, and experimental results demonstrate that our method significantly outperforms state-of-the-art algorithms in both NE and EFPE-finding tasks."
2508.08036,"In this paper, we study a truthful two-obnoxious-facility location problem, in which each agent has a private location in [0, 1] and a public optional preference over two obnoxious facilities, and there is a minimum distance constraint d between the two facilities. Each agent wants to be as far away as possible from the facilities that affect her, and the utility of each agent is the total distance from her to these facilities. The goal is to decide how to place the facilities in [0, 1] so as to incentivize agents to report their private locations truthfully as well as maximize the social utility. First, we consider the special setting where d = 0, that is, the two facilities can be located at any point in [0, 1]. We propose a deterministic strategyproof mechanism with approximation ratio of at most 4 and a randomized strategyproof mechanism with approximation ratio of at most 2, respectively. Then we study the general setting. We propose a deterministic strategyproof mechanism with approximation ratio of at most 8 and a randomized strategyproof mechanism with approximation ratio of at most 4, respectively. Furthermore, we provide lower bounds of 2 and 14/13 on the approximation ratio for any deterministic and any randomized strategyproof mechanism, respectively."
2508.08045,"We study a constrained distributed heterogeneous two-facility location problem, where a set of agents with private locations on the real line are divided into disjoint groups. The constraint means that the facilities can only be built in a given multiset of candidate locations and at most one facility can be built at each candidate location. Given the locations of the two facilities, the cost of an agent is the distance from her location to the farthest facility (referred to as max-variant). Our goal is to design strategyproof distributed mechanisms that can incentivize all agents to truthfully report their locations and approximately optimize some social objective. A distributed mechanism consists of two steps: for each group, the mechanism chooses two candidate locations as the representatives of the group based only on the locations reported by agents therein; then, it outputs two facility locations among all the representatives. We focus on a class of deterministic strategyproof distributed mechanisms and analyze upper and lower bounds on the distortion under the Average-of-Average cost (average of the average individual cost of agents in each group), the Max-of-Max cost (maximum individual cost among all agents), the Average-of-Max cost (average of the maximum individual cost among all agents in each group) and the Max-of-Average cost (maximum of the average individual cost of all agents in each group). Under four social objectives, we obtain constant upper and lower distortion bounds."
2508.08669,"Risk-aversion and bounded rationality are two key characteristics of human decision-making. Risk-averse quantal-response equilibrium (RQE) is a solution concept that incorporates these features, providing a more realistic depiction of human decision making in various strategic environments compared to a Nash equilibrium. Furthermore a class of RQE has recently been shown inarXiv:2406.14156to be universally computationally tractable in all finite-horizon Markov games, allowing for the development of multi-agent reinforcement learning algorithms with convergence guarantees. In this paper, we expand upon the study of RQE and analyze their computation in both two-player normal form games and discounted infinite-horizon Markov games. For normal form games we adopt a monotonicity-based approach allowing us to generalize previous results. We first show uniqueness and Lipschitz continuity of RQE with respect to player's payoff matrices under monotonicity assumptions, and then provide conditions on the players' degrees of risk aversion and bounded rationality that ensure monotonicity. We then focus on discounted infinite-horizon Markov games. We define the risk-averse quantal-response Bellman operator and prove its contraction under further conditions on the players' risk-aversion, bounded rationality, and temporal discounting. This yields a Q-learning based algorithm with convergence guarantees for all infinite-horizon general-sum Markov games."
2508.08682,"We consider the problem of resolving the envy of a given initial allocation by adding elements from a pool of goods. We give a characterization of the instances where envy can be resolved by adding an arbitrary number of copies of the items in the pool. From this characterization, we derive a polynomial-time algorithm returning a respective solution if it exists. If the number of copies or the total number of added items are bounded, the problem becomes computationally intractable even in various restricted cases. We perform a parameterized complexity analysis, focusing on the number of agents and the pool size as parameters. Notably, although not every instance admits an envy-free solution, our approach allows us to efficiently determine, in polynomial time, whether a solution exists-an aspect that is both theoretically interesting and far from trivial."
2508.08772,"Online bidding is crucial in mobile ecosystems, enabling real-time ad allocation across billions of devices to optimize performance and user experience. Improving ad allocation efficiency is a long-standing research problem, as it directly enhances the economic outcomes for all participants in advertising platforms. This paper investigates the design of optimal boost factors in online bidding while incorporating quality value (the impact of displayed ads on publishers' long-term benefits). To address the divergent interests on quality, we establish a three-party auction framework with a unified welfare metric of advertiser and publisher. Within this framework, we derive the theoretical efficiency lower bound for C-competitive boost in second-price single-slot auctions, then design a novel quality-involved Boosting (q-Boost) algorithm for computing the optimal boost factor. Experimental validation on Alibaba's public dataset (AuctionNet) demonstrates 2%-6% welfare improvements over conventional approaches, proving our method's effectiveness in real-world settings."
2508.0881,"We study a temporal voting model where voters have dynamic preferences over a set of public chores -- projects that benefit society, but impose individual costs on those affected by their implementation. We investigate the computational complexity of optimizing utilitarian and egalitarian welfare. Our results show that while optimizing the former is computationally straightforward, minimizing the latter is computationally intractable, even in very restricted cases. Nevertheless, we identify several settings where this problem can be solved efficiently, either exactly or by an approximation algorithm. We also examine the effects of enforcing temporal fairness and its impact on social welfare, and analyze the competitive ratio of online algorithms. We then explore the strategic behavior of agents, providing insights into potential malfeasance in such decision-making environments. Finally, we discuss a range of fairness measures and their suitability for our setting."
2508.0928,"Traffic is a significant source of global carbon emissions. In this paper, we study how carbon pricing can be used to guide traffic towards equilibria that respect given emission budgets. In particular, we consider a general multi-commodity flow model with flow-dependent externalities. These externalities may represent carbon emissions, entering a priced area, or the traversal of paths regulated by tradable credit schemes.We provide a complete characterization of all flows that can be attained as Wardrop equilibria when assigning a single price to each externality. More precisely, we show that every externality budget achievable by any feasible flow in the network can also be achieved as a Wardrop equilibrium by setting appropriate prices. For extremal and Pareto-minimal budgets, we show that there are prices such that all equilibria respect the budgets. Although the proofs of existence of these particular prices rely on fixed-point arguments and are non-constructive, we show that in the case where the equilibrium minimizes a convex potential, the prices can be obtained as Lagrange multipliers of a suitable convex program. In the case of a single externality, we prove that the total externality caused by the traffic flow is decreasing in the price. For increasing, continuous, and piecewise affine travel time functions with a single externality, we give an output-polynomial algorithm that computes all equilibria implementable by pricing the externality. Even though there are networks where the output size is exponential in the input size, we show that the minimal price obeying a given budget can be computed in polynomial time. This allows the efficient computation of the market price of tradable credit schemes. Overall, our results show that carbon pricing is a viable and (under mild assumptions) tractable approach to achieve all feasible emission goals in traffic networks."
2508.0934,"Classification algorithms based on Artificial Intelligence (AI) are nowadays applied in high-stakes decisions in finance, healthcare, criminal justice, or education. Individuals can strategically adapt to the information gathered about classifiers, which in turn may require algorithms to be re-trained. Which collective dynamics will result from users' adaptation and algorithms' retraining? We apply evolutionary game theory to address this question. Our framework provides a mathematically rigorous way of treating the problem of feedback loops between collectives of users and institutions, allowing to test interventions to mitigate the adverse effects of strategic adaptation. As a case study, we consider institutions deploying algorithms for credit lending. We consider several scenarios, each representing different interaction paradigms. When algorithms are not robust against strategic manipulation, we are able to capture previous challenges discussed in the strategic classification literature, whereby users either pay excessive costs to meet the institutions' expectations (leading to high social costs) or game the algorithm (e.g., provide fake information). From this baseline setting, we test the role of improving gaming detection and providing algorithmic recourse. We show that increased detection capabilities reduce social costs and could lead to users' improvement; when perfect classifiers are not feasible (likely to occur in practice), algorithmic recourse can steer the dynamics towards high users' improvement rates. The speed at which the institutions re-adapt to the user's population plays a role in the final outcome. Finally, we explore a scenario where strict institutions provide actionable recourse to their unsuccessful users and observe cycling dynamics so far unnoticed in the literature."
2508.09367,"In budget-feasible mechanism design, a buyer wishes to procure a set of items of maximum value from self-interested players. We have a valuation function $v:2^U \to \mathbb{R}_+$, where $U$ is the set of all items, where $v(S)$ specifies the value obtained from set $S$ of items. The entirety of current work on budget-feasible mechanisms has focused on the single-dimensional setting, wherein each player holds a single item $e$ and incurs a private cost $c_e$ for supplying item $e$.We introduce multidimensional budget feasible mechanism design: the universe $U$ is now partitioned into item-sets $\{G_i\}$ held by the different players, and each player $i$ incurs a private cost $c_i(S_i)$ for supplying the set $S_i\subseteq G_i$ of items. A budget-feasible mechanism is a mechanism that is truthful, and where the total payment made to the players is at most some given budget $B$. The goal is to devise a budget-feasible mechanism that procures a set of items of large value. We obtain the first approximation guarantees for multidimensional budget feasible mechanism design.Our contributions are threefold. First, we prove an impossibility result showing that the standard benchmark used in single-dimensional budget-feasible mechanism design, namely the algorithmic optimum is inadequate in that no budget-feasible mechanism can achieve good approximation relative to this. We identify that the chief underlying issue here is that there could be a monopolist which prevents a budget-feasible mechanism from obtaining good guarantees. Second, we devise an alternate benchmark, $OPT_{Bench}$, that allows for meaningful approximation guarantees, thereby yielding a metric for comparing mechanisms. Third, we devise budget-feasible mechanisms that achieve constant-factor approximation guarantees with respect to this benchmark for XOS valuations."
2508.09741,"We introduce the framework of project submission games, capturing the behavior of project proposers in participatory budgeting (and multiwinner elections). Here, each proposer submits a subset of project proposals, aiming at maximizing the total cost of those that get funded. We focus on finding conditions under which pure Nash equilibria (NE) exist in our games, and on the complexity of checking whether they exist. We also seek algorithms for computing best responses for the proposers"
2508.09869,"We consider a resource allocation problem with agents that have additive ternary valuations for a set of indivisible items, and bound the price of envy-free up to one item (EF1) allocations. For a large number $n$ of agents, we show a lower bound of $\Omega(\sqrt{n})$, implying that the price of EF1 is no better than when the agents have general subadditive valuations. We then focus on instances with few agents and show that the price of EF1 is $12/11$ for $n=2$, and between $1.2$ and $1.256$ for $n=3$."
2508.10129,"We study parameterized approximability of three optimization problems related to stable matching: (1) Min-BP-SMI: Given a stable marriage instance and a number k, find a size-at-least-k matching that minimizes the number $\beta$ of blocking pairs; (2) Min-BP-SRI: Given a stable roommates instance, find a matching that minimizes the number $\beta$ of blocking pairs; (3) Max-SMTI: Given a stable marriage instance with preferences containing ties, find a maximum-size stable matching.The first two problems are known to be NP-hard to approximate to any constant factor and W[1]-hard with respect to $\beta$, making the existence of an EPTAS or FPT-algorithms unlikely. We show that they are W[1]-hard with respect to $\beta$ to approximate to any function of $\beta$. This means that unless FPT=W[1], there is no FPT-approximation scheme for the parameter $\beta$. The last problem (Max-SMTI) is known to be NP-hard to approximate to factor-29/33 and W[1]-hard with respect to the number of ties. We complement this and present an FPT-approximation scheme for the parameter ""number of agents with ties""."
2508.10189,"We consider the problem of routing for logistics purposes, in a contested environment where an adversary attempts to disrupt the vehicle along the chosen route. We construct a game-theoretic model that captures the problem of optimal routing in such an environment. While basic robust deterministic routing plans are already challenging to devise, they tend to be predictable, which can limit their effectiveness. By introducing calculated randomness via modeling the route planning process as a two-player zero-sum game, we compute immediately deployable plans that are diversified and harder to anticipate. Although solving the game exactly is intractable in theory, our use of the double-oracle framework enables us to achieve computation times on the order of seconds, making the approach operationally viable. In particular, the framework is modular enough to accommodate specialized routing algorithms as oracles. We evaluate our method on real-world scenarios, showing that it scales effectively to realistic problem sizes and significantly benefits from explicitly modeling the adversary's capabilities, as demonstrated through ablation studies and comparisons with baseline approaches."
2508.10204,"Equilibria of realistic multiplayer games constitute a key solution concept both in practical applications, such as online advertising auctions and electricity markets, and in analytical frameworks used to study strategic voting in elections or assess policy impacts in integrated assessment models. However, efficiently computing these equilibria requires games to have a carefully designed structure and satisfy numerous restrictions; otherwise, the computational complexity becomes prohibitive. In particular, finding even approximate Nash equilibria in general-sum normal-form games with two or more players is known to be PPAD-complete. Current state-of-the-art algorithms for computing Nash equilibria in multiplayer normal-form games either suffer from poor scalability due to their reliance on non-convex optimization solvers, or lack guarantees of convergence to a true equilibrium. In this paper, we propose a formulation of the Nash equilibrium computation problem as a polynomial complementarity problem and develop a complete and sound spatial branch-and-bound algorithm based on this formulation. We provide a qualitative analysis arguing why one should expect our approach to perform well, and show the relationship between approximate solutions to our formulation and that of computing an approximate Nash equilibrium. Empirical evaluations demonstrate that our algorithm substantially outperforms existing complete methods."
2508.11359,"This work asks whether a human interacting with a generative AI system can merge into a single individual through iterative, information-driven interactions. We model the interactions between a human, a generative AI system, and the human's wider environment as a three-player stochastic game. We use information-theoretic measures (entropy, mutual information, and transfer entropy) to show that our modelled human and generative AI are able to form an aggregate individual in the sense of Krakauer et al. (2020). The model we present is able to answer interesting questions around the symbiotic nature of humans and AI systems, including whether LLM-driven chatbots are acting as parasites, feeding on the information provided by humans."
2508.11874,"Algorithm design and analysis is a cornerstone of computer science, but it confronts a major challenge. Proving an algorithm's performance guarantee across all inputs has traditionally required extensive and often error-prone human effort. While AI has shown great success in finding solutions to specific problem instances, automating the discovery of general algorithms with such provable guarantees has remained a significant barrier. This challenge stems from the difficulty of integrating the creative process of algorithm design with the rigorous process of formal analysis. To address this gap, we propose LegoNE, a framework that tightly fuses these two processes for the fundamental and notoriously difficult problem of computing approximate Nash equilibria. LegoNE automatically translates any algorithm written by a simple Python-like language into a constrained optimization problem. Solving this problem derives and proves the algorithm's approximation bound. Using LegoNE, a state-of-the-art large language model rediscovered the state-of-the-art algorithm for two-player games within hours, a feat that had taken human researchers 15 years to achieve. For three-player games, the model discovered a novel algorithm surpassing all existing human-designed ones. This work demonstrates a new human-machine collaborative paradigm for theoretical science: humans reason at a higher-abstract level, using symbols to compress the search space, and AI explores within it, achieving what neither could alone."
2508.12453,"Although approximate notions of envy-freeness-such as envy-freeness up to one good (EF1)-have been extensively studied for indivisible goods, the seemingly simpler fairness concept of proportionality up to one good (PROP1) has received far less attention. For additive valuations, every EF1 allocation is PROP1, and well-known algorithms such as Round-Robin and Envy-Cycle Elimination compute such allocations in polynomial time. PROP1 is also compatible with Pareto efficiency, as maximum Nash welfare allocations are EF1 and hence PROP1.We ask whether these favorable properties extend to non-additive valuations. We study a broad class of allocation instances with {\em satiating goods}, where agents have non-negative valuation functions that need not be monotone, allowing for negative marginal values. We present the following results:- EF1 implies PROP1 for submodular valuations over satiating goods, ensuring existence and efficient computation via Envy-Cycle Elimination for monotone submodular valuations;- Round-robin computes a partial PROP1 allocation after the second-to-last round for satiating submodular goods and a complete PROP1 for monotone submodular valuations;- PROP1 allocations for satiating subadditive goods can be computed in polynomial-time;- Maximum Nash welfare allocations are PROP1 for monotone submodular goods, revealing yet another facet of their ``unreasonable fairness.''"
2508.12549,"We consider the problem of assigning items to platforms where each item has a utility associated with each of the platforms to which it can be assigned. Each platform has a soft constraint over the total number of items it serves, modeled via a convex cost function. Additionally, items are partitioned into groups, and each platform also incurs group-specific convex cost over the number of items from each group that can be assigned to the platform. These costs promote group fairness by penalizing imbalances, yielding a soft variation of fairness notions introduced in prior work, such as Restricted Dominance and Minority protection. Restricted Dominance enforces upper bounds on group representation, while Minority protection enforces lower bounds. Our approach replaces such hard constraints with cost-based penalties, allowing more flexible trade-offs. Our model also captures Nash Social Welfare kind of objective.The cost of an assignment is the sum of the values of all the cost functions across all the groups and platforms. The objective is to find an assignment that minimizes the cost while achieving a total utility that is at least a user-specified threshold. The main challenge lies in balancing the overall platform cost with group-specific costs, both governed by convex functions, while meeting the utility constraint. We present an efficient polynomial-time approximation algorithm, supported by theoretical guarantees and experimental evaluation. Our algorithm is based on techniques involving linear programming and network flows. We also provide an exact algorithm for a special case with uniform utilities and establish the hardness of the general problem when the groups can intersect arbitrarily."
2508.13432,"We study the fair allocation of indivisible goods across groups of agents, where each agent fully enjoys all goods allocated to their group. We focus on groups of two (couples) and other groups of small size. For two couples, an EF1 allocation -- one in which all agents find their group's bundle no worse than the other group's, up to one good -- always exists and can be found efficiently. For three or more couples, EF1 allocations need not exist.Turning to proportionality, we show that, whenever groups have size at most $k$, a PROP$k$ allocation exists and can be found efficiently. In fact, our algorithm additionally guarantees (fractional) Pareto optimality, and PROP1 to the first agent in each group, PROP2 to the second, etc., for an arbitrary agent ordering. In special cases, we show that there are PROP1 allocations for any number of couples."
2508.13473,"Recommendation systems are used in a range of platforms to maximize user engagement through personalization and the promotion of popular content. It has been found that such recommendations may shape users' opinions over time. In this paper, we ask whether reactive users, who are cognizant of the influence of the content they consume, can prevent such changes by adaptively adjusting their content consumption choices. To this end, we study users' opinion dynamics under two types of stochastic policies: a passive policy where the probability of clicking on recommended content is fixed and a reactive policy where clicking probability adaptively decreases following large opinion drifts. We analytically derive the expected opinion and user utility under these policies. We show that the adaptive policy can help users prevent opinion drifts and that when a user prioritizes opinion preservation, the expected utility of the adaptive policy outperforms the fixed policy. We validate our theoretical findings through numerical simulations. These findings help better understand how user-level strategies can challenge the biases induced by recommendation systems."
2508.13841,"We study strategic candidate positioning in multidimensional spatial-voting elections. Voters and candidates are represented as points in $\mathbb{R}^d$, and each voter supports the candidate that is closest under a distance induced by an $\ell_p$-norm. We prove that computing an optimal location for a new candidate is NP-hard already against a single opponent, whereas for a constant number of issues the problem is tractable: an $O(n^{d+1})$ hyperplane-enumeration algorithm and an $O(n \log n)$ radial-sweep routine for $d=2$ solve the task exactly. We further derive the first approximation guarantees for the general multi-candidate case and show how our geometric approach extends seamlessly to positional-scoring rules such as $k$-approval and Borda. These results clarify the algorithmic landscape of multidimensional spatial elections and provide practically implementable tools for campaign strategy."
2508.13868,"Weighted voting games are a popular class of coalitional games that are widely used to model real-life situations of decision-making. They can be applied, for instance, to analyze legislative processes in parliaments or voting in corporate structures. Various ways of tampering with these games have been studied, among them merging or splitting players, fiddling with the quota, and controlling weighted voting games by adding or deleting players. While the complexity of control by adding players to such games so as to change or maintain a given player's power has been recently settled, the complexity of control by deleting players from such games (with the same goals) remained open. We show that when the players' power is measured by the probabilistic Penrose-Banzhaf index, some of these problems are complete for NP^PP -- the class of problems solvable by NP machines equipped with a PP (""probabilistic polynomial time"") oracle. Our results optimally improve the currently known lower bounds of hardness for much smaller complexity classes, thus providing protection against SAT-solving techniques in practical applications."
2508.1396,"The latest developments in AI focus on agentic systems where artificial and human agents cooperate to realize global goals. An example is collaborative learning, which aims to train a global model based on data from individual agents. A major challenge in designing such systems is to guarantee safety and alignment with human values, particularly a fair distribution of rewards upon achieving the global goal. Cooperative game theory offers useful abstractions of cooperating agents via value functions, which assign value to each coalition, and via reward functions. With these, the idea of fair allocation can be formalized by specifying fairness axioms and designing concrete mechanisms. Classical cooperative game theory, exemplified by the Shapley value, does not fully capture scenarios like collaborative learning, as it assumes nonreplicable resources, whereas data and models can be replicated. Infinite replicability requires a generalized notion of fairness, formalized through new axioms and mechanisms. These must address imbalances in reciprocal benefits among participants, which can lead to strategic exploitation and unfair allocations. The main contribution of this paper is a mechanism and a proof that it fulfills the property of mutual fairness, formalized by the Balanced Reciprocity Axiom. It ensures that, for every pair of players, each benefits equally from the participation of the other."
2508.14194,"In the roommate matching model, given a set of 2n agents and n rooms, we find an assignment of a pair of agents to a room. Although the roommate matching problem is well studied, the study of the model when agents have preference over both rooms and roommates was recently initiated by Chan et al. [11]. We study two types of stable roommate assignments, namely, 4-person stable (4PS) and 2-person stable (2PS) in conjunction with efficiency and strategy-proofness. We design a simple serial dictatorship based algorithm for finding a 4PS assignment that is Pareto optimal and strategy-proof. However, the serial dictatorship algorithm is far from being 2PS. Next, we study top trading cycle (TTC) based algorithms. We show that variations of TTC cannot be strategy-proof or PO. Finally, as Chan et al. (2016) showed that deciding the existence of 2PS assignment is NP-complete, we identify preference structures where a 2PS assignment can be found in polynomial time."
2508.14196,"The optimal signaling schemes in information design (Bayesian persuasion) problems often involve non-explainable randomization or disconnected partitions of state space, which are too intricate to be audited or communicated. We propose explainable information design in the context of information design with a continuous state space, restricting the information designer to use $K$-partitional signaling schemes defined by deterministic and monotone partitions of the state space, where a unique signal is sent for all states in each part. We first prove that the price of explainability (PoE) -- the ratio between the performances of the optimal explainable signaling scheme and unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning that partitional signaling schemes are never worse than arbitrary signaling schemes by a factor of 2.We then study the complexity of computing optimal explainable signaling schemes. We show that the exact optimization problem is NP-hard in general. But for Lipschitz utility functions, an $\varepsilon$-approximately optimal explainable signaling scheme can be computed in polynomial time. And for piecewise constant utility functions, we provide an efficient algorithm to find an explainable signaling scheme that provides a $1/2$ approximation to the optimal unrestricted signaling scheme, which matches the worst-case PoE bound.A technical tool we develop is a conversion from any optimal signaling scheme (which satisfies a bi-pooling property) to a partitional signaling scheme that achieves $1/2$ fraction of the expected utility of the former. We use this tool in the proofs of both our PoE result and algorithmic result."
2508.14439,"We study the task of electing egalitarian sequences of $\tau$ committees given a set of agents with additive utilities for candidates available on each of $\tau$ levels. We introduce several rules for electing an egalitarian committee sequence as well as properties for such rules. We settle the computational complexity of finding a winning sequence for our rules and classify them against our properties. Additionally, we transform sequential election data from existing election data from the literature. Using this data set, we compare our rules empirically and test them experimentally against our properties."
2508.14705,"We study payoff manipulation in repeated multi-objective Stackelberg games, where a leader may strategically influence a follower's deterministic best response, e.g., by offering a share of their own payoff. We assume that the follower's utility function, representing preferences over multiple objectives, is unknown but linear, and its weight parameter must be inferred through interaction. This introduces a sequential decision-making challenge for the leader, who must balance preference elicitation with immediate utility maximisation. We formalise this problem and propose manipulation policies based on expected utility (EU) and long-term expected utility (longEU), which guide the leader in selecting actions and offering incentives that trade off short-term gains with long-term impact. We prove that under infinite repeated interactions, longEU converges to the optimal manipulation. Empirical results across benchmark environments demonstrate that our approach improves cumulative leader utility while promoting mutually beneficial outcomes, all without requiring explicit negotiation or prior knowledge of the follower's utility function."
2508.14927,"This position paper argues for two claims regarding AI testing and evaluation. First, to remain informative about deployment behaviour, evaluations need account for the possibility that AI systems understand their circumstances and reason strategically. Second, game-theoretic analysis can inform evaluation design by formalising and scrutinising the reasoning in evaluation-based safety cases. Drawing on examples from existing AI systems, a review of relevant research, and formal strategic analysis of a stylised evaluation scenario, we present evidence for these claims and motivate several research directions."
2508.15296,"Two-sided matching, such as matching between students and schools, has been applied to various aspects of real life and has been the subject of much research, however, it has been plagued by the fact that efficiency and fairness are incompatible. In particular, Pareto efficiency and justified-envy-freeness are known to be incompatible even in the simplest one-to-one matching, i.e., the stable marriage problem. In previous research, the primary approach to improving efficiency in matchings has been to tolerate students' envy, thereby relaxing fairness constraints. In this study, we take a different approach to relaxing fairness. Specifically, it focuses on addressing only the envy that students may experience or prioritize more highly and seeks matchings without such envy. More specifically, this study assumes that envy towards students who are not acquaintances has less impact compared to envy towards students who are acquaintances. Accordingly, we assume that the students know each other or not, represented by an undirected graph, and define a local envy as a justified envy toward an acquaintance or a neighbor in the graph. We then propose the property that there is no local envy as a new relaxed concept of fairness, called local envy-freeness. We analyze whether Pareto-efficient matching can be achieved while maintaining local envy-freeness by meaningfully restricting the graph structure and the school's preferences. To analyze in detail the fairness that can achieve Pareto-efficient matching, we introduce a local version of the relaxed fairness recently proposed by Cho et al. (AAMAS 2024), which parameterizes the level of local envy-freeness by nonnegative integers. We then clarify the level of local envy-freeness that can be achieved by Pareto-efficient mechanisms for graphs that are ``close'' to trees and single-peaked preferences on the graphs."
2508.15356,"A strategy profile in a multi-player game is a Nash equilibrium if no player can unilaterally deviate to achieve a strictly better payoff. A profile is an $\epsilon$-Nash equilibrium if no player can gain more than $\epsilon$ by unilaterally deviating from their strategy. In this work, we use $\epsilon$-Nash equilibria to approximate the computation of Nash equilibria. Specifically, we focus on turn-based, multiplayer stochastic games played on graphs, where players are restricted to stationary strategies -- strategies that use randomness but not memory.The problem of deciding the constrained existence of stationary Nash equilibria -- where each player's payoff must lie within a given interval -- is known to be $\exists\mathbb{R}$-complete in such a setting (Hansen and Sølvsten, 2020). We extend this line of work to stationary $\epsilon$-Nash equilibria and present an algorithm that solves the following promise problem: given a game with a Nash equilibrium satisfying the constraints, compute an $\epsilon$-Nash equilibrium that $\epsilon$-satisfies those same constraints -- satisfies the constraints up to an $\epsilon$ additive error. Our algorithm runs in FNP^NP time.To achieve this, we first show that if a constrained Nash equilibrium exists, then one exists where the non-zero probabilities are at least an inverse of a double-exponential in the input. We further prove that such a strategy can be encoded using floating-point representations, as in the work of Frederiksen and Miltersen (2013), which finally gives us our FNP^NP algorithm.We further show that the decision version of the promise problem is NP-hard. Finally, we show a partial tightness result by proving a lower bound for such techniques: if a constrained Nash equilibrium exists, then there must be one that where the probabilities in the strategies are double-exponentially small."
2508.1538,"We study the problem of fair allocation of a set of indivisible goods among $n$ agents with $k$ distinct additive valuations, with the goal of achieving approximate envy-freeness up to any good ($\alpha-\mathrm{EFX}$).It is known that EFX allocations exist for $n$ agents when there are at most three distinct valuations due to HV et al. Furthermore, Amanatidis et al. showed that a $\frac{2}{3}-\mathrm{EFX}$ allocation is guaranteed to exist when number of agents is at most seven. In this paper, we show that a $\frac{2}{3}-\mathrm{EFX}$ allocation exists for any number of agents when there are at most four distinct valuations.Secondly, we consider a relaxation called $\mathrm{EFX}$ with charity, where some goods remain unallocated such that no agent envies the set of unallocated goods. Akrami et al. showed that for $n$ agents and any $\varepsilon \in \left(0, \frac{1}{2}\right]$, there exists a $(1-\varepsilon)-\mathrm{EFX}$ allocation with at most $\tilde{\mathcal{O}}((n/\varepsilon)^{\frac{1}{2}})$ goods to charity. In this paper, we show that a $(1-\varepsilon)-\mathrm{EFX}$ allocation with a $\tilde{\mathcal{O}}(k/\varepsilon)^{\frac{1}{2}}$ charity exists for any number of agents when there are at most $k$ distinct valuations."
2508.15844,"Ransomware attacks have become a pervasive and costly form of cybercrime, causing tens of millions of dollars in losses as organizations increasingly pay ransoms to mitigate operational disruptions and financial risks. While prior research has largely focused on proactive defenses, the post-infection negotiation dynamics between attackers and victims remains underexplored. This paper presents a formal analysis of attacker-victim interactions in modern ransomware incidents using a finite-horizon alternating-offers bargaining game model. Our analysis demonstrates how bargaining alters the optimal strategies of both parties. In practice, incomplete information-attackers lacking knowledge of victims' data valuations and victims lacking knowledge of attackers' reservation ransoms-can prolong negotiations and increase victims' business interruption costs. To address this, we design a Bayesian incentive-compatible mechanism that facilitates rapid agreement on a fair ransom without requiring either party to disclose private valuations. We further implement this mechanism using secure two-party computation based on garbled circuits, thereby eliminating the need for trusted intermediaries and preserving the privacy of both parties throughout the negotiation. To the best of our knowledge, this is the first automated, privacy-preserving negotiation mechanism grounded in a formal analysis of ransomware negotiation dynamics."
2508.16007,"We study the problem of data selling for Retrieval Augmented Generation (RAG) tasks in Generative AI applications. We model each buyer's valuation of a dataset with a natural coverage-based valuation function that increases with the inclusion of more relevant data points that would enhance responses to anticipated queries. Motivated by issues such as data control and prior-free revenue maximization, we focus on the scenario where each data point can be allocated to only one buyer. We show that the problem of welfare maximization in this setting is NP-hard even with two bidders, but design a polynomial-time $(1-1/e)$ approximation algorithm for any number of bidders. Unfortunately, however, this efficient allocation algorithm fails to be incentive compatible. The crux of our approach is a carefully tailored post-processing step called data burning which retains the $(1-1/e)$ approximation factor but achieves incentive compatibility. Our thorough experiments on synthetic and real-world image and text datasets demonstrate the practical effectiveness of our algorithm compared to popular baseline algorithms for combinatorial auctions."
2508.16177,"In rank aggregation, the task is to aggregate multiple weighted input rankings into a single output ranking. While numerous methods, so-called social welfare functions (SWFs), have been suggested for this problem, all of the classical SWFs tend to be majoritarian and are thus not acceptable when a proportional ranking is required. Motivated by this observation, we will design SWFs that guarantee that every input ranking is proportionally represented by the output ranking. Specifically, our central fairness condition requires that the number of pairwise comparisons between candidates on which an input ranking and the output ranking agree is proportional to the weight of the input ranking. As our main contribution, we present a simple SWF called the Proportional Sequential Borda rule, which satisfies this condition. Moreover, we introduce two variants of this rule: the Ranked Method of Equal Shares, which has a more utilitarian flavor while still satisfying our fairness condition, and the Flow-adjusting Borda rule, which satisfies an even stronger fairness condition. Many of our axioms and techniques are inspired by results on approval-based committee voting and participatory budgeting, where the concept of proportional representation has been studied in depth."
2508.16195,"Social decision schemes (SDSs) map the voters' preferences over multiple alternatives to a probability distribution over these alternatives. In a seminal result, Gibbard (1977) has characterized the set of SDSs that are strategyproof with respect to all utility functions and his result implies that all such SDSs are either unfair to the voters or alternatives, or they require a significant amount of randomization. To circumvent this negative result, we propose the notion of $U$-strategyproofness which postulates that only voters with a utility function in a predefined set $U$ cannot manipulate. We then analyze the tradeoff between $U$-strategyproofness and various decisiveness notions that restrict the amount of randomization of SDSs. In particular, we show that if the utility functions in the set $U$ value the best alternative much more than other alternatives, there are $U$-strategyproof SDSs that choose an alternative with probability $1$ whenever all but $k$ voters rank it first. On the negative side, we demonstrate that $U$-strategyproofness is incompatible with Condorcet-consistency if the set $U$ satisfies minimal symmetry conditions. Finally, we show that no ex post efficient and $U$-strategyproof SDS can be significantly more decisive than the uniform random dictatorship if the voters are close to indifferent between their two favorite alternatives."
2508.16245,"A Bayesian player acting in an infinite multi-player game learns to predict the other players' strategies if his prior assigns positive probability to their play (or contains a grain of truth). Kalai and Lehrer's classic grain of truth problem is to find a reasonably large class of strategies that contains the Bayes-optimal policies with respect to this class, allowing mutually-consistent beliefs about strategy choice that obey the rules of Bayesian inference. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of strategies wide enough to contain all computable strategies as well as Bayes-optimal strategies for every reasonable prior over the class. When the ""environment"" is a known repeated stage game, we show convergence in the sense of [KL93a] and [KL93b]. When the environment is unknown, agents using Thompson sampling converge to play $\varepsilon$-Nash equilibria in arbitrary unknown computable multi-agent environments. Finally, we include an application to self-predictive policies that avoid planning. While these results use computability theory only as a conceptual tool to solve a classic game theory problem, we show that our solution can naturally be computationally approximated arbitrarily closely."
2508.16251,"With rapid advancements in large language models (LLMs), AI-generated content (AIGC) has emerged as a key driver of technological innovation and economic transformation. Personalizing AIGC services to meet individual user demands is essential but challenging for AIGC service providers (ASPs) due to the subjective and complex demands of mobile users (MUs), as well as the computational and communication resource constraints faced by ASPs. To tackle these challenges, we first develop a novel multi-dimensional quality-of-experience (QoE) metric. This metric comprehensively evaluates AIGC services by integrating accuracy, token count, and timeliness. We focus on a mobile edge computing (MEC)-enabled AIGC network, consisting of multiple ASPs deploying differentiated AIGC models on edge servers and multiple MUs with heterogeneous QoE requirements requesting AIGC services from ASPs. To incentivize ASPs to provide personalized AIGC services under MEC resource constraints, we propose a QoE-driven incentive mechanism. We formulate the problem as an equilibrium problem with equilibrium constraints (EPEC), where MUs as leaders determine rewards, while ASPs as followers optimize resource allocation. To solve this, we develop a dual-perturbation reward optimization algorithm, reducing the implementation complexity of adaptive pricing. Experimental results demonstrate that our proposed mechanism achieves a reduction of approximately $64.9\%$ in average computational and communication overhead, while the average service cost for MUs and the resource consumption of ASPs decrease by $66.5\%$ and $76.8\%$, respectively, compared to state-of-the-art benchmarks."
2508.16285,"The Optimism Retroactive Project Funding (RetroPGF) is a key initiative within the blockchain ecosystem that retroactively rewards projects deemed valuable to the Ethereum and Optimism communities. Managed by the Optimism Collective, a decentralized autonomous organization (DAO), RetroPGF represents a large-scale experiment in decentralized governance. Funding rewards are distributed in OP tokens, the native digital currency of the ecosystem. As of this writing, four funding rounds have been completed, collectively allocating over 100M dollars, with an additional 1.3B dollars reserved for future rounds. However, we identify significant shortcomings in the current allocation system, underscoring the need for improved governance mechanisms given the scale of funds involved.Leveraging computational social choice techniques and insights from multiagent systems, we propose improvements to the voting process by recommending the adoption of a utilitarian moving phantoms mechanism. This mechanism, originally introduced by Freeman et al. in 2019, is designed to enhance social welfare (using the L1 norm) while satisfying strategyproofness -- two key properties aligned with the application's governance requirements. Our analysis provides a formal framework for designing improved funding mechanisms for DAOs, contributing to the broader discourse on decentralized governance and public goods allocation."
2508.16823,"The rise of auto-bidding has created challenges for ensuring advertiser incentive compatibility, particularly when advertisers delegate bidding to agents with high-level constraints. One challenge in defining incentive compatibility is the multiplicity of equilibria. After advertisers submit reports, it is unclear what the result will be and one only has knowledge of a range of possible results. Nevertheless, Alimohammadi et al. proposed a notion of Auto-bidding Incentive Compatibility (AIC) which serves to highlight that auctions may not incentivize truthful reporting of constraints. However, their definition of AIC is very stringent as it requires that the worst-case outcome of an advertiser's truthful report is at least as good as the best-case outcome of any of the advertiser's possible deviations. Indeed, they show both First-Price Auction and Second-Price Auction are not AIC. Moreover, the AIC definition precludes having ordinal preferences on the possible constraints that the advertiser can report.In this paper, we introduce two refined and relaxed concepts: Risk-Averse Auto-bidding Incentive Compatibility (RAIC) and Optimistic Auto-bidding Incentive Compatibility (OAIC). RAIC (OAIC) stipulates that truthful reporting is preferred if its least (most) favorable equilibrium outcome is no worse than the least (most) favorable equilibrium outcome from any misreport. This distinction allows for a clearer modeling of ordinal preferences for advertisers with differing attitudes towards equilibrium uncertainty. We demonstrate that SPA satisfies both RAIC and OAIC. Furthermore, we show that SPA also meets these conditions for two advertisers when they are assumed to employ uniform bidding. These findings provide new insights into the incentive properties of SPA in auto-bidding environments, particularly when considering advertisers' perspectives on equilibrium selection."
2508.17111,"Traditional user profiling techniques rely on browsing history or purchase records to identify users' willingness to pay. This enables sellers to offer personalized prices to profiled users while charging only a uniform price to non-profiled users. However, the emergence of privacy-enhancing technologies has caused users to actively avoid on-site data tracking. Today, major online sellers have turned to public platforms such as online social networks to better track users' profiles from their product-related discussions. This paper presents the first analytical study on how users should best manage their social activities against potential personalized pricing, and how a seller should strategically adjust her pricing scheme to facilitate user profiling in social networks. We formulate a dynamic Bayesian game played between the seller and users under asymmetric information. The key challenge of analyzing this game comes from the double couplings between the seller and the users as well as among the users. Furthermore, the equilibrium analysis needs to ensure consistency between users' revealed information and the seller's belief under random user profiling. We address these challenges by alternately applying backward and forward induction, and successfully characterize the unique perfect Bayesian equilibrium (PBE) in closed form. Our analysis reveals that as the accuracy of profiling technology improves, the seller tends to raise the equilibrium uniform price to motivate users' increased social activities and facilitate user profiling. However, this results in most users being worse off after the informed consent policy is imposed to ensure users' awareness of data access and profiling practices by potential sellers. This finding suggests that recent regulatory evolution towards enhancing users' privacy awareness may have unintended consequences of reducing users' payoffs."
2508.17177,"Given a set of items and a set of evaluators who all individually rank them, how do we aggregate these evaluations into a single societal ranking? Work in social choice and statistics has produced many aggregation methods for this problem, each with its desirable properties, but also with its limitations. Further, existing impossibility results rule out designing a single method that achieves every property of interest. Faced with this trade-off between incompatible desiderata, how do we decide which aggregation rule to use, i.e., what is a good rule picking rule?In this paper, we formally address this question by introducing a novel framework for rule picking rules (RPRs). We then design a data-driven RPR that identifies the best aggregation method for each specific setting, without assuming any generative model. The principle behind our RPR is to pick the rule which maximizes the consistency of the output ranking if the data collection process were repeated. We introduce several consistency-related axioms for RPRs and show that our method satisfies them, including those failed by a wide class of natural RPRs. While we prove that the algorithmic problem of maximizing consistency is computationally hard, we provide a sampling-based implementation of our RPR that is efficient in practice. We run this implementation on known statistical models and find that, when possible, our method selects the maximum likelihood estimator of the data. Finally, we show that our RPR can be used in many real-world settings to gain insights about how the rule currently being used can be modified or replaced to substantially improve the consistency of the process.Taken together, our work bridges an important gap between the axiomatic and statistical approaches to rank aggregation, laying a robust theoretical and computational foundation for principled rule picking."
2508.17206,"We present a Stackelberg game model to investigate how individuals make their decisions on timing and route selection. Group formation can naturally result from these decisions, but only when individuals arrive at the same time and choose the same route. Although motivated by bird migration, our model applies to scenarios such as traffic planning, disaster evacuation, and other animal movements. Early arrivals secure better territories, while traveling together enhances navigation accuracy, foraging efficiency, and energy efficiency. Longer or more difficult migration routes reduce predation risks but increase travel costs, such as higher elevations and scarce food resources. Our analysis reveals a richer set of subgame perfect equilibria (SPEs) and heightened competition, compared to earlier models focused only on timing. By incorporating individual differences in travel costs, our model introduces a ""neutrality"" state in addition to ""cooperation"" and ""competition."""
2508.17489,"We introduce a model for collaborative text aggregation in which an agent community coauthors a document, modeled as an unordered collection of paragraphs, using a dynamic mechanism: agents propose paragraphs and vote on those suggested by others. We formalize the setting and explore its realizations, concentrating on voting mechanisms that aggregate votes into a single, dynamic document. We focus on two desiderata: the eventual stability of the process and its expected social welfare. Following an impossibility result, we describe several aggregation methods and report on agent-based simulations that utilize natural language processing (NLP) and large-language models (LLMs) to model agents and their contexts. Using these simulations, we demonstrate promising results regarding the possibility of rapid convergence to a high social welfare collaborative text."
2508.17557,"How hard is it to achieve consensus in a social network under uncertainty? In this paper we model this problem as a social graph of agents where each vertex is initially colored red or blue. The goal of the agents is to achieve consensus, which is when the colors of all agents align. Agents attempt to do this locally through steps in which an agent changes their color to the color of the majority of their neighbors. In real life, agents may not know exactly how many of their neighbors are red or blue, which introduces uncertainty into this process. Modeling uncertainty as perturbations of relative magnitude $1+\varepsilon$ to these color neighbor counts, we show that even small values of $\varepsilon$ greatly hinder the ability to achieve consensus in a social network. We prove theoretically tight upper and lower bounds on the price of uncertainty, a metric defined by Balcan et al. to quantify the effect of uncertainty in network games."
2508.17671,"The goal of agents in multi-agent environments is to maximize total reward against the opposing agents that are encountered. Following a game-theoretic solution concept, such as Nash equilibrium, may obtain a strong performance in some settings; however, such approaches fail to capitalize on historical and observed data from repeated interactions against our opponents. Opponent modeling algorithms integrate machine learning techniques to exploit suboptimal opponents utilizing available data; however, the effectiveness of such approaches in imperfect-information games to date is quite limited. We show that existing opponent modeling approaches fail to satisfy a simple desirable property even against static opponents drawn from a known prior distribution; namely, they do not guarantee that the model approaches the opponent's true strategy even in the limit as the number of game iterations approaches infinity. We develop a new algorithm that is able to achieve this property and runs efficiently by solving a convex minimization problem based on the sequence-form game representation using projected gradient descent. The algorithm is guaranteed to efficiently converge to the opponent's true strategy given observations from gameplay and possibly additional historical data if it is available."
2508.17907,"Competitions are widely used to identify top performers in judgmental forecasting and machine learning, and the standard competition design ranks competitors based on their cumulative scores against a set of realized outcomes or held-out labels. However, this standard design is neither incentive-compatible nor very statistically efficient. The main culprit is noise in outcomes/labels that experts are scored against; it allows weaker competitors to often win by chance, and the winner-take-all nature incentivizes misreporting that improves win probability even if it decreases expected score. Attempts to achieve incentive-compatibility rely on randomized mechanisms that add even more noise in winner selection, but come at the cost of determinism and practical adoption. To tackle these issues, we introduce a novel deterministic mechanism: WOMAC (Wisdom of the Most Accurate Crowd). Instead of scoring experts against noisy outcomes, as is standard, WOMAC scores experts against the best ex-post aggregate of peer experts' predictions given the noisy outcomes. WOMAC is also more efficient than the standard competition design in typical settings. While the increased complexity of WOMAC makes it challenging to analyze incentives directly, we provide a clear theoretical foundation to justify the mechanism. We also provide an efficient vectorized implementation and demonstrate empirically on real-world forecasting datasets that WOMAC is a more reliable predictor of experts' out-of-sample performance relative to the standard mechanism. WOMAC is useful in any competition where there is substantial noise in the outcomes/labels."
2508.17945,"In this work, we model Moving Target Defence (MTD) as a partially observable stochastic game between an attacker and a defender. The attacker tries to compromise the system through probing actions, while the defender minimizes the risk by reimaging the system, balancing between performance cost and security level. We demonstrate that the optimal strategies for both players follow a threshold structure. Based on this insight, we propose a structure-aware policy gradient reinforcement learning algorithm that helps both players converge to the Nash equilibrium. This approach enhances the defender's ability to adapt and effectively counter evolving threats, improving the overall security of the system. Finally, we validate the proposed method through numerical simulations."
2508.18325,"We consider a setting where goods are allocated to agents by way of an allocation platform (e.g., a matching platform). An ``allocation facilitator'' aims to increase the overall utility/social-good of the allocation by encouraging (some of the) agents to relax (some of) their restrictions. At the same time, the advice must not hurt agents who would otherwise be better off. Additionally, the facilitator may be constrained by a ``bound'' (a.k.a. `budget'), limiting the number and/or type of restrictions it may seek to relax. We consider the facilitator's optimization problem of choosing an optimal set of restrictions to request to relax under the aforementioned constraints. Our contributions are three-fold: (i) We provide a formal definition of the problem, including the participation guarantees to which the facilitator should adhere. We define a hierarchy of participation guarantees and also consider several social-good functions. (ii) We provide polynomial algorithms for solving various versions of the associated optimization problems, including one-to-one and many-to-one allocation settings. (iii) We demonstrate the benefits of such facilitation and relaxation, and the implications of the different participation guarantees, using extensive experimentation on three real-world datasets."
2508.18449,"We propose a class of cooperative games, called d Partitioned Compbinatorial Optimization Games (PCOGs). The input of PCOG consists of a set of agents and a combinatorial structure (typically a graph) with a fixed optimization goal on this structure (e.g., finding a minimum dominating set on a graph) such that the structure is divided among the agents. The value of each coalition of agents is derived from the optimal solution for the part of the structure possessed by the coalition. We study two fundamental questions related to the core: Core Stability Verification and Core Stability Existence. We analyze the algorithmic complexity of both questions for four classic graph optimization tasks: minimum vertex cover, minimum dominating set, minimum spanning tree, and maximum matching."
2508.186,"Large language models (LLMs) are increasingly used to simulate human decision-making, but their intrinsic biases often diverge from real human behavior--limiting their ability to reflect population-level diversity. We address this challenge with a persona-based approach that leverages individual-level behavioral data from behavioral economics to adjust model biases. Applying this method to the ultimatum game--a standard but difficult benchmark for LLMs--we observe improved alignment between simulated and empirical behavior, particularly on the responder side. While further refinement of trait representations is needed, our results demonstrate the promise of persona-conditioned LLMs for simulating human-like decision patterns at scale."
2509.00179,"Optimization under uncertainty is a fundamental problem in learning and decision-making, particularly in multi-agent systems. Previously, Feldman, Kalai, and Tennenholtz [2010] demonstrated the ability to efficiently compete in repeated symmetric two-player matrix games without observing payoffs, as long as the opponents actions are observed. In this paper, we introduce and formalize a new class of zero-sum symmetric Markov games, which extends the notion of symmetry from matrix games to the Markovian setting. We show that even without observing payoffs, a player who knows the transition dynamics and observes only the opponents sequence of actions can still compete against an adversary who may have complete knowledge of the game. We formalize three distinct notions of symmetry in this setting and show that, under these conditions, the learning problem can be reduced to an instance of online learning, enabling the player to asymptotically match the return of the opponent despite lacking payoff observations. Our algorithms apply to both matrix and Markov games, and run in polynomial time with respect to the size of the game and the number of episodes. Our work broadens the class of games in which robust learning is possible under severe informational disadvantage and deepens the connection between online learning and adversarial game theory."
2509.00439,"We study the mechanism design problem of facility location on a metric space in the learning-augmented framework, where mechanisms have access to an imperfect prediction of optimal facility locations. Our goal is to design strategyproof (SP) mechanisms to elicit agent preferences on the facility locations truthfully and, leveraging the given imperfect prediction, determine the facility location that approximately minimizes the maximum cost among all agents. In particular, we seek SP mechanisms whose approximation guarantees depend on the prediction errors -- achieve improved guarantees when the prediction is accurate (known as the \emph{consistency}), while still ensuring robust worst-case performance when the prediction is arbitrarily inaccurate (known as the \emph{robustness}).When the metric space is the real line, we characterize all deterministic SP mechanisms with consistency strictly less than 2 and bounded robustness: such mechanisms must be the MinMaxP mechanism, which returns the prediction location if it lies between the two extreme agent locations and, otherwise, returns the closest agent location to the prediction. We further show that, for any prediction error $\eta\ge 0$, while MinMaxP is $(1+\min(1, \eta))$-approximation, no deterministic SP mechanism can achieve a better approximation. In two-dimensional spaces with the $l_p$ metric, we analyze the approximation guarantees of a deterministic mechanism that runs MinMaxP independently on each coordinate, as well as a randomized mechanism that selects between two deterministic ones with specific probabilities. Finally, we discuss the group strategyproofness of the considered mechanisms."
2509.00506,"A \emph{bidding} game is played on a graph as follows. A token is placed on an initial vertex and both players are allocated budgets. In each turn, the players simultaneously submit bids that do not exceed their available budgets, the higher bidder moves the token, and pays the bid to the lower bidder. We focus on \emph{discrete}-bidding, which are motivated by practical applications and restrict the granularity of the players' bids, e.g, bids must be given in cents. We study, for the first time, discrete-bidding games with {\em mean-payoff} and {\em energy} objectives. In contrast, mean-payoff {\em continuous}-bidding games (i.e., no granularity restrictions) are understood and exhibit a rich mathematical structure. The {\em threshold} budget is a necessary and sufficient initial budget for winning an energy game or guaranteeing a target payoff in a mean-payoff game. We first establish existence of threshold budgets; a non-trivial property due to the concurrent moves of the players. Moreover, we identify the structure of the thresholds, which is key in obtaining compact strategies, and in turn, showing that finding threshold is in \NP~and \coNP even in succinctly-represented games."
2509.01582,"Decision-making in automated driving must consider interactions with surrounding agents to be effective. However, traditional methods often neglect or oversimplify these interactions because they are difficult to model and solve, which can lead to overly conservative behavior of the ego vehicle. To address this gap, we propose two quantum game models, QG-U1 (Quantum Game - Unitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware decision-making. These models extend classical game theory by incorporating principles of quantum mechanics, such as superposition, interference, and entanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games with two strategies per player and can be executed in real time on a standard computer without requiring quantum hardware. We evaluate both models in merging and roundabout scenarios and compare them with classical game-theoretic methods and baseline approaches (IDM, MOBIL, and a utility-based technique). Results show that QG-G4 achieves lower collision rates and higher success rates compared to baseline methods, while both quantum models yield higher expected payoffs than classical game approaches under certain parameter settings."
2509.0187,"We consider a multi-player non-zero-sum turn-based game (abbreviated as multi-player game) on a finite directed graph. A secure equilibrium (SE) is a strategy profile in which no player has the incentive to deviate from the strategy because no player can increase her own payoff or lower the payoff of another player. SE is a promising refinement of Nash equilibrium in which a player does not care the payoff of another player. In this paper, we discuss the decidability and complexity of the problem of deciding whether a secure equilibrium with constraints (a payoff profile specifying which players must win) exists for a given multi-player game."
2509.01953,"The prevalence of low-quality content on online platforms is often attributed to the absence of meaningful entry requirements. This motivates us to investigate whether implicit or explicit entry barriers, alongside appropriate reward mechanisms, can enhance content quality. We present the first game-theoretic analysis of two distinct types of entry barriers in online content platforms. The first, a structural barrier, emerges from the collective behaviour of incumbent content providers which disadvantages new entrants. We show that both rank-order and proportional-share reward mechanisms induce such a structural barrier at Nash equilibrium. The second, a strategic barrier, involves the platform proactively imposing entry fees to discourage participation from low-quality contributors. We consider a scheme in which the platform redirects some or all of the entry fees into the reward pool. We formally demonstrate that this approach can improve overall content quality. Our findings establish a theoretical foundation for designing reward mechanisms coupled with entry fees to promote higher-quality content and support healthier online ecosystems."
2509.0238,"The nucleolus is a fundamental solution concept in cooperative game theory, yet computing it is NP-hard in general. In convex games-where players' marginal contributions grow with coalition size-the only existing polynomial-time algorithm relies on the ellipsoid method. We re-examine a reduced game approach, refuting a previously claimed polynomial-time implementation and clarifying why it fails. By developing new algorithmic ideas and exploiting the structure of least core polyhedra, we show that reduced games can in fact be used effectively. This yields the first combinatorial and strongly polynomial algorithm for computing the nucleolus in convex games."
2509.02493,"Incentive design deals with interaction between a principal and an agent where the former can shape the latter's utility through a policy commitment. It is well known that the principal faces an information rent when dealing with an agent that has informational advantage. In this work, we embark on a systematic study of the effect of information asymmetry in incentive design games. Specifically, we first demonstrate that it is in principal's interest to decrease this information asymmetry. To mitigate this uncertainty, we let the principal gather information either by letting the agent shape her belief (aka Information Design), or by paying to acquire it. Providing solutions to all these cases we show that while introduction of uncertainty increases the principal's cost, letting the agent shape its belief can be advantageous. We study information asymmetry and information acquisition in both matrix games and quadratic Gaussian game setups."
2509.02519,"Polarization is a major concern for a well-functioning society. Often, mass polarization of a society is driven by polarizing political representation, even when the latter is easily preventable. The existing computational social choice methods for the task of committee selection are not designed to address this issue. We enrich the standard approach to committee selection by defining two quantitative measures that evaluate how well a given committee interconnects the voters. Maximizing these measures aims at avoiding polarizing committees. While the corresponding maximization problems are NP-complete in general, we obtain efficient algorithms for profiles in the voter-candidate interval domain. Moreover, we analyze the compatibility of our goals with other representation objectives, such as excellence, diversity, and proportionality. We identify trade-offs between approximation guarantees, and describe algorithms that achieve simultaneous constant-factor approximations."
2509.02619,"In decision-dependent games, multiple players optimize their decisions under a data distribution that shifts with their joint actions, creating complex dynamics in applications like market pricing. A practical consequence of these dynamics is the \textit{performatively stable equilibrium}, where each player's strategy is a best response under the induced distribution. Prior work relies on $\beta$-smoothness, assuming Lipschitz continuity of loss function gradients with respect to the data distribution, which is impractical as the data distribution maps, i.e., the relationship between joint decision and the resulting distribution shifts, are typically unknown, rendering $\beta$ unobtainable. To overcome this limitation, we propose a gradient-based sensitivity measure that directly quantifies the impact of decision-induced distribution shifts. Leveraging this measure, we derive convergence guarantees for performatively stable equilibria under a practically feasible assumption of strong monotonicity. Accordingly, we develop a sensitivity-informed repeated retraining algorithm that adjusts players' loss functions based on the sensitivity measure, guaranteeing convergence to performatively stable equilibria for arbitrary data distribution maps. Experiments on prediction error minimization game, Cournot competition, and revenue maximization game show that our approach outperforms state-of-the-art baselines, achieving lower losses and faster convergence."
2509.03348,"Auto-bidding is central to computational advertising, achieving notable commercial success by optimizing advertisers' bids within economic constraints. Recently, large generative models show potential to revolutionize auto-bidding by generating bids that could flexibly adapt to complex, competitive environments. Among them, diffusers stand out for their ability to address sparse-reward challenges by focusing on trajectory-level accumulated rewards, as well as their explainable capability, i.e., planning a future trajectory of states and executing bids accordingly. However, diffusers struggle with generation uncertainty, particularly regarding dynamic legitimacy between adjacent states, which can lead to poor bids and further cause significant loss of ad impression opportunities when competing with other advertisers in a highly competitive auction environment. To address it, we propose a Causal auto-Bidding method based on a Diffusion completer-aligner framework, termed CBD. Firstly, we augment the diffusion training process with an extra random variable t, where the model observes t-length historical sequences with the goal of completing the remaining sequence, thereby enhancing the generated sequences' dynamic legitimacy. Then, we employ a trajectory-level return model to refine the generated trajectories, aligning more closely with advertisers' objectives. Experimental results across diverse settings demonstrate that our approach not only achieves superior performance on large-scale auto-bidding benchmarks, such as a 29.9% improvement in conversion value in the challenging sparse-reward auction setting, but also delivers significant improvements on the Kuaishou online advertising platform, including a 2.0% increase in target cost."
2509.04143,"Trust is often thought to increase cooperation. However, game-theoretic models often fail to distinguish between cooperative behaviour and trust. This makes it difficult to measure trust and determine its effect in different social dilemmas. We address this here by formalising trust as a cognitive shortcut in repeated games. This functions by avoiding checking a partner's actions once a threshold level of cooperativeness has been observed. We consider trust-based strategies that implement this heuristic, and systematically analyse their evolution across the space of two-player symmetric social dilemma games. We find that where it is costly to check whether another agent's actions were cooperative, as is the case in many real-world settings, then trust-based strategies can outcompete standard reciprocal strategies such as Tit-for-Tat in many social dilemmas. Moreover, the presence of trust increases the overall level of cooperation in the population, especially in cases where agents can make unintentional errors in their actions. This occurs even in the presence of strategies designed to build and then exploit trust. Overall, our results demonstrate the individual adaptive benefit to an agent of using a trust heuristic, and provide a formal theory for how trust can promote cooperation in different types of social interaction. We discuss the implications of this for interactions between humans and artificial intelligence agents."
2509.05498,"Cognitive vulnerabilities shape human decision-making and arise primarily from two sources: (1) cognitive capabilities, which include disparities in knowledge, education, expertise, or access to information, and (2) cognitive biases, such as rational inattention, confirmation bias, and base rate neglect, which influence how individuals perceive and process information. Exploiting these vulnerabilities allows an entity with superior cognitive awareness to gain a strategic advantage, a concept referred to as cognitive arbitrage. This paper investigates how to exploit the cognitive vulnerabilities of Advanced Persistent Threat (APT) attackers and proposes cognition-aware defenses that leverage windows of superiority to counteract attacks. Specifically, the proposed bi-level cyber warfare game focuses on ""strategic-level"" design for defensive deception mechanisms, which then facilitates ""operational-level"" actions and tactical-level execution of Tactics, Techniques, and Procedures (TTPs). Game-theoretic reasoning and analysis play a significant role in the cross-echelon quantitative modeling and design of cognitive arbitrage strategies. Our numerical results demonstrate that although the defender's initial advantage diminishes over time, strategically timed and deployed deception techniques can turn a negative value for the attacker into a positive one during the planning phase, and achieve at least a 40% improvement in total rewards during execution. This demonstrates that the defender can amplify even small initial advantages, sustain a strategic edge over the attacker, and secure long-term objectives, such as protecting critical assets throughout the attacker's lifecycle."
2509.05956,"We formulate the Knapsack Contracts problem -- a strategic version of the classic Stochastic Knapsack problem, which builds upon the inherent randomness shared by stochastic optimization and contract design. In this problem, the principal incentivizes agents to perform jobs with stochastic processing times, the realization of which depends on the agents' efforts.Algorithmically, we show that Knapsack Contracts can be viewed as Stochastic Knapsack with costs and multi-choice, features that introduce significant new challenges. We identify a crucial and economically meaningful parameter -- the Return on Investment (ROI) value. We show that the Inverse of ROI (or IOR for short) precisely characterizes the extent to which the approximation guarantees for Stochastic Knapsack extend to its strategic counterpart.For IOR of $\alpha$, we develop an algorithm that finds an $O(\alpha)$-approximation policy that does not rely on adaptivity. We establish matching $\Omega(\alpha)$ lower bounds, both on the adaptivity gap, and on what can be achieved without full distributional knowledge of the processing times. Taken together, our results show that IOR is fundamental to understanding the complexity and approximability of Knapsack Contracts, and bounding it is both necessary and sufficient for achieving non-trivial approximation guarantees. Our results highlight the computational challenges arising when stochasticity in optimization problems is controlled by strategic effort."
2509.06187,"In this paper, we introduce a family of sequential decision-making problems, collectively called the Keychain Problem, that involve exploring a set of actions to maximize expected payoff when only a subset of actions are available in each stage. In an instance of the Keychain Problem, a locksmith faces a sequence of choices, each of which involves selecting one key from a specified subset (a keychain) to attempt to open a lock. Given a Bayesian prior on the effectiveness of keys, the locksmith's goal is to maximize the expected number of rounds in which the lock is opened -- or equivalently, minimize the opportunity cost which is the expected number of rounds in which the chain has a correct key but our selected key is incorrect. We investigate Keychain Problems under three assumptions on the order in which keychains are tested by the locksmith: a fixed, known order; a random order sampled from a known distribution on a set of ``scenarios''; or an order selected by the locksmith themself. We present an exact algorithm for the simplest of these settings, and we present approximation algorithms and hardness results for the others. In the Probabilistic Scenarios setting, our approximation algorithm is based on a novel connection between combinatorial auctions and policy design for sequential decision-making problems. To illustrate the generality of this technique, we apply the same ideas to obtain Philosopher Inequalities for Online Bipartite Matching and some of its extensions."
2509.0752,"Prominent opinion formation models such as the one by Friedkin and Johnsen (FJ) concentrate on the effects of peer pressure on public opinions. In practice, opinion formation is also based on information about the state of the world and persuasion efforts. In this paper, we analyze an approach of Bayesian persuasion in the FJ model. There is an unknown state of the world that influences the preconceptions of n agents. A sender S can (partially) reveal information about the state to all agents. The agents update their preconceptions, and an equilibrium of public opinions emerges. We propose algorithms for the sender to reveal information in order to optimize various aspects of the emerging equilibrium. For many natural sender objectives, we show that there are simple optimal strategies. We then focus on a general class of range-based objectives with desired opinion ranges for each agent. We provide efficient algorithms in several cases, e.g., when the matrix of preconceptions in all states has constant rank, or when there is only a polynomial number of range combinations that lead to positive value for S. This generalizes, e.g., instances with a constant number of states and/or agents, or instances with a logarithmic number of ranges. In general, we show that subadditive range-based objectives allow a simple n-approximation, and even for additive ones, obtaining an $n^{1-c}$-approximation is NP-hard, for any constant $c > 0$."
2509.07557,"In citizens' assemblies, a group of constituents is randomly selected to weigh in on policy issues. We study a two-stage sampling problem faced by practitioners in countries such as Germany, in which constituents' contact information is stored at a municipal level. As a result, practitioners can only select constituents from a bounded number of cities ex post, while ensuring equal selection probability for constituents ex ante.We develop several algorithms for this problem. Although minimizing the number of contacted cities is NP-hard, we provide a pseudo-polynomial time algorithm and an additive 1-approximation, both based on separation oracles for a linear programming formulation. Recognizing that practical objectives go beyond minimizing city count, we further introduce a simple and more interpretable greedy algorithm, which additionally satisfies an ex-post monotonicity property and achieves an additive 2-approximation. Finally, we explore a notion of ex-post proportionality, for which we propose two practical algorithms: an optimal algorithm based on column generation and integer linear programming and a simple heuristic creating particularly transparent distributions. We evaluate these algorithms on data from Germany, and plan to deploy them in cooperation with a leading nonprofit organization in this space."
2509.0765,"Human interactions are influenced by emotions, temperament, and affection, often conflicting with individuals' underlying preferences. Without explicit knowledge of those preferences, judging whether behaviour is appropriate becomes guesswork, leaving us highly prone to misinterpretation. Yet, such understanding is critical if autonomous agents are to collaborate effectively with humans. We frame the problem with multi-agent inverse reinforcement learning and show that even a simple model, where agents weigh their own welfare against that of others, can cover a wide range of social behaviours. Using novel Bayesian techniques, we find that intrinsic rewards and altruistic tendencies can be reliably identified by placing agents in different groups. Crucially, this disentanglement of intrinsic motivation from altruism enables the synthesis of new behaviours aligned with any desired level of altruism, even when demonstrations are drawn from restricted behaviour profiles."
2509.07929,"We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds upon the industry standard Fast Finish (FF) feature in budget pacing systems that depletes remaining advertising budget as quickly as possible towards the end of some fixed time period. SFF dynamically updates system parameters such as start time and throttle rate depending on historical ad-campaign data. SFF is currently in use at DoorDash, one of the largest delivery platforms in the US, and is part of its budget pacing system. We show via online budget-split experimentation data and offline simulations that SFF is a robust solution for overdelivery mitigation when pacing budget."
2509.08767,"Budget aggregation deals with the social choice problem of distributing an exogenously given budget among a set of public projects, given agents' preferences. Taking a game-theoretic perspective, we initialize the study of budget-aggregation games where each agent has virtual decision power over some fraction of the budget. This paper investigates the structure and shows efficient computability of Nash equilibria in this setting for various preference models. In particular, we show that Nash equilibria for Leontief utilities can be found in polynomial time, solving an open problem from Brandt et al. [2023]."
2509.08976,"Cyber warfare has become a central element of modern conflict, especially within multi-domain operations. As both a distinct and critical domain, cyber warfare requires integrating defensive and offensive technologies into coherent strategies. While prior research has emphasized isolated tactics or fragmented technologies, a holistic understanding is essential for effective resource deployment and risk mitigation. Game theory offers a unifying framework for this purpose. It not only models attacker-defender interactions but also provides quantitative tools for equilibrium analysis, risk assessment, and strategic reasoning. Integrated with modern AI techniques, game-theoretic models enable the design and optimization of strategies across multiple levels of cyber warfare, from policy and strategy to operations, tactics, and technical implementations. These models capture the paradoxical logic of conflict, where more resources do not always translate into greater advantage, and where nonlinear dynamics govern outcomes. To illustrate the approach, this chapter examines RedCyber, a synthetic cyber conflict, demonstrating how game-theoretic methods capture the interdependencies of cyber operations. The chapter concludes with directions for future research on resilience, cros-echelon planning, and the evolving role of AI in cyber warfare."
2509.09099,"We study a Bayesian persuasion setting in which a sender wants to persuade a critical mass of receivers by revealing partial information about the state to them. The homogeneous binary-action receivers are located on a communication network, and each observes the private messages sent to them and their immediate neighbors. We examine how the sender's expected utility varies with increased communication among receivers. We show that for general families of networks, extending the network can strictly benefit the sender. Thus, the sender's gain from persuasion is not monotonic in network density. Moreover, many network extensions can achieve the upper bound on the sender's expected utility among all networks, which corresponds to the payoff in an empty network. This is the case in networks reflecting a clear informational hierarchy (e.g., in global corporations), as well as in decentralized networks in which information originates from multiple sources (e.g., influencers in social media). Finally, we show that a slight modification to the structure of some of these networks precludes the possibility of such beneficial extensions. Overall, our results caution against presuming that more communication necessarily leads to better collective outcomes."
2509.09561,"We initiate the study of mechanism design with outliers, where the designer can discard $z$ agents from the social cost objective. This setting is particularly relevant when some agents exhibit extreme or atypical preferences. As a natural case study, we consider facility location on the line: $n$ strategic agents report their preferred locations, and a mechanism places a facility to minimize a social cost function. In our setting, the $z$ agents farthest from the chosen facility are excluded from the social cost. While it may seem intuitive that discarding outliers improves efficiency, our results reveal that the opposite can hold.We derive tight bounds for deterministic strategyproof mechanisms under the two most-studied objectives: utilitarian and egalitarian social cost. Our results offer a comprehensive view of the impact of outliers. We first show that when $z \ge n/2$, no strategyproof mechanism can achieve a bounded approximation for either objective. For egalitarian cost, selecting the $(z + 1)$-th order statistic is strategyproof and 2-approximate. In fact, we show that this is best possible by providing a matching lower bound. Notably, this lower bound of 2 persists even when the mechanism has access to a prediction of the optimal location, in stark contrast to the setting without outliers. For utilitarian cost, we show that strategyproof mechanisms cannot effectively exploit outliers, leading to the counterintuitive outcome that approximation guarantees worsen as the number of outliers increases. However, in this case, access to a prediction allows us to design a strategyproof mechanism achieving the best possible trade-off between consistency and robustness. Finally, we also establish lower bounds for randomized mechanisms that are truthful in expectation."
2509.09641,"We study the fair allocation of indivisible items to $n$ agents to maximize the utilitarian social welfare, where the fairness criterion is envy-free up to one item and there are only two different utility functions shared by the agents. We present a $2$-approximation algorithm when the two utility functions are normalized, improving the previous best ratio of $16 \sqrt{n}$ shown for general normalized utility functions; thus this constant ratio approximation algorithm confirms the APX-completeness in this special case previously shown APX-hard. When there are only three agents, i.e., $n = 3$, the previous best ratio is $3$ shown for general utility functions, and we present an improved and tight $\frac 53$-approximation algorithm when the two utility functions are normalized, and a best possible and tight $2$-approximation algorithm when the two utility functions are unnormalized."
2509.10112,"There is a broad recognition that commitment-based mechanisms can promote coordination and cooperative behaviours in both biological populations and self-organised multi-agent systems by making individuals' intentions explicit prior to engagement. Yet their effectiveness depends on sustained compliance supported by institutions, especially in one-off interactions. Despite advances in quantitative studies of cooperation and commitment, most applied analyses and policy debates remain largely qualitative, with limited attention to the allocation of scarce institutional resources between enhancing participation and ensuring commitment compliance. Herein, we develop an evolutionary game-theoretic model that explicitly examines the strategic distribution of a limited budget for institutional incentives, namely rewards or punishments, aimed at these two critical objectives within pre-commitment frameworks. Our findings reveal that a reward-based incentive approach consistently yields greater coordination success than a punishment-based approach, with optimal outcomes arising when resources are appropriately distributed between participation promotion and compliance assurance. These findings offer novel insights for designing institutional incentives to promote broad, coordinated adoption of new technologies."
2509.10983,"Cyber defense operations increasingly require long-term strategic planning under uncertainty and resource constraints. We propose a new use of combinatorial auctions for allocating defensive action bundles in a realistic cyber environment, using host-specific valuations derived from reinforcement learning (RL) Q-values. These Q-values encode long-term expected utility, allowing upstream planning. We train CAFormer, a differentiable Transformer-based auction mechanism, to produce allocations that are approximately incentive-compatible under misreporting. Rather than benchmarking against existing agents, we explore the qualitative and strategic properties of the learned mechanisms. Compared to oracle and heuristic allocations, our method achieves competitive revenue while offering robustness to misreporting. In addition, we find that allocation patterns correlate with adversarial and defensive activity, suggesting implicit alignment with operational priorities. Our results demonstrate the viability of auction-based planning in cyber defense and highlight the interpretability benefits of RL-derived value structures."
2509.10989,"Correlated equilibrium generalizes Nash equilibrium by allowing a central coordinator to guide players' actions through shared recommendations, similar to how routing apps guide drivers. We investigate how a coordinator can learn a correlated equilibrium in convex games where each player minimizes a convex cost function that depends on other players' actions, subject to convex constraints without knowledge of the players' cost functions. We propose a learning framework that learns an approximate correlated equilibrium by actively querying players' regrets, \emph{i.e.}, the cost saved by deviating from the coordinator's recommendations. We first show that a correlated equilibrium in convex games corresponds to a joint action distribution over an infinite joint action space that minimizes all players' regrets. To make the learning problem tractable, we introduce a heuristic that selects finitely many representative joint actions by maximizing their pairwise differences. We then apply Bayesian optimization to learn a probability distribution over the selected joint actions by querying all players' regrets. The learned distribution approximates a correlated equilibrium by minimizing players' regrets. We demonstrate the proposed approach via numerical experiments on multi-user traffic assignment games in a shared transportation network."
2509.11261,"A perfect clone in an ordinal election (i.e., an election where the voters rank the candidates in a strict linear order) is a set of candidates that each voter ranks consecutively. We consider different relaxations of this notion: independent or subelection clones are sets of candidates that only some of the voters recognize as a perfect clone, whereas approximate clones are sets of candidates such that every voter ranks their members close to each other, but not necessarily consecutively. We establish the complexity of identifying such imperfect clones, and of partitioning the candidates into families of imperfect clones. We also study the parameterized complexity of these problems with respect to a set of natural parameters such as the number of voters, the size or the number of imperfect clones we are searching for, or their level of imperfection."
2509.11294,"Decentralized data-feed systems enable blockchain-based smart contracts to access off-chain information by aggregating values from multiple oracles. To improve accuracy, these systems typically use an aggregation function, such as majority voting, to consolidate the inputs they receive from oracles and make a decision. Depending on the final decision and the values reported by the oracles, the participating oracles are compensated through shared rewards. However, such incentive mechanisms are vulnerable to mirroring attacks, where a single user controls multiple oracles to bias the decision of the aggregation function and maximize rewards. This paper analyzes the impact of mirroring attacks on the reliability and dependability of majority voting-based data-feed systems. We demonstrate how existing incentive mechanisms can unintentionally encourage rational users to implement such attacks. To address this, we propose a new incentive mechanism that discourages Sybil behavior. We prove that the proposed mechanism leads to a Nash Equilibrium in which each user operates only one oracle. Finally, we discuss the practical implementation of the proposed incentive mechanism and provide numerical examples to demonstrate its effectiveness."
2509.13637,"Humans exhibit time-inconsistent behavior, in which planned actions diverge from executed actions. Understanding time inconsistency and designing appropriate interventions is a key research challenge in computer science and behavioral economics. Previous work focuses on progress-based tasks and derives a closed-form description of agent behavior, from which they obtain optimal intervention strategies. They model time-inconsistency using the $\beta$-$\delta$ discounting (quasi-hyperbolic discounting), but the analysis is limited to the case $\delta = 1$. In this paper, we relax that constraint and show that a closed-form description of agent behavior remains possible for the general case $0 < \delta \le 1$. Based on this result, we derive the conditions under which agents abandon tasks and develop efficient methods for computing optimal interventions. Our analysis reveals that agent behavior and optimal interventions depend critically on the value of $\delta$, suggesting that fixing $\delta = 1$ in many prior studies may unduly simplify real-world decision-making processes."
2509.13653,"Regret minimization is a powerful method for finding Nash equilibria in Normal-Form Games (NFGs) and Extensive-Form Games (EFGs), but it typically guarantees convergence only for the average strategy. However, computing the average strategy requires significant computational resources or introduces additional errors, limiting its practical applicability. The Reward Transformation (RT) framework was introduced to regret minimization to achieve last-iterate convergence through reward function regularization. However, it faces practical challenges: its performance is highly sensitive to manually tuned parameters, which often deviate from theoretical convergence conditions, leading to slow convergence, oscillations, or stagnation in local optima.Inspired by previous work, we propose an adaptive technique to address these issues, ensuring better consistency between theoretical guarantees and practical performance for RT Regret Matching (RTRM), RT Counterfactual Regret Minimization (RTCFR), and their variants in solving NFGs and EFGs more effectively. Our adaptive methods dynamically adjust parameters, balancing exploration and exploitation while improving regret accumulation, ultimately enhancing asymptotic last-iterate convergence and achieving linear convergence. Experimental results demonstrate that our methods significantly accelerate convergence, outperforming state-of-the-art algorithms."
2509.14032,"We study the existence and computation of Nash equilibria in continuous static games where the players' admissible strategies are subject to shared coupling constraints, i.e., constraints that depend on their \emph{joint} strategies. Specifically, we focus on a class of games characterized by playerwise concave utilities and playerwise concave constraints. Prior results on the existence of Nash equilibria are not applicable to this class, as they rely on strong assumptions such as joint convexity of the feasible set. By leveraging topological fixed point theory and novel structural insights into the contractibility of feasible sets under playerwise concave constraints, we give an existence proof for Nash equilibria under weaker conditions. Having established existence, we then focus on the computation of Nash equilibria via independent gradient methods under the additional assumption that the utilities admit a potential function. To account for the possibly nonconvex feasible region, we employ a log barrier regularized gradient ascent with adaptive stepsizes. Starting from an initial feasible strategy profile and under exact gradient feedback, the proposed method converges to an $\epsilon$-approximate constrained Nash equilibrium within $\mathcal{O}(\epsilon^{-3})$ iterations."
2509.14091,"Classic reachability games on graphs are zero-sum games, where the goal of one player, Eve, is to visit a vertex from a given target set, and that of other player, Adam, is to prevent this. Generalised reachability games, studied by Fijalkow and Horn, are a generalisation of reachability objectives, where instead of a single target set, there is a family of target sets and Eve must visit all of them in any order. In this work, we further study the complexity of solving two-player games on graphs with generalised reachability objectives. Our results are twofold: first, we provide an improved complexity picture for generalised reachability games, expanding the known tractable class from games in which all target sets are singleton to additionally allowing a logarithmic number of target sets of arbitrary size. Second, we study optimisation variants of generalised reachability with a focus on the size of the target sets. For these problems, we show intractability for most interesting cases. Particularly, in contrast to the tractability in the classic variant for singleton target sets, the optimisation problem is NP-hard when Eve tries to maximise the number of singleton target sets that are visited. Tractability can be recovered in the optimisation setting when all target sets are singleton by requiring that Eve pledges a maximum sized subset of target sets that she can guarantee to visit."
2509.14112,"Algorithmic analysis of Markov decision processes (MDP) and stochastic games (SG) in practice relies on value-iteration (VI) algorithms. Since basic VI does not provide guarantees on the precision of the result, variants of VI have been proposed that offer such guarantees. In particular, sound value iteration (SVI) not only provides precise lower and upper bounds on the result, but also converges faster in the presence of probabilistic cycles. Unfortunately, it is neither applicable to SG, nor to MDP with end components. In this paper, we extend SVI and cover both cases. The technical challenge consists mainly in proper treatment of end components, which require different handling than in the literature. Moreover, we provide several optimizations of SVI. Finally, we evaluate our prototype implementation experimentally to demonstrate its potential on systems with probabilistic cycles."
2509.14411,"Understanding the formation of opinions on interconnected topics within social networks is of significant importance. It offers insights into collective behavior and decision-making, with applications in Graph Neural Networks. Existing models propose that individuals form opinions based on a weighted average of their peers' opinions and their own beliefs. This averaging process, viewed as a best-response game, can be seen as an individual minimizing disagreements with peers, defined by a quadratic penalty, leading to an equilibrium. Bindel, Kleinberg, and Oren (FOCS 2011) provided tight bounds on the ""price of anarchy"" defined as the maximum overall disagreement at equilibrium relative to a social optimum. Bhawalkar, Gollapudi, and Munagala (STOC 2013) generalized the penalty function to non-quadratic penalties and provided tight bounds on the price of anarchy.When considering multiple topics, an individual's opinions can be represented as a vector. Parsegov, Proskurnikov, Tempo, and Friedkin (2016) proposed a multidimensional model using the weighted averaging process, but with constant interdependencies between topics. However, the question of the price of anarchy for this model remained open. We address this by providing tight bounds on the multidimensional model, while also generalizing it to more complex interdependencies. Following the work of Bhawalkar, Gollapudi, and Munagala, we provide tight bounds on the price of anarchy under non-quadratic penalties. Surprisingly, these bounds match the scalar model. We further demonstrate that the bounds remain unchanged even when adding another layer of complexity, involving groups of individuals minimizing their overall internal and external disagreement penalty, a common occurrence in real-life scenarios."
2509.14466,"We study the problem of pure exploration in matching markets under uncertain preferences, where the goal is to identify a stable matching with confidence parameter $\delta$ and minimal sample complexity. Agents learn preferences via stochastic rewards, with expected values indicating preferences. This finds use in labor market platforms like Upwork, where firms and freelancers must be matched quickly despite noisy observations and no prior knowledge, in a stable manner that prevents dissatisfaction. We consider markets with unique stable matching and establish information-theoretic lower bounds on sample complexity for (1) one-sided learning, where one side of the market knows its true preferences, and (2) two-sided learning, where both sides are uncertain. We propose a computationally efficient algorithm and prove that it asymptotically ($\delta\to 0$) matches the lower bound to a constant for one-sided learning. Using the insights from the lower bound, we extend our algorithm to the two-sided learning setting and provide experimental results showing that it closely matches the lower bound on sample complexity. Finally, using a system of ODEs, we characterize the idealized fluid path that our algorithm chases."
2509.15812,"In the k-Kemeny problem, we are given an ordinal election, i.e., a collection of votes ranking the candidates from best to worst, and we seek the smallest number of swaps of adjacent candidates that ensure that the election has at most k different rankings. We study this problem for a number of structured domains, including the single-peaked, single-crossing, group-separable, and Euclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny remains intractable under most of these domains, even for k=2, and (2) we use k-Kemeny to rank these domains in terms of their diversity."
2509.16075,"The strategy improvement algorithm for mean payoff games and parity games is a local improvement algorithm, just like the simplex algorithm for linear programs. Their similarity has turned out very useful: many lower bounds on running time for the simplex method have been created from lower bounds for strategy improvement. However, earlier connections between these algorithms required constructing an intermediate Markov decision process, which is not always possible. We prove a formal, direct connection between the two algorithms, showing that many variants of strategy improvement for parity and mean payoff games are truly an instance of the simplex algorithm, under mild nondegeneracy assumptions. As a result of this, we derive some combinatorial properties of the structure of strategy sets of various related games on graphs. In particular, we show a connection to lopsided sets."
2509.16157,"Liquidity providers (LPs) are essential figures in the operation of automated market makers (AMMs); in exchange for transaction fees, LPs lend the liquidity that allows AMMs to operate. While many prior works have studied the incentive structures of LPs in general, we currently lack a principled understanding of a special class of LPs known as Just-In-Time (JIT) LPs. These are strategic agents who momentarily supply liquidity for a single swap, in an attempt to extract disproportionately high fees relative to the remaining passive LPs. This paper provides the first formal, transaction-level model of JIT liquidity provision for a widespread class of AMMs known as Concentrated Liquidity Market Makers (CLMMs), as seen in Uniswap V3, for instance. We characterize the landscape of price impact and fee allocation in these systems, formulate and analyze a non-linear optimization problem faced by JIT LPs, and prove the existence of an optimal strategy. By fitting our optimal solution for JIT LPs to real-world CLMMs, we observe that in liquidity pools (particularly those with risky assets), there is a significant gap between observed and optimal JIT behavior. Existing JIT LPs often fail to account for price impact; doing so, we estimate they could increase earnings by up to 69% on average over small time windows. We also show that JIT liquidity, when deployed strategically, can improve market efficiency by reducing slippage for traders, albeit at the cost of eroding average passive LP profits by up to 44% per trade."
2509.1645,"The rapid growth of data-driven technologies and the emergence of various data-sharing paradigms have underscored the need for efficient and stable data exchange protocols. In any such exchange, agents must carefully balance the benefit of acquiring valuable data against the cost of sharing their own. Ensuring stability in these exchanges is essential to prevent agents -- or groups of agents -- from departing and conducting local (and potentially more favorable) exchanges among themselves. To address this, we study a model where agents participate in a data exchange. Each agent has an associated payoff for the data acquired from other agents and a cost incurred during sharing its own data. The net utility of an agent is payoff minus the cost. We adapt the classical notion of core-stability from cooperative game theory to data exchange. A data exchange is core-stable if no subset of agents has any incentive to deviate to a different exchange. We show that a core-stable data exchange is guaranteed to exist when agents have concave payoff functions and convex cost functions -- a setting typical in domains like PAC learning and random discovery models. We show that relaxing either of the foregoing conditions may result in the nonexistence of core-stable data exchanges. Then, we prove that finding a core-stable exchange is PPAD-hard, even when the potential blocking coalitions are restricted to constant size. To the best of our knowledge, this provides the first known PPAD-hardness result for core-like guarantees in data economics. Finally, we show that data exchange can be modelled as a balanced $n$-person game. This immediately gives a pivoting algorithm via Scarf's theorem \cite{Scarf1967core}. We show that the pivoting algorithm works well in practice through our empirical results."
2509.16802,"We extend the notion of combinatorial discrepancy to \emph{non-additive} functions. Our main result is an upper bound of $O(\sqrt{n \log(nk)})$ on the non-additive $k$-color discrepancy when $k$ is a prime power. We demonstrate two applications of this result to problems in fair division. First, we establish a bound for a consensus halving problem, where fairness is measured by the minimum number of items that must be transferred between the two parts to eliminate envy. Second, we improve the upper bound on the total subsidy required to achieve an envy-free allocation when the number of agents is a prime power, obtaining an $O(n \sqrt{n \log n})$ bound. This constitutes the first known subquadratic guarantee in this setting."
2509.17134,"We study metric distortion in distributed voting, where $n$ voters are partitioned into $k$ groups, each selecting a local representative, and a final winner is chosen from these representatives (or from the entire set of candidates). This setting models systems like U.S. presidential elections, where state-level decisions determine the national outcome. We focus on four cost objectives from \citep{anshelevich2022distortion}: $\avgavg$, $\avgmax$, $\maxavg$, and $\maxmax$. We present improved distortion bounds for both deterministic and randomized mechanisms, offering a near-complete characterization of distortion in this model.For deterministic mechanisms, we reduce the upper bound for $\avgmax$ from $11$ to $7$, establish a tight lower bound of $5$ for $\maxavg$ (improving on $2+\sqrt{5}$), and tighten the upper bound for $\maxmax$ from $5$ to $3$.For randomized mechanisms, we consider two settings: (i) only the second stage is randomized, and (ii) both stages may be randomized. In case (i), we prove tight bounds: $5\!-\!2/k$ for $\avgavg$, $3$ for $\avgmax$ and $\maxmax$, and $5$ for $\maxavg$. In case (ii), we show tight bounds of $3$ for $\maxavg$ and $\maxmax$, and nearly tight bounds for $\avgavg$ and $\avgmax$ within $[3\!-\!2/n,\ 3\!-\!2/(kn^*)]$ and $[3\!-\!2/n,\ 3]$, respectively, where $n^*$ denotes the largest group size."
2509.18338,"Restaking protocols expand validator responsibilities beyond consensus, but their security depends on resistance to Sybil attacks. We introduce a formal framework for Sybil-proofness in restaking networks, distinguishing between two types of attacks, one in which other Sybil identities are kept out of an attack and one where multiple Sybil identities attack. We analyze marginal and multiplicative slashing mechanisms and characterize the conditions under which each deters Sybil strategies. We then prove an impossibility theorem: no slashing mechanism can simultaneously prevent both attack types. Finally, we study the impact of network structure through random graph models: while Erdös-Rényi networks remain Sybil-proof, even minimal heterogeneity in a two-block stochastic block model makes Sybil attacks profitable. These results reveal fundamental limits of mechanism design for restaking and highlight the critical role of network topology."
2509.18343,"We discuss an algorithmic intervention aimed at increasing equity and economic efficiency at a crowdfunding platform that gives cash subsidies to grantees. Through a blend of technical and qualitative methods, we show that the previous algorithm used by the platform -- Quadratic Funding (QF) -- suffered problems because its design was rooted in a model of individuals as isolated and selfish. We present an alternative algorithm -- Connection-Oriented Quadratic Funding (CO-QF) -- rooted in a theory of plurality and prosocial utilities, and show that it qualitatively and quantitatively performs better than QF. CO-QF has achieved an 89% adoption rate at the platform and has distributed over $4 Million to date. In simulations we show that it provides better social welfare than QF. While our design for CO-QF was responsive to the needs of a specific community, we also extrapolate out of this context to show that CO-QF is a potentially helpful tool for general-purpose public decision making."
2509.18551,"This paper develops a game-theoretic model and an agent-based model to study group formation driven by resource pooling, spatial cohesion, and heterogeneity. We focus on cross-sector partnerships (CSPs) involving public, private, and nonprofit organizations, each contributing distinct resources. Group formation occurs as agents strategically optimize their choices in response to others within a competitive setting. We prove the existence of stable group equilibria and simulate formation dynamics under varying spatial and resource conditions. The results show that limited individual resources lead to groups that form mainly among nearby actors, while abundant resources allow groups to move across larger distances. Increased resource heterogeneity and spatial proximity promote the formation of larger and more diverse groups. These findings reveal key trade-offs shaping group size and composition, guiding strategies for effective cross-sector collaborations and multi-agent systems."
2509.18673,"The existence of allocations that are fair and efficient, simultaneously, is a central inquiry in fair division literature. A prominent result in discrete fair division shows that the complementary desiderata of fairness and efficiency can be achieved together when allocating indivisible items with nonnegative values; specifically, for indivisible goods and among agents with additive valuations, there always exists an allocation that is both envy-free up to one item (EF1) and Pareto efficient (PO). While a recent breakthrough extends the EF1 and PO guarantee to indivisible chores (items with negative values), the question remains open for indivisible mixed manna, i.e., for indivisible items whose values can be positive, negative, or zero. The current work makes notable progress in resolving this central question.For indivisible mixed manna and additive valuations, we establish the existence of allocations that are PO and introspectively envy-free up to one item (IEF1). In an IEF1 allocation, each agent can eliminate its envy towards all the other agents by either adding an item or removing an item from its own bundle. The notion of IEF1 coincides with EF1 for indivisible chores, and hence, our result generalizes the aforementioned existence guarantee for chores. Our techniques can be adopted to obtain an alternative proof for the existence of EF1 and PO allocations of indivisible goods. Hence, along with the result for mixed manna, we provide a unified approach for establishing the EF1 and PO guarantee for indivisible goods and indivisible chores. We also utilize our result for indivisible items to develop a distinct proof of the noted EF and PO guarantee for divisible mixed manna. Our work highlights an interesting application of the Knaster-Kuratowski-Mazurkiewicz (KKM) Theorem in discrete fair division and develops multiple, novel structural insights and algorithmic ideas."
2509.19279,"Much research in electoral control -- one of the most studied form of electoral attacks, in which an entity running an election alters the structure of that election to yield a preferred outcome -- has focused on giving decision complexity results, e.g., membership in P, NP-completeness, or fixed-parameter tractability. Approximation algorithms on the other hand have received little attention in electoral control, despite their prevalence in the study of other forms of electoral attacks, such as manipulation and bribery. Early work established some preliminary results with respect to popular voting rules such as plurality, approval, and Condorcet. In this paper, we establish for each of the ``standard'' control problems under plurality, approval, and Condorcet, whether they are approximable, and we prove our results in both the weighted and unweighted voter settings. For each problem we study under either approval or Condorcet, we show that any approximation algorithm we give is optimal, unless P=NP. Our approximation algorithms leverage the fact that Covering Integer Programs (CIPs) can be approximated within a factor of $O(\log n)$. Under plurality, we give an $O(m)$-approximation algorithm, and give as lower bound $\Omega(m^{1/4})$, by using a known lower bound on the Minimum $k$-Union (M$k$U) problem. To our knowledge, this is the first application of M$k$U in computational social choice. We also generalize our $O(m)$-approximation algorithm to work with respect to an infinite family of voting rules using an axiomatic approach. Our work closes a long list of open problems established 18 years ago."
2509.20147,"Consider N players and K games taking place simultaneously. Each of these games is modeled as a Tug-of-War (ToW) game where increasing the action of one player decreases the reward for all other players. Each player participates in only one game at any given time. At each time step, a player decides the game in which they wish to participate in and the action they take in that game. Their reward depends on the actions of all players that are in the same game. This system of K games is termed `Meta Tug-of-War' (Meta-ToW) game. These games can model scenarios such as power control, distributed task allocation, and activation in sensor networks. We propose the Meta Tug-of-Peace algorithm, a distributed algorithm where the action updates are done using a simple stochastic approximation algorithm, and the decision to switch games is made using an infrequent 1-bit communication between the players. We prove that in Meta-ToW games, our algorithm converges to an equilibrium that satisfies a target Quality of Service reward vector for the players. We then demonstrate the efficacy of our algorithm through simulations for the scenarios mentioned above."
2509.20329,"In this paper, we present a novel, game-theoretic model of deception in two-player, zero-sum games. Our framework leverages an information asymmetry: one player (the deceiver) has access to accurate payoff information, while the other (the victim) observes a modified version of these payoffs due to the deception strategy employed. The deceiver's objective is to choose a deception-action pair that optimally exploits the victim's best response to the altered payoffs, subject to a constraint on the deception's magnitude. We characterize the optimal deceptive strategy as the solution to a bi-level optimization problem, and we provide both an exact solution and an efficient method for computing a high-quality feasible point. Finally, we demonstrate the effectiveness of our approach on numerical examples inspired by honeypot deception."
2509.20919,"We examine the problem of efficiently learning coarse correlated equilibria (CCE) in polyhedral games, that is, normal-form games with an exponentially large number of actions per player and an underlying combinatorial structure. Prominent examples of such games are the classical Colonel Blotto and congestion games. To achieve computational efficiency, the learning algorithms must exhibit regret and per-iteration complexity that scale polylogarithmically in the size of the players' action sets. This challenge has recently been addressed in the full-information setting, primarily through the use of kernelization. However, in the case of the realistic, but mathematically challenging, partial-information setting, existing approaches result in suboptimal and impractical runtime complexity to learn CCE. We tackle this limitation by building a framework based on the kernelization paradigm. We apply this framework to prominent examples of polyhedral games -- namely the Colonel Blotto, graphic matroid and network congestion games -- and provide computationally efficient payoff-based learning algorithms, which significantly improve upon prior works in terms of the runtime for learning CCE in these settings."
2509.20932,"This paper uses category theory to develop an entirely new approach to approximate game theory. Game theory is the study of how different agents within a multi-agent system take decisions. At its core, game theory asks what an optimal decision is in a given scenario. Thus approximate game theory asks what is an approximately optimal decision in a given scenario. This is important in practice as -- just like in much of computing -- exact answers maybe too difficult to compute or even impossible to compute given inherent uncertainty in input.We consider first ""Selection Functions"" which are functions and develop a simple yet robust model of approximate equilibria. We develop the algebraic properties of approximation wrt selection functions and also relate approximation to the compositional structure of selection functions. We then repeat this process successfully for Open Games -- a more advanced model of game theory."
2509.2157,"Long studied as a toy model, quantum zero-sum games have recently resurfaced as a canonical playground for modern areas such as non-local games, quantum interactive proofs, and quantum machine learning. In this simple yet fundamental setting, two competing quantum players send iteratively mixed quantum states to a referee, who performs a joint measurement to determine their payoffs. In 2025, Vasconcelos et al. [arXiv:2311.10859] connected quantum communication channels with a hierarchy of quantum optimization algorithms that generalize Matrix Multiplicative Weights Update ($\texttt{MMWU}$) through extra-gradient mechanisms, establishing an average-iterate convergence rate of $\mathcal{O}(1/\epsilon)$ iterations to $\epsilon$-Nash equilibria. While a long line of work has shown that bilinear games over polyhedral domains admit gradient methods with linear last-iterate convergence rates of $\mathcal{O}(\log(1/\epsilon))$, it has been conjectured that a fundamental performance gap must persist between quantum feasible sets (spectraplexes) and classical polyhedral sets (simplices). We resolve this conjecture in the negative. We prove that matrix variants of $\textit{Nesterov's iterative smoothing}$ ($\texttt{IterSmooth}$) and $\textit{Optimistic Gradient Descent-Ascent}$ ($\texttt{OGDA}$) achieve last-iterate convergence at a linear rate in quantum zero-sum games, thereby matching the classical polyhedral case. Our analysis relies on a new generalization of error bounds in semidefinite programming geometry, establishing that (SP-MS) holds for monotone operators over spectrahedra, despite their uncountably many extreme points. Finally, as a byproduct, we obtain an exponential speed-up over the classical Jain-Watrous [arXiv:0808.2775] method for parallel approximation of strictly positive semidefinite programs."
2509.21612,"Federated learning promises significant sample-efficiency gains by pooling data across multiple agents, yet incentive misalignment is an obstacle: each update is costly to the contributor but boosts every participant. We introduce a game-theoretic framework that captures heterogeneous data: an agent's utility depends on who supplies each sample, not just how many. Agents aim to meet a PAC-style accuracy threshold at minimal personal cost. We show that uncoordinated play yields pathologies: pure equilibria may not exist, and the best equilibrium can be arbitrarily more costly than cooperation. To steer collaboration, we analyze the cost-minimizing contribution vector, prove that computing it is NP-hard, and derive a polynomial-time linear program that achieves a logarithmic approximation. Finally, pairing the LP with a simple pay-what-you-contribute rule - each agent receives a payment equal to its sample cost - yields a mechanism that is strategyproof and, within the class of contribution-based transfers, is unique."
2509.22563,"Bilateral trade models the task of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. We study this problem from the perspective of a broker, in a regret minimization framework. At each time step, a new seller and buyer arrive, and the broker has to propose a mechanism that is incentive-compatible and individually rational, with the goal of maximizing profit.We propose a learning algorithm that guarantees a nearly tight $\tilde{O}(\sqrt{T})$ regret in the stochastic setting when seller and buyer valuations are drawn i.i.d. from a fixed and possibly correlated unknown distribution. We further show that it is impossible to achieve sublinear regret in the non-stationary scenario where valuations are generated upfront by an adversary. Our ambitious benchmark for these results is the best incentive-compatible and individually rational mechanism. This separates us from previous works on efficiency maximization in bilateral trade, where the benchmark is a single number: the best fixed price in hindsight.A particular challenge we face is that uniform convergence for all mechanisms' profits is impossible. We overcome this difficulty via a careful chaining analysis that proves convergence for a provably near-optimal mechanism at (essentially) optimal rate. We further showcase the broader applicability of our techniques by providing nearly optimal results for the joint ads problem."
2509.23157,"In game theory and multi-agent reinforcement learning (MARL), each agent selects a strategy, interacts with the environment and other agents, and subsequently updates its strategy based on the received payoff. This process generates a sequence of joint strategies $(s^t)_{t \geq 0}$, where $s^t$ represents the strategy profile of all agents at time step $t$. A widely adopted principle in MARL algorithms is ""win-stay, lose-shift"", which dictates that an agent retains its current strategy if it achieves the best response. This principle exhibits a fixed-point property when the joint strategy has become an equilibrium. The sequence of joint strategies under this principle is referred to as a satisficing path, a concept first introduced in [40] and explored in the context of $N$-player games in [39]. A fundamental question arises regarding this principle: Under what conditions does every initial joint strategy $s$ admit a finite-length satisficing path $(s^t)_{0 \leq t \leq T}$ where $s^0=s$ and $s^T$ is an equilibrium? This paper establishes a sufficient condition for such a property, and demonstrates that any finite-state Markov game, as well as any $N$-player game, guarantees the existence of a finite-length satisficing path from an arbitrary initial strategy to some equilibrium. These results provide a stronger theoretical foundation for the design of MARL algorithms."
2509.23747,"Game theory has grown into a major field over the past few decades, and poker has long served as one of its key case studies. Game-Theory-Optimal (GTO) provides strategies to avoid loss in poker, but pure GTO does not guarantee maximum profit. To this end, we aim to develop a model that outperforms GTO strategies to maximize profit in No Limit Holdem, in heads-up (two-player) and multi-way (more than two-player) situations. Our model finds the GTO foundation and goes further to exploit opponents. The model first navigates toward many simulated poker hands against itself and keeps adjusting its decisions until no action can reliably beat it, creating a strong baseline that is close to the theoretical best strategy. Then, it adapts by observing opponent behavior and adjusting its strategy to capture extra value accordingly. Our results indicate that Monte-Carlo Counterfactual Regret Minimization (CFR) performs best in heads-up situations and CFR remains the strongest method in most multi-way situations. By combining the defensive strength of GTO with real-time exploitation, our approach aims to show how poker agents can move from merely not losing to consistently winning against diverse opponents."
2509.24398,"In the realm of evolutionary game theory, standard frameworks typically presuppose that every player possesses comprehensive knowledge and unrestricted access to the entire strategy space. However, real-world human society inherently harbors diverse levels of knowledge, experience, and background among individuals. Hypergames incorporate this heterogeneity by permitting individuals to differ in their access to the full strategy set, reflecting cognitive or informational constraints and giving rise to asymmetric strategic interactions. Yet, their evolutionary consequences remain underexplored. Our inquiry employs prototype models featuring three available strategies, focusing on social dilemmas involving cooperation, defection, and loner. These strategies manifest cyclic dominance, akin to the well-studied rock-paper-scissors dynamics, a foundational model in game theory. Our study spans both well-mixed and spatial lattice populations, delving into the intricacies of learning and evolution of the strategy set within the evolutionary hypergame dynamics. In stark contrast to traditional evolutionary game dynamics, our findings unveil nuanced and intricate phases, encompassing scenarios of loner dominance, coexistence of multiple strategy sets, combinations of cooperation and loner dominance, and more. Remarkably, we discern that heightened rationality significantly promotes cooperative behaviors."
2509.2472,"We study the optimal dynamic pricing of an expiring ticket or voucher, sold by a time-sensitive seller to strategic buyers who arrive stochastically with private values. The expiring nature creates a conflict: the seller's urgency to sell before expiration drives price reductions, which in turn incentivize buyers to wait. We seek the seller's optimal pricing policy that resolves this tension. The main analytical challenge is that buyer type is two-dimensional (valuation and arrival time), which makes equilibrium intractable under general strategies. To address this, we introduce the Value-Based Threshold (VBT) strategy, a tractable framework that decouples these two dimensions. Using this framework, we prove equilibrium existence via an ordinary differential equation and provide a constructive procedure for its characterization. We then derive near-optimal pricing policies for two stylized regimes: a constant price in thin markets and a linear discount in thick markets. Numerical frontier analysis confirms these benchmarks and shows how optimal policy adapts as the seller's time sensitivity changes. Our findings clarify the conflict between quick sales and strategic waiting. Sellers facing thick markets or high time sensitivity benefit from linear discounts, while in thin markets a constant price neutralizes buyers' incentive to wait. We also show this simple policy remains robust across broad conditions. For patient sellers, a quasi-auction schedule that maintains a high price until a sharp final drop is most effective in aggregating demand."
2509.24806,"In this paper, we study a multi-agent scheduling problem for organising the operations within the operating room department. The head of the surgeon group and individual surgeons are together responsible for the surgeon schedule and surgical case planning. The surgeon head allocates time blocks to individual surgeons, whereas individual surgeons determine the planning of surgical cases independently, which might degrade the schedule quality envisaged by the surgeon head. The bilevel optimisation under study seeks an optimal Nash equilibrium solution -- a surgeon schedule and surgical case plan that optimise the objectives of the surgeon head, while ensuring that no individual surgeon can improve their own objective within the allocated time blocks. We propose a dedicated branch-and-price that adds lazy constraints to the formulation of surgeon-specific pricing problems to ensure an optimal bilevel feasible solution is retrieved. In this way, the surgeon head respects the objective requirements of the individual surgeons and the solution space can be searched efficiently. In the computational experiments, we validate the performance of the proposed algorithm and its dedicated components and provide insights into the benefits of attaining an equilibrium solution under different scenarios by calculating the price of stability and the price of decentralisation."
2509.24849,"Ethereum's upcoming Glamsterdam upgrade introduces EIP-7732 enshrined Proposer--Builder Separation (ePBS), which improves the block production pipeline by addressing trust and scalability challenges. Yet it also creates a new liveness risk: builders gain a short-dated ``free'' option to prevent the execution payload they committed to from becoming canonical, without incurring an additional penalty. Exercising this option renders an empty block for the slot in question, thereby degrading network liveness.We present the first systematic study of the free option problem. Our theoretical results predict that option value and exercise probability grow with market volatility, the length of the option window, and the share of block value derived from external signals such as external market prices. The availability of a free option will lead to mispricing and LP losses. The problem would be exacerbated if Ethereum further scales and attracts more liquidity. Empirical estimates of values and exercise probabilities on historical blocks largely confirm our theoretical predictions. While the option is rarely profitable to exercise on average (0.82\% of blocks assuming an 8-second option time window), it becomes significant in volatile periods, reaching up to 6\% of blocks on high-volatility days -- precisely when users most require timely execution.Moreover, builders whose block value relies heavily on CEX-DEX arbitrage are more likely to exercise the option. We demonstrate that mitigation strategies -- shortening the option window or penalizing exercised options -- effectively reduce liveness risk."
2509.24946,"Vehicular Cloud Computing (VCC) leverages the idle computing capacity of vehicles to execute end-users' offloaded tasks without requiring new computation infrastructure. Despite its conceptual appeal, VCC adoption is hindered by the lack of quantitative evidence demonstrating its profitability and environmental advantages in real-world scenarios. This paper tackles the fundamental question: Can VCC be both profitable and sustainable? We address this problem by proposing a management scheme for VCC that combines energy-aware task allocation with a game-theoretic revenue-sharing mechanism. Our framework is the first to jointly model latency, energy consumption, monetary incentives, and carbon emissions within urban mobility and 5G communication settings. The task allocation strategy maximizes the aggregate stakeholder utility while satisfying deadlines and minimizing energy costs. The payoffs are distributed via a coalitional game theory adapted to dynamic vehicular environments, to prevent disincentivizing participants with potentially negative contributions. Extensive simulations demonstrate that our approach supports low-latency task execution, enables effective monetization of vehicular resources, and reduces CO2 emissions by more than 99% compared to conventional edge infrastructures, making VCC a practical and sustainable alternative to edge computing."
2509.2515,"We study popular matchings in three classical settings: the house allocation problem, the marriage problem, and the roommates problem. In the popular matching problem, (a subset of) the vertices in a graph have preference orderings over their potential matches. A matching is popular if it gets a plurality of votes in a pairwise election against any other matching. Unfortunately, popular matchings typically do not exist. So we study a natural relaxation, namely popular winning sets which are a set of matchings that collectively get a plurality of votes in a pairwise election against any other matching. The $\textit{popular dimension}$ is the minimum cardinality of a popular winning set, in the worst case over the problem class.We prove that the popular dimension is exactly $2$ in the house allocation problem, even if the voters are weighted and ties are allowed in their preference lists. For the marriage problem and the roommates problem, we prove that the popular dimension is between $2$ and $3$, when the agents are weighted and/or their preferences orderings allow ties. In the special case where the agents are unweighted and have strict preference orderings, the popular dimension of the marriage problem is known to be exactly $1$ and we prove the popular dimension of the roommates problem is exactly $2$."
2509.25565,"Information design is typically studied through the lens of Bayesian signaling, where signals shape beliefs based on their correlation with the true state of the world. However, Behavioral Economics and Psychology emphasize that human decision-making is more complex and can depend on how information is framed. This paper formalizes a language-based notion of framing and bridges this to the popular Bayesian-persuasion model. We model framing as a possibly non-Bayesian, linguistic way to influence a receiver's belief, while a signaling (or recommendation) scheme can further refine this belief in the classic Bayesian way. A key challenge in systematically optimizing in this framework is the vast space of possible framings and the difficulty of predicting their effects on receivers. Based on growing evidence that Large Language Models (LLMs) can effectively serve as proxies for human behavior, we formulate a theoretical model based on access to a framing-to-belief oracle. This model then enables us to precisely characterize when solely optimizing framing or jointly optimizing framing and signaling is tractable. We substantiate our theoretical analysis with an empirical algorithm that leverages LLMs to (1) approximate the framing-to-belief oracle, and (2) optimize over language space using a hill-climbing method. We apply this to two marketing-inspired case studies and validate the effectiveness through analytical and human evaluation."
2509.25618,"There has been significant recent progress in algorithms for approximation of Nash equilibrium in large two-player zero-sum imperfect-information games and exact computation of Nash equilibrium in multiplayer strategic-form games. While counterfactual regret minimization and fictitious play are scalable to large games and have convergence guarantees in two-player zero-sum games, they do not guarantee convergence to Nash equilibrium in multiplayer games. We present an approach for exact computation of Nash equilibrium in multiplayer imperfect-information games that solves a quadratically-constrained program based on a nonlinear complementarity problem formulation from the sequence-form game representation. This approach capitalizes on recent advances for solving nonconvex quadratic programs. Our algorithm is able to quickly solve three-player Kuhn poker after removal of dominated actions. Of the available algorithms in the Gambit software suite, only the logit quantal response approach is successfully able to solve the game; however, the approach takes longer than our algorithm and also involves a degree of approximation. Our formulation also leads to a new approach for computing Nash equilibrium in multiplayer strategic-form games which we demonstrate to outperform a previous quadratically-constrained program formulation."
2509.25921,"Efficient coordination in multi-agent systems often incurs high communication overhead or slow convergence rates, making scalable welfare optimization difficult. We propose Single-Bit Coordination Dynamics for Pareto-Efficient Outcomes (SBC-PE), a decentralized learning algorithm requiring only a single-bit satisfaction signal per agent each round. Despite this extreme efficiency, SBC-PE guarantees convergence to the exact optimal solution in arbitrary finite games. We establish explicit regret bounds, showing expected regret grows only logarithmically with the horizon, i.e., O(log T). Compared with prior payoff-based or bandit-style rules, SBC-PE uniquely combines minimal signaling, general applicability, and finite-time guarantees. These results show scalable welfare optimization is achievable under minimal communication constraints."
2510.00162,"The necklace splitting problem is a classic problem in fair division with many applications, including data-informed fair hash maps. We extend necklace splitting to a dynamic setting, allowing for relocation, insertion, and deletion of beads. We present linear-time, optimal algorithms for the two-color case that support all dynamic updates. For more than two colors, we give linear-time, optimal algorithms for relocation subject to a restriction on the number of agents. Finally, we propose a randomized algorithm for the two-color case that handles all dynamic updates, guarantees approximate fairness with high probability, and runs in polylogarithmic time when the number of agents is small."
2510.00239,"Studying the impact of cooperation in strategic settings is one of the cornerstones of algorithmic game theory. Intuitively, allowing more cooperation yields equilibria that are more beneficial for the society of agents. However, for many games it is still an open question how much cooperation is actually needed to ensure socially good equilibria. We contribute to this research endeavor by analyzing the benefits of cooperation in a network formation game that models the creation of communication networks via the interaction of selfish agents. In our game, agents that correspond to nodes of a network can buy incident edges of a given weighted host graph to increase their centrality in the formed network. The cost of an edge is proportional to its length, and both endpoints must agree and pay for an edge to be created. This setting is known for having a high price of anarchy.To uncover the impact of cooperation, we investigate the price of anarchy of our network formation game with respect to multiple solution concepts that allow for varying amounts of cooperation. On the negative side, we show that on host graphs with arbitrary edge weights even the strongest form of cooperation cannot improve the price of anarchy. In contrast to this, as our main result, we show that cooperation has a significant positive impact if the given host graph has metric edge weights. For this, we prove asymptotically tight bounds on the price of anarchy via a novel proof technique that might be of independent interest and can be applied in other models with metric weights."
2510.00472,"We examine formal games that we call ""capital games"" in which player payoffs are known, but their payoffs are not guaranteed to be von Neumann-Morgenstern utilities. In capital games, the dynamics of player payoffs determine their utility functions. Different players can have different payoff dynamics. We make no assumptions about where these dynamics come from, but implicitly assume that they come from the players' actions and interactions over time. We define an equilibrium concept called ""growth equilibrium"" and show a correspondence between the growth equilibria of capital games and the Nash equilibria of standard games."
2510.01387,"In a multi-follower Bayesian Stackelberg game, a leader plays a mixed strategy over $L$ actions to which $n\ge 1$ followers, each having one of $K$ possible private types, best respond. The leader's optimal strategy depends on the distribution of the followers' private types. We study an online learning version of this problem: a leader interacts for $T$ rounds with $n$ followers with types sampled from an unknown distribution every round. The leader's goal is to minimize regret, defined as the difference between the cumulative utility of the optimal strategy and that of the actually chosen strategies. We design learning algorithms for the leader under different feedback settings. Under type feedback, where the leader observes the followers' types after each round, we design algorithms that achieve $\mathcal O\big(\sqrt{\min\{L\log(nKA T), nK \} \cdot T} \big)$ regret for independent type distributions and $\mathcal O\big(\sqrt{\min\{L\log(nKA T), K^n \} \cdot T} \big)$ regret for general type distributions. Interestingly, those bounds do not grow with $n$ at a polynomial rate. Under action feedback, where the leader only observes the followers' actions, we design algorithms with $\mathcal O( \min\{\sqrt{ n^L K^L A^{2L} L T \log T}, K^n\sqrt{ T } \log T \} )$ regret. We also provide a lower bound of $\Omega(\sqrt{\min\{L, nK\}T})$, almost matching the type-feedback upper bounds."
2510.01434,"In Bayesian persuasion, an informed sender, who observes a state, commits to a randomized signaling scheme that guides a self-interested receiver's actions. Classical models assume the receiver knows the commitment. We, instead, study the setting where the receiver infers the scheme from repeated interactions. We bound the sender's performance loss relative to the known-commitment case by a term that grows with the signal space size and shrinks as the receiver's optimal actions become more distinct. We then lower bound the samples required for the sender to approximately achieve their known-commitment performance in the inference setting. We show that the sender requires more samples in persuasion compared to the leader in a Stackelberg game, which includes commitment but lacks signaling. Motivated by these bounds, we propose two methods for designing inferable signaling schemes, one being stochastic gradient descent (SGD) on the sender's inference-setting utility, and the other being optimization with a boundedly-rational receiver model. SGD performs best in low-interaction regimes, but modeling the receiver as boundedly-rational and tuning the rationality constant still provides a flexible method for designing inferable schemes. Finally, we apply SGD to a safety alert example and show it to find schemes that have fewer signals and make citizens' optimal actions more distinct compared to the known-commitment case."
2510.01689,"We study fair division problems with strategic agents capable of gaining advantages by manipulating their reported preferences. Although several impossibility results have revealed the incompatibility of truthfulness with standard fairness criteria, subsequent works have circumvented this limitation through the incentive ratio framework. Previous studies demonstrate that fundamental mechanisms like Maximum Nash Welfare (MNW) and Probabilistic Serial (PS) for divisible goods, and Round-Robin (RR) for indivisible goods achieve an incentive ratio of $2$, implying that no individual agent can gain more than double his truthful utility through manipulation. However, collusive manipulation by agent groups remains unexplored.In this work, we define strong group incentive ratio (SGIR) and group incentive ratio (GIR) to measure the gain of collusive manipulation, where SGIR and GIR are respectively the maximum and minimum of the incentive ratios of corrupted agents. Then, we tightly characterize the SGIRs and GIRs of MNW, PS, and RR. In particular, the GIR of MNW is $2$ regardless of the coalition size. Moreover, for coalition size $c \geq 1$, the SGIRs of MNW and PS, and the GIRs of PS and RR are $c + 1$. Finally, the SGIR of RR is unbounded for coalition size $c \geq 2$. Our results reveal fundamental differences of these three mechanisms in their vulnerability to collusion."
2510.01766,"This paper proposes a novel algorithm to approximate the core of transferable utility (TU) cooperative games via linear programming. Given the computational hardness of determining the full core, our approach provides a tractable approximation by sampling extreme points through randomized linear problems (LPs). We analyze its convergence and computational complexity, and validate its effectiveness through extensive simulations on various game models. Our results show that the method is scalable and achieves high accuracy in terms of core reconstruction."
2510.02078,"This paper presents a model of multi-group Bayesian games (MBGs) to describe the group behavior in Bayesian games, and gives methods to find (strongly) multi-group Bayesian Nash equilibria (MBNE) of this model with a proposed transformation. MBNE represent the optimal strategy \textit{profiles} under the situation where players within a group play a cooperative game, while strongly MBNE characterize the optimal strategy \textit{profiles} under the situation where players within a group play a noncooperative game. Firstly, we propose a model of MBGs and give a transformation to convert any MBG into a multi-group ex-ante agent game (MEAG) which is a normal-form game. Secondly, we give a sufficient and necessary condition for a MBG's MEAG to be (strongly) potential. If it is (strongly) potential, all its (strongly) Nash equilibria can be found, and then all (strongly) MBNE of the MBG can be obtained by leveraging the transformation's good properties. Finally, we provide algorithms for finding (strongly) MBNE of a MBG whose MEAG is (strongly) potential and use an illustrative example to verify the correctness of our results."
2510.02714,"We study decision-making with rational inattention in settings where agents have perception constraints. In such settings, inaccurate prior beliefs or models of others may lead to inattention blindness, where an agent is unaware of its incorrect beliefs. We model this phenomenon in two-player zero-sum stochastic games, where Player 1 has perception constraints and Player 2 deceptively deviates from its security policy presumed by Player 1 to gain an advantage. We formulate the perception constraints as an online sensor selection problem, develop a value-weighted objective function for sensor selection capturing rational inattention, and propose the greedy algorithm for selection under this monotone objective function. When Player 2 does not deviate from the presumed policy, this objective function provides an upper bound on the expected value loss compared to the security value where Player 1 has perfect information of the state. We then propose a myopic decision-making algorithm for Player 2 to exploit Player 1's beliefs by deviating from the presumed policy and, thereby, improve upon the security value. Numerical examples illustrate how Player 1 persistently chooses sensors that are consistent with its priors, allowing Player 2 to systematically exploit its inattention."
2510.02984,"In repeated games, players choose actions concurrently at each step. We consider a parameterized setting of repeated games in which the players form a population of an arbitrary size. Their utility functions encode a reachability objective. The problem is whether there exists a uniform coalition strategy for the players so that they are sure to win independently of the population size. We use algebraic tools to show that the problem can be solved in polynomial space. First we exhibit a finite semigroup whose elements summarize strategies over a finite interval of population sizes. Then, we characterize the existence of winning strategies by the existence of particular elements in this semigroup. Finally, we provide a matching complexity lower bound, to conclude that repeated population games with reachability objectives are PSPACE-complete."
2510.03446,"Game theory has traditionally had a relatively limited view of risk based on how a player's expected reward is impacted by the uncertainty of the actions of other players. Recently, a new game-theoretic approach provides a more holistic view of risk also considering the reward-variance. However, these variance-based approaches measure variance of the reward on both the upside and downside. In many domains, such as finance, downside risk only is of key importance, as this represents the potential losses associated with a decision. In contrast, large upside ""risk"" (e.g. profits) are not an issue. To address this restrictive view of risk, we propose a novel solution concept, downside risk aware equilibria (DRAE) based on lower partial moments. DRAE restricts downside risk, while placing no restrictions on upside risk, and additionally, models higher-order risk preferences. We demonstrate the applicability of DRAE on several games, successfully finding equilibria which balance downside risk with expected reward, and prove the existence and optimality of this equilibria."
2510.03855,"We study the alternating gradient descent-ascent (AltGDA) algorithm in two-player zero-sum games. Alternating methods, where players take turns to update their strategies, have long been recognized as simple and practical approaches for learning in games, exhibiting much better numerical performance than their simultaneous counterparts. However, our theoretical understanding of alternating algorithms remains limited, and results are mostly restricted to the unconstrained setting. We show that for two-player zero-sum games that admit an interior Nash equilibrium, AltGDA converges at an $O(1/T)$ ergodic convergence rate when employing a small constant stepsize. This is the first result showing that alternation improves over the simultaneous counterpart of GDA in the constrained setting. For games without an interior equilibrium, we show an $O(1/T)$ local convergence rate with a constant stepsize that is independent of any game-specific constants. In a more general setting, we develop a performance estimation programming (PEP) framework to jointly optimize the AltGDA stepsize along with its worst-case convergence rate. The PEP results indicate that AltGDA may achieve an $O(1/T)$ convergence rate for a finite horizon $T$, whereas its simultaneous counterpart appears limited to an $O(1/\sqrt{T})$ rate."
2510.04343,"When selling many goods with independent valuations, we develop a distributionally robust framework, consisting of a two-player game between seller and nature. The seller has only limited knowledge about the value distribution. The seller selects a revenue-maximizing mechanism, after which nature chooses a revenue-minimizing distribution from all distributions that comply with the limited knowledge. When the seller knows the mean and variance of valuations, bundling is known to be an asymptotically optimal deterministic mechanism, achieving a normalized revenue close to the mean. Moving beyond this variance assumption, we assume knowledge of the mean absolute deviation (MAD), accommodating more dispersion and heavy-tailed valuations with infinite variance. We show for a large range of MAD values that bundling remains optimal, but the seller can only guarantee a revenue strictly smaller than the mean. Another noteworthy finding is indifference to the order of play, as both the max-min and min-max versions of the problem yield identical values. This contrasts with deterministic mechanisms and the separate sale of goods, where the order of play significantly impacts outcomes. We further underscore the universality of the optimal bundling price by demonstrating its efficacy in optimizing not only absolute revenue but also the absolute regret and ratio objective among all bundling prices"
2510.04407,"A considerable chasm has been looming for decades between theory and practice in zero-sum game solving through first-order methods. Although a convergence rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox algorithm and Nesterov's excessive gap technique in the early 2000s, the most effective paradigm in practice is *counterfactual regret minimization*, which is based on *regret matching* and its modern variants. In particular, the state of the art across most benchmarks is *predictive* regret matching$^+$ (PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can exhibit slower $\Omega(T^{-1/2})$ convergence even in self-play.In this paper, we close the gap between theory and practice. We propose a new scale-invariant and parameter-free variant of PRM$^+$, which we call IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$ (i.e., optimal) average-iterate convergence guarantees, while also being on par with PRM$^+$ on benchmark games. From a technical standpoint, we draw an analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive* learning rate. The basic flaw of PRM$^+$ is that the ($\ell_2$-)norm of the regret vector -- which can be thought of as the inverse of the learning rate -- can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the invariance that the norm of the regret vector is nondecreasing. This enables us to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does not rely on introducing additional hyperparameters to enforce smoothness.Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive version of optimistic gradient descent that we introduce whose learning rate depends on the misprediction error, demystifying the effectiveness of the regret matching family *vis-a-vis* more standard optimization techniques."
2510.04425,"We study a fundamental fair allocation problem, where the agent's value is determined by the number of bins either used to pack or cover the items allocated to them. Fairness is evaluated using the maximin share (MMS) criterion. This problem is not only motivated by practical applications, but also serves as a natural framework for studying group fairness. As MMS is not always satisfiable, we consider two types of approximations: cardinal and ordinal. For cardinal approximation, we relax the requirements of being packed or covered for a bin, and for ordinal approximation, we relax the number of bins that are packed or covered. For all models of interest, we provide constant approximation algorithms."
2510.04624,"We study a sequential decision-making model where a set of items is repeatedly matched to the same set of agents over multiple rounds. The objective is to determine a sequence of matchings that either maximizes the utility of the least advantaged agent at the end of all rounds (optimal) or at the end of every individual round (anytime optimal). We investigate the computational challenges associated with finding (anytime) optimal outcomes and demonstrate that these problems are generally computationally intractable. However, we provide approximation algorithms, fixed-parameter tractable algorithms, and identify several special cases whereby the problem(s) can be solved efficiently. Along the way, we also establish characterizations of Pareto-optimal/maximum matchings, which may be of independent interest to works in matching theory and house allocation."
2510.04915,"We consider the problem of the existence of an envy-free allocation up to any good (EFX) for linear valuations and establish new results by connecting this problem to a fixed point framework. Specifically, we first use randomized rounding to extend the discrete EFX constraints into a continuous space and show that an EFX allocation exists if and only if the optimal value of the continuously extended objective function is nonpositive. In particular, we demonstrate that this optimization problem can be formulated as an unconstrained difference of convex (DC) program, which can be further simplified to the minimization of a piecewise linear concave function over a polytope. Leveraging this connection, we show that the proposed DC program has a nonpositive optimal objective value if and only if a well-defined continuous vector map admits a fixed point. Crucially, we prove that the reformulated fixed point problem satisfies all the conditions of Brouwer's fixed point theorem, except that self-containedness is violated by an arbitrarily small positive constant. To address this, we propose a slightly perturbed continuous map that always admits a fixed point. This fixed point serves as a proxy for the fixed point (if it exists) of the original map, and hence for an EFX allocation through an appropriate transformation. Our results offer a new approach to establishing the existence of EFX allocations through fixed point theorems. Moreover, the equivalence with DC programming enables a more efficient and systematic method for computing such allocations (if one exists) using tools from nonlinear optimization. Our findings bridge the discrete problem of finding an EFX allocation with two continuous frameworks: solving an unconstrained DC program and identifying a fixed point of a continuous vector map."
2510.05429,"We present a simple local search algorithm for computing EFX (envy-free up to any good) allocations of $m$ indivisible goods among $n$ agents with additive valuations. EFX is a compelling fairness notion, and whether such allocations always exist remains a major open question in fair division.Our algorithm employs simulated annealing with the total number of EFX violations as an objective function together with a single-transfer neighborhood structure to move through the space of allocations. It found an EFX allocation in all the instances tested, which included thousands of randomly generated inputs, and scaled to settings with hundreds of agents and/or thousands of items. The algorithm's simplicity, along with its strong empirical performance makes it a simple benchmark for evaluating future approaches.On the theoretical side, we provide a potential function for identical additive valuations, which ensures that any strict-descent procedure under the single-transfer neighborhood ends at an EFX allocation. This represents an alternative proof of existence for identical valuations."
2510.05434,"We study the classical rent division problem, where $n$ agents must allocate $n$ indivisible rooms and split a fixed total rent $R$. The goal is to compute an envy-free (EF) allocation, where no agent prefers another agent's room and rent to their own. This problem has been extensively studied under standard assumptions, where efficient algorithms for computing EF allocations are known.We extend this framework by introducing two practically motivated constraints: (i) lower and upper bounds on room rents, and (ii) room-specific budget for agents. We develop efficient combinatorial algorithms that either compute a feasible EF allocation or certify infeasibility.We further design algorithms to optimize over EF allocations using natural fairness objectives such as maximin utility, leximin utility, and minimum utility spread. Our approach unifies both constraint types within a single algorithmic framework, advancing the applicability of fair division methods in real-world platforms such as Spliddit."
2510.0546,"We consider the matching problem in the metric distortion framework. There are $n$ agents and $n$ items occupying points in a shared metric space, and the goal is to design a matching mechanism that outputs a low-cost matching between the agents and items, using only agents' ordinal rankings of the candidates by distance. A mechanism has distortion $\alpha$ if it always outputs a matching whose cost is within a factor of $\alpha$ of the optimum, in every instance regardless of the metric space.Typically, the cost of a matching is measured in terms of the total distance between matched agents and items, but this measure can incentivize unfair outcomes where a handful of agents bear the brunt of the cost. With this in mind, we consider how the metric distortion problem changes when the cost is instead measured in terms of the maximum cost of any agent. We show that while these two notions of distortion can in general differ by a factor of $n$, the distortion of a variant of the state-of-the-art mechanism, RepMatch, actually improves from $O(n^2)$ under the sum objective to $O(n^{1.58})$ under the max objective. We also show that for any fairness objective defined by a monotone symmetric norm, this algorithm guarantees distortion $O(n^2)$."
2510.05474,"Myerson's seminal characterization of the revenue-optimal auction for a single item \cite{myerson1981optimal} remains a cornerstone of mechanism design. However, generalizing this framework to multi-item settings has proven exceptionally challenging. Even under restrictive assumptions, closed-form characterizations of optimal mechanisms are rare and are largely confined to the single-agent case \cite{pavlov2011optimal,hart2017approximate, daskalakis2018transport, GIANNAKOPOULOS2018432}, departing from the two-item setting only when prior distributions are uniformly distributed \cite{manelli2006bundling, daskalakis2017strong,giannakopoulos2018sjm}. In this work, we build upon the bi-valued setting introduced by Yao \cite{YAO_BIC_DSIC}, where each item's value has support 2 and lies in $\{a, b\}$. Yao's result provides the only known closed-form optimal mechanism for multiple agents. We extend this line of work along three natural axes, establishing the first closed-form optimal mechanisms in each of the following settings: (i) $n$ i.i.d. agents and $m$ i.i.d. items (ii) $n$ non-i.i.d. agents and two i.i.d. items and (iii) $n$ i.i.d. agents and two non-i.i.d. items. Our results lie at the limit of what is considered possible, since even with a single agent and m bi-valued non-i.i.d. items, finding the optimal mechanism is $\#P$-Hard \cite{daskalakis2014complexity, xi2018soda}. We finally generalize the discrete analog of a result from~\cite{daskalakis2017strong}, showing that for a single agent with $m$ items drawn from arbitrary (non-identical) discrete distributions, grand bundling is optimal when all item values are sufficiently large. We further show that for any continuous product distribution, grand bundling achieves $\mathrm{OPT} - \epsilon$ revenue for large enough values."
2510.05504,"Decentralized coordination and digital contracting are becoming critical in complex industrial ecosystems, yet existing approaches often rely on ad hoc heuristics or purely technical blockchain implementations without a rigorous economic foundation. This study develops a mechanism design framework for smart contract-based resource allocation that explicitly embeds efficiency and fairness in decentralized coordination. We establish the existence and uniqueness of contract equilibria, extending classical results in mechanism design, and introduce a decentralized price adjustment algorithm with provable convergence guarantees that can be implemented in real time. To evaluate performance, we combine extensive synthetic benchmarks with a proof-of-concept real-world dataset (MovieLens). The synthetic tests probe robustness under fee volatility, participation shocks, and dynamic demand, while the MovieLens case study illustrates how the mechanism can balance efficiency and fairness in realistic allocation environments. Results demonstrate that the proposed mechanism achieves substantial improvements in both efficiency and equity while remaining resilient to abrupt perturbations, confirming its stability beyond steady state analysis. The findings highlight broad managerial and policy relevance for supply chains, logistics, energy markets, healthcare resource allocation, and public infrastructure, where transparent and auditable coordination is increasingly critical. By combining theoretical rigor with empirical validation, the study shows how digital contracts can serve not only as technical artifacts but also as institutional instruments for transparency, accountability, and resilience in high-stakes resource allocation."
2510.05786,"We generalize the concept of Möbius inversion and Shapley values to directed acyclic multigraphs and weighted versions thereof. We further allow value functions (games) and thus their Möbius transforms (synergy function) and Shapley values to have values in any abelian group that is a module over a ring that contains the graph weights, e.g. vector-valued functions. To achieve this and overcome the obstruction that the classical axioms (linearity, efficiency, null player, symmetry) are not strong enough to uniquely determine Shapley values in this more general setting, we analyze Shapley values from two novel points of view: 1) We introduce projection operators that allow us to interpret Shapley values as the recursive projection and re-attribution of higher-order synergies to lower-order ones; 2) we propose a strengthening of the null player axiom and a localized symmetry axiom, namely the weak elements and flat hierarchy axioms. The former allows us to remove coalitions with vanishing synergy while preserving the rest of the hierarchical structure. The latter treats player-coalition bonds uniformly in the corner case of hierarchically flat graphs. Together with linearity these axioms already imply a unique explicit formula for the Shapley values, as well as classical properties like efficiency, null player, symmetry, and novel ones like the projection property. This whole framework then specializes to finite inclusion algebras, lattices, partial orders and mereologies, and also recovers certain previously known cases as corner cases, and presents others from a new perspective. The admission of general weighted directed acyclic multigraph structured hierarchies and vector-valued functions and Shapley values opens up the possibility for new analytic tools and application areas, like machine learning, language processing, explainable artificial intelligence, and many more."
2510.05986,"Transaction Fee Mechanisms (TFMs) study auction design in the Blockchain context, and emphasize robustness against miner and user collusion, moreso than traditional auction theory. \cite{chung2023foundations} introduce the notion of a mechanism being $c$-Side-Contract-Proof ($c$-SCP), i.e., robust to a collusion of the miner and $c$ users. Later work \cite{chung2024collusion,welfareIncreasingCollusion} shows a gap between the $1$-SCP and $2$-SCP classes. We show that the class of $2$-SCP mechanisms equals that of any $c$-SCP with $c\geq 2$, under a relatively minor assumption of consistent tie-breaking. In essence, this implies that any mechanism vulnerable to collusion, is also vulnerable to a small collusion."
2510.06581,"We study the fair allocation of indivisible chores among agents with asymmetric weights. Among the various fairness notions, weighted maximin share (WMMS) stands out as particularly compelling. However, whether WMMS admits a constant-factor approximation has remained unknown and is one of the important open problems in weighted fair division [ALMW22, Suk25]. So far, the best known approximation ratio is O(log n), where n is the number of agents. In this paper, we advance the state of the art and present the first constant-factor approximate WMMS algorithm. To this end, we introduce canonical instance reductions and different bounds of agents' valuations. We also prove that guaranteeing better than 2-approximation is not possible, which improves the best-known lower bound of 1.366."
2510.0743,"In this paper, we study advanced persistent threats (APT) with an insider who has different preferences. To address the uncertainty of the insider's preference, we propose the BG-FlipIn: a Bayesian game framework for FlipIt-insider models with an investigation on malicious, inadvertent, or corrupt insiders. We calculate the closed-form Bayesian Nash Equilibrium expression and further obtain three edge cases with deterministic insiders corresponding to their Nash Equilibrium expressions. On this basis, we further discover several phenomena in APT related to the defender's move rate and cost, as well as the insider's preferences. We then provide decision-making guidance for the defender, given different parametric conditions. Two applications validate that our BG-FlipIn framework enables the defender to make decisions consistently, avoiding detecting the insider's concrete preference or adjusting its strategy frequently."
2510.07572,"Suppose that $n$ computer devices are to be connected to a network via inhomogeneous Bernoulli trials. The Shapley value of a device quantifies how much the network's value increases due to the participation of that device. Characteristic functions of such games are naturally taken as the belief function (containment function) and Choquet capacity (hitting probability) of a random set (random network of devices).Traditionally, the Shapley value is either calculated as the expected marginal contribution over all possible coalitions (subnetworks), which results in exponential computational complexity, or approximated by the Monte Carlo sampling technique, where the performance is highly dependent on the stochastic sampling process.The purpose of this study is to design deterministic algorithms for games formulated via inhomogeneous Bernoulli trials that approximate the Shapley value in linear or quadratic time, with rigorous error analysis (Sections 3 and 4). Additionally, we provide a review of relevant literature on existing calculation methods in Remark 3.1 and Appendix I.A further goal is to supplement Shapley's original proof by deriving the Shapley value formula using a rigorous approach based on definite integrals and combinatorial analysis. This method explicitly highlights the roles of the Binomial Theorem and the Beta function in the proof, addressing a gap in Shapley's work (Appendix II)."
2510.08453,"This paper argues that the finite horizon paradox, where game theory contradicts intuition, stems from the limitations of standard number systems in modelling the cognitive perception of infinity. To address this issue, we propose a new framework based on Alternative Set Theory (AST). This framework represents different cognitive perspectives on a long history of events using distinct topologies. These topologies define an indiscernibility equivalence that formally treats huge, indistinguishable quantities as equivalent. This offers criterion-dependent resolutions to long-standing paradoxes, such as Selten's chain store paradox and Rosenthal's centipede game. Our framework reveals new intuitive subgame perfect equilibria, the characteristics of which depend on the chosen temporal perspective and payoff evaluation. Ultimately, by grounding its mathematical foundation in different modes of human cognition, our work expands the explanatory power of game theory for long-horizon scenarios."
2510.08742,"Sequential auctions for identical items with unit-demand, private-value buyers are common and often occur periodically without end, as new bidders replace departing ones. We model bidder uncertainty by introducing a probability that a bidder must exit the auction in each period. Treating the sequential auction as a Markov process, we demonstrate the existence of a unique steady state.In the absence of uncertainty, the steady state resembles a posted-price mechanism: bidders with values above a threshold almost surely win items by repeatedly bidding the threshold price, while those below the threshold almost surely do not. The equilibrium price corresponds to the threshold value that balances supply (bidders with values above the threshold) and demand (auction winners).When uncertainty is introduced, the threshold value persists but becomes less precise, growing ""fuzzier"" as uncertainty increases. This uncertainty benefits low-value bidders, those below the threshold, by giving them a significant chance of winning. Surprisingly, high-value bidders also benefit from uncertainty, up to a certain value limit, as it lowers equilibrium bids and increases their expected utility. On the other hand, this bidder uncertainty often reduces the auctioneer's utility."
2510.08788,"Managing millions of digital auctions is an essential task for modern advertising auction systems. The main approach to managing digital auctions is an autobidding approach, which depends on the Click-Through Rate and Conversion Rate values. While these quantities are estimated with ML models, their prediction uncertainty directly impacts advertisers' revenue and bidding strategies. To address this issue, we propose RobustBid, an efficient method for robust autobidding taking into account uncertainty in CTR and CVR predictions. Our approach leverages advanced, robust optimization techniques to prevent large errors in bids if the estimates of CTR/CVR are perturbed. We derive the analytical solution of the stated robust optimization problem, which leads to the runtime efficiency of the RobustBid method. The synthetic, iPinYou, and BAT benchmarks are used in our experimental evaluation of RobustBid. We compare our method with the non-robust baseline and the RiskBid algorithm in terms of total conversion volume (TCV) and average cost-per-click ($CPC_{avg}$) performance metrics. The experiments demonstrate that RobustBid provides bids that yield larger TCV and smaller $CPC_{avg}$ than competitors in the case of large perturbations in CTR/CVR predictions."
2510.08869,"Data valuation methods assign marginal utility to each data point that has contributed to the training of a machine learning model. If used directly as a payout mechanism, this creates a hidden cost of valuation, in which contributors with near-zero marginal value would receive nothing, even though their data had to be collected and assessed. To better formalize this cost, we introduce a conceptual and game-theoretic model, the Information Disclosure Game, between a Data Union (sometimes also called a data trust), a member-run agent representing contributors, and a Data Consumer (e.g., a platform). After first aggregating members' data, the DU releases information progressively by adding Laplacian noise under a differentially-private mechanism. Through simulations with strategies guided by data Shapley values and multi-armed bandit exploration, we demonstrate on a Yelp review helpfulness prediction task that data valuation inherently incurs an explicit acquisition cost and that the DU's collective disclosure policy changes how this cost is distributed across members."
2510.09084,"In a typical \emph{billboard advertisement} technique, a number of digital billboards are owned by an \emph{influence provider}, and several commercial houses approach the influence provider for a specific number of views of their advertisement content on a payment basis. If the influence provider provides the demanded or more influence, then he will receive the full payment else a partial payment. In the context of an influence provider, if he provides more or less than the advertisers demanded influence, it is a loss for him. This is formalized as 'Regret', and naturally, in the context of the influence provider, the goal will be to allocate the billboard slots among the advertisers such that the total regret is minimized. In this paper, we study this problem as a discrete optimization problem and propose two solution approaches. The first one selects the billboard slots from the available ones in an incremental greedy manner, and we call this method the Budget Effective Greedy approach. In the second one, we introduce randomness in the first one, where we do it for a sample of slots instead of calculating the marginal gains of all the billboard slots. We analyze both algorithms to understand their time and space complexity. We implement them with real-life datasets and conduct a number of experiments. We observe that the randomized budget effective greedy approach takes reasonable computational time while minimizing the regret."
2510.09814,"The assignment game models a housing market where buyers and sellers are matched, and transaction prices are set so that the resulting allocation is stable. Shapley and Shubik showed that every stable allocation is necessarily built on a maximum social welfare matching. In practice, however, stable allocations are rarely attainable, as matchings are often sub-optimal, particularly in online settings where eagents arrive sequentially to the market. In this paper, we introduce and compare two complementary measures of instability for allocations with sub-optimal matchings, establish their connections to the optimality ratio of the underlying matching, and use this framework to study the stability performances of randomized algorithms in online assignment games."
2510.10335,"We consider the problem of allocating $m$ indivisible chores among $n$ agents with possibly different weights, aiming for a solution that is both fair and efficient. Specifically, we focus on the classic fairness notion of proportionality and efficiency notion of Pareto-optimality. Since proportional allocations may not always exist in this setting, we allow the use of subsidies (monetary compensation to agents) to ensure agents are proportionally-satisfied, and aim to minimize the total subsidy required. Wu and Zhou (WINE 2024) showed that when each chore has disutility at most 1, a total subsidy of at most $n/3 - 1/6$ is sufficient to guarantee proportionality. However, their approach is based on a complex technique, which does not guarantee economic efficiency - a key desideratum in fair division.In this work, we give a polynomial-time algorithm that achieves the same subsidy bound while also ensuring Pareto-optimality. Moreover, both our algorithm and its analysis are significantly simpler than those of Wu and Zhou (WINE 2024). Our approach first computes a proportionally-fair competitive equilibrium, and then applies a rounding procedure guided by minimum-pain-per-buck edges."
2510.10423,"The maximin share ($\textsf{MMS}$) is the most prominent share-based fairness notion in the fair allocation of indivisible goods. Recent years have seen significant efforts to improve the approximation guarantees for $\textsf{MMS}$ for different valuation classes, particularly for additive valuations. For the additive setting, it has been shown that for some instances, no allocation can guarantee a factor better than $1-\tfrac{1}{n^4}$ of maximin share value to all agents. However, the best currently known algorithm achieves an approximation guarantee of $\tfrac{3}{4} + \tfrac{3}{3836}$ for $\textsf{MMS}$. In this work, we narrow this gap and improve the best-known approximation guarantee for $\textsf{MMS}$ to $\tfrac{10}{13}$."
2510.10698,"We consider the problem of assigning indivisible chores to agents with different entitlements in the maximin share value (\MMS) context. While constant-\MMS\ allocations/assignments are guaranteed to exist for both goods and chores in the symmetric setting, the situation becomes much more complex when agents have different entitlements. For the allocation of indivisible goods, it has been proven that an $n$-\WMMS\ (weighted \MMS) guarantee is the best one can hope for. For indivisible chores, however, it was recently discovered that an $O(\log n)$-\WMMS\ assignment is guaranteed to exist. In this work, we improve this upper bound to a constant-\WMMS\ guarantee.\footnote{We prove the existence of a 20-\WMMS\ assignment, but we did not attempt to optimize the constant factor. We believe our methods already yield a slightly better bound with a tighter analysis.}"
2510.10929,"We analyze an infinite-horizon deterministic joint replenishment model from a non-cooperative game-theoretical approach. In this model, a group of retailers can choose to jointly place an order, which incurs a major setup cost independent of the group, and a minor setup cost for each retailer. Additionally, each retailer is associated with a holding cost. Our objective is to design cost allocation rules that minimize the long-run average system cost while accounting for the fact that each retailer independently selects its replenishment interval to minimize its own cost. We introduce a class of cost allocation rules that distribute the major setup cost among the associated retailers in proportion to their predefined weights. For these rules, we establish a monotonicity property of agent better responses, which enables us to prove the existence of a payoff dominant pure Nash equilibrium that can also be computed efficiently. We then analyze the efficiency of these equilibria by examining the price of stability (PoS), the ratio of the best Nash equilibrium's system cost to the social optimum, across different information settings. In particular, our analysis reveals that one rule, which leverages retailers' own holding cost rates, achieves a near-optimal PoS of 1.25, while another rule that does not require access to retailers' private information also yields a favorable PoS."
2510.11253,"Firms (businesses, service providers, entertainment organizations, political parties, etc.) advertise on social networks to draw people's attention and improve their awareness of the brands of the firms. In all such cases, the competitive nature of their engagements gives rise to a game where the firms need to decide how to distribute their budget over the agents on a network to maximize their brand's awareness. The firms (players) therefore need to optimize how much budget they should put on the vertices of the network so that the spread improves via direct (via advertisements or free promotional offers) and indirect marketing (words-of-mouth). We propose a two-timescale model of decisions where the communication between the vertices happen in a faster timescale and the strategy update of the firms happen in a slower timescale. We show that under fairly standard conditions, the best response dynamics of the firms converge to a pure strategy Nash equilibrium. However, such equilibria can be away from a socially optimal one. We provide a characterization of the contest success functions and provide examples for the designers of such contests (e.g., regulators, social network providers, etc.) such that the Nash equilibrium becomes unique and social welfare maximizing. Our experiments show that for realistic scenarios, such contest success functions perform fairly well."
2510.11255,"Classical cooperative game theory assumes that the worth of a coalition depends only on the set of agents involved, but in practice, it may also depend on the order in which agents arrive. Motivated by such scenarios, we introduce temporal cooperative games (TCG), where the worth $v$ becomes a function of the sequence of agents $\pi$ rather than just the set $S$. This shift calls for rethinking the underlying axioms. A key property in this temporal framework is the incentive for optimal arrival (I4OA), which encourages agents to join in the order maximizing total worth. Alongside, we define two additional properties: online individual rationality (OIR), incentivizing earlier agents to invite more participants, and sequential efficiency (SE), ensuring that the total worth of any sequence is fully distributed among its agents. We identify a class of reward-sharing mechanisms uniquely characterized by these three properties. The classical Shapley value does not directly apply here, so we construct its natural analogs in two variants: the sequential world, where rewards are defined for each sequence-player pair, and the extended world, where rewards are defined for each player alone. Properties of efficiency, additivity, and null player uniquely determine these Shapley analogs in both worlds. Importantly, the Shapley analogs are disjoint from mechanisms satisfying I4OA, OIR, and SE, and this conflict persists even for restricted classes such as convex and simple TCGs. Our findings thus uncover a fundamental tension: when players arrive sequentially, reward-sharing mechanisms satisfying desirable temporal properties must inherently differ from Shapley-inspired ones, opening new questions for defining fair and efficient solution concepts in TCGs."
2510.1155,"We study the problem of computing stationary Nash equilibria in discounted perfect information stochastic games from the viewpoint of computational complexity. For two-player games we prove the problem to be in PPAD, which together with a previous PPAD-hardness result precisely classifies the problem as PPAD-complete. In addition to this we give an improved and simpler PPAD-hardness proof for computing a stationary epsilon-Nash equilibrium. For 3-player games we construct games showing that rational-valued stationary Nash equilibria are not guaranteed to exist, and we use these to prove SqrtSum-hardness of computing a stationary Nash equilibrium in 4-player games."
2510.11625,"In multiwinner approval elections with many candidates, voters may struggle to determine their preferences over the entire slate of candidates. It is therefore of interest to explore which (if any) fairness guarantees can be provided under reduced communication. In this paper, we consider voters with one-dimensional preferences: voters and candidates are associated with points in $\mathbb R$, and each voter's approval set forms an interval of $\mathbb R$. We put forward a probabilistic preference model, where the voter set consists of $\sigma$ different groups; each group is associated with a distribution over an interval of $\mathbb R$, so that each voter draws the endpoints of her approval interval from the distribution associated with her group. We present an algorithm for computing committees that provide Proportional Justified Representation + (PJR+), which proceeds by querying voters' preferences, and show that, in expectation, it makes $\mathcal{O}(\log( \sigma\cdot k))$ queries per voter, where $k$ is the desired committee size."
2510.11866,"Decentralized storage is one of the most natural applications built on blockchains and a central component of the Web3 ecosystem. Yet despite a decade of active development -- from IPFS and Filecoin to more recent entrants -- most of these storage protocols have received limited formal analysis of their incentive properties. Claims of incentive compatibility are sometimes made, but rarely proven. This gap matters: without well-designed incentives, a system may distribute storage but fail to truly decentralize it.We analyze Shelby -- a storage network protocol recently proposed by Aptos Labs and Jump Crypto -- and provide the first formal proof of its incentive properties. Our game-theoretic model shows that while off-chain audits alone collapse to universal shirking, Shelby's combination of peer audits with occasional on-chain verification yields incentive compatibility under natural parameter settings. We also examine coalition behavior and outline a simple modification that strengthens the protocol's collusion-resilience."
2510.12158,"We study the fair division of indivisible items. In the general model, the goal is to allocate $m$ indivisible items to $n$ agents while satisfying fairness criteria such as MMS, EF1, and EFX. We also study a recently-introduced graphical model that represents the fair division problem as a multigraph, in which vertices correspond to agents and edges to items. The graphical model stipulates that an item can have non-zero marginal utility to an agent only if its corresponding edge is incident to the agent's corresponding vertex. We study orientations (allocations that allocate each edge to an endpoint) in this model, as they are particularly desirable.Our first contribution concerns MMS allocations of mixed manna (i.e. a mixture of goods and chores) in the general model. It is known that MMS allocations of goods exist when $m \leq n+5$. We generalize this and show that when $m \leq n+5$, MMS allocations of mixed manna exist as long as $n \leq 3$, there is an agent whose MMS threshold is non-negative, or every item is a chore. Remarkably, our result leaves only the case where every agent has a negative MMS threshold unanswered.Our second contribution concerns EFX orientations of multigraphs of goods. We show that deciding whether EFX orientations exist for multigraphs is NP-complete, even for symmetric bi-valued multigraphs. Complementarily, we show symmetric bi-valued multigraphs that do not contain non-trivial odd multitrees have EFX orientations that can be found in polynomial time.Our third contribution concerns EF1 and EFX orientations of graphs and multigraphs of chores. We obtain polynomial-time algorithms for deciding whether such graphs have EF1 and EFX orientations, resolving a previous conjecture and showing a fundamental difference between goods and chores division. In addition, we show that the analogous problems for multigraphs are NP-hard."
2510.12641,"We study stability in additively separable hedonic games when coalition sizes have to respect fixed size bounds. We consider four classic notions of stability based on single-agent deviations, namely, Nash stability, individual stability, contractual Nash stability, and contractual individual stability. For each stability notion, we consider two variants: in one, the coalition left behind by a deviator must still be of a valid size, and in the other there is no such constraint. We provide a full picture of the existence of stable outcomes with respect to given size parameters. Additionally, when there are only upper bounds, we fully characterize the computational complexity of the associated existence problem. In particular, we obtain polynomial-time algorithms for contractual individual stability and contractual Nash stability, where the latter requires an upper bound of 2. We obtain further results for Nash stability and contractual individual stability, when the lower bound is at least 2."
2510.12846,"A long-standing open problem in algorithmic game theory asks whether or not there is a polynomial time algorithm to compute a Nash equilibrium in a random bimatrix game. We study random win-lose games, where the entries of the $n\times n$ payoff matrices are independent and identically distributed (i.i.d.) Bernoulli random variables with parameter $p=p(n)$. We prove that, for nearly all values of the parameter $p=p(n)$, there is an expected polynomial-time algorithm to find a Nash equilibrium in a random win-lose game. More precisely, if $p\sim cn^{-a}$ for some parameters $a,c\ge 0$, then there is an expected polynomial-time algorithm whenever $a\not\in \{1/2, 1\}$. In addition, if $a = 1/2$ there is an efficient algorithm if either $c \le e^{-52} 2^{-8} $ or $c\ge 0.977$. If $a=1$, then there is an expected polynomial-time algorithm if either $c\le 0.3849$ or $c\ge \log^9 n$."
2510.12862,"User Equilibrium is the standard representation of the so-called routing game in which drivers adjust their route choices to arrive at their destinations as fast as possible. Asking whether this Equilibrium is strong or not was meaningless for human drivers who did not form coalitions due to technical and behavioral constraints. This is no longer the case for connected autonomous vehicles (CAVs), which will be able to communicate and collaborate to jointly form routing coalitions.We demonstrate this for the first time on a carefully designed toy-network example, where a `club` of three autonomous vehicles jointly decides to deviate from the user equilibrium and benefit (arrive faster). The formation of such a club has negative consequences for other users, who are not invited to join it and now travel longer, and for the system, making it suboptimal and disequilibrated, which triggers adaptation dynamics.This discovery has profound implications for the future of our cities. We demonstrate that, if not prevented, CAV operators may intentionally disequilibrate traffic systems from their classic Nash equilibria, benefiting their own users and imposing costs on others. These findings suggest the possible emergence of an exclusive CAV elite, from which human-driven vehicles and non-coalition members may be excluded, potentially leading to systematically longer travel times for those outside the coalition, which would be harmful for the equity of public road networks."
2510.12952,"Automated Market Makers (AMMs) are used to provide liquidity for combinatorial prediction markets that would otherwise be too thinly traded. They offer both buy and sell prices for any of the doubly exponential many possible securities that the market can offer. The problem of setting those prices is known to be #P-hard for the original and most well-known AMM, the logarithmic market scoring rule (LMSR) market maker [Chen et al., 2008]. We focus on another natural AMM, the Constant Log Utility Market Maker (CLUM). Unlike LMSR, whose worst-case loss bound grows with the number of outcomes, CLUM has constant worst-case loss, allowing the market to add outcomes on the fly and even operate over countably infinite many outcomes, among other features. Simpler versions of CLUM underpin several Decentralized Finance (DeFi) mechanisms including the Uniswap protocol that handles billions of dollars of cryptocurrency trades daily. We first establish the computational complexity of the problem: we prove that pricing securities is #P-hard for CLUM, via a reduction from the model counting 2-SAT problem. In order to make CLUM more practically viable, we propose an approximation algorithm for pricing securities that works with high probability. This algorithm assumes access to an oracle capable of determining the maximum shares purchased of any one outcome and the total number of outcomes that has that maximum amount purchased. We then show that this oracle can be implemented in polynomial time when restricted to interval securities, which are used in designing financial options."
2510.13088,"This paper considers behavior-based price discrimination in the repeated sale of a non-durable good to a single long-lived buyer, by a seller without commitment power. We assume that there is a mixed population of forward-looking ``sophisticated'' buyers and myopic ``naive'' buyers. We investigate the impact of these dynamics on the seller's ability to learn about the buyer and exploit this learning for revenue. We obtain conclusions that differ dramatically with the time horizon of the interactions. To understand short time horizons, we analyze a two-period model, and find that the strategic demand reduction observed with fully sophisticated buyers is robust to the introduction of naive types. In fact, despite the inability of naive buyers to game the pricing algorithm, their introduction can further harm the seller's revenue, due to more intense demand reduction overall. For long horizons, we consider an infinite-horizon model with time discounting. We find that the extreme demand reduction predicted by previous work does not survive the introduction of naive buyers. Instead, we observe equilibria where the seller learns meaningfully despite the sophisticated buyers' demand reduction. We prove that for a natural family of such equilibria, the seller's revenue is not just high, but approximates the revenue attainable with commitment power, even when the fraction of naive types is vanishingly small."
2510.13261,"Collaborative machine learning enables multiple data owners to jointly train models for improved predictive performance. However, ensuring incentive compatibility and fair contribution-based rewards remains a critical challenge. Prior work by Sim and colleagues (Rachel Hwee Ling Sim et al: Collaborative machine learning with incentive-aware model rewards. In: International conference on machine learning. PMLR. 2020, pp. 8927-8963) addressed this by allocating model rewards, which are non-monetary and freely replicable, based on the Shapley value of each party's data contribution, measured via information gain. In this paper, we introduce a ratio-based Shapley value that replaces the standard additive formulation with a relative contribution measure. While our overall reward framework, including the incentive definitions and model-reward setting, remains aligned with that of Sim and colleagues, the underlying value function is fundamentally different. Our alternative valuation induces a different distribution of model rewards and offers a new lens through which to analyze incentive properties. We formally define the ratio-based value and prove that it satisfies the same set of incentive conditions as the additive formulation, including adapted versions of fairness, individual rationality, and stability. Like the original approach, our method faces the same fundamental trade-offs between these incentives. Our contribution is a mathematically grounded alternative to the additive Shapley framework, potentially better suited to contexts where proportionality among contributors is more meaningful than additive differences."
2510.13518,"We study a dynamic routing game motivated by traffic flows. The base model for an edge is the Vickrey bottleneck model. That is, edges are equipped with a free flow transit time and a capacity. When the inflow into an edge exceeds its capacity, a queue forms and the following particles experience a waiting time. In this paper, we enhance the model by introducing tolls, i.e., a cost each flow particle must pay for traversing an edge. In this setting we consider non-atomic equilibria, which means flows over time in which every particle is on a cheapest path, when summing up toll and travel time. We first show that unlike in the non-tolled version of this model, dynamic equilibria are not unique in terms of costs and do not necessarily reach a steady state. As a main result, we provide a procedure to compute steady states in the model with tolls."
2510.13633,"We study the problem of fairly allocating $m$ indivisible items arriving online, among $n$ (offline) agents. Although envy-freeness has emerged as the archetypal fairness notion, envy-free (EF) allocations need not exist with indivisible items. To bypass this, a prominent line of research demonstrates that there exist allocations that can be made envy-free by allowing a subsidy. Extensive work in the offline setting has focused on finding such envy-freeable allocations with bounded subsidy. We extend this literature to an online setting where items arrive one at a time and must be immediately and irrevocably allocated. Our contributions are two-fold:1. Maintaining EF Online: We show that envy-freeability cannot always be preserved online when the valuations are submodular or supermodular, even with binary marginals. In contrast, we design online algorithms that maintain envy-freeability at every step for the class of additive valuations, and for its superclasses including $k$-demand and SPLC valuations.2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to guarantee envy-freeness online. Surprisingly, even for additive valuations, the minimum subsidy may be as large as $\Omega(mn)$, in contrast to the offline setting, where the bound is $O(n)$. On the positive side, we identify valuation classes where the minimum subsidy is small (i.e., does not depend on $m$), including $k$-valued, rank-one, restricted additive, and identical valuations, and we obtain (mostly) tight subsidy bounds for these classes."
2510.1445,"Previous studies have shown that Instant-Runoff Voting (IRV) is highly resistant to coalitional manipulation (CM), though the theoretical reasons for this remain unclear. To address this gap, we analyze the susceptibility to CM of three major voting rules-Plurality, Two-Round System, and IRV-within the Perturbed Culture model. Our findings reveal that each rule undergoes a phase transition at a critical value theta\_c of the concentration of preferences: the probability of CM for large electorates converges exponentially fast to 1 below theta\_c and to 0 above theta\_c. We introduce the Super Condorcet Winner (SCW), showing that its presence is a key factor of IRV's resistance to coalitional manipulation, both theoretically and empirically. Notably, we use this notion to prove that for IRV, theta\_c = 0, making it resistant to CM with even minimal preference concentration."
2510.14555,"The introduction of new services, such as Mobile Edge Computing (MEC), requires a massive investment that cannot be assumed by a single stakeholder, for instance the Infrastructure Provider (InP). Service Providers (SPs) however also have an interest in the deployment of such services. We hence propose a co-investment scheme in which all stakeholders, i.e., the InP and the SPs, form the so-called grand coalition composed of all the stakeholders with the aim of sharing costs and revenues and maximizing their payoffs. The challenge comes from the fact that future revenues are uncertain. We devise in this case a novel stochastic coalitional game formulation which builds upon robust game theory and derive a lower bound on the probability of the stability of the grand coalition, wherein no player can be better off outside of it. In the presence of some correlated fluctuations of revenues however, stability can be too conservative. In this case, we make use also of profitability, in which payoffs of players are non-negative, as a necessary condition for co-investment. The proposed framework is showcased for MEC deployment, where computational resources need to be deployed in nodes at the edge of a telecommunication network. Numerical results show high lower bound on the probability of stability when the SPs' revenues are of similar magnitude and the investment period is sufficiently long, even with high levels of uncertainty. In the case where revenues are highly variable however, the lower bound on stability can be trivially low whereas co-investment is still profitable."
2510.14642,"In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\% of available profits when deployed alongside existing searchers and 81\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike."
2510.14752,"Traditionally, the problem of apportioning the seats of a legislative body has been viewed as a one-shot process with no dynamic considerations. While this approach is reasonable for some settings, dynamic aspects play an important role in many others. We initiate the study of apportionment problems in an online setting. Specifically, we introduce a framework for proportional apportionment with no information about the future. In this model, time is discrete and there are $n$ parties that receive a certain share of the votes at each time step. An online algorithm needs to irrevocably assign a prescribed number of seats at each time, ensuring that each party receives its fractional share rounded up or down, and that the cumulative number of seats allocated to each party remains close to its cumulative share up to that time.We study deterministic and randomized online apportionment methods. For deterministic methods, we construct a family of adversarial instances that yield a lower bound, linear in $n$, on the worst-case deviation between the seats allocated to a party and its cumulative share. We show that this bound is best possible and is matched by a natural greedy method. As a consequence, a method guaranteeing that the cumulative number of seats assigned to each party up to any step equals its cumulative share rounded up or down (global quota) exists if and only if $n\leq 3$. Then, we turn to randomized allocations and show that, for $n\leq 3$, we can randomize over methods satisfying global quota with the additional guarantee that each party receives, in expectation, its proportional share in every step. Our proof is constructive: Any method satisfying these properties can be obtained from a flow on a recursively constructed network. We showcase the applicability of our results to obtain approximate solutions in the context of online dependent rounding procedures."
2510.14907,"We extend the study of learning in games to dynamics that exhibit non-asymptotic stability. We do so through the notion of uniform stability, which is concerned with equilibria of individually utility-seeking dynamics. Perhaps surprisingly, it turns out to be closely connected to economic properties of collective rationality. Under mild non-degeneracy conditions and up to strategic equivalence, if a mixed equilibrium is not uniformly stable, then it is not weakly Pareto optimal: there is a way for all players to improve by jointly deviating from the equilibrium. On the other hand, if it is locally uniformly stable, then the equilibrium must be weakly Pareto optimal. Moreover, we show that uniform stability determines the last-iterate convergence behavior for the family of incremental smoothed best-response dynamics, used to model individual and corporate behaviors in the markets. Unlike dynamics around strict equilibria, which can stabilize to socially-inefficient solutions, individually utility-seeking behaviors near mixed Nash equilibria lead to collective rationality."
2510.15065,"Contract theory studies how a principal can incentivize agents to exert costly, unobservable effort through performance-based payments. While classical economic models provide elegant characterizations of optimal solutions, modern applications, ranging from online labor markets and healthcare to AI delegation and blockchain protocols, call for an algorithmic perspective. The challenge is no longer only which contracts induce desired behavior, but whether such contracts can be computed efficiently. This viewpoint has given rise to \emph{algorithmic contract design}, paralleling the rise of algorithmic mechanism design two decades ago.This article focuses on \emph{combinatorial contracts}, an emerging frontier within algorithmic contract design, where agents may choose among exponentially many combinations of actions, or where multiple agents must work together as a team, and the challenge lies in selecting the right composition. These models capture a wide variety of real-world contracting environments, from hospitals coordinating physicians across treatment protocols to firms hiring teams of engineers for interdependent tasks. We review three combinatorial settings: (i) a single agent choosing multiple actions, (ii) multiple agents with binary actions, and (iii) multiple agents each selecting multiple actions. For each, we highlight structural insights, algorithmic techniques, and complexity barriers. Results include tractable cases such as gross substitutes reward functions, hardness results, and approximation guarantees under value- and demand-oracle access. By charting these advances, the article maps the emerging landscape of combinatorial contract design, and highlights fundamental open questions and promising directions for future work."
2510.15094,"Hand abstraction is crucial for scaling imperfect-information games (IIGs) such as Texas Hold'em, yet progress is limited by the lack of a formal task model and by evaluations that require resource-intensive strategy solving. We introduce signal observation ordered games (SOOGs), a subclass of IIGs tailored to hold'em-style games that cleanly separates signal from player action sequences, providing a precise mathematical foundation for hand abstraction. Within this framework, we define a resolution bound-an information-theoretic upper bound on achievable performance under a given signal abstraction. Using the bound, we show that mainstream outcome-based imperfect-recall algorithms suffer substantial losses by arbitrarily discarding historical information; we formalize this behavior via potential-aware outcome Isomorphism (PAOI) and prove that PAOI characterizes their resolution bound. To overcome this limitation, we propose full-recall outcome isomorphism (FROI), which integrates historical information to raise the bound and improve policy quality. Experiments on hold'em-style benchmarks confirm that FROI consistently outperforms outcome-based imperfect-recall baselines. Our results provide a unified formal treatment of hand abstraction and practical guidance for designing higher-resolution abstractions in IIGs."
2510.15214,"Motivated by the problem of selling large, proprietary data, we consider an information pricing problem proposed by Bergemann et al. that involves a decision-making buyer and a monopolistic seller. The seller has access to the underlying state of the world that determines the utility of the various actions the buyer may take. Since the buyer gains greater utility through better decisions resulting from more accurate assessments of the state, the seller can therefore promise the buyer supplemental information at a price. To contend with the fact that the seller may not be perfectly informed about the buyer's private preferences (or utility), we frame the problem of designing a data product as one where the seller designs a revenue-maximizing menu of statistical experiments.Prior work by Cai et al. showed that an optimal menu can be found in time polynomial in the state space, whereas we observe that the state space is naturally exponential in the dimension of the data. We propose an algorithm which, given only sampling access to the state space, provably generates a near-optimal menu with a number of samples independent of the state space. We then analyze a special case of high-dimensional Gaussian data, showing that (a) it suffices to consider scalar Gaussian experiments, (b) the optimal menu of such experiments can be found efficiently via a semidefinite program, and (c) full surplus extraction occurs if and only if a natural separation condition holds on the set of potential preferences of the buyer."
2510.15238,"The E-commerce advertising platforms typically sell commercial traffic through either second-price auction (SPA) or first-price auction (FPA). SPA was historically prevalent due to its dominant strategy incentive-compatible (DSIC) for bidders with quasi-linear utilities, especially when budgets are not a binding constraint, while FPA has gained more prominence for offering higher revenue potential to publishers and avoiding the possibility for discriminatory treatment in personalized reserve prices. Meanwhile, on the demand side, advertisers are increasingly adopting platform-wide marketing solutions akin to QuanZhanTui, shifting from spending budgets solely on commercial traffic to bidding on the entire traffic for the purpose of maximizing overall sales. For automated bidding systems, such a trend poses a critical challenge: determining optimal strategies across heterogeneous auction channels to fulfill diverse advertiser objectives, such as maximizing return (MaxReturn) or meeting target return on ad spend (TargetROAS). To overcome this challenge, this work makes two key contributions. First, we derive an efficient solution for optimal bidding under FPA channels, which takes into account the presence of organic traffic - traffic can be won for free. Second, we introduce a marginal cost alignment (MCA) strategy that provably secures bidding efficiency across heterogeneous auction mechanisms. To validate performance of our developed framework, we conduct comprehensive offline experiments on public datasets and large-scale online A/B testing, which demonstrate consistent improvements over existing methods."
2510.15344,"Federated learning (FL) has gained prominence due to heightened concerns over data privacy. Privacy restrictions limit the visibility for data consumers (DCs) to accurately assess the capabilities and efforts of data owners (DOs). Thus, for open collaborative FL markets to thrive, effective incentive mechanisms are key as they can motivate data owners (DOs) to contribute to FL tasks. Contract theory is a useful technique for developing FL incentive mechanisms. Existing approaches generally assume that once the contract between a DC and a DO is signed, it remains unchanged until the FL task is finished. However, unforeseen circumstances might force a DO to be unable to fulfill the current contract, resulting in inefficient utilization of DCs' budgets. To address this limitation, we propose the Renegotiable Contract-Theoretic Incentive Mechanism (RC-TIM) for FL. Unlike previous approaches, it adapts to changes in DOs' behavior and budget constraints by supporting the renegotiation of contracts, providing flexible and dynamic incentives. Under RC-TIM, an FL system is more adaptive to unpredictable changes in the operating environment that can affect the quality of the service provided by DOs. Extensive experiments on three benchmark datasets demonstrate that RC-TIM significantly outperforms four state-of-the-art related methods, delivering up to a 45.76% increase in utility on average."
2510.15384,"Technologies such as Mobile Edge Computing (MEC) depend on the availability of infrastructure. We define the Infrastructure Provider (InP) as the actor responsible for deploying and maintaining this infrastructure, while Service Providers (SPs) operate applications over it to serve end users and earn revenues. Deploying such infrastructure requires however a significant investment, and the InP may be reluctant to bear it alone. We propose co-investment to overcome this barrier, allowing players, the InP and multiple SPs, to share costs and revenues. However, committing to a co-investment over a long period may be too constraining for players: in an unforeseeable future, players may realize that they could make more profit outside the co-investment (such a profit is called opportunity cost). For this reason, we propose a scheme, based on coalitional game theory, which is dynamic in terms of (i)allowing players to join, remain in, or leave the co-investment, (ii) adjusting the infrastructure capacity and resource sharing over time. We propose a method to compute entry fees and exit penalties in order to appropriately compensate players remaining in the co-investment. We numerically show that our dynamic scheme encourages player participation and increases profit (in case of high opportunity cost)."
2510.15438,"Direct reciprocity, typically studied using the Iterated Prisoner's Dilemma (IPD), is central to understanding how cooperation evolves. In the 1980s, Robert Axelrod organized two influential IPD computer tournaments, where Tit for Tat (TFT) emerged as the winner. Yet the archival record is incomplete: for the first tournament only a report survives, and for the second the submitted Fortran strategies remain but not the final tournament code. This gap raises questions about the reproducibility of these historically influential results. We recreate the second tournament by restoring the surviving Fortran implementations to compile with modern compilers and by building a Python interface that calls the original strategy functions without modification. Using the open-source Axelrod-Python library to run tournaments, we reproduce Axelrod's main findings: TFT prevails, and successful play tends to be cooperative, responsive to defection, and willing to forgive. Strategy rankings remain mostly unchanged. We then assess the robustness of the originally submitted strategies by incorporating additional strategies, and we run one of the largest IPD tournaments to date. We find that the original tournament was especially favorable to TFT and that it is difficult to dethrone TFT when the original submissions make up the majority of the field. We also observe that several lesser-known submissions perform strongly in more diverse settings and under noise. Our contributions are: (i) the first systematic reproduction of Axelrod's second tournament; (ii) a contemporary reassessment of the original results in light of new strategies and settings; and (iii) a preserved, easy-to-use implementation of the second-tournament strategies within Axelrod-Python to support future research."
2510.15582,"Inverse game theory is utilized to infer the cost functions of all players based on game outcomes. However, existing inverse game theory methods do not consider the learner as an active participant in the game, which could significantly enhance the learning process. In this paper, we extend inverse game theory to active inverse methods. For Stackelberg games with bounded rationality, the leader, acting as a learner, actively chooses actions to better understand the follower's cost functions. First, we develop a method of active learning by leveraging Fisher information to maximize information gain about the unknown parameters and prove the consistency and asymptotic normality. Additionally, when leaders consider its cost, we develop a method of active inverse game to balance exploration and exploitation, and prove the consistency and asymptotic Stackelberg equilibrium with quadratic cost functions. Finally, we verify the properties of these methods through simulations in the quadratic case and demonstrate that the active inverse game method can achieve Stackelberg equilibrium more quickly through active exploration."
2510.16385,"The stable roommates problem is a non-bipartite version of the stable matching problem in a bipartite graph. In this paper, we consider the stable roommates problem with ties. In particular, we focus on strong stability, which is one of the main stability concepts in the stable roommates problem with ties. We propose a new polynomial-time algorithm for the problem of checking the existence of a strongly stable matching in the stable roommates problem with ties. More concretely, we extend the linear programming approach of Abeledo and Blum to the stable roommates problem with strict preferences to our problem."
2510.16869,"Automated bidding to optimize online advertising with various constraints, e.g. ROI constraints and budget constraints, is widely adopted by advertisers. A key challenge lies in designing algorithms for non-truthful mechanisms with ROI constraints. While prior work has addressed truthful auctions or non-truthful auctions with weaker benchmarks, this paper provides a significant improvement: We develop online bidding algorithms for repeated first-price auctions with ROI constraints, benchmarking against the optimal randomized strategy in hindsight. In the full feedback setting, where the maximum competing bid is observed, our algorithm achieves a near-optimal $\widetilde{O}(\sqrt{T})$ regret bound, and in the bandit feedback setting (where the bidder only observes whether the bidder wins each auction), our algorithm attains $\widetilde{O}(T^{3/4})$ regret bound."
2510.17067,"Regret matching (RM} -- and its modern variants -- is a foundational online algorithm that has been at the heart of many AI breakthrough results in solving benchmark zero-sum games, such as poker. Yet, surprisingly little is known so far in theory about its convergence beyond two-player zero-sum games. For example, whether regret matching converges to Nash equilibria in potential games has been an open problem for two decades. Even beyond games, one could try to use RM variants for general constrained optimization problems. Recent empirical evidence suggests that they -- particularly regret matching$^+$ (RM$^+$) -- attain strong performance on benchmark constrained optimization problems, outperforming traditional gradient descent-type algorithms.We show that alternating RM$^+$ converges to an $\epsilon$-KKT point after $O_\epsilon(1/\epsilon^4)$ iterations, establishing for the first time that it is a sound and fast first-order optimizer. Our argument relates the KKT gap to the accumulated regret, two quantities that are entirely disparate in general but interact in an intriguing way in our setting, so much so that when regrets are bounded, our complexity bound improves all the way to $O_\epsilon(1/\epsilon^2)$. From a technical standpoint, while RM$^+$ does not have the usual one-step improvement property in general, we show that it does in a certain region that the algorithm will quickly reach and remain in thereafter. In sharp contrast, our second main result establishes a lower bound: RM, with or without alternation, can take an exponential number of iterations to reach a crude approximate solution even in two-player potential games. This represents the first worst-case separation between RM and RM$^+$. Our lower bound shows that convergence to coarse correlated equilibria in potential games is exponentially faster than convergence to Nash equilibria."
2510.17285,"We study resource allocation problems in which a central planner allocates resources among strategic agents with private cost functions in order to minimize a social cost, defined as an aggregate of the agents' costs. This setting poses two main challenges: (i) the agents' cost functions may be unknown to them or difficult to specify explicitly, and (ii) agents may misreport their costs strategically. To address these challenges, we propose an algorithm that combines preference-based learning with Vickrey-Clarke-Groves (VCG) payments to incentivize truthful reporting. Our algorithm selects informative preference queries via D-optimal design, estimates cost parameters through maximum likelihood, and computes VCG allocations and payments based on these estimates. In a one-shot setting, we prove that the mechanism is approximately truthful, individually rational, and efficient up to an error of $\tilde{\mathcal O}(K^{-1/2})$ for $K$ preference queries per agent. In an online setting, these guarantees hold asymptotically with sublinear regret at a rate of $\tilde{\mathcal O}(T^{2/3})$ after $T$ rounds. Finally, we validate our approach through a numerical case study on demand response in local electricity markets."
2510.18062,"The well-known Condorcet Jury Theorem states that, under majority rule, the better of two alternatives is chosen with probability approaching one as the population grows. We study an asymmetric setting where voters face varying participation costs and share a possibly heuristic belief about their pivotality (ability to influence the outcome).In a costly voting setup where voters abstain if their participation cost is greater than their pivotality estimate, we identify a single property of the heuristic belief -- weakly vanishing pivotality -- that gives rise to multiple stable equilibria in which elections are nearly tied. In contrast, strongly vanishing pivotality (as in the standard Calculus of Voting model) yields a unique, trivial equilibrium where only zero-cost voters participate as the population grows. We then characterize when nontrivial equilibria satisfy a version of the Jury Theorem: below a sharp threshold, the majority-preferred candidate wins with probability approaching one; above it, both candidates either win with equal probability."
2510.18567,"In this work, we introduce and study contextual search in general principal-agent games, where a principal repeatedly interacts with agents by offering contracts based on contextual information and historical feedback, without knowing the agents' true costs or rewards. Our model generalizes classical contextual pricing by accommodating richer agent action spaces. Over $T$ rounds with $d$-dimensional contexts, we establish an asymptotically tight exponential $T^{1 - \Theta(1/d)}$ bound in terms of the pessimistic Stackelberg regret, benchmarked against the best utility for the principal that is consistent with the observed feedback.We also establish a lower bound of $\Omega(T^{\frac{1}{2}-\frac{1}{2d}})$ on the classic Stackelberg regret for principal-agent games, demonstrating a surprising double-exponential hardness separation from the contextual pricing problem (a.k.a, the principal-agent game with two actions), which is known to admit a near-optimal $O(d\log\log T)$ regret bound [Kleinberg and Leighton, 2003, Leme and Schneider, 2018, Liu et al., 2021]. In particular, this double-exponential hardness separation occurs even in the special case with three actions and two-dimensional context. We identify that this significant increase in learning difficulty arises from a structural phenomenon that we call contextual action degeneracy, where adversarially chosen contexts can make some actions strictly dominated (and hence unincentivizable), blocking the principal's ability to explore or learn about them, and fundamentally limiting learning progress."
2511.0334,"Generalized Nash equilibrium problems with mixed-integer variables constitute an important class of games in which each player solves a mixed-integer optimization problem, where both the objective and the feasible set is parameterized by the rivals' strategies. However, such games are known for failing to admit exact equilibria and also the assumption of all players being able to solve nonconvex problems to global optimality is questionable. This motivates the study of approximate equilibria. In this work, we consider an approximation concept that incorporates both multiplicative and additive relaxations of optimality. We propose a branch-and-cut (B&C) method that computes such approximate equilibria or proves its non-existence. For this, we adopt the idea of intersection cuts and show the existence of such cuts under the condition that the constraints are linear and each player's cost function is either convex in the entire strategy profile, or, concave in the entire strategy profile and linear in the rivals' strategies. For the special case of standard Nash equilibrium problems, we introduce an alternative type of cut and show that the method terminates finitely, provided that each player has only finitely many distinct best-response sets. Finally, on the basis of the B&C method, we introduce a single-tree binary-search method to compute best-approximate equilibria under some simplifying assumptions. We implemented these methods and present numerical results for a class of mixed-integer flow games."
2511.0381,"We study the fundamental problem of fairly allocating a multiset $\mathcal{M}$ of $t$ types of indivisible items among $d$ groups of agents, where all agents within a group have identical additive valuations. Gorantla et al. [GMV23] showed that for every such instance, there exists a finite number $\mu$ such that, if each item type appears at least $\mu$ times, an envy-free allocation exists. Their proof is non-constructive and only provides explicit upper bounds on $\mu$ for the cases of two groups ($d=2$) or two item types ($t=2$).In this work, we resolve one of the main open questions posed by Gorantla et al. [GMV23] by deriving explicit upper bounds on $\mu$ that hold for arbitrary numbers of groups and item types. We introduce a significantly simpler, yet powerful technique that not only yields constructive guarantees for indivisible goods but also extends naturally to chores and continuous domains, leading to new results in related fair division settings such as cake cutting."
2511.00835,"We investigate optimal social welfare allocations of $m$ items to $n$ agents with binary additive or submodular valuations. For binary additive valuations, we prove that the set of optimal allocations coincides with the set of so-called \emph{stable allocations}, as long as the employed criterion for evaluating social welfare is strongly Pigou-Dalton (SPD) and symmetric. Many common criteria are SPD and symmetric, such as Nash social welfare, leximax, leximin, Gini index, entropy, and envy sum. We also design efficient algorithms for finding a stable allocation, including an $O(m^2n)$ time algorithm for the case of indivisible items, and an $O(m^2n^5)$ time one for the case of divisible items. The first is faster than the existing algorithms or has a simpler analysis. The latter is the first combinatorial algorithm for that problem. It utilizes a hidden layer partition of items and agents admitted by all stable allocations, and cleverly reduces the case of divisible items to the case of indivisible items.In addition, we show that the profiles of different optimal allocations have a small Chebyshev distance, which is 0 for the case of divisible items under binary additive valuations, and is at most 1 for the case of indivisible items under binary submodular valuations."
2511.00847,"The widespread adoption of Large Language Models (LLMs) through Application Programming Interfaces (APIs) induces a critical vulnerability: the potential for dishonest manipulation by service providers. This manipulation can manifest in various forms, such as secretly substituting a proclaimed high-performance model with a low-cost alternative, or inflating responses with meaningless tokens to increase billing. This work tackles the issue through the lens of algorithmic game theory and mechanism design. We are the first to propose a formal economic model for a realistic user-provider ecosystem, where a user can iteratively delegate $T$ queries to multiple model providers, and providers can engage in a range of strategic behaviors. As our central contribution, we prove that for a continuous strategy space and any $\epsilon\in(0,\frac12)$, there exists an approximate incentive-compatible mechanism with an additive approximation ratio of $O(T^{1-\epsilon}\log T)$, and a guaranteed quasi-linear second-best user utility. We also prove an impossibility result, stating that no mechanism can guarantee an expected user utility that is asymptotically better than our mechanism. Furthermore, we demonstrate the effectiveness of our mechanism in simulation experiments with real-world API settings."
2511.00986,"We study deliberative social choice, where voters refine their preferences through small-group discussions before collective aggregation. We introduce a simple and easily implementable deliberation-via-matching protocol: for each pair of candidates, we form an arbitrary maximum matching among voters who disagree on that pair, and each matched pair deliberates. The resulting preferences (individual and deliberative) are then appropriately weighted and aggregated using the weighted uncovered set tournament rule.We show that our protocol has a tight distortion bound of $3$ within the metric distortion framework. This breaks the previous lower bound of $3.11$ for tournament rules without deliberation and matches the lower bound for deterministic social choice rules without deliberation. Our result conceptually shows that tournament rules are just as powerful as general social choice rules, when the former are given the minimal added power of pairwise deliberations. We prove our bounds via a novel bilinear relaxation of the non-linear program capturing optimal distortion, whose vertices we can explicitly enumerate, leading to an analytic proof. Loosely speaking, our key technical insight is that the distortion objective, as a function of metric distances to any three alternatives, is both supermodular and convex. We believe this characterization provides a general analytical framework for studying the distortion of other deliberative protocols, and may be of independent interest."
2511.01157,"We study the welfare of a mechanism in a dynamic environment where a learning investor can make a costly investment to change her value. In many real-world problems, the common assumption that the investor always makes the best responses, i.e., choosing her utility-maximizing investment option, is unrealistic due to incomplete information in a dynamically evolving environment. To address this, we consider an investor who uses a no-regret online learning algorithm to adaptively select investments through repeated interactions with the environment. We analyze how the welfare guarantees of approximation allocation algorithms extend from static to dynamic settings when the investor learns rather than best-responds, by studying the approximation ratio for optimal welfare as a measurement of an algorithm's performance against different benchmarks in the dynamic learning environment. First, we show that the approximation ratio in the static environment remains unchanged in the dynamic environment against the best-in-hindsight benchmark. Second, we provide tight characterizations of the approximation upper and lower bounds relative to a stronger time-varying benchmark. Bridging mechanism design with online learning theory, our work shows how robust welfare guarantees can be maintained even when an agent cannot make best responses but learns their investment strategies in complex, uncertain environments."
2511.01421,"Urban traffic congestion stems from the misalignment between self-interested routing decisions and socially optimal flows. Intersections, as critical bottlenecks, amplify these inefficiencies because existing control schemes often neglect drivers' strategic behavior. Autonomous intersections, enabled by vehicle-to-infrastructure communication, permit vehicle-level scheduling based on individual requests. Leveraging this fine-grained control, we propose a non-monetary mechanism that strategically adjusts request timestamps-delaying or advancing passage times-to incentivize socially efficient routing. We present a hierarchical architecture separating local scheduling by roadside units from network-wide timestamp adjustments by a central planner. We establish an experimentally validated analytical model, prove the existence and essential uniqueness of equilibrium flows and formulate the planner's problem as an offline bilevel optimization program solvable with standard tools. Experiments on the Sioux Falls network show up to a 68% reduction in the efficiency gap between equilibrium and optimal flows, demonstrating scalability and effectiveness."
2511.01852,"Learning and computation of equilibria are central problems in game theory, theory of computation, and artificial intelligence. In this work, we introduce proximal regret, a new notion of regret based on proximal operators that lies strictly between external and swap regret. When every player employs a no-proximal-regret algorithm in a general convex game, the empirical distribution of play converges to proximal correlated equilibria (PCE), a refinement of coarse correlated equilibria. Our framework unifies several emerging notions in online learning and game theory-such as gradient equilibrium and semicoarse correlated equilibrium-and introduces new ones. Our main result shows that the classic Online Gradient Descent (GD) algorithm achieves an optimal $O(\sqrt{T})$ bound on proximal regret, revealing that GD, without modification, minimizes a stronger regret notion than external regret. This provides a new explanation for the empirically superior performance of gradient descent in online learning and games. We further extend our analysis to Mirror Descent in the Bregman setting and to Optimistic Gradient Descent, which yields faster convergence in smooth convex games."
2511.02157,"No-regret learning dynamics play a central role in game theory, enabling decentralized convergence to equilibrium for concepts such as Coarse Correlated Equilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the convergence rate to CCE in general-sum Markov games, reducing it from the previously best-known rate of $\mathcal{O}(\log^5 T / T)$ to a sharper $\mathcal{O}(\log T / T)$. This matches the best known convergence rate for CE in terms of $T$, number of iterations, while also improving the dependence on the action set size from polynomial to polylogarithmic-yielding exponential gains in high-dimensional settings. Our approach builds on recent advances in adaptive step-size techniques for no-regret algorithms in normal-form games, and extends them to the Markovian setting via a stage-wise scheme that adjusts learning rates based on real-time feedback. We frame policy updates as an instance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for value-iteration-based learning. The resulting self-play algorithm achieves, to our knowledge, the fastest known convergence rate to CCE in Markov games."
2511.02746,"In many real-life settings, algorithms play the role of assistants, while humans ultimately make the final decision. Often, algorithms specifically act as curators, narrowing down a wide range of options into a smaller subset that the human picks between: consider content recommendation or chatbot responses to questions with multiple valid answers. Crucially, humans may not know their own preferences perfectly either, but instead may only have access to a noisy sampling over preferences. Algorithms can assist humans by curating a smaller subset of items, but must also face the challenge of misalignment: humans may have different preferences from each other (and from the algorithm), and the algorithm may not know the exact preferences of the human they are facing at any point in time. In this paper, we model and theoretically study such a setting. Specifically, we show instances where humans benefit by collaborating with a misaligned algorithm. Surprisingly, we show that humans gain more utility from a misaligned algorithm (which makes different mistakes) than from an aligned algorithm. Next, we build on this result by studying what properties of algorithms maximize human welfare when the goals could be either utilitarian welfare or ensuring all humans benefit. We conclude by discussing implications for designers of algorithmic tools and policymakers."
2511.03629,"We consider the problem of fairly allocating the vertices of a graph among $n$ agents, where the value of a bundle is determined by its cut value -- the number of edges with exactly one endpoint in the bundle. This model naturally captures applications such as team formation and network partitioning, where valuations are inherently non-monotonic: the marginal values may be positive, negative, or zero depending on the composition of the bundle. We focus on the fairness notion of envy-freeness up to one item (EF1) and explore its compatibility with several efficiency concepts such as Transfer Stability (TS) that prohibits single-item transfers that benefit one agent without making the other worse-off. For general graphs, our results uncover a non-monotonic relationship between the number of agents $n$ and the existence of allocations satisfying EF1 and transfer stability (TS): such allocations always exist for $n=2$, may fail to exist for $n=3$, but exist again for all $n\geq 4$. We further show that existence can be guaranteed for any $n$ by slightly weakening the efficiency requirement or by restricting the graph to forests. All of our positive results are achieved via efficient algorithms."
2511.03968,"The complexity of computing equilibrium refinements has been at the forefront of algorithmic game theory research, but it has remained open in the seminal class of potential games; we close this fundamental gap in this paper.We first establish that computing a pure-strategy perfect equilibrium is $\mathsf{PLS}$-complete under different game representations -- including extensive-form games and general polytope games, thereby being polynomial-time equivalent to pure Nash equilibria. For normal-form proper equilibria, our main result is that a perturbed (proper) best response can be computed efficiently in extensive-form games. As a byproduct, we establish $\mathsf{FIXP}_a$-completeness of normal-form proper equilibria in extensive-form games, resolving a long-standing open problem. In stark contrast, we show that computing a normal-form proper equilibrium in polytope potential games is both $\mathsf{NP}$-hard and $\mathsf{coNP}$-hard.We next turn to more structured classes of games, namely symmetric network congestion and symmetric matroid congestion games. For both classes, we show that a perfect pure-strategy equilibrium can be computed in polynomial time, strengthening the existing results for pure Nash equilibria. On the other hand, we establish that, for a certain class of potential games, there is an exponential separation in the length of the best-response path between perfect and Nash equilibria.Finally, for mixed strategies, we prove that computing a point geometrically near a perfect equilibrium requires a doubly exponentially small perturbation even in $3$-player potential games in normal form. On the flip side, in the special case of polymatrix potential games, we show that equilibrium refinements are amenable to perturbed gradient descent dynamics, thereby belonging to the complexity class $\mathsf{CLS}$."
2511.04465,"We study a model of subscription-based platforms where users pay a fixed fee for unlimited access to content, and creators receive a share of the revenue. Existing approaches to detecting fraud predominantly rely on machine learning methods, engaging in an ongoing arms race with bad actors. We explore revenue division mechanisms that inherently disincentivize manipulation. We formalize three types of manipulation-resistance axioms and examine which existing rules satisfy these. We show that a mechanism widely used by streaming platforms, not only fails to prevent fraud, but also makes detecting manipulation computationally intractable. We also introduce a novel rule, ScaledUserProp, that satisfies all three manipulation-resistance axioms. Finally, experiments with both real-world and synthetic streaming data support ScaledUserProp as a fairer alternative compared to existing rules."
2511.04572,"The Fisher market equilibrium for private goods and the Lindahl equilibrium for public goods are classic and fundamental solution concepts for market equilibria. While Fisher market equilibria have been well-studied, the theoretical foundations for Lindahl equilibria remain substantially underdeveloped.In this work, we propose a unified duality framework for market equilibria. We show that Lindahl equilibria of a public goods market correspond to Fisher market equilibria in a dual Fisher market with dual utilities, and vice versa. The dual utility is based on the indirect utility, and the correspondence between the two equilibria works by exchanging the roles of allocations and prices.Using the duality framework, we address the gaps concerning the computation and dynamics for Lindahl equilibria and obtain new insights and developments for Fisher market equilibria. First, we leverage this duality to analyze welfare properties of Lindahl equilibria. For concave homogeneous utilities, we prove that a Lindahl equilibrium maximizes Nash Social Welfare (NSW). For concave non-homogeneous utilities, we show that a Lindahl equilibrium achieves $(1/e)^{1/e}$ approximation to the optimal NSW, and the approximation ratio is tight. Second, we apply the duality framework to market dynamics, including proportional response dynamics (PRD) and tâtonnement. We obtain new market dynamics for the Lindahl equilibria from market dynamics in the dual Fisher market. We also use duality to extend PRD to markets with total complements utilities, the dual class of gross substitutes utilities. Finally, we apply the duality framework to markets with chores. We propose a program for private chores for general convex homogeneous disutilities that avoids the ""poles"" issue, whose KKT points correspond to Fisher market equilibria. We also initiate the study of the Lindahl equilibrium for public chores."
2511.04817,"We consider the problem of repeatedly allocating multiple shareable public goods that have limited availability in an online setting without the use of money. In our setting, agents have additive values, and the value each agent receives from getting access to the goods in each period is drawn i.i.d. from some joint distribution $\mathcal{D}$ (that can be arbitrarily correlated between agents). The principal also has global constraints on the set of goods they can select over the horizon, which is represented via a submodular allocation-cost function. Our goal is to select the periods to allocate the good to ensure high value for each group of agents.We develop mechanisms for this problem using an artificial currency, where we give each agent a budget proportional to their (exogenous) fair share. The correlated value distribution makes this an especially challenging problem, as agents may attempt to free-ride by declaring low valuations for the good when they know other agents have high values-hoping those agents will bear a larger share of the cost of the resource. We offer a black-box reduction from monetary mechanisms for the allocation of a costly excludable public good. We focus on pacing strategies, the natural strategies when using AI agents, where agents report a scaled version of their value to the mechanism. Our main results show that when using a truthful monetary mechanism as our building block, the resulting online mechanism has a focal equilibrium in which each agent plays a pacing strategy whose outcome results in an allocation that is a $(\mathcal{H}_n-1)$-approximation of the core, where $\mathcal{H}_n$ is the Harmonic number, and $n$ is the number of agents. Remarkably, we are able to achieve an approximate core solution as a Nash outcome without explicit collaboration or coordination between the agents."
2511.04846,"In bipartite matching problems, agents on two sides of a graph want to be paired according to their preferences. The stability of a matching depends on these preferences, which in uncertain environments also reflect agents' beliefs about the underlying state of the world. We investigate how a principal -- who observes the true state of the world -- can strategically shape these beliefs through Bayesian persuasion to induce stable matching that maximizes a desired utility. Due to the general intractability of the underlying matching optimization problem as well as the multi-receiver persuasion problem, our main considerations are two important special cases: (1) when agents can be categorized into a small number of types based on their value functions, and (2) when the number of possible world states is small. For each case, we study both public and private signaling settings. Our results draw a complete complexity landscape: we show that private persuasion remains intractable even when the number of worlds is small, while all other settings admit polynomial-time algorithms. We present efficient algorithms for each tractable case and prove NP-hardness for the intractable ones. These results illuminate the algorithmic frontier of stable matching under information design and clarify when optimal persuasion is computationally feasible."
2511.04867,"Motivated by online platforms such as job markets, we study an agent choosing from a list of candidates, each with a hidden quality that determines match value. The agent observes only a noisy ranking of the candidates plus a binary signal that indicates whether each candidate is ""free"" or ""busy."" Being busy is positively correlated with higher quality, but can also reduce value due to decreased availability. We study the agent's optimal selection problem in the presence of ranking noise and free-busy signals and ask how the accuracy of the ranking tool impacts outcomes. In a setting with one high-valued candidate and an arbitrary number of low-valued candidates, we show that increased accuracy of the ranking tool can result in reduced social welfare. This can occur for two reasons: agents may be more likely to make offers to busy candidates, and (paradoxically) may be more likely to select lower-ranked candidates when rankings are more indicative of quality. We further discuss conditions under which these results extend to more general settings."
2511.04891,"We study the problem of fairly allocating indivisible items and a desirable heterogeneous divisible good (i.e., cake) to agents with additive utilities. In our paper, each indivisible item can be a good that yields non-negative utilities to some agents and a chore that yields negative utilities to the other agents. Given a fixed set of divisible and indivisible resources, we investigate almost envy-free allocations, captured by the natural fairness concept of envy-freeness for mixed resources (EFM). It requires that an agent $i$ does not envy another agent $j$ if agent $j$'s bundle contains any piece of cake yielding positive utility to agent $i$ (i.e., envy-freeness), and agent $i$ is envy-free up to one item (EF1) towards agent $j$ otherwise. We prove that with indivisible items and a cake, an EFM allocation always exists for any number of agents with additive utilities."
2511.04993,"While the auto-bidding literature predominantly considers independent bidding, we investigate the coordination problem among multiple auto-bidders in online advertising platforms. Two motivating scenarios are: collaborative bidding among multiple distinct bidders managed by a third-party bidding agent, and strategic bid selection for multiple ad campaigns managed by a single advertiser. We formalize this coordination problem as a theoretical model and demonstrate that a straightforward coordination mechanism, where only the highest-value bidder competes with outside bids, strictly dominates independent bidding, improving both Return-on-Spend (RoS) compliance and the total value accrued for each participating auto-bidder or ad campaign. Additionally, our simulations on synthetic and real-world datasets support the theoretical result that coordinated mechanism outperforms independent bidding. These findings highlight both the theoretical potential and the practical robustness of coordination in auto-bidding in online auctions."
2511.0529,"In this paper, we study cooperation in distributed games under network-constrained communication. Building on the framework of Monderer and Tennenholtz (1999), we derive a sufficient condition for cooperative equilibrium in settings where communication between agents is delayed by the underlying network topology. Each player deploys an agent at every location, and local interactions follow a Prisoner's Dilemma structure. We derive a sufficient condition that depends on the network diameter and the number of locations, and analyze extreme cases of instantaneous, delayed, and proportionally delayed communication. We also discuss the asymptotic case of scale-free communication networks, in which the network diameter grows sub-linearly in the number of locations. These insights clarify how communication latency and network design jointly determine the emergence of distributed cooperation."
2511.057,"We consider Stackelberg pricing games, which are also known as bilevel pricing problems, or combinatorial price-setting problems. This family of problems consists of games between two players: the leader and the follower. There is a market that is partitioned into two parts: the part of the leader and the part of the leader's competitors. The leader controls one part of the market and can freely set the prices for products. By contrast, the prices of the competitors' products are fixed and known in advance. The follower, then, needs to solve a combinatorial optimization problem in order to satisfy their own demands, while comparing the leader's offers to the offers of the competitors. Therefore, the leader has to hit the intricate balance of making an attractive offer to the follower, while at the same time ensuring that their own profit is maximized.Pferschy, Nicosia, Pacifici, and Schauer considered the Stackelberg pricing game where the follower solves a knapsack problem. They raised the question whether this problem is complete for the second level of the polynomial hierarchy, i.e., $\Sigma^p_2$-complete. The same conjecture was also made by Böhnlein, Schaudt, and Schauer. In this paper, we positively settle this conjecture. Moreover, we show that this result holds actually in a much broader context: The Stackelberg pricing game is $\Sigma^p_2$-complete for over 50 NP-complete problems, including most classics such as TSP, vertex cover, clique, subset sum, etc. This result falls in line of recent meta-theorems about higher complexity in the polynomial hierarchy by Grüne and Wulf."
2511.06218,"We study the fair allocation of indivisible goods with variable groups. In this model, the goal is to partition the agents into groups of given sizes and allocate the goods to the groups in a fair manner. We show that for any number of groups and corresponding sizes, there always exists an envy-free up to one good (EF1) outcome, thereby generalizing an important result from the individual setting. Our result holds for arbitrary monotonic utilities and comes with an efficient algorithm. We also prove that an EF1 outcome is guaranteed to exist even when the goods lie on a path and each group must receive a connected bundle. In addition, we consider a probabilistic model where the utilities are additive and drawn randomly from a distribution. We show that if there are $n$ agents, the number of goods $m$ is divisible by the number of groups $k$, and all groups have the same size, then an envy-free outcome exists with high probability if $m = \omega(\log n)$, and this bound is tight. On the other hand, if $m$ is not divisible by $k$, then an envy-free outcome is unlikely to exist as long as $m = o(\sqrt{n})$."
2511.0626,"Large language models (LLMs) are increasingly used as behavioral proxies for self-interested travelers in agent-based traffic models. Although more flexible and generalizable than conventional models, the practical use of these approaches remains limited by scalability due to the cost of calling one LLM for every traveler. Moreover, it has been found that LLM agents often make opaque choices and produce unstable day-to-day dynamics. To address these challenges, we propose to model each homogeneous traveler group facing the same decision context with a single representative LLM agent who behaves like the population's average, maintaining and updating a mixed strategy over routes that coincides with the group's aggregate flow proportions. Each day, the LLM reviews the travel experience and flags routes with positive reinforcement that they hope to use more often, and an interpretable update rule then converts this judgment into strategy adjustments using a tunable (progressively decaying) step size. The representative-agent design improves scalability, while the separation of reasoning from updating clarifies the decision logic while stabilizing learning. In classic traffic assignment settings, we find that the proposed approach converges rapidly to the user equilibrium. In richer settings with income heterogeneity, multi-criteria costs, and multi-modal choices, the generated dynamics remain stable and interpretable, reproducing plausible behavioral patterns well-documented in psychology and economics, for example, the decoy effect in toll versus non-toll road selection, and higher willingness-to-pay for convenience among higher-income travelers when choosing between driving, transit, and park-and-ride options."
2511.06518,"We study a class of two-player zero-sum Colonel Blotto games in which, after allocating soldiers across battlefields, players engage in (possibly distinct) normal-form games on each battlefield. Per-battlefield payoffs are parameterized by the soldier allocations. This generalizes the classical Blotto setting, where outcomes depend only on relative soldier allocations. We consider both discrete and continuous allocation models and examine two types of aggregate objectives: linear aggregation and worst-case battlefield value. For each setting, we analyze the existence and computability of Nash equilibrium. The general problem is not convex-concave, which limits the applicability of standard convex optimization techniques. However, we show that in several settings it is possible to reformulate the strategy space in a way where convex-concave structure is recovered. We evaluate the proposed methods on synthetic and real-world instances inspired by security applications, suggesting that our approaches scale well in practice."
2511.06559,"With the rapid advancement of generative AI (GenAI), mechanism design adapted to its unique characteristics poses new theoretical and practical challenges. Unlike traditional goods, content from one domain can enhance the training and performance of GenAI models in other domains. For example, OpenAI's video generation model Sora (Liu et al., 2024b) relies heavily on image data to improve video generation quality. In this work, we study nonlinear procurement mechanism design under data transferability, where online platforms employ both human creators and GenAI to satisfy cross-domain content demand. We propose optimal mechanisms that maximize either platform revenue or social welfare and identify the specific properties of GenAI that make such high-dimensional design problems tractable. Our analysis further reveals which domains face stronger competitive pressure and which tend to experience overproduction. Moreover, the growing role of data intermediaries, including labeling companies such as Scale AI and creator organizations such as The Wall Street Journal, introduces a third layer into the traditional platform-creator structure. We show that this three-layer market can result in a lose-lose outcome, reducing both platform revenue and social welfare, as large pre-signed contracts distort creators' incentives and lead to inefficiencies in the data market. These findings suggest a need for government regulation of the GenAI data ecosystem, and our theoretical insights are further supported by numerical simulations."
2511.06858,"As Aumann stated, cooperation and non-cooperation are different ways of viewing the same game, with the main difference being whether players can reach a binding cooperative agreement. In the real world, many games often coexist competition and cooperation. Based on the above reasons, we propose a method to transform strategic games into a biform game model, which retains the characteristics of cooperative games while considering the ultimate goal of players to maximize their own interests. Furthermore, based on this biform game model, we analyze the impact of two different distribution methods, namely marginalism and egalitarianism, on the game results. As an application, we analyze how food producers seek maximum profits through cooperative pricing."
2511.06934,"Can classical game-theoretic frameworks be extended to capture the bounded rationality and causal reasoning of AI agents? We investigate this question by extending Causal Normal Form Games (CNFGs) to sequential settings, introducing Sequential Causal Multi-Agent Systems (S-CMAS) that incorporate Pearl's Causal Hierarchy across leader-follower interactions. While theoretically elegant -- we prove PSPACE-completeness, develop equilibrium refinements, and establish connections to signaling theory -- our comprehensive empirical investigation reveals a critical limitation: S-CNE provides zero welfare improvement over classical Stackelberg equilibrium across all tested scenarios. Through 50+ Monte Carlo simulations and hand-crafted synthetic examples, we demonstrate that backward induction with rational best-response eliminates any strategic advantage from causal layer distinctions. We construct a theoretical example illustrating conditions where benefits could emerge ($\epsilon$-rational satisficing followers), though implementation confirms that even relaxed rationality assumptions prove insufficient when good instincts align with optimal play. This negative result provides valuable insight: classical game-theoretic extensions grounded in rational choice are fundamentally incompatible with causal reasoning advantages, motivating new theoretical frameworks beyond standard Nash equilibrium for agentic AI."
2511.07022,"House Allocations concern with matchings involving one-sided preferences, where houses serve as a proxy encoding valuable indivisible resources (e.g. organs, course seats, subsidized public housing units) to be allocated among the agents. Every agent must receive exactly one resource. We study algorithmic approaches towards ensuring fairness in such settings. Minimizing the number of envious agents is known to be NP-complete (Kamiyama et al. 2021). We present two tractable approaches to deal with the computational hardness. When the agents are presented with an initial allocation of houses, we aim to refine this allocation by reallocating a bounded number of houses to reduce the number of envious agents. We show an efficient algorithm when the agents express preference for a bounded number of houses. Next, we consider single peaked preference domain and present a polynomial time algorithm for finding an allocation that minimize the number of envious agents. We further extend it to satisfy Pareto efficiency. Our former algorithm works for other measures of envy such as total envy, or maximum envy, with suitable modifications. Finally, we present an empirical analysis recording the fairness-welfare trade-off of our algorithms."
