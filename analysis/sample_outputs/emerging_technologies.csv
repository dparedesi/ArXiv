paper_id,abstract
2501.01052,"Compute-in-memory (CiM) emerges as a promising solution to solve hardware challenges in artificial intelligence (AI) and the Internet of Things (IoT), particularly addressing the ""memory wall"" issue. By utilizing nonvolatile memory (NVM) devices in a crossbar structure, CiM efficiently accelerates multiply-accumulate (MAC) computations, the crucial operations in neural networks and other AI models. Among various NVM devices, Ferroelectric FET (FeFET) is particularly appealing for ultra-low-power CiM arrays due to its CMOS compatibility, voltage-driven write/read mechanisms and high ION/IOFF ratio. Moreover, subthreshold-operated FeFETs, which operate at scaling voltages in the subthreshold region, can further minimize the power consumption of CiM array. However, subthreshold-FeFETs are susceptible to temperature drift, resulting in computation accuracy degradation. Existing solutions exhibit weak temperature resilience at larger array size and only support 1-bit. In this paper, we propose TReCiM, an ultra-low-power temperature-resilient multibit 2FeFET-1T CiM design that reliably performs MAC operations in the subthreshold-FeFET region with temperature ranging from 0 to 85 degrees Celcius at scale. We benchmark our design using NeuroSim framework in the context of VGG-8 neural network architecture running the CIFAR-10 dataset. Benchmarking results suggest that when considering temperature drift impact, our proposed TReCiM array achieves 91.31% accuracy, with 1.86% accuracy improvement compared to existing 1-bit 2T-1FeFET CiM array. Furthermore, our proposed design achieves 48.03 TOPS/W energy efficiency at system level, comparable to existing designs with smaller technology feature sizes."
2501.01154,"In probability theory, the partition function is a factor used to reduce any probability function to a density function with total probability of one. Among other statistical models used to represent joint distribution, Markov random fields (MRF) can be used to efficiently represent statistical dependencies between variables. As the number of terms in the partition function scales exponentially with the number of variables, the potential of each configuration cannot be computed exactly in a reasonable time for large instances. In this paper, we aim to take advantage of the exponential scalability of quantum computing to speed up the estimation of the partition function of a MRF representing the dependencies between operating variables of an airborne radar. For that purpose, we implement a quantum algorithm for partition function estimation in the one clean qubit model. After proposing suitable formulations, we discuss the performances and scalability of our approach in comparison to the theoretical performances of the algorithm."
2501.01729,"For over a decade, linear and symmetric weight updates have remained the elusive holy grail in neuromorphic computing. Here, we unveil a kinetically controlled molecular mechanism driving a near-ideal neuromorphic element, capable of precisely modulating conductance linearly across 16,500 analog levels spanning four orders of magnitude. Our findings, supported by experimental data and mathematical modelling, demonstrate how nonlinear processes such as nucleation can be orchestrated within small perturbation regimes to achieve linearity. This establishes a groundwork for routinely realizing these long-sought neuromorphic features across a broad range of material systems."
2501.02162,"The rapid increase in symbolic data has underscored the significance of pattern matching and regular expression processing. While nondeterministic finite automata (NFA) are commonly used for these tasks, they are limited to detecting matches without determining the optimal one. This research expands on the NAPOLY pattern-matching accelerator by introducing NAPOLY+, which adds registers to each processing element to store variables like scores, weights, or edge costs. This enhancement allows NAPOLY+ to identify the highest score corresponding to the best match in sequence alignment tasks through the new-added arithmetic unit in each processor element. The design was evaluated against the original NAPOLY, with results showing that NAPOLY+ offers superior functionality and improved performance in identifying the best match. The design was implemented and tested on zynq102 and zynq104 FPGA devices, with performance metrics compared across array sizes from 1K to 64K processing elements. The results showed that memory usage increased proportionally with array size with Fmax decreasing as the array size grew on both platforms. The reported findings focus specifically on the core array, excluding the impact of buffers and DRAMs."
2501.02715,"Data encoding is a fundamental step in emerging computing paradigms, particularly in stochastic computing (SC) and hyperdimensional computing (HDC), where it plays a crucial role in determining the overall system performance and hardware cost efficiency. This study presents an advanced encoding strategy that leverages a hardware-friendly class of low-discrepancy (LD) sequences, specifically powers-of-2 bases of Van der Corput (VDC) sequences (VDC-2^n), as sources for random number generation. Our approach significantly enhances the accuracy and efficiency of SC and HDC systems by addressing challenges associated with randomness. By employing LD sequences, we improve correlation properties and reduce hardware complexity. Experimental results demonstrate significant improvements in accuracy and energy savings for SC and HDC systems. Our solution provides a robust framework for integrating SC and HDC in resource-constrained environments, paving the way for efficient and scalable AI implementations."
2501.02742,"Reconfigurable Intelligent Surface (RIS) technology has emerged as a transformative solution for enhancing satellite networks in next-generation wireless communication. The integration of RIS in satellite networks addresses critical challenges such as limited spectrum resources and high path loss, making it an ideal candidate for next-generation Internet of Things (IoT) networks. This paper provides a new framework based on transmissive beyond diagonal RIS (T-BD-RIS) mounted low earth orbit (LEO) satellite networks with non-orthogonal multiple access (NOMA). The NOMA power allocation at LEO and phase shift design at T-BD-RIS are optimized to maximize the system's spectral efficiency. The optimization problem is formulated as non-convex, which is first transformed using successive convex approximation and then divided into two problems. A closed-form solution is obtained for LEO satellite transmit power using KKT conditions, and a semi-definite relaxation approach is adopted for the T-BD-RIS phase shift design. Numerical results are obtained based on Monte Carlo simulations, which demonstrate the advantages of T-BD-RIS in satellite networks."
2501.03075,"This comprehensive survey examines how Reconfigurable Intelligent Surfaces (RIS) revolutionize resource allocation in various network frameworks. It begins by establishing a theoretical foundation with an overview of RIS technologies, including passive RIS, active RIS, and Simultaneously Transmitting and Reflecting RIS (STAR-RIS). The core of the survey focuses on RIS's role in optimizing resource allocation within Single-Input Multiple-Output (SIMO), Multiple-Input Single-Output (MISO), and Multiple-Input Multiple-Output (MIMO) systems. It further explores RIS integration in complex network environments, such as Heterogeneous Wireless Networks (HetNets) and Non-Orthogonal Multiple Access (NOMA) frameworks. Additionally, the survey investigates RIS applications in advanced communication domains like Terahertz (THz) networks, Vehicular Communication (VC), and Unmanned Aerial Vehicle (UAV) communications, highlighting the synergy between RIS and Artificial Intelligence (AI) for enhanced network efficiency. Summary tables provide comparative insights into various schemes. The survey concludes with lessons learned, future research directions, and challenges, emphasizing critical open issues."
2501.04314,"Organic memories, with small dimension, fast speed and long retention features, are considered as promising candidates for massive data archiving. In order to satisfy the re-quirements for ultra-low power and high-security information storage, we design a concep-tual molecular hard-disk (HDD) logic scheme that is capable to execute in-situ encryption of massive data in pW/bit power-consumption range. Beneficial from the coupled mechanism of counter-balanced redox reaction and local ion drifting, the basic HDD unit consisting of ~ 200 self-assembled RuXLPH molecules in a monolayer (SAM) configuration undergoes unique conductance modulation with continuous, symmetric and low-power switching char-acteristics. 96-state memory performance, which allows 6-bit data storage and single-unit one-step XOR operation, is realized in the RuXLPH SAM sample. Through single-unit XOR manipulation of the pixel information, in-situ bitwise encryption of the Mogao Grottoes mural images stored in the molecular HDD is demonstrated."
2501.04783,"This paper proposes an approach to perform travel demand calibration for high-resolution stochastic traffic simulators. It employs abundant travel times at the path-level, departing from the standard practice of resorting to scarce segment-level sensor counts. The proposed approach is shown to tackle high-dimensional instances in a sample-efficient way. For the first time, case studies on 6 metropolitan highway networks are carried out, considering a total of 54 calibration scenarios. This is the first work to show the ability of a calibration algorithm to systematically scale across networks. Compared to the state-of-the-art simultaneous perturbation stochastic approximation (SPSA) algorithm, the proposed approach enhances fit to field data by an average 43.5% with a maximum improvement of 80.0%, and does so within fewer simulation calls."
2501.04971,"Ising machines (IM) are physics-inspired alternatives to von Neumann architectures for solving hard optimization tasks. By mapping binary variables to coupled Ising spins, IMs can naturally solve unconstrained combinatorial optimization problems such as finding maximum cuts in graphs. However, despite their importance in practical applications, constrained problems remain challenging to solve for IMs that require large quadratic energy penalties to ensure the correspondence between energy ground states and constrained optimal solutions. To relax this requirement, we propose a self-adaptive IM that iteratively shapes its energy landscape using a Lagrange relaxation of constraints and avoids prior tuning of penalties. Using a probabilistic-bit (p-bit) IM emulated in software, we benchmark our algorithm with multidimensional knapsack problems (MKP) and quadratic knapsack problems (QKP), the latter being an Ising problem with linear constraints. For QKP with 300 variables, the proposed algorithm finds better solutions than state-of-the-art IMs such as Fujitsu's Digital Annealer and requires 7,500x fewer samples. Our results show that adapting the energy landscape during the search can speed up IMs for constrained optimization."
2501.05374,"Verifying computational processes in decentralized networks poses a fundamental challenge, particularly for Graphics Processing Unit (GPU) computations. Our investigation reveals significant limitations in existing approaches: exact recomputation fails due to computational non-determinism across GPU nodes, Trusted Execution Environments (TEEs) require specialized hardware, and Fully Homomorphic Encryption (FHE) faces prohibitive computational costs. To address these challenges, we explore three verification methodologies adapted from adjacent technical domains: model fingerprinting techniques, semantic similarity analysis, and GPU profiling. Through systematic exploration of these approaches, we develop novel probabilistic verification frameworks, including a binary reference model with trusted node verification and a ternary consensus framework that eliminates trust requirements. These methodologies establish a foundation for ensuring computational integrity across untrusted networks while addressing the inherent challenges of non-deterministic execution in GPU-accelerated workloads."
2501.05664,"Fabric has been a fundamental part of human life for thousands of years, providing comfort, protection, and aesthetic expression. While modern advancements have enhanced fabric's functionality, it remains static and unchangeable, failing to adapt to our evolving body shapes and preferences. This lack of adaptability can lead to unsustainable practices, as consumers often buy more items to meet their changing needs. In this paper, we propose ExoFabric, a re-moldable fabric system for customized soft goods applications. We created ExoFabric by embedding thermoplastic threads into fabric through computerized embroidery to allow for tunability between rigid plastic and conformable fabric. We defined a library of design primitives to enable geometric formability, stiffness, and stretchability by identifying suitable fabrics, threads, embroidery parameters, and machine limitations. To facilitate practical applications, we demonstrated practical methods for linking parameters to application requirements, showcasing form-fitting wearables, structural support, and shape-changeable furniture for repeatable or one-time customization."
2501.06177,"Micromobility vehicles, such as e-scooters, are increasingly popular in urban communities but present significant challenges in terms of road safety, user privacy, infrastructure planning, and civil engineering. Addressing these critical issues requires a large-scale and easily accessible research infrastructure to collect diverse mobility and contextual data from micromobility users in realistic settings. To this end, we present ScooterLab, a community research testbed comprising a fleet of customizable battery-powered micromobility vehicles retrofitted with advanced sensing, communication, and control capabilities. ScooterLab enables interdisciplinary research at the intersection of computing, mobility, and urban planning by providing researchers with tools to design and deploy customized sensing experiments and access curated datasets. The testbed will enable advances in machine learning, privacy, and urban transportation research while promoting sustainable mobility."
2501.06306,"Being widely adopted by the transportation and planning practitioners, the fundamental diagram (FD) is the primary tool used to relate the key macroscopic traffic variables of speed, flow, and density. We empirically analyze the relation between vehicular space-mean speeds and flows given different signal settings and postulate a parsimonious parametric function form of the traditional FD where its function parameters are explicitly modeled as a function of the signal plan factors. We validate the proposed formulation using data from signalized urban road segments in Salt Lake City, Utah, USA. The proposed formulation builds our understanding of how changes to signal settings impact the FDs, and more generally the congestion patterns, of signalized urban segments."
2501.06334,"Employing wireless systems with dual sensing and communications functionalities is becoming critical in next generation of wireless networks. In this paper, we propose a robust design for over-the-air federated edge learning (OTA-FEEL) that leverages sensing capabilities at the parameter server (PS) to mitigate the impact of target echoes on the analog model aggregation. We first derive novel expressions for the Cramer-Rao bound of the target response and mean squared error (MSE) of the estimated global model to measure radar sensing and model aggregation quality, respectively. Then, we develop a joint scheduling and beamforming framework that optimizes the OTA-FEEL performance while keeping the sensing and communication quality, determined respectively in terms of Cramer-Rao bound and achievable downlink rate, in a desired range. The resulting scheduling problem reduces to a combinatorial mixed-integer nonlinear programming problem (MINLP). We develop a low-complexity hierarchical method based on the matching pursuit algorithm used widely for sparse recovery in the literature of compressed sensing. The proposed algorithm uses a step-wise strategy to omit the least effective devices in each iteration based on a metric that captures both the aggregation and sensing quality of the system. It further invokes alternating optimization scheme to iteratively update the downlink beamforming and uplink post-processing by marginally optimizing them in each iteration. Convergence and complexity analysis of the proposed algorithm is presented. Numerical evaluations on MNIST and CIFAR-10 datasets demonstrate the effectiveness of our proposed algorithm. The results show that by leveraging accurate sensing, the target echoes on the uplink signal can be effectively suppressed, ensuring the quality of model aggregation to remain intact despite the interference."
2501.06479,"This paper contains information about the universal shift register. In the early stages of this paper, this paper introduces different types of flip flops and calculates the delay. After that, different types of flip flops are used to make a universal shift register, and the high-speed universal shift register is measured using a timing diagram. In addition, a complete memory system was designed at the end of this paper. A universal shift register with 4-bit Alu was added to complete the memory system. As a result, this method has created an accurate memory storage device with high-speed characteristics."
2501.06526,"Unmanned aerial vehicle (UAV)-based integrated sensing and communication (ISAC) systems are poised to revolutionize next-generation wireless networks by enabling simultaneous sensing and communication (S\&C). This survey comprehensively reviews UAV-ISAC systems, highlighting foundational concepts, key advancements, and future research directions. We explore recent advancements in UAV-based ISAC systems from various perspectives and objectives, including advanced channel estimation (CE), beam tracking, and system throughput optimization under joint sensing and communication S\&C constraints. Additionally, we examine weighted sum rate (WSR) and sensing trade-offs, delay and age of information (AoI) minimization, energy efficiency (EE), and security enhancement. These applications highlight the potential of UAV-based ISAC systems to improve spectrum utilization, enhance communication reliability, reduce latency, and optimize energy consumption across diverse domains, including smart cities, disaster relief, and defense operations. The survey also features summary tables for comparative analysis of existing methodologies, emphasizing performance, limitations, and effectiveness in addressing various challenges. By synthesizing recent advancements and identifying open research challenges, this survey aims to be a valuable resource for developing efficient, adaptive, and secure UAV-based ISAC systems."
2501.06921,"This work presents a novel monolithic 3D (M3D) FPGA architecture that leverages stackable back-end-of-line (BEOL) transistors to implement configuration memory and pass gates, significantly improving area, latency, and power efficiency. By integrating n-type (W-doped In_2O_3) and p-type (SnO) amorphous oxide semiconductor (AOS) transistors in the BEOL, Si SRAM configuration bits are substituted with a less leaky equivalent that can be programmed at logic-compatible voltages. BEOL-compatible AOS transistors are currently under extensive research and development in the device community, with investment by leading foundries, from which reported data is used to develop robust physics-based models in TCAD that enable circuit design. The use of AOS pass gates reduces the overhead of reconfigurable circuits by mapping FPGA switch block (SB) and connection block (CB) matrices above configurable logic blocks (CLBs), thereby increasing the proximity of logic elements and reducing latency. By interfacing with the latest Verilog-to-Routing (VTR) suite, an AOS-based M3D FPGA design implemented in 7 nm technology is demonstrated with 3.4x lower area-time squared product (AT^2), 27% lower critical path latency, and 26% lower reconfigurable routing block power on benchmarks including hyperdimensional computing and large language models (LLMs)."
2501.07683,"Cryogenic neuromorphic systems, inspired by the brains unparalleled efficiency, present a promising paradigm for next generation computingthis http URLwork introduces a fully integrated neuromorphic framework that combines superconducting memristor(SM) based spiking neurons and synapse topologies to achieve a low power neuromorphic network with non volatile synapticthis http URLneurosynaptic framework is validated by implementing the cart pole control task, a dynamic decision making problem requiring real timethis http URLdetailed simulations, we demonstrate the network's ability to execute this task with an average fitness of 5965 timesteps across 1000 randomized test episodes, with 40 percent achieving the target fitness of 15,000 timesteps (0.02s per timestep).The system achieves 23 distinct spiking rates across neurons, ensuring efficient informationthis http URLfindings establish the potential of SM based cryogenic neuromorphic systems to address the energy and scalability limitations of traditional computing, paving the way for biologically inspired, ultra low power computational frameworks."
2501.07733,"Solving optimization problems is a highly demanding workload requiring high-performance computing systems. Optimization solvers are usually difficult to parallelize in conventional digital architectures, particularly when stochastic decisions are involved. Recently, analog computing architectures for accelerating stochastic optimization solvers have been presented, but they were limited to academic problems in quadratic polynomial format. Here we present KLIMA, a k-Local In-Memory Accelerator with resistive Content Addressable Memories (CAMs) and Dot-Product Engines (DPEs) to accelerate the solution of high-order industry-relevant optimization problems, in particular Boolean Satisfiability. By co-designing the optimization heuristics and circuit architecture we improve the speed and energy to solution up to 182x compared to the digital state of the art."
2501.07917,"This roadmap consolidates recent advances while exploring emerging applications, reflecting the remarkable diversity of hardware platforms, neuromorphic concepts, and implementation philosophies reported in the field. It emphasizes the critical role of cross-disciplinary collaboration in this rapidly evolving field."
2501.08664,"Consensus ranking is a technique used to derive a single ranking that best represents the preferences of multiple individuals or systems. It aims to aggregate different rankings into one that minimizes overall disagreement or distance from each of the individual rankings. Kemeny ranking aggregation, in particular, is a widely used method in decision-making and social choice, with applications ranging from search engines to music recommendation systems. It seeks to determine a consensus ranking of a set of candidates based on the preferences of a group of individuals. However, existing quantum annealing algorithms face challenges in efficiently processing large datasets with many candidates. In this paper, we propose a method to improve the performance of quantum annealing for Kemeny rank aggregation. Our approach identifies the pairwise preference matrix that represents the solution list and subsequently reconstructs the ranking using classical methods. This method already yields better results than existing approaches. Furthermore, we present a range of enhancements that significantly improve the proposed method's performance, thereby increasing the number of candidates that can be effectively handled. Finally, we evaluate the efficiency of our approach by comparing its performance and execution time with that of KwikSort, a well-known approximate algorithm."
2501.10093,"This paper presents a comprehensive analysis of the energy consumption characteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node developed in the initial phase of the SUPERIOT project, focusing on key operating states, including Bluetooth Low Energy (BLE) communication, Narrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display. Extensive measurements were conducted to establish a detailed energy profile, which serves as a benchmark for evaluating the effectiveness of subsequent optimizations and future node iterations. To minimize the energy consumption, multiple optimizations were implemented at both the software and hardware levels, achieving a reduction of over 60% in total energy usage through software modifications alone. Further improvements were realized by optimizing the E-ink display driving waveform and implementing a very low-power mode for non-communication activities. Based on the measured data, three measurement-based energy consumption models were developed to characterize the energy behavior of the node under: (i) normal, unoptimized operation, (ii) low-power, software-optimized operation, and (iii) very low-power, hardware-optimized operation. These models, validated with new measurement data, achieved an accuracy exceeding 97%, confirming their reliability for predicting energy consumption in diverse configurations."
2501.10201,"We consider the unsourced random access problem with multiple receivers and propose a cell-free type solution for that. In our proposed scheme, the active users transmit their signals to the access points (APs) distributed in a geographical area and connected to a central processing unit (CPU). The transmitted signals are composed of a pilot and polar codeword, where the polar codeword bits occupy a small fraction of the data part of the transmission frame. The receiver operations of pilot detection and channel and symbol estimation take place at the APs, while the actual message bits are detected at the CPU by combining the symbol estimates from the APs forwarded over the fronthaul. The effect of the successfully decoded messages is then subtracted at the APs. Numerical examples illustrate that the proposed scheme can support up to 1400 users with a high energy efficiency, and the distributed structure decreases the error probability by more than two orders of magnitude."
2501.10376,"In this paper, we examine the problem of information storage on memristors affected by resistive drift noise under energy constraints. We introduce a novel, fundamental trade-off between the information lifetime of memristive states and the energy that must be expended to bring the device into a particular state. We then treat the storage problem as one of communication over a noisy, energy-constrained channel, and propose a joint source-channel coding (JSCC) approach to storing images in an analogue fashion. To design an encoding scheme for natural images and to model the memristive channel, we make use of data-driven techniques from the field of deep learning for communications, namely deep joint source-channel coding (DeepJSCC), employing a generative model of resistive drift as a computationally tractable differentiable channel model for end-to-end optimisation. We introduce a modified version of generalised divisive normalisation (GDN), a biologically inspired form of normalisation, that we call conditional GDN (cGDN), allowing for conditioning on continuous channel characteristics, including the initial resistive state and the delay between storage and reading. Our results show that the delay-conditioned network is able to learn an energy-aware coding scheme that achieves a higher and more balanced reconstruction quality across a range of storage delays."
2501.10392,"Molecular communication (MC) is an emerging paradigm that takes inspiration from biological processes, enabling communication at the nanoscale and facilitating the development of the Internet of Bio-Nano Things (IoBNT). Traditional models of MC often rely on idealized assumptions that overlook practical challenges related to noise and signal behavior. This paper proposes and evaluates the first physical MC ion transmitter (ITX) using an ion exchange membrane. The circuit network model is used to simulate ion transport and analyze both transient and steady-state behavior. This analysis includes the effects of noise sources such as thermal and shot noise on signal integrity and SNR. The main contributions of this paper are to demonstrate how a practical MC ITX can produce a realistic waveform and to highlight future research challenges associated with a physical membrane-based ITX."
2501.10431,"Principal component analysis is commonly used for dimensionality reduction, feature extraction, denoising, and visualization. The most commonly used principal component analysis method is based upon optimization of the L2-norm, however, the L2-norm is known to exaggerate the contribution of errors and outliers. When optimizing over the L1-norm, the components generated are known to exhibit robustness or resistance to outliers in the data. The L1-norm components can be solved for with a binary optimization problem. Previously, L1-BF has been used to solve the binary optimization for multiple components simultaneously. In this paper we propose QAPCA, a new method for finding principal components using quantum annealing hardware which will optimize over the robust L1-norm. The conditions required for convergence of the annealing problem are discussed. The potential speedup when using quantum annealing is demonstrated through complexity analysis and experimental results. To showcase performance against classical principal component analysis techniques experiments upon synthetic Gaussian data, a fault detection scenario and breast cancer diagnostic data are studied. We find that the reconstruction error when using QAPCA is comparable to that when using L1-BF."
2501.10702,"Binary matrix-vector multiplication (BMVM) is a key operation in post-quantum cryptography schemes like the Classic McEliece cryptosystem. Conventional computing architectures incur significant energy efficiency loss due to data movement of large matrices when handling such tasks. Resistive memory (RRAM) non-volatile compute-in-memory (nvCIM) is an ideal technology for high energy-efficient BMVM processing but faces challenges, including signal margin degradation in high input-parallelism arrays due to device non-idealities and high hardware overhead from current readout and XOR operations. This work presents a RRAM nvCIM architecture featuring: 1) 1T1R cells with high-resistive-state compensation modules; and 2) pulsed current-sensing parity checkers. Based on the 180nm process and test results from RRAM devices, the computing accuracy and efficiency of the architecture are verified by simulation. The proposed architecture performs high-precision current accumulation with a maximum MAC value of 10 and achieves an energy efficiency of 1.51TOPS/W, offering approximately 1.62 times improvement compared to an advanced 28nm FPGA platform."
2501.12247,"The ever increasing challenges introduced by the diversity of current and envisioned network technologies and IT infrastructure draw a highly distributed and heterogeneous topology where innovative services must be optimally deployed to guarantee maximum level of quality for users. Indeed, paradigms such as the cloud continuum, bringing together edge and cloud computing, along with the new opportunities coming out by considering non-terrestrial networks connecting future 6G ecosystems, all with no doubt facilitate the development of innovative services in many different areas and verticals. However, considering the intensive data and quality requirements demanded by these services, the distribution of the execution tasks must be optimally designed. On the infrastructure side, several initiatives are already active aimed at providing a Meta-OS that may seamlessly manage the different actors (services, infrastructure and users) playing under this paradigm. However, several aspects remain yet limited, particularly when referring to the mapping of resources into services, where innovative technologies based on bidirectional coordination and modeling may be pivotal for an optimal performance. In addition, the upcoming demands coming from the adoption of network technologies easing users connection with high levels of quality, such as 6G, as well the study of NTN open up the traditional cloud continuum to include also satellites that may extend the cloud paradigm further than ever considered. This paper shows a seed work toward an extendable paradigm so called as plastic computing whose main objective is to optimize service performance and users satisfaction, through considering a bidirectional strategy, easily extendable to adopt novel network and IT technologies and paradigms. Finally, two examples are briefly introduced to highlight the potential benefits of the plastic computing adoption"
2501.12644,"The unprecedented advancement of artificial intelligence has placed immense demands on computing hardware, but traditional silicon-based semiconductor technologies are approaching their physical and economic limit, prompting the exploration of novel computing paradigms. Memristor offers a promising solution, enabling in-memory analog computation and massive parallelism, which leads to low latency and power consumption. This manuscript reviews the current status of memristor-based machine learning accelerators, highlighting the milestones achieved in developing prototype chips, that not only accelerate neural networks inference but also tackle other machine learning tasks. More importantly, it discusses our opinion on current key challenges that remain in this field, such as device variation, the need for efficient peripheral circuitry, and systematic co-design and optimization. We also share our perspective on potential future directions, some of which address existing challenges while others explore untouched territories. By addressing these challenges through interdisciplinary efforts spanning device engineering, circuit design, and systems architecture, memristor-based accelerators could significantly advance the capabilities of AI hardware, particularly for edge applications where power efficiency is paramount."
2501.12652,"Quantum computing (QC) is expected to solve incredibly difficult problems, including finding optimal solutions to combinatorial optimization problems. However, to date, QC alone is still far to demonstrate this capability except on small-sized problems. Hybrid approaches where QC and classical computing work together have shown the most potential for solving real-world scale problems. This work aims to show that we can enhance a classical optimization algorithm with QC so that it can overcome this limitation. We present a new hybrid quantum-classical tabu search (HQTS) algorithm to solve the capacitated vehicle routing problem (CVRP). Based on our prior work, HQTS leverages QC for routing within a classical tabu search framework. The quantum component formulates the traveling salesman problem (TSP) for each route as a QUBO, solved using D-Wave's Advantage system. Experiments investigate the impact of quantum routing frequency and starting solution methods. While different starting solution methods, including quantum-based and classical heuristics methods, it shows minimal overall impact. HQTS achieved optimal or near-optimal solutions for several CVRP problems, outperforming other hybrid CVRP algorithms and significantly reducing the optimality gap compared to preliminary research. The experimental results demonstrate that more frequent quantum routing improves solution quality and runtime. The findings highlight the potential of integrating QC within meta-heuristic frameworks for complex optimization in vehicle routing problems."
2501.15305,"Edge sensing and computing is rapidly becoming part of intelligent infrastructure architecture leading to operational reliance on such systems in disaster or emergency situations. In such scenarios there is a high chance of power supply failure due to power grid issues, and communication system issues due to base stations losing power or being damaged by the elements, e.g., flooding, wildfires etc. Mobile edge computing in the form of unmanned aerial vehicles (UAVs) has been proposed to provide computation offloading from these devices to conserve their battery, while the use of UAVs as relay network nodes has also been investigated previously. This paper considers the use of UAVs with further constraints on power and connectivity to prolong the life of the network while also ensuring that the data is received from the edge nodes in a timely manner. Reinforcement learning is used to investigate numerous scenarios of various levels of power and communication failure. This approach is able to identify the device most likely to fail in a given scenario, thus providing priority guidance for maintenance personnel. The evacuations of a rural town and urban downtown area are also simulated to demonstrate the effectiveness of the approach at extending the life of the most critical edge devices."
2501.17416,"Decentralized physical infrastructure networks (DePINs) are an emerging vertical within ""Web3"" replacing the traditional method that physical infrastructures are constructed. Yet, the boundaries between DePIN and traditional method of building crowd-sourced infrastructures such as citizen science initiatives or other Web3 verticals are not always so clear cut. In this work, we systematically analyze the differences between DePIN and other Web2 and Web3 verticals. For this, the study proposes a novel decision tree for classifying systems as DePIN. This tree is informed by prior studies and differentiates DePIN from related concepts using criteria such as the presence of a three-sided market, token-based incentives for supply, and the requirement for physical asset placement in those systems.The paper demonstrates the application of the decision tree to various blockchain systems, including Helium and Bitcoin, showcasing its practical utility in differentiating DePIN systems.This research offers significant contributions towards establishing a more objective and systematic approach to identifying and categorizing DePIN systems. It lays the groundwork for creating a comprehensive and unbiased database of DePIN systems, which will inform future research and development within this emerging sector."
2501.18905,"While quantum computing holds immense potential for tackling previously intractable problems, its current practicality remains limited. A critical aspect of realizing quantum utility is the ability to efficiently interface with data from the classical world. This research focuses on the crucial phase of quantum encoding, which enables the transformation of classical information into quantum states for processing within quantum systems. We focus on three prominent encoding models: Phase Encoding, Qubit Lattice, and Flexible Representation of Quantum Images (FRQI) for cost and efficiency analysis. The aim of quantifying their different characteristics is to analyze their impact on quantum processing workflows. This comparative analysis offers valuable insights into their limitations and potential to accelerate the development of practical quantum computing solutions."
2501.19341,"Molecular Communications (MC), transferring information via chemical signals, holds promise for transformative healthcare applications within the Internet of Bio-Nano Things (IoBNT) framework. Despite promising advances toward practical MC systems, progress has been constrained by experimental testbeds that are costly, difficult to customize, and require labor-intensive fabrication. Here, we address these challenges by introducing a low-cost ($\sim$\$1 per unit), rapidly fabricated ($<$1 hour), and highly customizable microfluidic testbed that integrates hydrodynamic gating and screen-printed potentiometric sensors. This platform enables precise spatiotemporal control over chemical signals and supports reconfigurable channel architectures along with on-demand sensor functionalization. As a proof of concept, we demonstrate a pH-based MC system combining a polyaniline (PANI)-functionalized sensor for real-time signal detection with a programmable hydrodynamic gating architecture, patterned in a double-sided adhesive tape, as the transmitter. By dynamically mixing phosphate-buffered saline (PBS) with an acidic solution (pH 3), the testbed reliably generates pH-encoded pulses. Experimental results confirm robust control over pulse amplitude and pulse width, enabling the simulation of end-to-end MC scenarios with 4-ary concentration shift keying (CSK) modulation. By combining affordability and rapid prototyping without compromising customizability, this platform is poised to accelerate the translation of MC concepts into practical IoBNT applications."
2502.00831,"We present a fluid-based experimental molecular communication (MC) testbed which uses media modulation. Motivated by the natural human cardiovascular system, the testbed operates in a closed-loop tube system. The proposed system is designed to be biocompatible, resource-efficient, and controllable from outside the tube. As signaling molecule, the testbed employs the green fluorescent protein variant ""Dreiklang"" (GFPD). GFPDs can be reversibly switched via light of different wavelengths between a bright fluorescent state and a less fluorescent state. GFPDs in solution are filled into the testbed prior to the start of information transmission and remain there for an entire experiment. For information transmission, an optical transmitter (TX) and an optical eraser (EX), which are located outside the tube, are used to write and erase the information encoded in the state of the GFPDs, respectively. At the receiver (RX), the state of the GFPDs is read out by fluorescence detection. In our testbed, due to the closed-loop setup, we observe new forms of inter-symbol interferences (ISI), which do not occur in short experiments and open-loop systems. For the testbed, we developed a communication scheme, which includes blind transmission start detection, symbol-by-symbol synchronization, and adaptive threshold detection. We comprehensively analyze our MC experiments using different performance metrics. Moreover, we experimentally demonstrate the error-free transmission of 5370 bit at a data rate of 36 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$ using 8-ary modulation and the error-free binary transmission of around 90000 bit at a data rate of 12 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$. For the latter experiment, data was transmitted for a period of 125 hours. All signals recorded and parts of the evaluation code are publicly available on Zenodo and Github, respectively."
2502.02125,"In this paper, we present an approach for estimating significant financial metrics within risk management by utilizing quantum phenomena for random number generation. We explore Quantum-Enhanced Monte Carlo, a method that combines traditional and quantum techniques for enhanced precision through Quantum Random Numbers Generation (QRNG). The proposed methods can be based on the use of photonic phenomena or quantum processing units to generate random numbers. The results are promising, hinting at improved accuracy with the proposed methods and slightly lower estimates (both for VaR and CVaR estimation) using the quantum-based methodology."
2502.0252,"The use of Self-Sovereign Identity (SSI) systems for digital identity management is gaining traction and interest. Countries such as Bhutan have already implemented an SSI infrastructure to manage the identity of their citizens. The EU, thanks to the revised eIDAS regulation, is opening the door for SSI vendors to develop SSI systems for the planned EU digital identity wallet. These developments, which fall within the sovereign domain, raise questions about individual privacy. The design of SSI systems is complex, often characterized by a large number of components and architectural choices because the current SSI communities differ on how to create identifiers, how to build and present credentials, and even how to design a user wallet. SSI stacks developed by different organizations provide different privacy features for different privacy needs. This paper performs a systematic mapping and review of SSI components and technologies into a novel four-layer privacy framework to address the design complexity of SSI systems. Based on this review, we provide an accompanying Design Assistance Dashboard (DAD). The DAD shows the interdependencies between SSI components in different layers, and maps these components to different privacy requirements and considerations, even providing a simple privacy class for each component."
2502.03086,"This study explores the implementation of large Quantum Restricted Boltzmann Machines (QRBMs), a key advancement in Quantum Machine Learning (QML), as generative models on D-Wave's Pegasus quantum hardware to address dataset imbalance in Intrusion Detection Systems (IDS). By leveraging Pegasus's enhanced connectivity and computational capabilities, a QRBM with 120 visible and 120 hidden units was successfully embedded, surpassing the limitations of default embedding tools. The QRBM synthesized over 1.6 million attack samples, achieving a balanced dataset of over 4.2 million records. Comparative evaluations with traditional balancing methods, such as SMOTE and RandomOversampler, revealed that QRBMs produced higher-quality synthetic samples, significantly improving detection rates, precision, recall, and F1 score across diverse classifiers. The study underscores the scalability and efficiency of QRBMs, completing balancing tasks in milliseconds. These findings highlight the transformative potential of QML and QRBMs as next-generation tools in data preprocessing, offering robust solutions for complex computational challenges in modern information systems."
2502.03167,"Interest in non-algorithmic, unconventional computing is rising in recent years due to more and more apparent short comings of classic stored-program digital computers, such as energy efficiency, degree of parallelism in computations, clock frequency limitations, integration density, silicon utilization, etc. One notable such unconventional approach are oscillator based Ising machines, i.e., systems consisting of a number of oscillators which can be coupled in order to create an analogue for some problem to be solved, while the actual information is encoded in the phase relationships of these oscillators with respect to some reference (typically one of these oscillators). It has been shown that machines of this type are capable of solving NP-hard problems such as max-cut, etc. In the following an experimental Ising machine is presented together with experimental results obtained from this machine."
2502.03445,"A quantum processing unit (QPU) must contain a large number of high quality qubits to produce accurate results for problems at useful scales. In contrast, most scientific and industry classical computation workloads happen in parallel on distributed systems, which rely on copying data across multiple cores. Unfortunately, copying quantum data is theoretically prohibited due to the quantum non-cloning theory. Instead, quantum circuit cutting techniques cut a large quantum circuit into multiple smaller subcircuits, distribute the subcircuits on parallel QPUs and reconstruct the results with classical computing. Such techniques make distributed hybrid quantum computing (DHQC) a possibility but also introduce an exponential classical co-processing cost in the number of cuts and easily become intractable. This paper presents TensorQC, which leverages classical tensor networks to bring an exponential runtime advantage over state-of-the-art parallelization post-processing techniques. As a result, this paper demonstrates running benchmarks that are otherwise intractable for a standalone QPU and prior circuit cutting techniques. Specifically, this paper runs six realistic benchmarks using QPUs available nowadays and a single GPU, and reduces the QPU size and quality requirements by more than $10\times$ over purely quantum platforms."
2502.03637,"Reconfigurable intelligent surface (RIS) technology has emerged as a promising enabler for next-generation wireless networks, offering a paradigm shift from passive environments to programmable radio wave propagation. Despite the potential of diagonal RIS (D-RIS), its limited wave manipulation capability restricts performance gains. In this paper, we investigate the burgeoning concept of beyond-diagonal RIS (BD-RIS), which incorporates non-diagonal elements in its scattering matrix to deliver more fine-grained control of electromagnetic wavefronts. We begin by discussing the limitations of traditional D-RIS and introduce key BD-RIS architectures with different operating modes. We then highlight the features that make BD-RIS particularly advantageous for 6G IoT applications, including advanced beamforming, enhanced interference mitigation, and flexible coverage. A case study on BD-RIS-assisted vehicle-to-vehicle (V2V) communication in an underlay cellular network demonstrates considerable improvements in spectral efficiency when compared to D-RIS and conventional systems. Lastly, we present current challenges such as hardware design complexity, channel estimation, and non-ideal hardware effects, and propose future research directions involving AI-driven optimization, joint communication and sensing, and physical layer security. Our findings illustrate the transformative potential of BD-RIS in shaping high-performance, scalable, and reliable 6G IoT networks."
2502.04524,"Analog in-memory computing is an emerging paradigm designed to efficiently accelerate deep neural network workloads. Recent advancements have focused on either inference or training acceleration. However, a unified analog in-memory technology platform-capable of on-chip training, weight retention, and long-term inference acceleration-has yet to be reported. This work presents an all-in-one analog AI accelerator, combining these capabilities to enable energy-efficient, continuously adaptable AI systems. The platform leverages an array of analog filamentary conductive-metal-oxide (CMO)/HfOx resistive switching memory cells (ReRAM) integrated into the back-end-of-line (BEOL). The array demonstrates reliable resistive switching with voltage amplitudes below 1.5V, compatible with advanced technology nodes. The array multi-bit capability (over 32 stable states) and low programming noise (down to 10nS) enable a nearly ideal weight transfer process, more than an order of magnitude better than other memristive technologies. Inference performance is validated through matrix-vector multiplication simulations on a 64x64 array, achieving a root-mean-square error improvement by a factor of 20 at 1 second and 3 at 10 years after programming, compared to state-of-the-art. Training accuracy closely matching the software equivalent is achieved across different datasets. The CMO/HfOx ReRAM technology lays the foundation for efficient analog systems accelerating both inference and training in deep neural networks."
2502.04527,"Droplet-based communications has been investigated as a more robust alternative to diffusion-based molecular communications (MC), yet most existing demonstrations employ large ""plug-like"" droplets or simple T-junction designs for droplet generation, restricting modulation strategies and achievable data rates. Here, we report a microfluidic communication system that encodes information via the generation rate of sub-100 $\mu$m water-in-oil microdroplets using a microfabricated flow focusing architecture. By precisely tuning the flow rate of the dispersed-phase (water) via a pressure-regulated flow controller, we implement frequency shift keying modulation with four symbols (4-FSK). A high-speed optical detection and video processing setup serves as the receiver, tracking system response in the microfluidic channel across different symbol durations (20 s and 12 s) and quantifying error performance. Despite the miniaturized device and channel architecture, our experiments demonstrate programmable and reliable data transmission with minimal symbol errors. Beyond water-in-oil systems, the same encoding principles can be extended to other compartmentalized carriers (e.g., giant unilamellar vesicles, polymersomes) that can also be synthesized via flow focusing techniques, paving the way for biocompatible, robust, and high-capacity communication in intrabody networks and the emerging Internet of Bio-Nano Things."
2502.05586,"The global manufacturing landscape is undergoing a fundamental shift from resource-intensive mass production to sustainable, localised manufacturing. This paper presents a comprehensive analysis of a Cloud Crafting Platform that enables Manufacturing as a Service (MaaS) through additive manufacturing technologies. The platform connects web shops with local three-dimensional (3D) printing facilities, allowing customers to purchase products that are manufactured on-demand in their vicinity. We present the platform's Service-Oriented Architecture (SOA), deployment on the Microsoft Azure cloud, and integration with three different 3D printer models in a testbed environment. A detailed cost-benefit analysis demonstrates the economic viability of the approach, which generates significant profit margins. The platform implements a weighted profit-sharing model that fairly compensates all stakeholders based on their investment and operational responsibilities. Our results show that on-demand, localised manufacturing through MaaS is not only technically feasible but also economically viable, while reducing environmental impact through shortened supply chains and elimination of inventory waste. The platform's extensible architecture allows for future integration of additional manufacturing technologies beyond 3D printing."
2502.05787,"Pattern search is crucial in numerous analytic applications for retrieving data entries akin to the query. Content Addressable Memories (CAMs), an in-memory computing fabric, directly compare input queries with stored entries through embedded comparison logic, facilitating fast parallel pattern search in memory. While conventional CAM designs offer exact match functionality, they are inadequate for meeting the approximate search needs of emerging data-intensive applications. Some recent CAM designs propose approximate matching functions, but they face limitations such as excessively large cell area or the inability to precisely control the degree of approximation. In this paper, we propose TAP-CAM, a novel ferroelectric field effect transistor (FeFET) based ternary CAM (TCAM) capable of both exact and tunable approximate matching. TAP-CAM employs a compact 2FeFET-2R cell structure as the entry storage unit, and similarities in Hamming distances between input queries and stored entries are measured using an evaluation transistor associated with the matchline of CAM array. The operation, robustness and performance of the proposed design at array level have been discussed and evaluated, respectively. We conduct a case study of K-nearest neighbor (KNN) search to benchmark the proposed TAP-CAM at application level. Results demonstrate that compared to 16T CMOS CAM with exact match functionality, TAP-CAM achieves a 16.95x energy improvement, along with a 3.06% accuracy enhancement. Compared to 2FeFET TCAM with approximate match functionality, TAP-CAM achieves a 6.78x energy improvement."
2502.05948,"With the escalating demand for power-efficient neural network architectures, non-volatile compute-in-memory designs have garnered significant attention. However, owing to the nature of analog computation, susceptibility to noise remains a critical concern. This study confronts this challenge by introducing a detailed model that incorporates noise factors arising from both ADCs and RRAM devices. The experimental data is derived from a 40nm foundry RRAM test-chip, wherein different reference voltage configurations are applied, each tailored to its respective module. The mean and standard deviation values of HRS and LRS cells are derived through a randomized vector, forming the foundation for noise simulation within our analytical framework. Additionally, the study examines the read-disturb effects, shedding light on the potential for accuracy deterioration in neural networks due to extended exposure to high-voltage stress. This phenomenon is mitigated through the proposed low-voltage read mode. Leveraging our derived comprehensive fault model from the RRAM test-chip, we evaluate CIM noise impact on both supervised learning (time-independent) and reinforcement learning (time-dependent) tasks, and demonstrate the effectiveness of reference tuning to mitigate noise impacts."
2502.05981,"In this paper we show that every combinatorial problem has an exact explicit equation that returns its solution. We present a method to obtain an equation that solves exactly any combinatorial problem, both inversion, constraint satisfaction and optimization, by obtaining its equivalent tensor network. This formulation only requires a basic knowledge of classical logical operators, at a first year level of any computer science degree. These equations are not necessarily computable in a reasonable time, nor do they allow to surpass the state of the art in computational complexity, but they allow to have a new perspective for the mathematical analysis of these problems. These equations computation can be approximated by different methods such as Matrix Product State compression. We also present the equations for numerous combinatorial problems. This work proves that, if there is a physical system capable of contracting in polynomial time the tensor networks presented, every NP-Hard problem can be solved in polynomial time."
2502.06736,"This work introduces a spike-based wearable analytics system utilizing Spiking Neural Networks (SNNs) deployed on an In-memory Computing engine based on RRAM crossbars, which are known for their compactness and energy-efficiency. Given the hardware constraints and noise characteristics of the underlying RRAM crossbars, we propose online adaptation of pre-trained SNNs in real-time using Direct Feedback Alignment (DFA) against traditional backpropagation (BP). Direct Feedback Alignment (DFA) learning, that allows layer-parallel gradient computations, acts as a fast, energy & area-efficient method for online adaptation of SNNs on RRAM crossbars, unleashing better algorithmic performance against those adapted using BP. Through extensive simulations using our in-house hardware evaluation engine called DFA_Sim, we find that DFA achieves upto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1x reduction in latency compared to BP, while delivering upto 7.55% higher inference accuracy on human activity recognition (HAR) tasks."
2502.07493,"Wireless Fidelity or Wi-Fi, has completely transfigured wireless networking by offering a smooth connection to the internet and networks, particularly when dealing with enclosed environments. As with the majority of wireless technology, it functions through radio communication. This makes it possible for Wi-Fi to operate effectively close to an Access Point. However, a device's ability to receive Wi-Fi signals can vary greatly. These discrepancies arise because of impediments or motions between the device and the access point. We have creatively used these variances as unique opportunities for applications that can be used to detect movement in confined areas. As this approach makes use of the current wireless infrastructure, no additional hardware is required. These applications could potentially be leveraged to enable sophisticated robots or enhance security systems."
2502.07954,"Wireless communication channels in Vehicular Ad-hoc NETworks (VANETs) suffer from packet losses, which severely influences the performance of their applications. There are several reasons for this loss, including but not limited to signal interference with itself after being reflected from the ground and other objects, the doppler effect caused by the speed of the vehicle, and buildings and other vehicles blocking the signal. As a result, VANET simulators must be calibrated in order to mimic the behavior of real-world vehicular communication channels effectively. In this paper, we calibrated an OMNET++(Objective Modular Network Testbed in C++)/Veins simulator for VANET's dedicated short-range communications (DSRC) protocol using the field data from the urban testbed in Downtown Chattanooga, TN. Channel propagation models, as well as physical layer parameters, were calibrated using a Genetic Algorithm (GA). The performance of the calibrated simulator was improved significantly in comparison with the default settings in Veins. The final results were compared to the real-world data collected from the testbed and performance shows that the final calibrated channel model performs better than uncalibrated models in simulating the packet delivery pattern of DSRC channels."
2502.08566,"Recent advancements in Augmented Reality (AR) have demonstrated applications in architecture, design, and fabrication. Compared to conventional 2D construction drawings, AR can be used to superimpose contextual instructions, display 3D spatial information and enable on-site engagement. Despite the potential of AR, the widespread adoption of the technology in the industry is limited by its precision. Precision is important for projects requiring strict construction tolerances, design fidelity, and fabrication feedback. For example, the manufacturing of glulam beams requires tolerances of less than 2mm. The goal of this project is to explore the industrial application of using multiple fiducial markers for high-precision AR fabrication. While the method has been validated in lab settings with a precision of 0.97, this paper focuses on fabricating glulam beams in a factory setting with an industry manufacturer, Unalam Factory."
2502.08603,"Many hardware proposals have aimed to accelerate inference in AI workloads. Less attention has been paid to hardware acceleration of training, despite the enormous societal impact of rapid training of AI models. Physics-based computers, such as thermodynamic computers, offer an efficient means to solve key primitives in AI training algorithms. Optimizers that normally would be computationally out-of-reach (e.g., due to expensive matrix inversions) on digital hardware could be unlocked with physics-based hardware. In this work, we propose a scalable algorithm for employing thermodynamic computers to accelerate a popular second-order optimizer called Kronecker-factored approximate curvature (K-FAC). Our asymptotic complexity analysis predicts increasing advantage with our algorithm as $n$, the number of neurons per layer, increases. Numerical experiments show that even under significant quantization noise, the benefits of second-order optimization can be preserved. Finally, we predict substantial speedups for large-scale vision and graph problems based on realistic hardware characteristics."
2502.08968,"Dysphonia, a prevalent medical condition, leads to voice loss, hoarseness, or speech interruptions. To assess it, researchers have been investigating various machine learning techniques alongside traditional medical assessments. Convolutional Neural Networks (CNNs) have gained popularity for their success in audio classification and speech recognition. However, the limited availability of speech data, poses a challenge for CNNs. This study evaluates the performance of CNNs against a novel hybrid quantum-classical approach, Quanvolutional Neural Networks (QNNs), which are well-suited for small datasets. The audio data was preprocessed into Mel spectrograms, comprising 243 training samples and 61 testing samples in total, and used in ten experiments. Four models were developed (two QNNs and two CNNs) with the second models incorporating additional layers to boost performance. The results revealed that QNN models consistently outperformed CNN models in accuracy and stability across most experiments."
2502.10403,"Agile healthcare frameworks, derived from methodologies in IT and manufacturing, offer transformative potential for low-income regions. This study explores Agile integration in resource-constrained environments, focusing on Ghana. Key benefits include adaptability, iterative planning, and stakeholder collaboration to address infrastructure gaps, workforce shortages, and the ""know-do gap."" Digital tools like mobile health (mHealth) applications and the District Health Information Management System (DHIMS) demonstrate Agile scalability and efficacy in improving outcomes and resource allocation. Policy alignment, such as through Ghana's National Health Insurance Scheme (NHIS), is crucial for sustaining these practices. Findings reveal Agile ability to enable real-time decision-making, foster community engagement, and drive interdisciplinary collaboration. This paper provides actionable strategies and systemic innovations, positioning Agile as a scalable model for equitable, high-quality care delivery in other low-income regions."
2502.11126,"Reservoir computing (RC) is an innovative paradigm in neuromorphic computing that leverages fixed, randomized, internal connections to address the challenge of overfitting. RC has shown remarkable effectiveness in signal processing and pattern recognition tasks, making it well-suited for hardware implementations across various physical substrates, which promise enhanced computation speeds and reduced energy consumption. However, achieving optimal performance in RC systems requires effective parameter optimization. Traditionally, this optimization has relied on software modeling, limiting the practicality of physical computing approaches. Here, we report an \emph{in situ} optimization method for an optoelectronic delay-based RC system with digital delayed feedback. By simultaneously optimizing five parameters, normalized mean squared error (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks: waveform classification, time series prediction, and speech recognition outperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329, respectively) in the two of the three tasks. This method marks a significant advancement in physical computing, facilitating the optimization of RC and neuromorphic systems without the need for simulation, thus enhancing their practical applicability."
2502.11316,"Quantum computing QC emulation is crucial for advancing QC applications, especially given the scalability constraints of current devices. FPGA-based designs offer an efficient and scalable alternative to traditional large-scale platforms, but most are tightly integrated with high-performance systems, limiting their use in mobile and edge environments. This study introduces a compact, standalone FPGA-based QC emulator designed for embedded systems, leveraging the Quantum Approximate Optimization Algorithm (QAOA) to solve the Weighted-MaxCut problem. By restructuring QAOA operations for hardware compatibility, the proposed design reduces time complexity from O(N^2) to O(N), where N equals 2^n for n qubits. This reduction, coupled with a pipeline architecture, significantly minimizes resource consumption, enabling support for up to nine qubits on mid-tier FPGAs, roughly three times more than comparable designs. Additionally, the emulator achieved energy savings ranging from 1.53 times for two-qubit configurations to up to 852 times for nine-qubit configurations, compared to software-based QAOA on embedded processors. These results highlight the practical scalability and resource efficiency of the proposed design, providing a robust foundation for QC emulation in resource-constrained edge devices."
2502.12012,"Variational quantum algorithms, such as the Recursive Quantum Approximate Optimization Algorithm (RQAOA), have become increasingly popular, offering promising avenues for employing Noisy Intermediate-Scale Quantum devices to address challenging combinatorial optimization tasks like the maximum cut problem. In this study, we utilize an evolutionary algorithm equipped with a unique fitness function. This approach targets hard maximum cut instances within the latent space of a Graph Autoencoder, identifying those that pose significant challenges or are particularly tractable for RQAOA, in contrast to the classic Goemans and Williamson algorithm. Our findings not only delineate the distinct capabilities and limitations of each algorithm but also expand our understanding of RQAOA's operational limits. Furthermore, the diverse set of graphs we have generated serves as a crucial benchmarking asset, emphasizing the need for more advanced algorithms to tackle combinatorial optimization challenges. Additionally, our results pave the way for new avenues in graph generation research, offering exciting opportunities for future explorations."
2502.13386,"Magnetic induction (MI) communication, with stable channel conditions and small antenna size, is considered as a promising solution for underwater communication network. However, the narrowband nature of the MI link can cause significant delays in the network. To comprehensively ensure the timeliness and effectiveness of the MI network, in this paper we introduce a statistical quality of service (QoS) framework for MI communication, aiming to maximize the achievable rate while provisioning delay and queue-length requirements. Specifically, we employ effective capacity theory to model underwater MI communication. Based on convex optimization theory, we propose a current control strategy that maximizes the effective capacity under the constraints of limited channel capacity and limited power. Simulations demonstrate that the current control strategy proposed for MI communication differs significantly from that in the conventional statistical QoS provisioning framework. In addition, compared to other current control strategies, the proposed strategy substantially improves the achievable rate under various delay QoS requirements."
2502.15192,"Mobile Augmented Reality (MAR) applications face performance challenges due to their high computational demands and need for low-latency responses. Traditional approaches like on-device storage or reactive data fetching from the cloud often result in limited AR experiences or unacceptable lag. Edge caching, which caches AR objects closer to the user, provides a promising solution. However, existing edge caching approaches do not consider AR-specific features such as AR object sizes, user interactions, and physical location. This paper investigates how to further optimize edge caching by employing AR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and Association-based Prefetching policy specifically designed for MAR Caches. SPAARC intelligently prioritizes the caching of virtual objects based on their association with other similar objects and the user's proximity to them. It also considers the recency of associations and uses a lazy fetching strategy to efficiently manage edge resources and maximize Quality of Experience (QoE).Through extensive evaluation using both synthetic and real-world workloads, we demonstrate that SPAARC significantly improves cache hit rates compared to standard caching algorithms, achieving gains ranging from 3% to 40% while reducing the need for on-demand data retrieval from the cloud. Further, we present an adaptive tuning algorithm that automatically tunes SPAARC parameters to achieve optimal performance. Our findings demonstrate the potential of SPAARC to substantially enhance the user experience in MAR applications by ensuring the timely availability of virtual objects."
2502.15386,"Electronic Design Automation (EDA) plays a crucial role in classical chip design and significantly influences the development of quantum chip design. However, traditional EDA tools cannot be directly applied to quantum chip design due to vast differences compared to the classical realm. Several EDA products tailored for quantum chip design currently exist, yet they only cover partial stages of the quantum chip design process instead of offering a fully comprehensive solution. Additionally, they often encounter issues such as limited automation, steep learning curves, challenges in integrating with actual fabrication processes, and difficulties in expanding functionality. To address these issues, we developed a full-stack EDA tool specifically for quantum chip design, called EDA-Q. The design workflow incorporates functionalities present in existing quantum EDA tools while supplementing critical design stages such as device mapping and fabrication process mapping, which users expect. EDA-Q utilizes a unique architecture to achieve exceptional scalability and flexibility. The integrated design mode guarantees algorithm compatibility with different chip components, while employing a specialized interactive processing mode to offer users a straightforward and adaptable command interface. Application examples demonstrate that EDA-Q significantly reduces chip design cycles, enhances automation levels, and decreases the time required for manual intervention. Multiple rounds of testing on the designed chip have validated the effectiveness of EDA-Q in practical applications."
2502.15481,"Recently, employing single-modality large language models based on mechanical vibration signals as Tuning Predictors has introduced new perspectives in intelligent fault diagnosis. However, the potential of these methods to leverage multimodal data remains underexploited, particularly in complex mechanical systems where relying on a single data source often fails to capture comprehensive fault information. In this paper, we present FaultGPT, a novel model that generates fault diagnosis reports directly from raw vibration signals. By leveraging large vision-language models (LVLM) and text-based supervision, FaultGPT performs end-to-end fault diagnosis question answering (FDQA), distinguishing itself from traditional classification or regression approaches. Specifically, we construct a large-scale FDQA instruction dataset for instruction tuning of LVLM. This dataset includes vibration time-frequency image-text label pairs and human instruction-ground truth pairs. To enhance the capability in generating high-quality fault diagnosis reports, we design a multi-scale cross-modal image decoder to extract fine-grained fault semantics and conducted instruction tuning without introducing additional training parameters into the LVLM. Extensive experiments, including fault diagnosis report generation, few-shot and zero-shot evaluation across multiple datasets, validate the superior performance and adaptability of FaultGPT in diverse industrial scenarios."
2502.16271,"With the advent of the 6G mobile communication network era, the existing non-orthogonal multiple-access (NOMA) technology faces the challenge of high successive interference in multi-user scenarios, which limits its ability to support more user access. To address this, this paper proposes a novel power-domain sparse-dimensional constellation multiple-access scheme (PD-SDCMA). Through the signal space dimension selection strategy (S2D-strategy), this scheme sparsely superposes low-dimensional constellations onto high-dimensional signal spaces, and reduces the high-order interference caused by SC by taking advantage of the non-correlation between dimensions. Specifically, PD-SDCMA reduces the successive interference between users by sparsifying the dimension allocation of constellation superposition and designs a sparse superposition method based on the theory of vector space signal representation. Simulation results show that, under the AWGN channel, PD-SDCMA significantly outperforms the traditional PD-NOMA in terms of the number of supported users under QPSK and 16QAM modulations, and also has better BER performance. This paper provides a new solution for efficient spectrum utilization in future scenarios with large-scale user access."
2502.16489,"Setting out a path to use quantum computing within a company is not as straightforward as the implementation of classical ICT-projects. The technology is fundamentally different and not mature yet, which makes the development and use uncertain, non-linear and more complex. Being also a potential disruptive technology makes it for a company important to be aware of the possible business value generated by using quantum computing and be prepared for taking this technology in production. In this article we present Quantum Organisational Readiness Levels to determine the degree of readiness for implementation of quantum computing."
2502.18639,"The digitization of healthcare presents numerous challenges, including the complexity of biological systems, vast data generation, and the need for personalized treatment plans. Traditional computational methods often fall short, leading to delayed and sometimes ineffective diagnoses and treatments. Quantum Computing (QC) and Quantum Machine Learning (QML) offer transformative advancements with the potential to revolutionize medicine. This paper summarizes areas where QC promises unprecedented computational power, enabling faster, more accurate diagnostics, personalized treatments, and enhanced drug discovery processes. However, integrating quantum technologies into precision medicine also presents challenges, including errors in algorithms and high costs. We show that mathematically-based techniques for specifying, developing, and verifying software (formal methods) can enhance the reliability and correctness of QC. By providing a rigorous mathematical framework, formal methods help to specify, develop, and verify systems with high precision. In genomic data analysis, formal specification languages can precisely (1) define the behavior and properties of quantum algorithms designed to identify genetic markers associated with diseases. Model checking tools can systematically explore all possible states of the algorithm to (2) ensure it behaves correctly under all conditions, while theorem proving techniques provide mathematical (3) proof that the algorithm meets its specified properties, ensuring accuracy and reliability. Additionally, formal optimization techniques can (4) enhance the efficiency and performance of quantum algorithms by reducing resource usage, such as the number of qubits and gate operations. Therefore, we posit that formal methods can significantly contribute to enabling QC to realize its full potential as a game changer in precision medicine."
2502.19399,"Many combinatorial problems can be mapped to Ising machines, i.e., networks of coupled oscillators that settle to a minimum-energy ground state, from which the problem solution is inferred. This work proposes DROID, a novel event-driven method for simulating the evolution of a CMOS Ising machine to its ground state. The approach is accurate under general delay-phase relations that include the effects of the transistor nonlinearities and is computationally efficient. On a realistic-size all-to-all coupled ring oscillator array, DROID is nearly four orders of magnitude faster than a traditional HSPICE simulation in predicting the evolution of a coupled oscillator system and is demonstrated to attain a similar distribution of solutions as the hardware."
2502.19528,"This paper introduces a novel approach to demand estimation that utilizes partial observations of segment-level track counts. Building on established simulation-based demand estimation methods, we present a modified formulation that integrates sample track counts as a regularization term. This approach effectively addresses the underdetermination challenge in demand estimation, moving beyond the conventional reliance on a prior OD matrix. The proposed formulation aims to preserve the distribution of the observed track counts while optimizing the demand to align with observed path-level travel times. We tested this approach on Seattle's highway network with various congestion levels. Our findings reveal significant enhancements in the solution quality, particularly in accurately recovering ground truth demand patterns at both the OD and segment levels."
2502.20403,"Adversarial robustness in quantum classifiers is a critical area of study, providing insights into their performance compared to classical models and uncovering potential advantages inherent to quantum machine learning. In the NISQ era of quantum computing, circuit cutting is a notable technique for simulating circuits that exceed the qubit limitations of current devices, enabling the distribution of a quantum circuit's execution across multiple quantum processing units through classical communication. We examine how partitioning quantum classifiers through circuit cutting increase their susceptibility to adversarial attacks, establishing a link between attacking the state preparation channels in wire cutting and implementing adversarial gates within intermediate layers of a quantum classifier. We then proceed to study the latter problem from both a theoretical and experimental perspective."
2502.21104,"The OmpSs-2 programming model is used in HPC programs to parallelize code and offload code to accelerators. In this work, we extend the offloading capability to quantum computers. We explain the necessary changes to the Clang compiler and the Nanos6 runtime, which are both part of OmpSs-2. In addition, we develop a simulator that simulates a quantum computer in the network and receives the jobs offloaded by the runtime. Four detailed examples show how our programming model can be used to write hybrid quantum-classical software. The examples are random number generation, a parameter scan using the mean-field ansatz, a variational algorithm using this ansatz, and handwritten digit recognition using a hybrid convolutional neural network."
2503.00447,"Single ferroelectric memcapacitor-based time-domain (TD) content-addressable memory (CAM) is proposed and experimentally demonstrated for high reliability and density. The proposed TD CAM features the symmetric capacitance-voltage characteristics of a ferroelectric memcapacitor with a gated p-i-n diode structure. This CAM performs search operations based on the variable capacitance of cells. The propagation delay of the TD CAM output signal is linearly correlated with the Hamming distance (HD) between input and output vectors. The proposed TD CAM array exhibits exceptional reliability in HD computation and in-memory search tasks owing to this linearity, considerably outperforming the conventional nonlinear voltage-domain CAM."
2503.01177,"In recent years, hardware implementations of Ising machines have emerged as a viable alternative to quantum computing for solving hard optimization problems among other applications. Unlike quantum hardware, dense connectivity can be achieved in classical systems. However, we show that dense connectivity leads to severe frequency slowdowns and interconnect congestion scaling unfavorably with system sizes. As a scalable solution, we propose a systematic sparsification method for dense graphs by introducing copy nodes to limit the number of neighbors per graph node. In addition to solving interconnect congestion, this approach enables constant frequency scaling where all spins in a network can be updated in constant time. On the other hand, sparsification introduces new difficulties, such as constraint-breaking between copied spins and increased convergence times to solve optimization problems, especially if exact ground states are sought. Relaxing the exact solution requirements, we find that the overheads in convergence times are milder. We demonstrate these ideas by designing probabilistic bit Ising machines using ASAP7 (a predictive 7nm FinFET technology model) process design kits as well as Field Programmable Gate Array (FPGA)-based implementations. Finally, we show how formulating problems in naturally sparse networks (e.g., by invertible logic) sidesteps challenges introduced by sparsification methods. Our results are applicable to a broad family of Ising machines using different hardware implementations."
2503.01756,"Advancements in nanosatellite technology lead to more Earth-observation satellites in low-Earth orbit. We explore using nanosatellite constellations to achieve low-latency detection for time-critical events, such as forest fires, oil spills, and floods. The detection latency comprises three parts: capture, compute and transmission. Previous solutions reduce transmission latency, but we find that the bottleneck is capture latency, accounting for more than 90% of end-to-end latency. We present a measurement study on how various satellite and ground station design factors affect latency. We offer design guidance to operators on how to choose satellite orbital configurations and design an algorithm to choose ground station locations. For six use cases, our design guidance reduces end-to-end latency by 5.6 to 8.2 times compared to the existing system."
2503.02033,"Advances in novel hardware devices and architectures allow Spiking Neural Network evaluation using ultra-low power, mixed-signal, memristor crossbar arrays. As individual network sizes quickly scale beyond the dimensional capabilities of single crossbars, networks must be mapped onto multiple crossbars. Crossbar sizes within modern Memristor Crossbar Architectures are determined predominately not by device technology but by network topology; more, smaller crossbars consume less area thanks to the high structural sparsity found in larger, brain-inspired SNNs. Motivated by continuing increases in SNN sparsity due to improvements in training methods, we propose utilizing heterogeneous crossbar sizes to further reduce area consumption. This approach was previously unachievable as prior compiler studies only explored solutions targeting homogeneous MCAs. Our work improves on the state-of-the-art by providing Integer Linear Programming formulations supporting arbitrarily heterogeneous architectures. By modeling axonal interactions between neurons our methods produce better mappings while removing inhibitive a priori knowledge requirements. We first show a 16.7-27.6% reduction in area consumption for square-crossbar homogeneous architectures. Then, we demonstrate 66.9-72.7% further reduction when using a reasonable configuration of heterogeneous crossbar dimensions. Next, we present a new optimization formulation capable of minimizing the number of inter-crossbar routes. When applied to solutions already near-optimal in area an 11.9-26.4% routing reduction is observed without impacting area consumption. Finally, we present a profile-guided optimization capable of minimizing the number of runtime spikes between crossbars. Compared to the best-area-then-route optimized solutions we observe a further 0.5-14.8% inter-crossbar spike reduction while requiring 1-3 orders of magnitude less solver time."
2503.02167,"Digital twin technology is a transformative innovation driving the digital transformation and intelligent optimization of manufacturing systems. By integrating real-time data with computational models, digital twins enable continuous monitoring, simulation, prediction, and optimization, effectively bridging the gap between the physical and digital worlds. Recent advancements in communication, computing, and control technologies have accelerated the development and adoption of digital twins across various industries. However, significant challenges remain, including limited data for accurate system modeling, inefficiencies in system analysis, and a lack of explainability in the interactions between physical and digital systems. The rise of large language models (LLMs) offers new avenues to address these challenges. LLMs have shown exceptional capabilities across diverse domains, exhibiting strong generalization and emergent abilities that hold great potential for enhancing digital twins. This paper provides a comprehensive review of recent developments in LLMs and their applications to digital twin modeling. We propose a unified description-prediction-prescription framework to integrate digital twin modeling technologies and introduce a structured taxonomy to categorize LLM functionalities in these contexts. For each stage of application, we summarize the methodologies, identify key challenges, and explore potential future directions. To demonstrate the effectiveness of LLM-enhanced digital twins, we present an LLM-enhanced enterprise digital twin system, which enables automatic modeling and optimization of an enterprise. Finally, we discuss future opportunities and challenges in advancing LLM-enhanced digital twins, offering valuable insights for researchers and practitioners in related fields."
2503.0417,"In intelligent transportation systems (ITSs), incorporating pedestrians and vehicles in-the-loop is crucial for developing realistic and safe traffic management solutions. However, there is falls short of simulating complex real-world ITS scenarios, primarily due to the lack of a digital twin implementation framework for characterizing interactions between pedestrians and vehicles at different locations in different traffic environments. In this article, we propose a surveillance video assisted federated digital twin (SV-FDT) framework to empower ITSs with pedestrians and vehicles in-the-loop. Specifically, SVFDT builds comprehensive pedestrian-vehicle interaction models by leveraging multi-source traffic surveillance videos. Its architecture consists of three layers: (i) the end layer, which collects traffic surveillance videos from multiple sources; (ii) the edge layer, responsible for semantic segmentation-based visual understanding, twin agent-based interaction modeling, and local digital twin system (LDTS) creation in local regions; and (iii) the cloud layer, which integrates LDTSs across different regions to construct a global DT model in realtime. We analyze key design requirements and challenges and present core guidelines for SVFDT's system implementation. A testbed evaluation demonstrates its effectiveness in optimizing traffic management. Comparisons with traditional terminal-server frameworks highlight SV-FDT's advantages in mirroring delays, recognition accuracy, and subjective evaluation. Finally, we identify some open challenges and discuss future research directions."
2503.04239,"Molecular docking is a critical process for drug discovery and challenging due to the complexity and size of biomolecular systems, where the optimal binding configuration of a drug to a target protein is determined. Hybrid classical-quantum computing techniques offer a novel approach to address these challenges. The Quantum Approximate Optimization Algorithm (QAOA) and its variations are hybrid classical-quantum techniques, and a promising tool for combinatorial optimization challenges. This paper presents a Digitized Counterdiabatic QAOA (DC-QAOA) approach to molecular docking. Simulated quantum runs were conducted on a GPU cluster. We examined 14 and 17 nodes instances - to the best of our knowledge the biggest published instance is 12-node at Ding et al. and we present the results. Based on computational results, we conclude that binding interactions represent the anticipated exact solution. Additionally, as the size of the examined instance increases, the computational times exhibit a significant escalation."
2503.04387,"The synchronization of digital twins (DT) serves as the cornerstone for effective operation of the DT framework. However, the limitations of channel capacity can greatly affect the data transmission efficiency of wireless communication. Unlike traditional communication methods, semantic communication transmits the intended meanings of physical objects instead of raw data, effectively saving bandwidth resource and reducing DT synchronization latency. Hence, we are committed to integrating semantic communication into the DT synchronization framework within the mobile edge computing system, aiming to enhance the DT synchronization efficiency of user devices (UDs). Our goal is to minimize the average DT synchronization latency of all UDs by jointly optimizing the synchronization strategy, transmission power of UDs, and computational resource allocation for both UDs and base station. The formulated problem involves sequential decision-making across multiple coherent time slots. Furthermore, the mobility of UDs introduces uncertainties into the decision-making process. To solve this challenging optimization problem efficiently, we propose a soft actor-critic-based deep reinforcement learning algorithm to optimize synchronization strategy and resource allocation. Numerical results demonstrate that our proposed algorithm can reduce synchronization latency by up to 13.2\% and improve synchronization efficiency compared to other benchmark schemes."
2503.04397,"Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) can expand the coverage of mobile edge computing (MEC) services by reflecting and transmitting signals simultaneously, enabling full-space coverage. The orientation of the STAR-RIS plays a crucial role in optimizing the gain of received and transmitted signals, and a rotatable STAR-RIS offers potential enhancement for MEC systems. This paper investigates a rotatable STAR-RIS-assisted MEC system, operated under three protocols, namely energy splitting, mode switching, and time switching. The goal is to minimize energy consumption for multiple moving user devices through the joint optimization of STAR-RIS configurations, orientation, computation resource allocation, transmission power, and task offloading strategies. Considering the mobility of user devices, we model the original optimization problem as a sequential decision-making process across multiple time slots. The high-dimensional, highly coupled, and nonlinear nature makes it a challenging non-convex decision-making problem for traditional optimization algorithms. Therefore, a deep reinforcement learning (DRL) approach is employed, specifically utilizing soft actor-critic algorithm to train the DRL model. Simulation results demonstrate that the proposed algorithm outperforms the benchmarks in both convergence speed and energy efficiency, while reducing energy consumption by up to 52.7\% compared to the fixed STAR-RIS scheme. Among three operating protocols, the energy splitting yields the best performance."
2503.05629,"Human Activity Recognition has gained significant attention due to its diverse applications, including ambient assisted living and remote sensing. Wearable sensor-based solutions often suffer from user discomfort and reliability issues, while video-based methods raise privacy concerns and perform poorly in low-light conditions or long ranges. This study introduces a Frequency-Modulated Continuous Wave radar-based framework for human activity recognition, leveraging a 60 GHz radar and multi-dimensional feature maps. Unlike conventional approaches that process feature maps as images, this study feeds multi-dimensional feature maps -- Range-Doppler, Range-Azimuth, and Range-Elevation -- as data vectors directly into the machine learning (SVM, MLP) and deep learning (CNN, LSTM, ConvLSTM) models, preserving the spatial and temporal structures of the data. These features were extracted from a novel dataset with seven activity classes and validated using two different validation approaches. The ConvLSTM model outperformed conventional machine learning and deep learning models, achieving an accuracy of 90.51% and an F1-score of 87.31% on cross-scene validation and an accuracy of 89.56% and an F1-score of 87.15% on leave-one-person-out cross-validation. The results highlight the approach's potential for scalable, non-intrusive, and privacy-preserving activity monitoring in real-world scenarios."
2503.06203,"While a plethora of machine learning (ML) models are currently available, along with their implementation on disparate platforms, there is hardly any verifiable ML code which can be executed on public blockchains. We propose a novel approach named LMST that enables conversion of the inferencing path of an ML model as well as its weights trained off-chain into Solidity code using Large Language Models (LLMs). Extensive prompt engineering is done to achieve gas cost optimization beyond mere correctness of the produced code, while taking into consideration the capabilities and limitations of the Ethereum Virtual Machine. We have also developed a proof of concept decentralized application using the code so generated for verifying the accuracy claims of the underlying ML model. An extensive set of experiments demonstrate the feasibility of deploying ML models on blockchains through automated code translation using LLMs."
2503.06304,"The Last Level Cache (LLC) is the processor's critical bridge between on-chip and off-chip memory levels - optimized for high density, high bandwidth, and low operation energy. To date, high-density (HD) SRAM has been the conventional device of choice; however, with the slowing of transistor scaling, as reflected in the industry's almost identical HD SRAM cell size from 5 nm to 3 nm, alternative solutions such as 3D stacking with advanced packaging like hybrid bonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands necessitate ultra-large on-chip caches to decrease costly off-chip memory movement, pushing the exploration of device technology toward monolithic 3D (M3D) integration where transistors can be stacked in the back-end-of-line (BEOL) at the interconnect level. M3D integration requires fabrication techniques compatible with a low thermal budget (<400 degC). Among promising BEOL device candidates are amorphous oxide semiconductor (AOS) transistors, particularly desirable for their ultra-low leakage (<fA/um), enabling persistent data retention (>seconds) when used in a gain-cell configuration. This paper examines device, circuit, and system-level tradeoffs when optimizing BEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache early-exploration tool, NS-Cache, is developed to model caches in advanced 7 and 3 nm nodes and is integrated with the Gem5 simulator to systematically benchmark the impact of the newfound density/performance when compared to HD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
2503.0846,"The rapid development of Industry 4.0 technologies requires robust and comprehensive standardization to ensure interoperability, safety and efficiency in the Industry of the Future. This paper examines the fundamental role and functionality of standardization, with a particular focus on its importance in Europe's regulatory framework. Based on this, selected topics in context of standardization activities in context intelligent manufacturing and digital twins are highlighted and, by that, an overview of the Industry 4.0 standards framework is provided. This paper serves both as an informative guide to the existing standards in Industry 4.0 with respect to Artificial Intelligence and Digital Twins, and as a call to action for increased cooperation between standardization bodies and the research community. By fostering such collaboration, we aim to facilitate the continued development and implementation of standards that will drive innovation and progress in the manufacturing sector."
2503.09237,"We attempt to take a comprehensive look at the challenges of representing the spatio-temporal structures and dynamic processes defining a city's overall characteristics. For the task of urban planning and urban operation, we take the stance that even if the necessary representations of these structures and processes can be achieved, the most important representation of the relevant mindsets of the citizens are, unfortunately, mostly neglected.After a review of major ""traditional"" urban models of structures behind urban scale, form, and dynamics, we turn to major recent modeling approaches triggered by recent advances in AI that enable multi-modal generative models. Some of these models can create representations of geometries, networks and images, and reason flexibly at a human-compatible semantic level. They provide huge amounts of knowledge extracted from Terabytes of text and image documents and cover the required rich representation spectrum including geographic knowledge by different knowledge sources, degrees of granularity and scales.We then discuss what these new opportunities mean for the modeling challenges posed by cities, in particular with regard to the role and impact of citizens and their interactions within the city infrastructure. We propose to integrate these possibilities with existing approaches, such as agent-based models, which opens up new modeling spaces including rich citizen models which are able to also represent social interactions.Finally, we put forward some thoughts about a vision of a ""social AI in a city ecosystem"" that adds relevant citizen models to state-of-the-art structural and process models. This extended city representation will enable urban planners to establish citizen-oriented planning of city infrastructures for human culture, city resilience and sustainability."
2503.10866,"Beyond diagonal reconfigurable intelligent surfaces (BD-RIS) have emerged as a transformative technology for enhancing wireless communication by intelligently manipulating the propagation environment. Its interconnected elements offer enhanced control over signal redirection, making it a promising solution for integrated terrestrial and non-terrestrial networks (NTNs). This paper explores the potential of BD-RIS in improving cognitive radio enabled multilayer non-terrestrial networks. We formulate a joint optimization problem that maximizes the achievable spectral efficiency by optimizing BD-RIS phase shifts and secondary transmitter power allocation while controlling the interference temperature from the secondary network to the primary network. To solve this problem efficiently, we decouple the original problem and propose a novel solution based on an alternating optimization approach. Simulation results demonstrate the effectiveness of BD-RIS in cognitive radio enabled multilayer NTNs."
2503.11068,"Pharmaceutical process design and development for generic, innovative, or personalized drugs have always been a time-consuming, costly, rigorous process, that involves multi-stage evaluation for better quality control and assurance. Large language models (LLMs), a type of generative artificial intelligence system, can augment laboratory research in the pharmaceutical engineering process by helping scientists to extract knowledge from literature, design parameters, and collect and interpret experimental data ultimately accelerating scientific discovery. LLMs with prompt engineering technologies change the researchers thinking protocol from traditional empirical knowledge to streamlined thinking that connects the performance and structured parameters together. In this work, we investigate and evaluate how prompt engineering technologies can enhance the drug design process from different strategies such as zero-shot, few-shot, chain-of-thought, etc. The dissolution profile for specific drugs is predicted and suggested from the LLMs model. Furthermore, the fundamental physical properties such as PSD, aspect ratio, and specific surface area could be inversely designed from the LLMs model. Finally, all the results are evaluated and validated by real-world cases to prove the reliability of prompt engineering techniques. Initial evaluations show an MSE of 23.61 and R2 of 0.97 in zero-shot, an MSE of 114.89 and R2 of 0.90 in zero-shot-CoT, an MSE of 57.0 and R2 of 0.92 in few-shot, a MSE of 22.56 and R2 of 0.97 in few-shot-CoT and a MSE of 10.56 and R2 of 0.99 with the involvement of RAG. This work breaks down any barriers in developing a systematic framework where LLMs assist in formulation design, process control, and decision-making. Finally, we conclude the work by discussing open challenges and future research directions in pharmaceutical processes."
2503.11691,"Advancements in fabrication methods have shaped new computing device technologies. Among these methods, depositing electrical contacts to the channel material is fundamental to device characterization. Novel layered and two-dimensional (2D) materials are promising for next-generation computing electronic channel materials. Direct-write printing of conductive inks is introduced as a surprisingly effective, significantly faster, and cleaner method to contact different classes of layered materials, including graphene (semi-metal), MoS2 (semiconductor), Bi-2212 (superconductor), and Fe5GeTe2 (metallic ferromagnet). Based on the electrical response, the quality of the printed contacts is comparable to what is achievable with resist-based lithography techniques. These devices are tested by sweeping gate voltage, temperature, and magnetic field to show that the materials remain pristine post-processing. This work demonstrates that direct-write printing is an agile method for prototyping and characterizing the electrical properties of novel layered materials."
2503.12709,"Modular design maximizes utility by using standardized components in large-scale systems. From a manufacturing perspective, it supports green technology by reducing material waste and improving reusability. Industrially, it offers economic benefits through economies of scale, making it a practical design strategy. Typically, modularization selects a representative design from predefined candidates to meet all performance requirements. However, achieving effective modularization in mechanical mechanisms presents challenges. First, mechanisms depend on geometric relationships for functional motion, and varying loads lead to different optimal parameters, complicating representative design selection. Second, the chosen design often exceeds optimal parameters, causing over-specification and performance deviations, which worsen as scale increases. To address this, we propose a modular mechanism design framework using surrogate-based optimization. This approach finds optimal designs for large-scale systems and partitions them into groups, each assigned an optimized design. This multi-objective optimization (MOO) problem balances economies of scale and performance consistency. Unlike conventional methods based on predefined candidates and simple grouping, our framework optimizes design variables flexibly for modularization. Additionally, we analyze manufacturing cost parameters to develop a decision support system for selecting optimal strategies in diverse design scenarios. This enhances maintainability, improves interchangeability, and fosters environmentally sustainable manufacturing."
2503.13809,"The Immersive Archive is an initiative dedicated to preserve and restore the groundbreaking works from across Extended Reality (XR) history. Originating at the University of Southern California's Mobile and Environmental Media Lab, this archive is committed to developing and exhibiting simulations of influential XR devices that have shaped immersive media over time. This paper examines the challenges and strategies involved in archiving seminal XR technologies, with a focus on Morton Heilig's Sensorama and Ivan Sutherland's HeadMounted Display. As pioneering prototypes in virtual and augmented reality, these devices provide valuable insights into the evolution of immersive media, highlighting both technological innovation and sensory experimentation. Through collaborative archival efforts with institutions such as the HMH Moving Image Archive at University of Southern California and the Computer History Museum, this research integrates media archaeology with digital preservation techniques. Emphasis is placed on documentation practices, restoration of physical artifacts and developing simulations of these historic experiences for contemporary virtual reality platforms. Our interdisciplinary approach to archival methodologies, which captures the multisensory and interactive qualities of these pioneering devices, has been instrumental in developing a framework for future immersive media preservation initiatives. By preserving the immersive essence of these early experiences, we lay the groundwork for future generations to explore and learn from the origins of immersive media. Safeguarding this rich legacy is essential to ensure these visionary works continue to inspire and shape the future of media landscapes."
2503.13819,"The Internet of Things (IoT) in the sixth generation (6G) era is envisioned to evolve towards intelligence, ubiquity, and self-optimization. Large language models (LLMs) have demonstrated remarkable generalization capabilities across diverse domains, including natural language processing (NLP), computer vision (CV), and beyond. In this article, we propose an LLM-empowered IoT architecture for 6G networks to achieve intelligent autonomy while supporting advanced IoT applications. LLMs are pushed to the edge of the 6G network to support the synergy of LLMs and IoT. LLM solutions are tailored to both IoT application requirements and IoT management needs, i.e., LLM for IoT. On the other hand, edge inference and edge fine-tuning are discussed to support the deployment of LLMs, i.e., LLM on IoT. Furthermore, we propose a memory-efficient split federated learning (SFL) framework for LLM fine-tuning on heterogeneous IoT devices that alleviates memory pressures on both IoT devices and the edge server while achieving comparable performance and convergence time. Finally, a case study is presented, followed by a discussion about open issues of LLM-empowered IoT for 6G networks."
2503.14126,"We demonstrate the first hardware implementation of an oscillatory neural network (ONN) utilizing resistive memory (ReRAM) for coupling elements. A ReRAM crossbar array chip, integrated into the Back End of Line (BEOL) of CMOS technology, is leveraged to establish dense coupling elements between oscillator neurons, allowing phase-encoded analog information to be processed in-memory. We also realize an ONN architecture design with the coupling ReRAM array. To validate the architecture experimentally, we present a conductive metal oxide (CMO)/HfOx ReRAM array chip integrated with a 2-by-2 ring oscillator-based network. The system successfully retrieves patterns through correct binary phase locking. This proof of concept underscores the potential of ReRAM technology for large-scale, integrated ONNs."
2503.14186,"Teleoperated driving enables remote human intervention in autonomous vehicles, addressing challenges in complex driving environments. However, its effectiveness depends on ultra-low latency, high-reliability communication. This paper evaluates teleoperated driving over 5G networks, analyzing key performance metrics such as glass-to-glass (G2G) latency, RTT and steering command delay. Using a real-world testbed with a Kia Soul EV and a remote teleoperation platform, we assess the feasibility and limitations of 5G-enabled teleoperated driving. Our system achieved an average G2G latency of 202ms and an RTT of 47ms highlighting the G2G latency as the critical bottleneck. The steering control proved to be mostly accurate and responsive. Finally, this paper provides recommendations and outlines future work to improve future teleoperated driving deployments for safer and more reliable autonomous mobility."
2503.16823,"In this paper, we propose a novel federated framework for constructing the digital twin (DT) model, referring to a living and self-evolving visualization model empowered by artificial intelligence, enabled by distributed sensing under edge-cloud collaboration. In this framework, the DT model to be built at the cloud is regarded as a global one being split into and integrating from multiple functional components, i.e., partial-DTs, created at various edge servers (ESs) using feature data collected by associated sensors. Considering time-varying DT evolutions and heterogeneities among partial-DTs, we formulate an online problem that jointly and dynamically optimizes partial-DT assignments from the cloud to ESs, ES-sensor associations for partial-DT creation, and as well as computation and communication resource allocations for global-DT integration. The problem aims to maximize the constructed DT's model quality while minimizing all induced costs, including energy consumption and configuration costs, in long runs. To this end, we first transform the original problem into an equivalent hierarchical game with an upper-layer two-sided matching game and a lower-layer overlapping coalition formation game. After analyzing these games in detail, we apply the Gale-Shapley algorithm and particularly develop a switch rules-based overlapping coalition formation algorithm to obtain short-term equilibria of upper-layer and lower-layer subgames, respectively. Then, we design a deep reinforcement learning-based solution, called DMO, to extend the result into a long-term equilibrium of the hierarchical game, thereby producing the solution to the original problem. Simulations show the effectiveness of the introduced framework, and demonstrate the superiority of the proposed solution over counterparts."
2503.17548,"Analog Ising machines (IMs) occupy an increasingly prominent area of computer architecture research, offering high-quality and low latency/energy solutions to intractable computing tasks. However, IMs have a fixed capacity, with little to no utility in out-of-capacity problems. Previous works have proposed parallel, multi-IM architectures to circumvent this limitation. In this work we theoretically and numerically investigate tradeoffs in parallel IM networks to guide researchers in this burgeoning field. We propose formal models of parallel IM excution models, then provide theoretical guarantees for probabilistic convergence. Numerical experiments illustrate our findings and provide empirical insight into high and low synchronization frequency regimes. We also provide practical heuristics for parameter/model selection, informed by our theoretical and numerical findings."
2503.1977,"The aim of this paper is to give an overview of brain organoid computing, its characteristics, challenges, as well as possible advantages for future applications in the field of artificial intelligence. An important part is the extensive bibliography covering all relevant aspects and questions on this topic. Brain organoids - three-dimensional in vitro neural structures derived from human stem cells - have recently garnered attention not only in medical research but also as potential substrates for unconventional computing. Their biological nature allows them to exhibit learning behavior, plasticity, and parallel information processing, making them fundamentally different from traditional silicon-based systems. This opens up new perspectives on how intelligent systems might be designed in the future. Using brain organoids for computing presents a possible pathway towards more adaptive, energy-efficient, and biologically inspired forms of AI. However, challenges persist, particularly regarding lifespan, interfacing, reproducibility, and ethical concerns regarding the use of human-derived tissue. This paper aims to provide a foundational understanding for researchers exploring the convergence of human biology and computation."
2503.23298,"Emergence, the phenomenon of a rapid performance increase once the model scale reaches a threshold, has achieved widespread attention recently. The literature has observed that monosemantic neurons in neural networks gradually diminish as the model scale increases. Subsequently, Learning From Emergence is proposed to actively inhibit monosemantic neurons in relatively small neural networks (e.g., BERT and Swin-Transformer) for promoting model performance with fine-tuning. However, to ultimately achieve emergence, it is demanding to support the monosemantic neuron inhibition in the pretraining phase of large-scale models. Thus, this work further pushes the boundary of this research direction to be Learning Towards Emergence (L2E) and enables the training and validating of the impact of inhibiting monosemantic neurons on larger pre-trained neural networks (e.g., Pythia-70M, 410M, and 2.8B). More specifically, to bridge the gap in current research, we first conduct experiments on models of various scales (up to 6.9B) to validate the monosemantic ideas. Then, we present a novel method L2E to address the inefficient monosemantic neuron retrieval and ineffective monosemantic neuron inhibition when existing methods are applied in the pretraining phase of large-scale models. It employs an adjustable thresholding technique for efficient neuron retrieval, incorporates a False Killing Rate metric to assess inhibition effects, and proposes a regularization-style inhibition approach, which addresses the limitations of previous approaches in both efficiency and effectiveness. Experimental results demonstrate the effectiveness of L2E's monosemantic neuron inhibition and its efficiency in implementation with large-scale models."
2503.23685,"Neuromorphic vision sensors require efficient real-time pattern recognition, yet conventional architectures struggle with energy and latency constraints. Here, we present a novel in-situ spatiotemporal sequence detector that leverages vertical NAND storage to achieve massively parallel pattern detection. By encoding each cell with two single-transistor-based multi-level cell (MLC) memory elements, such as ferroelectric field-effect transistors (FeFETs), and mapping a pixel's temporal sequence onto consecutive word lines (WLs), we enable direct temporal pattern detection within NAND strings. Each NAND string serves as a dedicated reference for a single pixel, while different blocks store patterns for distinct pixels, allowing large-scale spatial-temporal pattern recognition via simple direct bit-line (BL) sensing, a well-established operation in vertical NAND storage. We experimentally validate our approach at both the cell and array levels, demonstrating that vertical NAND-based detector achieves more than six orders of magnitude improvement in energy efficiency and more than three orders of magnitude reduction in latency compared to conventional CPU-based methods. These findings establish vertical NAND storage as a scalable and energy-efficient solution for next-generation neuromorphic vision processing."
2503.23966,"Quantum or quantum-inspired Ising machines have recently shown promise in solving combinatorial optimization problems in a short time. Real-world applications, such as time division multiple access (TDMA) scheduling for wireless multi-hop networks and financial trading, require solving those problems sequentially where the size and characteristics change dynamically. However, using Ising machines involves challenges to shorten system-wide latency due to the transfer of large Ising model or the cloud access and to determine the parameters for each problem. Here we show a combinatorial optimization method using embedded Ising machines, which enables solving diverse problems at high speed without runtime parameter tuning. We customize the algorithm and circuit architecture of the simulated bifurcation-based Ising machine to compress the Ising model and accelerate computation and then built a machine learning model to estimate appropriate parameters using extensive training data. In TDMA scheduling for wireless multi-hop networks, our demonstration has shown that the sophisticated system can adapt to changes in the problem and showed that it has a speed advantage over conventional methods."
2503.24285,"In this paper, we explore the potential for quantum annealing to solve realistic routing problems. We focus on two NP-Hard problems, including the Traveling Salesman Problem with Time Windows and the Capacitated Vehicle Routing Problem with Time Windows. We utilize D-Wave's Quantum Annealer and Constrained Quadratic Model (CQM) solver within a hybrid framework to solve these problems. We demonstrate that while the CQM solver effectively minimizes route costs, it struggles to maintain time window feasibility as the problem size increases. To address this limitation, we implement a heuristic method that fixes infeasible solutions through a series of swapping operations. Testing on benchmark instances shows our method achieves promising results with an average optimality gap of 3.86%."
2503.24301,"Quantum computing holds transformative potential for optimizing large-scale drone fleet operations, yet its near-term limitations necessitate hybrid approaches blending classical and quantum techniques. This work introduces Quantum Unmanned Aerial Delivery Routing Optimization (QUADRO), a novel hybrid framework addressing the Energy-Constrained Capacitated Unmanned Aerial Vehicle Routing Problem and the Unmanned Aerial Vehicle Scheduling Problem. By formulating these challenges as Quadratic Unconstrained Binary Optimization problems, QUADRO leverages the Quantum Approximate Optimization Algorithm for routing and scheduling, enhanced by classical heuristics and post-processing. We minimize total transit time in routing, considering payload and battery constraints, and optimize makespan scheduling across various drone fleets. Evaluated on adapted Augerat benchmarks (16-51 nodes), QUADRO competes against classical and prior hybrid methods, achieving scalable solutions with fewer than one hundred qubits. The proposed results underscore the viability of hybrid quantum-classical strategies for real-world drone logistics, paving the way for quantum-enhanced optimization in the Noisy Intermediate Scale Quantum era."
2504.00298,"In this paper, we investigate the application of Time-Reversal Symmetry (TRS) in Quantum Wireless Sensor Networks (QWSNs) to enhance communication performance. QWSNs combine quantum communication principles with traditional wireless sensor network technologies, offering the potential for improved security, energy efficiency, and signal quality. TRS, a concept from signal processing and quantum mechanics, focuses transmitted signals back toward the receiver, compensating for noise, interference, and fading effects. By applying TRS to QWSNs, we aim to optimize throughput, reduce latency, and enhance energy efficiency in challenging communication environments. The paper presents a theoretical framework for integrating TRS into QWSNs, including mathematical formulations for its impact on key network performance metrics. We also consider real-world channel models, such as Rayleigh and Rician fading, along with network interference, to demonstrate how TRS can improve communication in practical settings. The discussion extends to the broader potential of TRS in quantum communication systems, particularly in **Quantum Key Distribution (QKD)**, **quantum entanglement**, and **quantum networking** applications. The findings highlight TRS as a promising approach to optimize quantum communication in sensor networks and provide a foundation for future research in the intersection of quantum technologies and wireless networks."
2504.00329,"We present an oscillator model with both phase and amplitude dynamics for oscillator-based Ising machines (OIMs). The model targets combinatorial optimization problems with polynomial cost functions of arbitrary order and addresses fundamental limitations of previous OIM models through a mathematically rigorous formulation with a well-defined energy function and corresponding dynamics. The model demonstrates monotonic energy decrease and reliable convergence to low-energy states. Empirical evaluations on 3-SAT problems show significant performance improvements over existing phase-amplitude models. Additionally, we propose a flexible, generalizable framework for designing higher-order oscillator interactions, from which we derive a practical method for oscillator binarization without compromising performance. This work strengthens both the theoretical foundation and practical applicability of oscillator-based Ising machines for complex optimization problems."
2504.00444,"As a key enabling technology of the Internet of Things (IoT) and 5G communication networks, millimeter wave (mmWave) backscatter has undergone noteworthy advancements and brought significant improvement to prevailing sensing and communication systems. Past few years have witnessed growing efforts in innovating mmWave backscatter transmitters (e.g., tags and metasurfaces) and the corresponding techniques, which provide efficient information embedding and fine-grained signal manipulation for mmWave backscatter technologies. These efforts have greatly enabled a variety of appealing applications, such as long-range localization, roadside-to-vehicle communication, coverage optimization and large-scale identification. In this paper, we carry out a comprehensive survey to systematically summarize the works related to the topic of mmWave backscatter. Firstly, we introduce the scope of this survey and provide a taxonomy to distinguish two categories of mmWave backscatter research based on the operating principle of the backscatter transmitter: modulation-based and relay-based. Furthermore, existing works in each category are grouped and introduced in detail, with their common applications, platforms and technologies, respectively. Finally, we elaborate on potential directions and discuss related surveys in this area."
2504.0156,"Recent research at the intersection of quantum computing and routing problems has been highly prolific. Much of this work focuses on classical problems such as the Traveling Salesman Problem and the Vehicle Routing Problem. The practical applicability of these problems depends on the specific objectives and constraints considered. However, it is undeniable that translating complex real-world requirements into these classical formulations often proves challenging. In this paper, we resort to our previously published quantum-classical technique for addressing real-world-oriented routing problems, known as Quantum for Real Package Delivery (Q4RPD), and elaborate on solving additional realistic problem instances. Accordingly, this paper emphasizes the following characteristics: i) simultaneous pickup and deliveries, ii) time-windows, and iii) mobility restrictions by vehicle type. To illustrate the application of Q4RPD, we have conducted an experimentation comprising seven instances, serving as a demonstration of the newly developed features."
2504.02162,"The integration of RIS into UAV networks presents a transformative solution for achieving energy-efficient and reliable communication, particularly within the rapidly expanding low-altitude economy (LAE). As UAVs facilitate diverse aerial services-spanning logistics to smart surveillance-their limited energy reserves create significant challenges. RIS effectively addresses this issue by dynamically shaping the wireless environment to enhance signal quality, reduce power consumption, and extend UAV operation time, thus enabling sustainable and scalable deployment across various LAE applications. This survey provides a comprehensive review of RIS-assisted UAV networks, focusing on energy-efficient design within LAE applications. We begin by introducing the fundamentals of RIS, covering its operational modes, deployment architectures, and roles in both terrestrial and aerial environments. Next, advanced EE-driven strategies for integrating RIS and UAVs. Techniques such as trajectory optimization, power control, beamforming, and dynamic resource management are examined. Emphasis is placed on collaborative solutions that incorporate UAV-mounted RIS, wireless energy harvesting (EH), and intelligent scheduling frameworks. We further categorize RIS-enabled schemes based on key performance objectives relevant to LAE scenarios. These objectives include sum rate maximization, coverage extension, QoS guarantees, secrecy rate improvement, latency reduction, and age of information (AoI) minimization. The survey also delves into RIS-UAV synergy with emerging technologies like MEC, NOMA, V2X communication, and WPT. These technologies are crucial to the LAE ecosystem. Finally, we outline open research challenges and future directions, emphasizing the critical role of energy-aware, RIS-enhanced UAV networks in shaping scalable, sustainable, and intelligent infrastructures within the LAE."
2504.02197,"The concept of an AI assistant for task guidance is rapidly shifting from a science fiction staple to an impending reality. Such a system is inherently complex, requiring models for perceptual grounding, attention, and reasoning, an intuitive interface that adapts to the performer's needs, and the orchestration of data streams from many sensors. Moreover, all data acquired by the system must be readily available for post-hoc analysis to enable developers to understand performer behavior and quickly detect failures. We introduce TIM, the first end-to-end AI-enabled task guidance system in augmented reality which is capable of detecting both the user and scene as well as providing adaptable, just-in-time feedback. We discuss the system challenges and propose design solutions. We also demonstrate how TIM adapts to domain applications with varying needs, highlighting how the system components can be customized for each scenario."
2504.03925,"Time-domain nonvolatile in-memory computing (TD-nvIMC) offers a promising pathway to reduce data movement and improve energy efficiency by encoding computation in delay rather than voltage or current. This work presents a fully integrated and reconfigurable TD-nvIMC macro, fabricated in 28 nm CMOS, that combines a ferroelectric FET (FeFET)-based content-addressable memory array, a cascaded delay element chain, and a time-to-digital converter. The architecture supports binary multiply-and-accumulate (MAC) operations using XOR- and AND-based matching, as well as in-memory Boolean logic and arithmetic functions. Sub-nanosecond MAC resolution is achieved through experimentally demonstrated 550 ps delay steps, representing a 2000$\times$ improvement over prior FeFET TD-nvIMC work, enabled by multilevel-state calibration with $\leq$100 ps resolution. Write-disturb resilience is ensured via isolated triple-well bulks. The proposed macro achieves a measured throughput of 222.2 MOPS/cell and energy efficiency of 1887 TOPS/W at 0.85 V, establishing a viable path toward scalable, energy-efficient TD-nvIMC accelerators."
2504.04543,"Probabilistic computing is an emerging quantum-inspired computing paradigm capable of solving combinatorial optimization and various other classes of computationally hard problems. In this work, we present pc-COP, an efficient and configurable probabilistic computing hardware accelerator with 2048 fully connected probabilistic bits (p-bits) implemented on Xilinx UltraScale+ FPGA. We propose a pseudo-parallel p-bit update architecture with speculate-and-select logic which improves overall performance by $4 \times$ compared to the traditional sequential p-bit update. Using our FPGA-based accelerator, we demonstrate the standard G-Set graph maximum cut benchmarks with near-99% average accuracy. Compared to state-of-the-art hardware implementations, we achieve similar performance and accuracy with lower FPGA resource utilization."
2504.05989,"Combinatorial optimization is essential across numerous disciplines. Traditional metaheuristics excel at exploring complex solution spaces efficiently, yet they often struggle with scalability. Deep learning has become a viable alternative for quickly generating high-quality solutions, particularly when metaheuristics underperform. In recent years, quantum-inspired approaches such as tensor networks have shown promise in addressing these challenges. Despite these advancements, a thorough comparison of the different paradigms is missing. This study evaluates eight algorithms on Weighted Max-Cut graphs ranging from 10 to 250 nodes. Specifically, we compare a Genetic Algorithm representing metaheuristics, a Graph Neural Network for deep learning, and the Density Matrix Renormalization Group as a tensor network approach. Our analysis focuses on solution quality and computational efficiency (i.e., time and memory usage). Numerical results show that the Genetic Algorithm achieves near-optimal results for small graphs, although its computation time grows significantly with problem size. The Graph Neural Network offers a balanced solution for medium-sized instances with low memory demands and rapid inference, yet it exhibits more significant variability on larger graphs. Meanwhile, the Tensor Network approach consistently yields high approximation ratios and efficient execution on larger graphs, albeit with increased memory consumption."
2504.06311,"WiFi-based device localization is a key enabling technology for smart applications, which has attracted numerous research studies in the past decade. Most of the existing approaches rely on Line-of-Sight (LoS) signals to work, while a critical problem is often neglected: In the real-world indoor environments, WiFi signals are everywhere, but very few of them are usable for accurate localization. As a result, the localization accuracy in practice is far from being satisfactory. This paper presents BIFROST, a novel hardware-software co-design for accurate indoor localization. The core idea of BIFROST is to reinvent WiFi signals, so as to provide sufficient LoS signals for localization. This is realized by exploiting the dispersion effect of signals emitted by the leaky wave antenna (LWA). We present a low-cost plug-in design of LWA that can generate orthogonal polarized signals: On one hand, LWA disperses signals of different frequencies to different angles, thus providing Angle-of-Arrival (AoA) information for the localized target. On the other hand, the target further leverages the antenna polarization mismatch to distinguish AoAs from different LWAs. In the software layer, fine-grained information in Channel State Information (CSI) is exploited to cope with multipath and noise. We implement BIFROST and evaluate its performance under various settings. The results show that the median localization error of BIFROST is 0.81m, which is 52.35% less than that of SpotFi, a state-of-the-art approach. SpotFi, when combined with BIFROST to work in the realistic settings, can reduce the localization error by 33.54%."
2504.06458,"This paper proposes a novel combinatorial optimization framework that reformulates existing power system problems into a format executable on quantum annealers. The proposed framework accommodates both normal and complex numbers and enables efficient handling of large-scale problems, thus ensuring broad applicability across power system problems. As a proof of concept, we demonstrate its applicability in two classical problems: (i) power system parameter identification, where we estimate the admittance matrix given voltage and current measurements, and (ii) power flow analysis, where we reformulate the nonlinear equations governing active and reactive power balance. The results show that the proposed framework effectively and efficiently solves both linear and nonlinear power system problems, and thus offers significant advantages in scenarios where traditional solvers face challenges, such as ill-conditioned systems and fault conditions."
2504.06476,"The Boolean satisfiability (SAT) problem is a computationally challenging decision problem central to many industrial applications. For SAT problems in cryptanalysis, circuit design, and telecommunication, solutions can often be found more efficiently by representing them with a combination of exclusive OR (XOR) and conjunctive normal form (CNF) clauses. We propose a hardware accelerator architecture that natively embeds and solves such hybrid CNF$-$XOR problems using in-memory computing hardware. To achieve this, we introduce an algorithm and demonstrate, both experimentally and through simulations, how it can be efficiently implemented with memristor crossbar arrays. Compared to the conventional approaches that translate CNF$-$XOR problems to pure CNF problems, our simulations show that the accelerator improves computation speed, energy efficiency, and chip area utilization by $\sim$10$\times$ for a set of hard cryptographic benchmarking problems. Moreover, the accelerator achieves a $\sim$10$\times$ speedup and a $\sim$1000$\times$ gain in energy efficiency over state-of-the-art SAT solvers running on CPUs."
2504.07623,"Platooning represents an advanced driving technology designed to assist drivers in traffic convoys of varying lengths, enhancing road safety, reducing driver fatigue, and improving fuel efficiency. Sophisticated automated driving assistance systems have facilitated this innovation. Recent advancements in platooning emphasize cooperative mechanisms within both centralized and decentralized architectures enabled by vehicular communication technologies. This study introduces a cooperative route planning optimization framework aimed at promoting the adoption of platooning through a centralized platoon formation strategy at the system level. This approach is envisioned as a transitional phase from individual (ego) driving to fully collaborative driving. Additionally, this research formulates and incorporates travel cost metrics related to fuel consumption, driver fatigue, and travel time, considering regulatory constraints on consecutive driving durations. The performance of these cost metrics has been evaluated using Dijkstra's and A* shortest path algorithms within a network graph framework. The results indicate that the proposed architecture achieves an average cost improvement of 14 % compared to individual route planning for long road trips."
2504.0834,"As the demand for efficient, low-power computing in embedded and edge devices grows, traditional computing methods are becoming less effective for handling complex tasks. Stochastic computing (SC) offers a promising alternative by approximating complex arithmetic operations, such as addition and multiplication, using simple bitwise operations, like majority or AND, on random bit-streams. While SC operations are inherently fault-tolerant, their accuracy largely depends on the length and quality of the stochastic bit-streams (SBS). These bit-streams are typically generated by CMOS-based stochastic bit-stream generators that consume over 80% of the SC system's power and area. Current SC solutions focus on optimizing the logic gates but often neglect the high cost of moving the bit-streams between memory and processor. This work leverages the physics of emerging ReRAM devices to implement the entire SC flow in place: (1) generating low-cost true random numbers and SBSs, (2) conducting SC operations, and (3) converting SBSs back to binary. Considering the low reliability of ReRAM cells, we demonstrate how SC's robustness to errors copes with ReRAM's variability. Our evaluation shows significant improvements in throughput (1.39x, 2.16x) and energy consumption (1.15x, 2.8x) over state-of-the-art (CMOS- and ReRAM-based) solutions, respectively, with an average image quality drop of 5% across multiple SBS lengths and image processing tasks."
2504.09012,"Field-coupled Nanocomputing (FCN) is a class of promising post-CMOS technologies that transmit information through electric or magnetic fields instead of current flow. They utilize basic building blocks called cells, which can form gates that implement Boolean functions. However, the design constraints for FCN circuits differ significantly from those for CMOS. One major challenge is that wires in FCN have to be realized as gates, i.e., they are constructed from cells and incur the same costs as gates. Additionally, all FCN technologies are fabricated on a single layer, e.g., a silicon surface, requiring all elements -- gates and wires -- to be placed within that same layer. Consequently, FCN employs special gates, called wire crossings, to enable signals to cross. While existing wire-crossing implementations are complex and were previously considered costly, initial efforts have aimed at minimizing their use. However, recent physical simulations and experiments on a quantum annealing platform have shown that currently used wire crossings in FCN significantly compromise signal stability, to the extent that circuits cannot function reliably. This work addresses that issue by introducing the first placement and routing algorithm that produces fully planar FCN circuits, eliminating the need for all wire crossings. For a comparative evaluation, a state-of-the-art placement and routing algorithm was also modified to enforce planarity. However, our proposed algorithm is more scalable and can handle inputs with up to 149k gates, enabling it to process circuits that are 182x more complex than those handled by the modified state-of-the-art algorithm."
2504.09229,"Researchers developed about a dozen semiconductor reversible (or adiabatic) logic chips since the early 1990s, validating circuit designs and proving the concept--but scale up required a further advance. This document shows that cryogenic inductors made of a new High Kinetic Inductance (HKI) material provide the advance. This material can be deposited as an integrated circuit layer, where it has enough energy recycling capacity to power a reversible circuit of the same size. This allows a designer to replicate and scale a complete reversible logic subsystem in accordance with Moore's law."
2504.09542,"The integration of electric vehicle (EV) charging with renewable energy sources is crucial for minimizing the carbon footprint of transportation. This study investigates whether real-time pro-environmental information displayed on a smartboard at EV charging stations can influence drivers to align their charging behavior with periods of high renewable energy availability. A pre-post-control quasi-experimental field trial was conducted in a sustainable neighborhood in Ghent, Belgium. A smartboard provided real-time signals indicating optimal charging times based on renewable energy production. The results demonstrate that the presence of real-time pro-environmental information on a smartboard was associated with significant increases in both the number of charging operations and the amount of energy charged during periods of high renewable energy availability. This approach offers a scalable, cost-effective method for optimizing energy consumption and reducing greenhouse gas emissions in residential settings."
2504.09713,"Ferroelectric memories have attracted significant interest due to their non-volatile storage, energy efficiency, and fast operation, making them prime candidates for future memory technologies. As commercial Dynamic Random Access Memory (DRAM) and NAND flash memory are transiting or have moved toward three-dimensional (3D) integration, 3D ferroelectric memory architectures are also emerging, provided they can achieve a competitive position within the modern memory hierarchy. Given the excellent scalability of ferroelectric HfO2, various dense 3D integrated ferroelectric memory architectures are feasible, each offering unique strengths and facing distinct challenges. In this work, we present a comprehensive classification of 3D ferroelectric memory architectures based on polarization sensing methods, highlighting their critical role in shaping memory cell design and operational efficiency. Through a systematic evaluation of these architectures, we develop a unified framework to assess their advantages and trade-offs. This classification not only enhances the understanding of current 3D ferroelectric memory technologies but also lays the foundation for designing next-generation architectures optimized for advanced computing and high-performance applications."
2504.11016,"Heating and hot water usage account for nearly 80% of household energy consumption in the European Union. In order to reach the EU New Deal goals, new policies to reduce heat energy consumption are indispensable. However, research targeting reductions concentrates either on technical building interventions without considerations of people's behavior, or psychological interventions with no technical interference. Such interventions can be promising, but their true potential for scaling up can only be realized by testing approaches that integrate behavioral and technical solutions in tandem rather than in isolation. In this research, we study a mix of psychological and technical interventions targeting heating and hot water demand among students in Polish university dormitories. We evaluate effects on building energy consumption, behavioral spillovers and on social beliefs and attitudes in a pre-post quasi-experimental mixed-method field study in three student dormitories. Our findings reveal that the most effective approaches to yield energy savings were a direct, collectively framed request to students to reduce thermostat settings for the environment, and an automated technical adjustment of the heating curve temperature. Conversely, interventions targeting domestic hot water had unintended effects, including increased energy use and negative spillovers, such as higher water consumption. Further, we find that informing students about their active, collective participation had a positive impact on perceived social norms. Our findings highlight the importance of trialing interventions in controlled real-world settings to understand the interplay between technical systems, behaviors, and social impacts to enable scalable, evidence-based policies driving an effective and sustainable energy transition."
2504.13294,"Ising solvers with hierarchical clustering have shown promise for large-scale Traveling Salesman Problems (TSPs), in terms of latency and energy. However, most of these methods still face unacceptable quality degradation as the problem size increases beyond a certain extent. Additionally, their hardware-agnostic adoptions limit their ability to fully exploit available hardware resources. In this work, we introduce TAXI -- an in-memory computing-based TSP accelerator with crossbar(Xbar)-based Ising macros. Each macro independently solves a TSP sub-problem, obtained by hierarchical clustering, without the need for any off-macro data movement, leading to massive parallelism. Within the macro, Spin-Orbit-Torque (SOT) devices serve as compact energy-efficient random number generators enabling rapid ""natural annealing"". By leveraging hardware-algorithm co-design, TAXI offers improvements in solution quality, speed, and energy-efficiency on TSPs up to 85,900 cities (the largest TSPLIB instance). TAXI produces solutions that are only 22% and 20% longer than the Concorde solver's exact solution on 33,810 and 85,900 city TSPs, respectively. TAXI outperforms a current state-of-the-art clustering-based Ising solver, being 8x faster on average across 20 benchmark problems from TSPLib."
2504.136,"Human brain processes sensory information in real-time with extraordinary efficiency compared to the possibilities of current artificial computing systems. It operates as a complex nonlinear system, composed of interacting dynamic units - neurons and synapses - that processes data-streams as time goes by, i.e. through time, using time as an internal self-standing variable. Here we report on a memristor-based compact chaotic circuit included in a computing architecture that can process information through time. We realized a hardware memristive version of the formally simplest chaotic circuit that, thanks to the nonlinearity of the nonvolatile memristor device, evolves with complex dynamics in response to a driving signal. The circuit is used in a single-node reservoir computing scheme to demonstrate nonlinear classification tasks and the processing of data streams through time. These results demonstrate that a simple memristor-based chaotic circuit has the potential to operate as a nonlinear dynamics-based computing system and to process temporal information through time."
2504.1402,"Hyperdimensional computing (HDC) is a brain-inspired paradigm valued for its noise robustness, parallelism, energy efficiency, and low computational overhead. Hardware accelerators are being explored to further enhance their performance, but current solutions are often limited by application specificity and the latency of encoding and similarity search. This paper presents a generalized, reconfigurable on-chip training and inference architecture for HDC, utilizing spin-orbit-torque magnetic random access memory (SOT-MRAM) based content-addressable memory (SOT-CAM). The proposed SOT-CAM array integrates storage and computation, enabling in-memory execution of key HDC operations: binding (bitwise multiplication), permutation (bit shfiting), and efficient similarity search. Furthermore, a novel bit drop method-based permutation backed by holographic information representation of HDC is proposed which replaces conventional permutation execution in hardware resulting in a 6x latency improvement, and an HDC-specific adder reduces energy and area by 1.51X and 1.43x, respectively. To mitigate the parasitic effect of interconnects in the similarity search, a four-stage voltage scaling scheme has been proposed to ensure an accurate representation of the Hamming distance. Benchmarked at 7nm, the architecture achieves energy reductions of 21.5x, 552.74x, 1.45x, and 282.57x for addition, permutation, multiplication, and search operations, respectively, compared to CMOS-based HDC. Against state-of-the-art HDC accelerators, it achieves a 2.27x lower energy consumption and outperforms CPU and eGPU implementations by 2702x and 23161x, respectively, with less than 3% drop in accuracy."
2504.1431,"The widespread adoption of large artificial intelligence (AI) models has enabled numerous applications of the Internet of Things (IoT). However, large AI models require substantial computational and memory resources, which exceed the capabilities of resource-constrained IoT devices. End-edge collaboration paradigm is developed to address this issue, where a small model on the end device performs inference tasks, while a large model on the edge server assists with model updates. To improve the accuracy of the inference tasks, the data generated on the end devices will be periodically uploaded to edge server to update model, and a distilled model of the updated one will be transmitted back to the end device. Subjected to the limited bandwidth for the communication link between the end device and the edge server, it is important to investigate whether the system should allocate more bandwidth to data upload or to model transmission. In this paper, we characterize the impact of data upload and model transmission on inference accuracy. Subsequently, we formulate a bandwidth allocation problem. By solving this problem, we derive an efficient optimization framework for the end-edge collaboration system. The simulation results demonstrate our framework significantly enhances mean average precision (mAP) under various bandwidths and datasizes."
2504.1436,"Large-scale adoption of commercial and personal Electric Vehicles (EVs) is expected to significantly affect traffic flow dynamics, emissions, and energy consumption in the transportation sector. Range anxiety and challenges associated with charging EVs are among the key issues that reduce the adoption rate of EVs and, in turn, limit their system-level impacts. A promising solution to address these challenges is the introduction of charging while driving (CWD) lanes. Although technological advancements have made it possible to charge vehicles wirelessly while driving, introducing such lanes to the traffic stream can potentially disturb traffic flow and result in new congestion patterns. This study puts forward a framework to investigate the effects of CWD lanes on traffic flow, considering %autonomy, speed harmonization, and environmental factors for different market penetration rates (MPRs) of personal and commercial EVs. Different policies have been investigated to suggest the best design for CWD lanes. Results indicate that introducing CWD lanes can decrease overall traffic throughput and increase congestion due to additional lane-changing maneuvers by electric vehicles aiming to utilize the CWD lane. Although higher MPRs of EVs help stabilize traffic flow and reduce the number of shockwaves, speed disruption tends to increase in the CWD lane and propagate to adjacent lanes. Emission analyses show significant reductions (up to 63\%) in pollution levels with increasing MPRs of personal and commercial EVs. Our analysis shows that while CWD lanes can facilitate the adoption of EVs, they can deteriorate traffic efficiency, emphasizing the importance of careful design and policy considerations."
2504.14466,"Neuromorphic systems seek to replicate the functionalities of biological neural networks to attain significant improvements in performance and efficiency of AI computing platforms. However, these systems have generally remained limited to emulation of simple neurons and synapses; and ignored higher order functionalities enabled by other components of the brain like astrocytes and dendrites. In this work, drawing inspiration from biology, we introduce a compact Double-Gate Ferroelectric Field Effect Transistor (DG-FeFET) cell that can emulate the dynamics of both astrocytes and dendrites within neuromorphic architectures. We demonstrate that with a ferroelectric top gate for synaptic weight programming as in conventional synapses and a non-ferroelectric back gate, the DG-FeFET realizes a synapse with a dynamic gain modulation mechanism. This can be leveraged as an analog for a compact astrocyte-tripartite synapse, as well as enabling dendrite-like gain modulation operations. By employing a fully-depleted silicon-on-insulator (FDSOI) FeFET as our double-gate device, we validate the linear control of the synaptic weight via the back gate terminal (i.e., the gate underneath the buried oxide (BOX) layer) through comprehensive theoretical and experimental studies. We showcase the promise such a tripartite synaptic device holds for numerous important neuromorphic applications, including autonomous self-repair of faulty neuromorphic hardware mediated by astrocytic functionality. Coordinate transformations based on dragonfly prey-interception circuitry models are also demonstrated based on dendritic function emulation by the device. This work paves the way forward for developing truly ""brain-like"" neuromorphic hardware that go beyond the current dogma focusing only on neurons and synapses."
2504.14601,"As cyber threats escalate, Zero Trust Architecture (ZTA) replaces outdated perimeter security with strict never trust, always verify protocols. However, ZTA's dual nature as both technical infrastructure and social intervention creates an unresolved tension: its very mechanisms for security may systematically erode the trust foundations enabling effective collaboration. This integrative research combines case study analysis, employee surveys, and social network mapping reveals how ZTA disrupts knowledge-sharing, disproportionately hindering low-altruism employees, while surveillance erodes collective psychological ownership. Networked organizations, reliant on fluid trust, face fragmentation risks. Mitigation strategies include adaptive authorization frameworks using behavioral analytics and transparent communication reframing security as shared responsibility. Interdepartmental collaboration in security design preserves organizational trust structures identified through sociometric mapping. This research provides a framework balancing technical rigor with cultural sensitivity, proving cybersecurity can coexist with innovation by aligning verification with organizational psychology. The findings pioneer a paradigm where security and trust evolve synergistically critical for digital resilience in hybrid work environments. Future security must harmonize protocols with trust cultivation, ensuring defenses adapt to social dynamics driving modern enterprises."
2504.15934,"Advances in third-generation sequencing have enabled portable and real-time genomic sequencing, but real-time data processing remains a bottleneck, hampering on-site genomic analysis due to prohibitive time and energy costs. These technologies generate a massive amount of noisy analog signals that traditionally require basecalling and digital mapping, both demanding frequent and costly data movement on von Neumann hardware. To overcome these challenges, we present a memristor-based hardware-software co-design that processes raw sequencer signals directly in analog memory, effectively combining the separated basecalling and read mapping steps. Here we demonstrate, for the first time, end-to-end memristor-based genomic analysis in a fully integrated memristor chip. By exploiting intrinsic device noise for locality-sensitive hashing and implementing parallel approximate searches in content-addressable memory, we experimentally showcase on-site applications including infectious disease detection and metagenomic classification. Our experimentally-validated analysis confirms the effectiveness of this approach on real-world tasks, achieving a state-of-the-art 97.15% F1 score in virus raw signal mapping, with 51x speed up and 477x energy saving compared to implementation on a state-of-the-art ASIC. These results demonstrate that memristor-based in-memory computing provides a viable solution for integration with portable sequencers, enabling truly real-time on-site genomic analysis for applications ranging from pathogen surveillance to microbial community profiling."
2504.15967,"With the continued growth of its core technologies, including the Internet of Things (IoT), artificial intelligence (AI), Big Data and data analytics, and edge computing, digital twin (DT) technology has witnessed a significant increase in industrial applications, helping the industry become more sustainable, smart, and adaptable. Hence, DT technology has emerged as a promising link between the physical and virtual worlds, enabling simulation, prediction, and real-time performance optimization. This work aims to explore the development of a high-fidelity digital twin framework, focusing on synchronization and accuracy between physical and digital systems to enhance data-driven decision making. To achieve this, we deploy several stationary UAVs in optimized locations to collect data from industrial IoT devices, which were used to monitor multiple physical entities and perform computations to evaluate their status. We consider a practical setup in which multiple IoT devices may monitor a single physical entity, and as a result, the measurements are combined and processed together to determine the status of the physical entity. The resulting status updates are subsequently uploaded from the UAVs to the base station, where the DT resides. In this work, we consider a novel metric based on the Age of Information (AoI), coined as the Age of Digital Twin (AoDT), to reflect the status freshness of the digital twin. Factoring AoDT in the problem formulation ensures that the DT reliably mirrors the physical system with high accuracy and synchronization. We formulate a mixed-integer non-convex program to maximize the total amount of data collected from all IoT devices while ensuring a constrained AoDT. Using successive convex approximations, we solve the problem, conduct extensive simulations and compare the results with baseline approaches to demonstrate the effectiveness of the proposed solution."
2504.17022,"Molecular Communication (MC) channels are characterized by significant memory and nonlinear dynamics arising from diffusion and receptor kinetics. While often viewed as impairments to reliable data transmission, this work introduces a paradigm shift by reconceptualizing these intrinsic physical properties as computational resources. We frame a canonical point-to-point MC channel, comprising ligand diffusion and reversible ligand-receptor binding at a spherical receiver, as a Physical Reservoir Computer (PRC). Utilizing deterministic mean-field modeling and particle-based spatial stochastic simulations, we demonstrate the MC system's inherent capability for complex temporal information processing on standard chaotic time-series benchmarks. We comprehensively evaluate performance using both task-specific Normalized Root Mean Square Error (NRMSE) and the task-independent Information Processing Capacity (IPC). Our results reveal a non-monotonic dependence of computational power on key biophysical parameters (receptor kinetic rates, diffusion coefficient, and transmitter-receiver distance), identifying optimal operational regimes where memory and nonlinearity are balanced. These findings establish the MC channel as a viable computational substrate, paving the way for novel architectures in \emph{wetware} artificial intelligence."
2504.17164,"In this paper, we present formal foundations for two wireless agility techniques: (1) Random Range Mutation (RNM) that allows for periodic changes of AP coverage range randomly, and (2) Ran- dom Topology Mutation (RTM) that allows for random motion and placement of APs in the wireless infrastructure. The goal of these techniques is to proactively defend against targeted attacks (e.g., DoS and eavesdropping) by forcing the wireless clients to change their AP association randomly. We apply Satisfiability Modulo The- ories (SMT) and Answer Set Programming (ASP) based constraint solving methods that allow for optimizing wireless AP mutation while maintaining service requirements including coverage, secu- rity and energy properties under incomplete information about the adversary strategies. Our evaluation validates the feasibility, scalability, and effectiveness of the formal methods based technical approaches."
2504.17554,"Persistent Memory (PM) introduces new opportunities for designing crash-consistent applications without the traditional storage overheads. However, ensuring crash consistency in PM demands intricate knowledge of CPU, cache, and memory interactions. Hardware and software mechanisms have been proposed to ease this burden, but neither proved sufficient, prompting a variety of bug detection tools.With the sunset of Intel Optane comes the rise of Compute Express Link (CXL) for PM. In this position paper, we discuss the impact of CXL's disaggregated and heterogeneous nature in the development of crash-consistent PM applications, and outline three research directions: hardware primitives, persistency frameworks, and bug detection tools."
2504.17752,"Modern edge devices, such as cameras, drones, and Internet-of-Things nodes, rely on deep learning to enable a wide range of intelligent applications, including object recognition, environment perception, and autonomous navigation. However, deploying deep learning models directly on the often resource-constrained edge devices demands significant memory footprints and computational power for real-time inference using traditional digital computing architectures. In this paper, we present WISE, a novel computing architecture for wireless edge networks designed to overcome energy constraints in deep learning inference. WISE achieves this goal through two key innovations: disaggregated model access via wireless broadcasting and in-physics computation of general complex-valued matrix-vector multiplications directly at radio frequency. Using a software-defined radio platform with wirelessly broadcast model weights over the air, we demonstrate that WISE achieves 95.7% image classification accuracy with ultra-low operation power of 6.0 fJ/MAC per client, corresponding to a computation efficiency of 165.8 TOPS/W. This approach enables energy-efficient deep learning inference on wirelessly connected edge devices, achieving more than two orders of magnitude improvement in efficiency compared to traditional digital computing."
2504.17923,"Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantum genetic algorithms leverage the principles of superposition and entanglement to enhance the performance of classical genetic algorithms. In this work, we propose a novel quantum genetic algorithm introducing an innovative crossover strategy to generate quantum circuits from a binary solution. We incorporate a heuristic method to encode entanglement patterns from parent solutions into circuits for the next generation. Our algorithm advances quantum genetic algorithms by utilizing a limited number of entanglements, enabling efficient exploration of optimal solutions without significantly increasing circuit depth, making it suitable for near-term applications. We test this approach on a portfolio optimization problem using an IBM 127 qubits Eagle processor (ibm_quebec) and simulators. Compared to state-of-the-art algorithms, our results show that the proposed method improves fitness values by 33.6% over classical genetic algorithm and 37.2% over quantum-inspired genetic algorithm, using the same iteration counts and population sizes with real quantum hardware employing 100 qubits. These findings highlight the potential of current quantum computers to address real-world utility-scale combinatorial optimization problems."
2504.18326,"Molecular Communication (MC) has long been envisioned to enable an Internet of Bio-Nano Things (IoBNT) with medical applications, where nanomachines within the human body conduct monitoring, diagnosis, and therapy at micro- and nanoscale levels. MC involves information transfer via molecules and is supported by well-established theoretical models. However, practically achieving reliable, energy-efficient, and bio-compatible communication at these scales still remains a challenge. Air-Based Molecular Communication (ABMC) is a type of MC that operates over larger, meter-scale distances and extends even outside the human body. Therefore, devices and techniques to realize ABMC are readily accessible, and associated use cases can be very promising in the near future. Exhaled breath analysis has previously been proposed. It provides a non-invasive approach for health monitoring, leveraging existing commercial sensor technologies and reducing deployment barriers. The breath contains a diverse range of molecules and particles that serve as biomarkers linked to various physiological and pathological conditions. The plethora of proven methods, models, and optimization approaches in MC enable macroscale breath analysis, treating human as the transmitter, the breath as the information carrier, and macroscale sensors as the receiver. Using ABMC to interface with the inherent dynamic networks of cells, tissues, and organs could create a novel Internet of Bio Things (IoBT), a preliminary macroscale stage of the IoBNT. This survey extensively reviews exhaled breath modeling and analysis through the lens of MC, offering insights into theoretical frameworks and practical implementations from ABMC, bringing the IoBT a step closer to real-world use."
2504.18813,"As technology advances, photonic integrated circuits (PICs) are rapidly scaling in size and complexity, with modern designs integrating thousands of components. However, the analog custom layout nature of photonics, the curvy waveguide structures, and single-layer routing resources impose stringent physical constraints, such as minimum bend radii and waveguide crossing penalties, which make manual layout the de facto standard. This manual process takes weeks to complete and is error-prone, which is fundamentally unscalable for large-scale PIC systems. Existing automation solutions have adopted force-directed placement on small benchmarks with tens of components, with limited routability and scalability. To fill this fundamental gap in the electronic-photonic design automation (EPDA) toolchain, we present the first GPU-accelerated, routing-informed placement framework. It features an asymmetric bending-aware wirelength function with explicit modeling of waveguide routing congestion and crossings for routability maximization. Meanwhile, conditional projection is employed to gradually enforce a variety of user-defined layout constraints, including alignment, spacing, etc. This constrained optimization is accelerated and stabilized by a custom blockwise adaptive Nesterov-accelerated optimizer, ensuring stable and high-quality convergence. Compared to existing methods, our method can generate high-quality layouts for large-scale PICs with an average routing success rate of 94.79% across all benchmarks within minutes. By tightly coupling placement with physical-aware routing, our method establishes a new paradigm for automated PIC design, bringing intelligent, scalable layout synthesis to the forefront of next-generation EPDA. Our code is open-sourced atthis https URL."
2504.19967,"Traffic flow prediction is a critical component of intelligent transportation systems, yet accurately forecasting traffic remains challenging due to the interaction between long-term trends and short-term fluctuations. Standard deep learning models often struggle with these challenges because their architectures inherently smooth over fine-grained fluctuations while focusing on general trends. This limitation arises from low-pass filtering effects, gate biases favoring stability, and memory update mechanisms that prioritize long-term information retention. To address these shortcomings, this study introduces a hybrid deep learning framework that integrates both long-term trend and short-term fluctuation information using two input features processed in parallel, designed to capture complementary aspects of traffic flow dynamics. Further, our approach leverages attention mechanisms, specifically Bahdanau attention, to selectively focus on critical time steps within traffic data, enhancing the model's ability to predict congestion and other transient phenomena. Experimental results demonstrate that features learned from both branches are complementary, significantly improving the goodness-of-fit statistics across multiple prediction horizons compared to a baseline model. Notably, the attention mechanism enhances short-term forecast accuracy by directly targeting immediate fluctuations, though challenges remain in fully integrating long-term trends. This framework can contribute to more effective congestion mitigation and urban mobility planning by advancing the robustness and precision of traffic prediction models."
2504.20175,"For the upcoming 6G wireless networks, reconfigurable intelligent surfaces are an essential technology, enabling dynamic beamforming and signal manipulation in both reflective and transmissive modes. It is expected to utilize frequency bands in the millimeter-wave and THz, which presents unique opportunities but also significant challenges. The selection of switching technologies that can support high-frequency operation with minimal loss and high efficiency is particularly complex. In this work, we demonstrate the potential of advanced components such as Schottky diodes, memristor switches, liquid metal-based switches, phase change materials, and RF-SOI technology in RIS designs as an alternative to overcome limitations inherent in traditional technologies in D-band (110-170 GHz)."
2504.2128,"Combinatorial optimization problems (COPs) are crucial in many applications but are computationally demanding. Traditional Ising annealers address COPs by directly converting them into Ising models (known as direct-E transformation) and solving them through iterative annealing. However, these approaches require vector-matrix-vector (VMV) multiplications with a complexity of $O(n^2)$ for Ising energy computation and complex exponential annealing factor calculations during annealing process, thus significantly increasing hardware costs. In this work, we propose a ferroelectric compute-in-memory (CiM) in-situ annealer to overcome aforementioned challenges. The proposed device-algorithm co-design framework consists of (i) a novel transformation method (first to our known) that converts COPs into an innovative incremental-E form, which reduces the complexity of VMV multiplication from $O(n^2)$ to $O(n)$, and approximates exponential annealing factor with a much simplified fractional form; (ii) a double gate ferroelectric FET (DG FeFET)-based CiM crossbar that efficiently computes the in-situ incremental-E form by leveraging the unique structure of DG FeFETs; (iii) %When feasible solutions are detected, a CiM annealer that approaches the solutions of COPs via iterative incremental-E computations within a tunable back gate-based in-situ annealing flow. Evaluation results show that our proposed CiM annealer significantly reduces hardware overhead, reducing energy consumption by 1503/1716$\times$ and time cost by 8.08/8.15$\times$ in solving 3000-node Max-Cut problems compared to two state-of-the-art annealers. It also exhibits high solving efficiency, achieving a remarkable average success rate of 98\%, whereas other annealers show only 50\% given the same iteration counts."
2504.21684,"Quantum computing has emerged as a powerful tool to efficiently solve computational challenges, particularly in simulation and optimisation. However, hardware limitations prevent quantum computers from achieving the full theoretical potential. Among the quantum algorithms, quantum annealing is a prime candidate to solve optimisation problems. This makes it a natural candidate for search-based software testing in the Cyber-Physical Systems (CPS) domain, which demands effective test cases due to their safety-critical nature. This work explores the use of quantum annealing to enhance test case generation for CPS through a mutation-based approach. We encode test case mutation as a binary optimisation problem, and use quantum annealing to identify and target critical regions of the test cases for improvement. Our approach mechanises this process into an algorithm that uses D-Wave's quantum annealer to find the solution. As a main contribution, we offer insights into how quantum annealing can advance software testing methodologies by empirically evaluating the correlation between problem size, hardware limitations, and the effectiveness of the results. Moreover, we compare the proposed method against state-of-the-art classical optimisation algorithms, targeting efficiency (time to generate test cases) and effectiveness (fault detection rates). Results indicate that quantum annealing enables faster test case generation while achieving comparable fault detection performance to state-of-the-art alternatives."
2505.01635,"Although inspired by neuronal systems in the brain, artificial neural networks generally employ point-neurons, which offer far less computational complexity than their biological counterparts. Neurons have dendritic arbors that connect to different sets of synapses and offer local non-linear accumulation - playing a pivotal role in processing and learning. Inspired by this, we propose a novel neuron design based on a multi-gate ferroelectric field-effect transistor that mimics dendrites. It leverages ferroelectric nonlinearity for local computations within dendritic branches, while utilizing the transistor action to generate the final neuronal output. The branched architecture paves the way for utilizing smaller crossbar arrays in hardware integration, leading to greater efficiency. Using an experimentally calibrated device-circuit-algorithm co-simulation framework, we demonstrate that networks incorporating our dendritic neurons achieve superior performance in comparison to much larger networks without dendrites ($\sim$17$\times$ fewer trainable weight parameters). These findings suggest that dendritic hardware can significantly improve computational efficiency, and learning capacity of neuromorphic systems optimized for edge applications."
2505.01735,"Recent advances in the fields of deep learning and quantum computing have paved the way for innovative developments in artificial intelligence. In this manuscript, we leverage these cutting-edge technologies to introduce a novel model that emulates the intricate functioning of the human brain, designed specifically for the detection of anomalies such as fraud in credit card transactions. Leveraging the synergies of Quantum Spiking Neural Networks (QSNN) and Quantum Long Short-Term Memory (QLSTM) architectures, our approach is developed in two distinct stages, closely mirroring the information processing mechanisms found in the brain's sensory and memory systems. In the initial stage, similar to the brain's hypothalamus, we extract low-level information from the data, emulating sensory data processing patterns. In the subsequent stage, resembling the hippocampus, we process this information at a higher level, capturing and memorizing correlated patterns. We will compare this model with other quantum models such as Quantum Neural Networks among others and their corresponding classical models."
2505.02285,"Despite the parallel in-memory search capabilities of content addressable memories (CAMs), their use in applications is constrained by their limited resolution that worsens as they are scaled to larger arrays or advanced nodes. In this work we present experimental results for a novel back-end-of-line compatible reference resistive device that can significantly improve the search resolution of CAMs implemented with CMOS and beyond-CMOS technologies to less than or equal to 5-bits."
2505.03036,"The Massive Internet of Things (MIoT) envisions an interconnected ecosystem of billions of devices, fundamentally transforming diverse sectors such as healthcare, smart cities, transportation, agriculture, and energy management. However, the vast scale of MIoT introduces significant challenges, including network scalability, efficient data management, energy conservation, and robust security mechanisms. This paper presents a thorough review of existing and emerging MIoT technologies designed to address these challenges, including Low-Power Wide-Area Networks (LPWAN), 5G/6G capabilities, edge and fog computing architectures, and hybrid access methodologies. We further investigate advanced strategies such as AI-driven resource allocation, federated learning for privacy-preserving analytics, and decentralized security frameworks using blockchain. Additionally, we analyze sustainable practices, emphasizing energy harvesting and integrating green technologies to reduce environmental impact. Through extensive comparative analysis, this study identifies critical innovations and architectural adaptations required to support efficient, resilient, and scalable MIoT deployments. Key insights include the role of network slicing and intelligent resource management for scalability, adaptive protocols for real-time data handling, and lightweight AI models suited to the constraints of MIoT devices. This research ultimately contributes to a deeper understanding of how MIoT systems can evolve to meet the growing demand for seamless, reliable connectivity while prioritizing sustainability, security, and performance across diverse applications. Our findings serve as a roadmap for future advancements, underscoring the potential of MIoT to support a globally interconnected, intelligent infrastructure."
2505.0354,"Microfluidic biochips are replacing the conventional biochemical analysers integrating the necessary functions on-chip. We are interested in Flow-Based Microfluidic Biochips (FBMB), where a continuous flow of liquid is manipulated using integrated microvalves. Using microvalves and channels, more complex Fluidic Units (FUs) such as switches, micropumps, mixers and separators can be constructed. When running a biochemical application on a FBMB, fluid volumes are dispensed from input reservoirs and used by the FUs. Given a biochemical application and a biochip, one of the key problems which we are discussing in this paper, is in determining the fluid volume assignment for each operation of the application, such that the FUs' volume requirements are satisfied, while over- and underflow are avoided and the total volume of fluid used is minimized. We illustrate the main problems using examples, and provide a review of related work on volume management. We present algorithms for optimizing fluid volume assignments and for reusing leftover fluids to reduce waste. This also includes the optimization of mixing operations which significantly impact the required fluid volumes. We identify the main challenges related to volume management and discuss possible solutions. Finally we compare the outcome of volume management using fixed- and arbitrary-ratio mixing technology, demonstrating significant reductions in fluid consumption for real biochemical assays."
2505.03629,"DNA-based data storage systems face practical challenges due to the high cost of DNA synthesis. A strategy to address the problem entails encoding data via topological modifications of the DNA sugar-phosphate backbone. The DNA Punchcards system, which introduces nicks (cuts) in the DNA backbone, encodes only one bit per nicking site, limiting density. We propose \emph{DNA Tails,} a storage paradigm that encodes nonbinary symbols at nicking sites by growing enzymatically synthesized single-stranded DNA of varied lengths. The average tail lengths encode multiple information bits and are controlled via a staggered nicking-tail extension process. We demonstrate the feasibility of this encoding approach experimentally and identify common sources of errors, such as calibration errors and stumped tail growth errors. To mitigate calibration errors, we use rank modulation proposed for flash memory. To correct stumped tail growth errors, we introduce a new family of rank modulation codes that can correct ``stuck-at'' errors. Our analytical results include constructions for order-optimal-redundancy permutation codes and accompanying encoding and decoding algorithms."
2505.04403,"Blockchain technology has rapidly expanded beyond its original use in cryptocurrencies to a broad range of applications, creating vast amounts of immutable, decentralized data. As blockchain adoption grows, so does the need for advanced data analytics techniques to extract insights for business intelligence, fraud detection, financial analysis and many more. While previous research has examined specific aspects of blockchain data analytics, such as transaction patterns, illegal activity detection, and data management, there remains a lack of comprehensive reviews that explore the full scope of blockchain data analytics. This study addresses this gap through a scoping literature review, systematically mapping the existing research landscape, identifying key topics, and highlighting emerging trends. Using established methodologies for literature reviews, we analyze 466 publications, clustering them into six major research themes: illegal activity detection, data management, financial analysis, user analysis, community detection, and mining analysis. Our findings reveal a strong focus on detecting illicit activities and financial applications, while holistic business intelligence use cases remain underexplored. This review provides a structured overview of blockchain data analytics, identifying research gaps and proposing future directions to enhance the fields impact."
2505.04772,"Satellite communications have emerged as one of the most feasible solutions to provide global wireless coverage and connect the unconnected. Starlink dominates the market with over 7,000 operational satellites in low Earth orbit (LEO) and offers global high-speed and low-latency Internet service for stationary and mobile use cases, including in-motion connectivity for vehicles, vessels, and aircraft. Starlink terminals are designed to handle extreme weather conditions. Starlink recommends a flat high performance (FHP) terminal for users living in areas with extreme weather conditions. The earlier studies evaluated Starlink's FHP throughput for stationary and in-motion users without providing a detailed analysis of how weather affects its performance. There remains a need to investigate the impact of weather on FHP's throughput. In this paper, we address this shortcoming by analyzing the impact of weather on Starlink's performance in Oulu, Finland, a city located in Northern Europe near the Arctic Circle. Our measurements reveal that rain degrades median uplink and downlink throughput by 52.27% and 37.84%, respectively. On the contrary, there was no noticeable impact on the round-trip time. Additionally, we also examine the impact of cloud cover on the Starlink throughput. The linear regression analysis reveals the negative relationship between throughput and cloud cover. The cloud cover of up to 12.5% has around 20% greater throughput than the cloud cover of 87.5%"
2505.0509,"The sixth generation of wireless networks defined several key performance indicators (KPIs) for assessing its networks, mainly in terms of reliability, coverage, and sensing. In this regard, remarkable attention has been paid recently to the integrated sensing and communication (ISAC) paradigm as an enabler for efficiently and jointly performing communication and sensing using the same spectrum and hardware resources. On the other hand, ensuring communication and data security has been an imperative requirement for wireless networks throughout their evolution. The physical-layer security (PLS) concept paved the way to catering to the security needs in wireless networks in a sustainable way while guaranteeing theoretically secure transmissions, independently of the computational capacity of adversaries. Therefore, it is of paramount importance to consider a balanced trade-off between communication reliability, sensing, and security in future networks, such as the 5G and beyond, and the 6G. In this paper, we provide a comprehensive and system-wise review of designed secure ISAC systems from a PLS point of view. In particular, the impact of various physical-layer techniques, schemes, and wireless technologies to ensure the sensing-security trade-off is studied from the surveyed work. Furthermore, the amalgamation of PLS and ISAC is analyzed in a broader impact by considering attacks targeting data confidentiality, communication covertness, and sensing spoofing. The paper also serves as a tutorial by presenting several theoretical foundations on ISAC and PLS, which represent a practical guide for readers to develop novel secure ISAC network designs."
2505.05693,"Field Effect Transistors (FETs) are ubiquitous in electronics. As we scale FETs to ever smaller sizes, it becomes natural to ask how small a practical FET might be. We propose and analyze an atomically precise molecular FET (herein referred to as an ""mFET"") with 7,694 atoms made only of hydrogen and carbon atoms. It uses metallic (4,4) carbon nanotubes as the conductive leads, a linear segment of Lonsdaleite (hexagonal diamond) as the channel, Lonsdaleite as the insulating layer between the channel and the gate, and a (20,20) metallic carbon nanotube as the surrounding gate. The (4,4) nanotube leads are bonded to the channel using a mix of 5- and 6-membered rings, and to the gate using 5-, 6- and 7-membered rings. Issues of component design assessment and optimization using quantum chemical methods are discussed throughout. A 10 watt sugar-cube-sized computer made with $10^{18}$ such mFETs could deliver $\sim 10^{25}$ switching operations per second."
2505.0681,"To mitigate the barren plateau problem, effective parameter initialization is crucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) in the near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-driven approaches leveraged the optimal parameter concentration phenomenon, utilizing medium values of previously optimized QAOA parameters stored in databases as initialization for new graphs. However, this medium-value-based strategy lacks generalization capability. Conversely, prior computer-science-based approaches employed graph neural networks (GNNs) trained on previously optimized QAOA parameters to predict initialization values for new graphs. However, these approaches neglect key physics-informed QAOA principles, such as parameter concentration, symmetry, and adiabatic evolution, resulting in suboptimal parameter predictions and limited performance improvements. Furthermore, no existing GNN-based methods support parameter initialization for QAOA circuits with variable depths or for solving weighted Max-Cut problems. This paper introduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameter prediction. Compared to prior physics- and computer-science-driven methods, QSeer improves the initial approximation ratio and convergence speed of QAOA circuits across diverse graphs by 6%-68% and 5x-10x, respectively."
2505.07126,"A promising type of Reconfigurable Intelligent Surface (RIS) employs tunable control of its varactors using biasing transmission lines below the RIS reflecting elements. Biasing standing waves (BSWs) are excited by a time-periodic signal and sampled at each RIS element to create a desired biasing voltage and control the reflection coefficients of the elements. A simple rectifier can be used to sample the voltages and capture the peaks of the BSWs over time. Like other types of RIS, attempting to model and accurately configure a wave-controlled RIS is extremely challenging due to factors such as device non-linearities, frequency dependence, element coupling, etc., and thus significant differences will arise between the actual and assumed performance. An alternative approach to solving this problem is data-driven: Using training data obtained by sampling the reflected radiation pattern of the RIS for a set of BSWs, a neural network (NN) is designed to create an input-output map between the BSW amplitudes and the resulting sampled radiation pattern. This is the approach discussed in this paper. In the proposed approach, the NN is optimized using a genetic algorithm (GA) to minimize the error between the predicted and measured radiation patterns. The BSW amplitudes are then designed via Simulated Annealing (SA) to optimize a signal-to-leakage-plus-noise ratio measure by iteratively forward-propagating the BSW amplitudes through the NN and using its output as feedback to determine convergence. The resulting optimal solutions are stored in a lookup table to be used both as settings to instantly configure the RIS and as a basis for determining more complex radiation patterns."
2505.0717,"This paper examines how decentralized energy systems can be enhanced using collaborative Edge Artificial Intelligence. Decentralized grids use local renewable sources to reduce transmission losses and improve energy security. Edge AI enables real-time, privacy-preserving data processing at the network edge. Techniques such as federated learning and distributed control improve demand response, equipment maintenance, and energy optimization. The paper discusses key challenges including data privacy, scalability, and interoperability, and suggests solutions such as blockchain integration and adaptive architectures. Examples from virtual power plants and smart grids highlight the potential of these technologies. The paper calls for increased investment, policy support, and collaboration to advance sustainable energy systems."
2505.07179,"Physics-inspired computing paradigms are receiving renewed attention to enhance efficiency in compute-intensive tasks such as artificial intelligence and optimization. Similar to Hopfield neural networks, oscillatory neural networks (ONNs) minimize an Ising energy function that embeds the solutions of hard combinatorial optimization problems. Despite their success in solving unconstrained optimization problems, Ising machines still face challenges with constrained problems as they can become trapped in infeasible local minima. In this paper, we introduce a Lagrange ONN (LagONN) designed to escape infeasible states based on the theory of Lagrange multipliers. Unlike existing oscillatory Ising machines, LagONN employs additional Lagrange oscillators to guide the system towards feasible states in an augmented energy landscape, settling only when constraints are met. Taking the maximum satisfiability problem with three literals as a use case (Max-3-SAT), we harness LagONN's constraint satisfaction mechanism to find optimal solutions for random SATlib instances with up to 200 variables and 860 clauses, which provides a deterministic alternative to simulated annealing for coupled oscillators. We benchmark LagONN with SAT solvers and further discuss the potential of Lagrange oscillators to address other constraints, such as phase copying, which is useful in oscillatory Ising machines with limited connectivity."
2505.07711,"We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gates when mapping quantum algorithms onto quantum hardware but face computational challenges that restrict their application to circuits with no more than 5-6 qubits, necessitating the need to partition large circuits before the application of noisy quantum gate minimization algorithms. The existing generation of these algorithms is heuristic in nature and does not account for downstream gate minimization tasks. Large language models (LLMs) have the potential to change this and help improve quantum circuit partitions. This paper investigates the use of LLMs, such as Llama and Mistral, for partitioning quantum circuits by capitalizing on their abilities to understand and generate code, including QASM. Specifically, we teach LLMs to partition circuits using the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through experimental evaluations, we show that careful fine-tuning of open source LLMs enables us to obtain an accuracy of 53.4% for the partition task while over-the-shelf LLMs are unable to correctly partition circuits, using standard 1-shot and few-shot training approaches."
2505.10248,"Integrated circuit implementations of coupled oscillator networks have recently gained increased attention. The focus is usually on using these networks for analogue computing, for example for solving computational optimization tasks. For use within analog computing, these networks are run close to critical dynamics. On the other hand, such networks are also used as an analogy of transport networks such as electrical power grids to answer the question of how exactly such critical dynamic states can be avoided. However, simulating large network of coupled oscillators is computationally intensive, with specifc regards to electronic ones. We have developed an integrated circuit using integrated Phase-Locked Loop (PLL) with modifications, that allows to flexibly vary the topology as well as a complexity parameter of the network during operation. The proposed architecture, inspired by the brain, employs a clustered architecture, with each cluster containing 7 PLLs featuring programmable coupling mechanisms. Additionally, the inclusion of a RISC-V processor enables future algorithmic implementations. Thus, we provide a practical alternative for large-scale network simulations both in the field of analog computing and transport network stability research."
2505.10771,"Neuromorphic computing, characterized by its event-driven computation and massive parallelism, is particularly effective for handling data-intensive tasks in low-power environments, such as computing the minimum spanning tree (MST) for large-scale graphs. The introduction of dynamic synaptic modifications provides new design opportunities for neuromorphic algorithms. Building on this foundation, we propose an SNN-based union-sort routine and a pipelined version of Kruskal's algorithm for MST computation. The event-driven nature of our method allows for the concurrent execution of two completely decoupled stages: neuromorphic sorting and union-find. Our approach demonstrates superior performance compared to state-of-the-art Prim 's-based methods on large-scale graphs from the DIMACS10 dataset, achieving speedups by 269.67x to 1283.80x, with a median speedup of 540.76x. We further evaluate the pipelined implementation against two serial variants of Kruskal's algorithm, which rely on neuromorphic sorting and neuromorphic radix sort, showing significant performance advantages in most scenarios."
2505.11009,"A system architecture is suggested for a System on Chip that will combine several different memristor-based, bio-inspired computation arrays with inter- and intra-chip communication. It will serve as a benchmark system for future developments. The architecture takes the special requirements into account which are caused by the memristor co-integration on commercial CMOS structures in a post processing step of the chip. The interface considers the necessary data bandwidth to monitor the internal Network on Chip at speed and provides enough flexibility to give different measurement options."
2505.11236,"Fluorinated compounds, often referred to as forever chemicals, are critical in various steps of semiconductor fabrication like lithography, etching, chamber cleaning, and others. Forever chemical emissions can exhibit global warming potentials thousands of times greater than carbon dioxide and persist in the atmosphere for millennia. Despite their severe impact, most sustainability works in computer systems have focused on carbon emissions alone. We address this gap by introducing ForgetMeNot, a modeling tool that quantifies fluorinated compound emissions by integrating fabrication facility-specific practices and hardware specifications, and validate its accuracy using real-world emission data from fabrication facilities. We show how ForgetMeNot can enable fabrication facilities to optimize design and material usage decisions for emission reduction and provide researchers with a methodology to calibrate emission estimates for hardware designs. When ForgetMeNot is applied to analyze emissions for manufacturing CPUs, DRAM, and storage, it illustrates how hardware generations, lithography techniques, and capacities impact fluorinated compound emissions. Finally, we demonstrate how datacenter operators can assemble low-emission servers while balancing performance demands. By factoring in fluorinated emissions into manufacturing decisions, ForgetMeNot paves the way for building more sustainable systems."
2505.1273,"In this work, we present a recent investigation on leveraging large reconfigurable intelligent surfaces (RIS) as anchors for positioning in wireless communication systems. Unlike existing approaches, we explicitly address the uncertainty arising from the substantial physical size of the RIS, particularly relevant when a user equipment resides in the near field, and propose a method that ensures accurate positioning under these conditions. We derive the corresponding Cramer-Rao bound for our scheme and validate the effectiveness of our scheme through numerical experiments, highlighting both the feasibility and potential of our approach."
2505.1283,"Memristors are promising devices for scalable and low power, in-memory computing to improve the energy efficiency of a rising computational demand. The crossbar array architecture with memristors is used for vector matrix multiplication (VMM) and acts as kernels in neuromorphic computing. The analog conductance control in a memristor is achieved by applying voltage or current through it. A basic 1T1R array is suitable to avoid sneak path issues but suffer from wire resistances, which affects the read and write procedures. A conductance control scheme with a regulated voltage source will improve the architecture and reduce the possible potential divider effects. A change in conductance is also possible with the provision of a regulated current source and measuring the voltage across the memristors. A regulated 2T1R memristor conductance control architecture is proposed in this work, which avoids the potential divider effect and virtual ground scenario in a regular crossbar scheme, as well as conductance control by passing a regulated current through memristors. The sneak path current is not allowed to pass by the provision of ground potential to both terminals of memristors."
2505.13451,"Recurrent Neural Networks (RNN) are extensively employed for processing sequential data such as time series. Reservoir computing (RC) has drawn attention as an RNN framework due to its fixed network that does not require training, making it an attractive platform for hardware based machine learning. We establish an explicit correspondence between the well-established mathematical RC implementations of Echo State Networks and Band-pass Networks with Leaky Integrator nodes on the one hand and a physical circuit containing iontronic simple volatile memristors on the other. These aqueous iontronic devices employ ion transport through water as signal carriers, and feature a voltage-dependent (memory) conductance. The activation function and the dynamics of the Leaky Integrator nodes naturally materialise as the (dynamic) conductance properties of iontronic memristors, while a simple fixed local current-to-voltage update rule at the memristor terminals facilitates the relevant matrix coupling between nodes. We process various time series, including pressure data from simulated airways during breathing that can be directly fed into the network due to the intrinsic responsiveness of iontronic devices to applied pressures. We accomplish this by employing established physical equations of motion of iontronic memristors for the internal dynamics of the circuit."
2505.14303,"Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory (CIM) architectures offers a promising solution to overcome the von Neumann bottleneck. Due to non-idealities like cell variability, RRAM crossbars are often operated in binary mode, utilizing only two states: Low Resistive State (LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary Neural Networks (TNNs) are well-suited for this hardware due to their efficient mapping. Existing software projects for RRAM-based CIM typically focus on only one aspect: compilation, simulation, or Design Space Exploration (DSE). Moreover, they often rely on classical 8 bit quantization. To address these limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end compiler stack, multiple mapping options, and simulators, enabling a DSE flow for accuracy estimation across different crossbar parameters and mappings. CIM-Explorer can accompany the entire design process, from early accuracy estimation for specific crossbar parameters, to selecting an appropriate mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case studies, we demonstrate the expected accuracy for various mappings and crossbar parameters. CIM-Explorer can be found on GitHub."
2505.14829,"The conventional computer architecture has been facing challenges answering the ever-increasing demands from emerging applications, such as AI, for energy-efficient computation and memory hardware systems. Computational Random Access Memory (CRAM) represents a true in-memory computing paradigm that integrates logic and memory functions within the same array. At its core, CRAM relies on Magnetic Tunnel Junctions (MTJs), which serve as the foundational building blocks for implementing both memory storage and logic operations. However, a key challenge in CRAM lies in the non-ideal error rates associated with switching dynamics of MTJs, necessitating innovative approaches to reduce errors and optimize logic margins. This work proposes a novel approach of utilizing the voltage-controlled magnetic anisotropy (VCMA) to steepen the switching probability transfer curve (SPTC), thereby significantly reducing the logic operation error rate in CRAM. Using several numerical modeling tools, we validate the effectiveness of VCMA in modulating the energy barrier and switching dynamics in MTJs. It is revealed that the VCMA effect significantly reduces the error rate of CRAM by 61.43% at a VCMA coefficient of 200 fJ/V/m compared to CRAM without VCMA. The reduction of error rate is further rapidly amplified with an increasing TMR ratio. Furthermore, the introduction of the VCMA effect decreases the logic voltage (Vlogic) required for logic operations in CRAM and results in reduction of energy consumption. Our work serves as a first exploration in reducing the error rate in CRAM by modifying SPTC in MTJs."
2505.15757,"Knowing how to reliably use memristors as information storage devices is crucial not only to their role as emerging memories, but also for their application in neural network acceleration and as components of novel neuromorphic systems. In order to better understand the dynamics of information storage on memristors, it is essential to be able to characterise and measure their state. To this end, in this paper we propose a general, physics-inspired modelling approach for characterising the state of self-directed channel (SDC) memristors. Additionally, to enable the identification of the proposed state from device data, we introduce a noise-aware approach to the minimum-variance estimation of the state from voltage and current pairs."
2505.15936,"Analog computers hold promise to significantly reduce the energy consumption of artificial intelligence algorithms, but commercialization has been hampered by a fundamental scientific challenge - how to reliably store and process analog information with high precision. We present an approach based upon metal oxide memory cells that undergo controlled self-heating during programming with a newly developed, electro-thermo-chemical gate. The gate uniformly spreads heat and electrochemical reactions to enable wide, bulk-vacancy modulation which yields nine orders of magnitude in tunable analog resistance - three orders greater than other devices reported, with thousands of states. The gating profoundly reduces noise and drift to enable precision programming to targeted states within a few operations, lowering conductance errors by two orders of magnitude relative to other devices reported. Simulations show improvement in computational energy efficiency by at least 10x over other devices due to far greater scalability at higher precision. The results overturn long-held assumptions about the poor reliability and precision of analog resistance devices and opens the door to manufacturable, bulk metal-oxide devices and new applications that leverage high precision."
2505.16769,"Approximate computing is an effective computing paradigm for improving energy efficiency of error-tolerant applications. Approximate logic synthesis (ALS) is an automatic process to generate approximate circuits with reduced area, delay, and power, while satisfying user-specified error constraints. This paper focuses on ALS under the maximum error constraint. As an essential error metric that provides a worst-case error guarantee, the maximum error is crucial for many applications such as image processing and machine learning. This work proposes an efficient simulation-guided ALS flow that handles this constraint. It utilizes logic simulation to 1) prune local approximate changes (LACs) with large errors that violate the error constraint, and 2) accelerate the SAT-based LAC selection process. Furthermore, to enhance scalability, our ALS flow iteratively selects a set of promising LACs satisfying the error constraint to improve the efficiency. The experimental results show that compared with the state-of-the-art method, our ALS flow accelerates by 30.6 times, and further reduces circuit area and delay by 18.2% and 4.9%, respectively. Notably, our flow scales to large EPFL benchmarks with up to 38540 nodes, which cannot be handled by any existing ALS method for maximum error."
2505.16813,"Reservoir Computing (RC) with physical systems requires an understanding of the underlying structure and internal dynamics of the specific physical reservoir. In this study, physical nano-electronic networks with neuromorphic dynamics are investigated for their use as physical reservoirs in an RC framework. These neuromorphic networks operate as dynamic reservoirs, with node activities in general coupled to the edge dynamics through nonlinear nano-electronic circuit elements, and the reservoir outputs influenced by the underlying network connectivity structure. This study finds that networks with varying degrees of sparsity generate more useful nonlinear temporal outputs for dynamic RC compared to dense networks. Dynamic RC is also tested on an autonomous multivariate chaotic time series prediction task with networks of varying densities, which revealed the importance of network sparsity in maintaining network activity and overall dynamics, that in turn enabled the learning of the chaotic Lorenz63 system's attractor behavior."
2505.17239,"Driven by innovations in photonic computing and interconnects, photonic integrated circuit (PIC) designs advance and grow in complexity. Traditional manual physical design processes have become increasingly cumbersome. Available PIC layout tools are mostly schematic-driven, which has not alleviated the burden of manual waveguide planning and layout drawing. Previous research in PIC automated routing is largely adapted from electronic design, focusing on high-level planning and overlooking photonic-specific constraints such as curvy waveguides, bending, and port alignment. As a result, they fail to scale and cannot generate DRV-free layouts, highlighting the need for dedicated electronic-photonic design automation tools to streamline PIC physical design. In this work, we present LiDAR, the first automated PIC detailed router for large-scale designs. It features a grid-based, curvy-aware A* engine with adaptive crossing insertion, congestion-aware net ordering, and insertion-loss optimization. To enable routing in more compact and complex designs, we further extend our router to hierarchical routing as LiDAR 2.0. It introduces redundant-bend elimination, crossing space preservation, and routing order refinement for improved conflict resilience. We also develop and open-source a YAML-based PIC intermediate representation and diverse benchmarks, including TeMPO, GWOR, and Bennes, which feature hierarchical structures and high crossing densities. Evaluations across various benchmarks show that LiDAR 2.0 consistently produces DRV-free layouts, achieving up to 16% lower insertion loss and 7.69x speedup over prior methods on spacious cases, and 9% lower insertion loss with 6.95x speedup over LiDAR 1.0 on compact cases. Our codes are open-sourced atthis https URL."
2505.1904,"Waste management is a critical global issue with significant environmental and public health implications. It has become more destructive during large-scale events such as the annual pilgrimage to Makkah, Saudi Arabia, one of the world's largest religious gatherings. This event's popularity has attracted millions worldwide, leading to significant and un-predictable accumulation of waste. Such a tremendous number of visitors leads to in-creased waste management issues at the Grand Mosque and other holy sites, highlighting the need for an effective solution other than traditional methods based on rigid collection schedules.To address this challenge, this research proposed an innovative solution that is context-specific and tailored to the unique requirements of pilgrimage season: a Smart Waste Management System, called TUHR, that utilizes the Internet of Things and Artificial Intelligence. This system encompasses ultrasonic sensors that monitor waste levels in each container at the performance sites. Once the container reaches full capacity, the sensor communicates with the microcontroller, which alerts the relevant authorities. Moreover, our system can detect harmful substances such as gas from the gas detector sensor. Such a proactive and dynamic approach promises to mitigate the environmental and health risks associated with waste accumulation and enhance the cleanliness of these sites. It also delivers economic benefits by reducing unnecessary gasoline consumption and optimizing waste management resources. Importantly, this research aligns with the principles of smart cities and exemplifies the innovative, sustainable, and health-conscious approach that Saudi Arabia is implementing as part of its Vision 2030 initiative."
2505.19322,"Artificial intelligence (AI) and wireless networking advancements have created new opportunities to enhance network efficiency and performance. In this paper, we introduce Next-Generation GPT (NextG-GPT), an innovative framework that integrates retrieval-augmented generation (RAG) and large language models (LLMs) within the wireless systems' domain. By leveraging state-of-the-art LLMs alongside a domain-specific knowledge base, NextG-GPT provides context-aware real-time support for researchers, optimizing wireless network operations. Through a comprehensive evaluation of LLMs, including Mistral-7B, Mixtral-8x7B, LLaMa3.1-8B, and LLaMa3.1-70B, we demonstrate significant improvements in answer relevance, contextual accuracy, and overall correctness. In particular, LLaMa3.1-70B achieves a correctness score of 86.2% and an answer relevancy rating of 90.6%. By incorporating diverse datasets such as ORAN-13K-Bench, TeleQnA, TSpec-LLM, and Spec5G, we improve NextG-GPT's knowledge base, generating precise and contextually aligned responses. This work establishes a new benchmark in AI-driven support for next-generation wireless network research, paving the way for future innovations in intelligent communication systems."
2505.19916,"Modern systems exhibit unprecedented complexity due to their increased scale, interconnectedness, and the heterogeneity of their digital and physical components. In response to scaling challenges, the system-of-systems (SoS) paradigm proposes flexible aggregations of subsystems into a larger whole, while maintaining the independence of subsystems to various degrees. In response to the cyber-physical convergence, the digital twin (DT) paradigm proposes a tight coupling between digital and physical components through computational reflection and precise control. As these two paradigms address distinct parts of the overall challenge, combining the two promises more comprehensive methods to engineer what we call systems of twinned systems (SoTS). The noticeably growing body of knowledge on SoTS calls for a review of the state of the art. In this work, we report on our systematic literature survey of SoTS. We screened over 2500 potential studies, of which we included 80 and investigated them in detail. To converge SoS and DT, we derive a classification framework for SoTS that is backward compatible with the currently accepted theories of SoS and DT."
2505.21919,"The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient caching critical to reducing redundancy and improving speed. We analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1] and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of tailored storage solution for KVC prefilling, underscores the need for an efficient distributed caching system with optimized metadata management for LLM workloads, and provides insights into designing improved KVC management systems for scalable, low-latency inference."
2505.22868,"In this paper, we propose the CrossNAS framework, an automated approach for exploring a vast, multidimensional search space that spans various design abstraction layers-circuits, architecture, and systems-to optimize the deployment of machine learning workloads on analog processing-in-memory (PIM) systems. CrossNAS leverages the single-path one-shot weight-sharing strategy combined with the evolutionary search for the first time in the context of PIM system mapping and optimization. CrossNAS sets a new benchmark for PIM neural architecture search (NAS), outperforming previous methods in both accuracy and energy efficiency while maintaining comparable or shorter search times."
2505.24583,"Cognitive radio rate-splitting multiple access (CR-RSMA) has emerged as a promising multiple access framework that can efficiently manage interference and adapt dynamically to heterogeneous quality-of-service (QoS) requirements. To effectively support such demanding access schemes, programmable wireless environments have attracted considerable attention, especially through simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs), which can enable full-space control of signal propagation in asymmetric user deployments. In this paper, we propose the cognitive radio (CR) functionality for STAR-RIS-assisted CR-RSMA systems, leveraging the unique capability of the STAR-RIS to combine element and power splitting for adaptive control of transmission and reflection in CR scenarios. Specifically, the proposed CR functionality partitions the STAR-RIS into two regions independently controlling the transmission and reflection of signals, simultaneously ensuring the required QoS for the primary user and enhancing the performance of the secondary user. To accurately characterize the system performance, we derive analytical expressions for the ergodic rate of the secondary user and the outage rate of the primary user under Nakagami-m fading. Finally, simulation results show that the proposed approach effectively manages interference, guarantees the QoS of the primary user, and significantly improves the throughput of the secondary user, highlighting STAR-RIS as an efficient solution for CR-RSMA-based services."
2506.01045,"Analog/Mixed-Signal (AMS) circuits and systems continually present significant challenges to designers with the increase of design complexity and aggressive technology scaling. This is due to the large number of design factors and parameters that must be taken into account as well as the process variations which are prominent in nano-CMOS circuits. Design optimization techniques while presenting an accurate and fast design flow which can perform design optimization in reasonable time are still lacking. Even with techniques such as metamodeling that aid the design phase, there is still the need to improve them for accuracy and time cost. As a trade-off of the accuracy and speed, this paper presents a design flow for ultra-fast variability-aware optimization of nano-CMOS based physical design of analog circuits. It combines a Kriging bootstrapped Artificial Neural Network (ANN) metamodel with a Particle Swarm Optimization (PSO) based algorithm in the design optimization flow. The Kriging bootstrapped ANN metamodel provides a trade-off between analog-quality accuracy and scalability and can be effectively used for large and complex AMS circuits. The proposed technique uses Kriging to bootstrap target samples used for the ANN training. This introduces Kriging characteristics, which account for correlation effects between design parameters, to the ANN. The effectiveness of the design flow is demonstrated using a PLL as a case study with as many as 21 design parameters. It is observed that the bootstrapped Kriging metamodeling is 24X faster than simple ANN metamodeling. The layout optimization for such a complex circuit can be performed effectively in a short time using this approach. The optimization flow could achieve significant reductions in the mean and standard deviation of the PLL characteristics. Thus, the proposed research is a major contribution to design for cost."
2506.0283,"Major domains such as logistics, healthcare, and smart cities increasingly rely on sensor technologies and distributed infrastructures to monitor complex processes in real time. These developments are transforming the data landscape from discrete, structured records stored in centralized systems to continuous, fine-grained, and heterogeneous event streams collected across distributed environments. As a result, traditional process mining techniques, which assume centralized event logs from enterprise systems, are no longer sufficient. In this paper, we discuss the conceptual and methodological foundations for this emerging field. We identify three key shifts: from offline to online analysis, from centralized to distributed computing, and from event logs to sensor data. These shifts challenge traditional assumptions about process data and call for new approaches that integrate infrastructure, data, and user perspectives. To this end, we define a research agenda that addresses six interconnected fields, each spanning multiple system dimensions. We advocate a principled methodology grounded in algorithm engineering, combining formal modeling with empirical evaluation. This approach enables the development of scalable, privacy-aware, and user-centric process mining techniques suitable for distributed environments. Our synthesis provides a roadmap for advancing process mining beyond its classical setting, toward a more responsive and decentralized paradigm of process intelligence."
2506.08053,"With the commercial deployment of 5G and the in-depth research of 6G, the demand for high-speed data services in the next-generation fiber optic access systems is growing increasingly. Passive optical networks (PONs) have become a research hotspot due to their characteristics of low loss, high bandwidth, and low cost. However, the traditional orthogonal multiple access (OMA-PON) has difficulty meeting the requirements of the next-generation PON for high spectral efficiency and flexibility. In this paper, a novel transmission technology, namely power-domain sparse dimension constellation multiple access (PD-SDCMA), is proposed for the first time. Through the signal space dimension selection strategy (S2D-strategy) in the high-dimensional signal space, the low-dimensional constellation is sparsely superimposed into the high-dimensional space, thereby reducing multi-user interference and enhancing the system capacity. PD-SDCMA supports higher-order modulation formats and more access groups, and is also compatible with the existing orthogonal frequency division multiplexing (OFDM) architecture. The simulation results show that in a 25 km single-mode fiber system, compared with PD-NOMA and 3D-NOMA, PD-SDCMA can support more users and significantly reduce BER. This technology provides an efficient and low-cost solution for the evolution of Flexible PONs."
2506.08702,"The rapid development of Large Language Models (LLMs) opens up the possibility of using them as personal tutors. This has led to the development of several intelligent tutoring systems and learning assistants that use LLMs as back-ends with various degrees of engineering. In this study, we seek to compare human tutors with LLM tutors in terms of engagement, empathy, scaffolding, and conciseness. We ask human tutors to annotate and compare the performance of an LLM tutor with that of a human tutor in teaching grade-school math word problems on these qualities. We find that annotators with teaching experience perceive LLMs as showing higher performance than human tutors in all 4 metrics. The biggest advantage is in empathy, where 80% of our annotators prefer the LLM tutor more often than the human tutors. Our study paints a positive picture of LLMs as tutors and indicates that these models can be used to reduce the load on human teachers in the future."
2506.0948,"The non-destructive capacitance read-out of ferroelectric capacitors (FeCaps) based on doped HfO$_2$ metal-ferroelectric-metal (MFM) structures offers the potential for low-power and highly scalable crossbar arrays. This is due to a number of factors, including the selector-less design, the absence of sneak paths, the power-efficient charge-based read operation, and the reduced IR drop. Nevertheless, a reliable capacitive readout presents certain challenges, particularly in regard to device variability and the trade-off between read yield and read disturbances, which can ultimately result in bit-flips. This paper presents a digital read macro for HfO$_2$ FeCaps and provides design guidelines for capacitive readout of HfO$_2$ FeCaps, taking device-centric reliability and yield challenges into account. An experimentally calibrated physics-based compact model of HfO$_2$ FeCaps is employed to investigate the reliability of the read-out operation of the FeCap macro through Monte Carlo simulations. Based on this analysis, we identify limitations posed by the device variability and propose potential mitigation strategies through design-technology co-optimization (DTCO) of the FeCap device characteristics and the CMOS circuit design. Finally, we examine the potential applications of the FeCap macro in the context of secure hardware. We identify potential security threats and propose strategies to enhance the robustness of the system."
2506.09963,"Quantum algorithms offer an exponential speedup over classical algorithms for a range of computational problems. The fundamental mechanisms underlying quantum computation required the development and construction of quantum computers. These devices are referred to as NISQ (Noisy Intermediate-Scale Quantum) devices. Not only are NISQ devices extremely limited in their qubit count but they also suffer from noise during computation and this problem only gets worse as the size of the circuit increases which limits the practical use of quantum computers for modern day applications. This paper will focus on utilizing quantum circuit partitioning to overcome the inherent issues of NISQ devices. Partitioning a quantum circuit into smaller subcircuits has allowed for the execution of quantum circuits that are too large to fit on one quantum device. There have been many previous approaches to quantum circuit partitioning and each of these approaches differ in how they work with some focusing on hardware-aware partitioning, optimal graph-based partitioning, multi-processor architectures and many more. These approaches achieve success in their objective but they often fail to scale well which impacts cost and noise. The ultimate goal of this paper is to mitigate these issues by minimizing 3 important metrics; noise, time and cost. To achieve this we use dynamic partitioning for practical circuit cutting and we take advantage of the benefits of hybrid execution where classical computation will be used alongside quantum hardware. This approach has proved to be beneficial with respect to noise with classical execution enabling a 42.30% reduction in noise and a 40% reduction in the number of qubits required in cases where a mixture of classical and quantum computation were required."
2506.11565,"This paper explores the application of the parameter-shift rule (PSR) for computing gradients in unitary optical neural networks (UONNs). While backpropagation has been fundamental to training conventional neural networks, its implementation in optical neural networks faces significant challenges due to the physical constraints of optical systems. We demonstrate how PSR, which calculates gradients by evaluating functions at shifted parameter values, can be effectively adapted for training UONNs constructed from Mach-Zehnder interferometer meshes. The method leverages the inherent Fourier series nature of optical interference in these systems to compute exact analytical gradients directly from hardware measurements. This approach offers a promising alternative to traditional in silico training methods and circumvents the limitations of both finite difference approximations and all-optical backpropagation implementations. We present the theoretical framework and practical methodology for applying PSR to optimize phase parameters in optical neural networks, potentially advancing the development of efficient hardware-based training strategies for optical computing systems."
2506.1221,"Machine intelligence on edge devices enables low-latency processing and improved privacy, but is often limited by the energy and delay of moving and converting data. Current systems frequently avoid local model storage by sending queries to a server, incurring uplink cost, network latency, and privacy risk. We present the opposite approach: broadcasting model weights to clients that perform inference locally using in-physics computation inside the radio receive chain. A base station transmits weights as radio frequency (RF) waveforms; the client encodes activations onto the waveform and computes the result using existing mixer and filter stages, RF components already present in billions of edge devices such as cellphones, eliminating repeated signal conversions and extra hardware. Analysis shows that thermal noise and nonlinearity create an optimal energy window for accurate analog inner products. Hardware-tailored training through a differentiable RF chain preserves accuracy within this regime. Circuit-informed simulations, consistent with a companion experiment, demonstrate reduced memory and conversion overhead while maintaining high accuracy in realistic wireless edge scenarios."
2506.12264,"As the technology node continues to shrink, nanosheet field effect transistors (NSFETs) and complementary FETs (CFETs) become valid candidates for the 3nm and sub-nanometre nodes. However, due to the shrinking device size, self-heating and inter-device thermal crosstalk of NSFETs and CFETs become more severe. It is important to accurately calculate the self-heating and thermal crosstalk of devices and to study the electrical and thermal characteristics of logic gates, etc. In this work, a thermal network model considering the thermal crosstalk of neighboring devices is proposed, which can accurately calculate the self-heating and thermal crosstalk. The electrical and thermal characteristics of NSFETs and CFETs are compared, and it is found that CFETs have more severe self-heating and thermal crosstalk. The electro-thermal characteristics of inverters, logic gates and ring oscillators composed of NSFETs and CFETs are further investigated. Compared with NSFETs, logic gates and ring oscillators composed of CFETs are more seriously affected by self-heating and should be given extra attention. The thermal network model proposed in this paper can be further used to study the thermal optimization strategy of devices and circuits to enhance the electrical performance, achieving the design technology co-optimizations (DTCO)."
2506.12794,"In this paper, we present a spatially consistent A2G channel model based on probabilistic LOS/NLOS segmentation to parameterize the deterministic path loss and stochastic shadow fading model. Motivated by the limitations of existing Unmanned Aerial Vehicle (UAV) channel models that overlook spatial correlation, our approach reproduces LOS/NLOS transitions along ground user trajectories in urban environments. This model captures environment-specific obstructions by means of azimuth and elevation-dependent LOS probabilities without requiring a full detailed 3D representation of the surroundings. We validate our framework against a geometry-based simulator by evaluating it across various urban settings. The results demonstrate its accuracy and computational efficiency, enabling further realistic derivations of path loss and shadow fading models and thorough outage analysis."
2506.12795,"Just like power, water and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient to unforeseen events, able to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Despite its critical importance, resilience remains an elusive concept, with its mathematical foundations still underdeveloped. Unlike robustness and reliability, resilience is premised on the fact that disruptions will inevitably happen. Resilience, in terms of elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents (or networks) that can flexibly expand their states, hypotheses and course of actions, by transforming through real-time adaptation and reconfiguration. This constant situational awareness and vigilance of adapting world models and counterfactually reasoning about potential system failures and the corresponding best responses, is a core aspect of resilience. This article seeks to first define resilience and disambiguate it from reliability and robustness, before delving into the mathematics of resilience. Finally, the article concludes by presenting nuanced metrics and discussing trade-offs tailored to the unique characteristics of network resilience."
2506.12962,"Fully Homomorphic Encryption (FHE) facilitates secure computations on encrypted data but imposes significant demands on memory bandwidth and computational power. While current FHE accelerators focus on optimizing computation, they often face bandwidth limitations that result in performance bottlenecks, particularly in memory-intensive operations. This paper presents OptoLink, a scalable photonic interconnect architecture designed to address these bandwidth and latency challenges in FHE systems. OptoLink achieves a throughput of 1.6 TB/s with 128 channels, providing 300 times the bandwidth of conventional electrical interconnects. The proposed architecture improves data throughput, scalability, and reduces latency, making it an effective solution for meeting the high memory and data transfer requirements of modern FHE accelerators."
2506.13744,"This article describes lcpy, an open-source python package that allows for advanced parametric Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) analysis. The package is designed to allow the user to model a process with a flexible, modular design based on dictionaries and lists. The modeling can consider in-time variations, uncertainty, and allows for dynamic analysis, uncertainty assessment, as well as conventional static LCA and LCC. The package is compatible with optimization and uncertainty analysis libraries as well as python packages for prospective LCA. Its goal is to allow for easy implementation of dynamic LCA and LCC and for simple integration with tools for uncertainty assessment and optimization towards a more widened implementation of advanced enviro-economic analysis. The open-source code can be found atthis https URL."
2506.14676,"Hardware implementations of the Ising model offer promising solutions to large-scale optimization tasks. In the literature, various nanodevices have been shown to emulate the spin dynamics for such Ising machines with remarkable effectiveness. Other nanodevices have been shown to implement spin-spin coupling with compact footprint and minimal energy dissipation. However, an ideal Ising machine would associate both types of nanodevices, and they must operate synergistically to support annealing: a progressive reduction of machine stochasticity that allows it to settle to energy minimum. Here, we report an Ising machine that combines two nanotechnologies: memristor crossbar -- storing multi-level couplings -- and stochastic magnetic tunnel junction (SMTJ), acting as thermally driven spins. Because the same read voltage that interrogates the crossbar also biases the SMTJs, increasing this voltage automatically lowers the effective temperature of the machine, providing an intrinsic, nearly circuit-free annealing technique. Operating at zero magnetic field, our prototype consistently reaches the global optimum of a 24-vertex weighted MAX-CUT and a 10-vertex, three-color graph-coloring problem. Given that both nanotechnologies in our demonstrator are CMOS-integrated, this approach is compatible with advanced 3D integration, offering a scalable pathway toward compact, fast, and energy-efficient large-scale Ising solvers."
2506.16,"Navigation is a very crucial aspect of autonomous vehicle ecosystem which heavily relies on collecting and processing large amounts of data in various states and taking a confident and safe decision to define the next vehicle maneuver. In this paper, we propose a novel architecture based on Quantum Artificial Intelligence by enabling quantum and AI at various levels of navigation decision making and communication process in Autonomous vehicles : Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum reinforcement learning for navigation policy optimization and finally post-quantum cryptographic protocols for secure communication. Quantum neural networks uses quantum amplitude encoding to fuse data from various sensors like LiDAR, radar, camera, GPS and weather etc., This approach gives a unified quantum state representation between heterogeneous sensor modalities. Nav-Q module processes the fused quantum states through variational quantum circuits to learn optimal navigation policies under swift dynamic and complex conditions. Finally, post quantum cryptographic protocols are used to secure communication channels for both within vehicle communication and V2X (Vehicle to Everything) communications and thus secures the autonomous vehicle communication from both classical and quantum security threats. Thus, the proposed framework addresses fundamental challenges in autonomous vehicles navigation by providing quantum performance and future proof security. Index Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion"
2506.16281,"Atmospheric sciences are crucial for understanding environmental phenomena ranging from air quality to extreme weather events, and climate change. Recent breakthroughs in sensing, communication, computing, and Artificial Intelligence (AI) have significantly advanced atmospheric sciences, enabling the generation of vast amounts of data through long-term Earth observations and providing powerful tools for analyzing atmospheric phenomena and predicting natural disasters. This paper contributes a critical interdisciplinary overview that bridges the fields of atmospheric science and computer science, highlighting the transformative potential of AI in atmospheric research. We identify key challenges associated with integrating AI into atmospheric research, including issues related to big data and infrastructure, and provide a detailed research roadmap that addresses both current and emerging challenges."
2506.1856,"Beamforming enhances signal strength and quality by focusing energy in specific directions. This capability is particularly crucial in cell-free integrated sensing and communication (ISAC) systems, where multiple distributed access points (APs) collaborate to provide both communication and sensing services. In this work, we first derive the distribution of joint target detection probabilities across multiple receiving APs under false alarm rate constraints, and then formulate the beam selection procedure as a Markov decision process (MDP). We establish a deep reinforcement learning (DRL) framework, in which reward shaping and sinusoidal embedding are introduced to facilitate agent learning. To eliminate the high costs and associated risks of real-time agent-environment interactions, we further propose a novel digital twin (DT)-assisted offline DRL approach. Different from traditional online DRL, a conditional generative adversarial network (cGAN)-based DT module, operating as a replica of the real world, is meticulously designed to generate virtual state-action transition pairs and enrich data diversity, enabling offline adjustment of the agent's policy. Additionally, we address the out-of-distribution issue by incorporating an extra penalty term into the loss function design. The convergency of agent-DT interaction and the upper bound of the Q-error function are theoretically derived. Numerical results demonstrate the remarkable performance of our proposed approach, which significantly reduces online interaction overhead while maintaining effective beam selection across diverse conditions including strict false alarm control, low signal-to-noise ratios, and high target velocities."
2506.18799,"Quantum computing has shown significant potential to address complex optimization problems; however, its application remains confined to specific problems at limited scales. Spatial regionalization remains largely unexplored in quantum computing due to its complexity and large number of variables. In this paper, we introduce the first hybrid quantum-classical method to spatial regionalization by decomposing the problem into manageable subproblems, leveraging the strengths of both classical and quantum computation. This study establishes a foundational framework for effectively integrating quantum computing methods into realistic and complex spatial optimization tasks. Our initial results show a promising quantum performance advantage for a broad range of spatial regionalization problems and their variants."
2506.19487,"As chiplet-based integration and many-core architectures become the norm in high-performance computing, on-chip wireless communication has emerged as a compelling alternative to traditional interconnects. However, scalable Medium Access Control (MAC) remains a fundamental challenge, particularly under dense traffic and limited spectral resources. This paper presents TRMAC, a novel cross-layer MAC protocol that exploits the spatial focusing capability of Time Reversal (TR) to enable multiple parallel transmissions over a shared frequency channel. By leveraging the quasi-deterministic nature of on-chip wireless channels, TRMAC pre-characterizes channel impulse responses to coordinate access using energy-based thresholds, eliminating the need for orthogonal resource allocation or centralized arbitration. Through detailed physical-layer simulation and system-level evaluation on diverse traffic, TRMAC demonstrates comparable or superior performance to existing multi-channel MAC protocols, achieving low latency, high throughput, and strong scalability across hundreds of cores. TRMAC provides a low-complexity, high-efficiency solution for future Wireless Networks-on-Chip (WNoCs), particularly in chiplet-based systems where spatial reuse and modularity are critical. With simulations we prove that TRMAC can be utilized for parallel transmissions with a single frequency channel with a similar throughput and latency as in using multiple frequency bands omitting the need for complex transceivers. This work establishes a new design direction for MAC protocols that are tightly integrated with the underlying channel physics to meet the demands of next-generation computing platforms."
2506.19491,"The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has expanded their deployment potential to indoor and hard-to-reach areas. However, this trend introduces distinct challenges, particularly in terms of flight dynamics and power consumption, which limit the UAVs' autonomy and mission capabilities. This paper presents a novel approach to overcoming these limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV systems for fine-grained 3-Dimensional (3D) digital reconstruction of small static objects. Specifically, we design, implement, and evaluate an N3DR-based pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and Splatfacto, to improve the quality of 3D reconstructions using images of the object captured by a fleet of small UAVs. We assess the performance of the considered models using various imagery and pointcloud metrics, comparing them against the baseline Structure from Motion (SfM) algorithm. The experimental results demonstrate that the N3DR-enhanced pipeline significantly improves reconstruction quality, making it feasible for small UAVs to support high-precision 3D mapping and anomaly detection in constrained environments. In more general terms, our results highlight the potential of N3DR in advancing the capabilities of miniaturized UAV systems."
2506.19642,"Threshold logic gates (TLGs) have been proposed as artificial counterparts of biological neurons with classification capabilities based on a linear predictor function combining a set of weights with the feature vector. The linearity of TLGs limits their classification capabilities requiring the use of networks for the accomplishment of complex tasks. A generalization of the TLG model called receptron, characterized by input-dependent weight functions allows for a significant enhancement of classification performances even with the use of a single unit. Here we formally demonstrate that a receptron, characterized by nonlinear input-dependent weight functions, exhibit intrinsic selective activation properties for analog inputs, when the input vector is within cubic domains in a 3D space. The proposed model can be extended to the n-dimensional case for multidimensional applications. Our results suggest that receptron-based networks can represent a new class of devices capable to manage a large number of analog inputs, for edge applications requiring high selectivity and classification capabilities without the burden of complex training."
2506.22052,"V2X communication has become crucial for enhancing road safety, especially for Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the increasing number of devices communicating on the same channels will lead to significant channel load. To address this issue this study evaluates the effectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM), focusing specifically on cyclists. The objective of RM is to minimize the transmission of redundant information. We conducted a simulation study using a urban scenario with a high bicycle density based on traffic data from Hannover, Germany. This study assessed the impact of RM on channel load, measured by Channel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in simulation. To evaluate the accuracy and reliability of the RM mechanisms, we analyzed the actual differences in position, speed, and heading between the ego VRU and the VRU, which was assumed to be redundant. Our findings indicate that while RM can reduce channel congestion, it also leads to a decrease in VPR. The analysis of actual differences revealed that the RM mechanism standardized by ETSI often uses outdated information, leading to significant discrepancies in position, speed, and heading, which could result in dangerous situations. To address these limitations, we propose an adapted RM mechanism that improves the balance between reducing channel load and maintaining VRU awareness. The adapted approach shows a significant reduction in maximum CBR and a less significant decrease in VPR compared to the standardized RM. Moreover, it demonstrates better performance in the actual differences in position, speed, and heading, thereby enhancing overall safety. Our results highlight the need for further research to optimize RM techniques and ensure they effectively enhance V2X communication without compromising the safety of VRUs."
2506.22227,"We present a fabricated and experimentally characterized memory stack that unifies memristive and memcapacitive behavior. Exploiting this dual functionality, we design a circuit enabling simultaneous control of spatial and temporal dynamics in recurrent spiking neural networks (RSNNs). Hardware-aware simulations highlight its promise for efficient neuromorphic processing."
2506.22677,"Accurate prediction of protein active site structures remains a central challenge in structural biology, particularly for short and flexible peptide fragments where conventional methods often fail. Here, we present a quantum computing framework specifically developed for utility-level quantum processors to address this problem. Starting from an amino acid sequence, we formulate the structure prediction task as a ground-state energy minimization problem using the Variational Quantum Eigensolver (VQE). Amino acid connectivity is encoded on a tetrahedral lattice model, and structural constraints-including steric, geometric, and chirality terms-are mapped into a problem-specific Hamiltonian expressed as sparse Pauli operators. The optimization is executed via a two-stage architecture separating energy estimation and measurement decoding, allowing noise mitigation under realistic quantum device conditions. We evaluate the framework on 23 randomly selected real protein fragments from the PDBbind dataset, as well as 7 real fragments from proteins with therapeutic potential, and run the experiments on the IBM-Cleveland Clinic quantum processor. Structural predictions are benchmarked against AlphaFold3 (AF3) using identical postprocessing and docking procedures. Our quantum method outperformed AF3 in both RMSD (Root-Mean-Square Deviation) and docking efficacy. This work demonstrates, for the first time, a complete end-to-end pipeline for biologically relevant structure prediction on real quantum hardware, highlighting its engineering feasibility and practical advantage over existing classical and deep learning approaches."
2506.23185,"Modern data-intensive applications demand memory solutions that deliver high-density, low-power, and integrated computational capabilities to reduce data movement overhead. This paper presents the use of Gain-Cell embedded DRAM (GC-eDRAM) - a compelling alternative to traditional SRAM and eDRAM - for stateful, in-memory logic. We propose a circuit design that exploits GC-eDRAM's dual-port architecture and nondestructive read operation to perform logic functions directly within the GC-eDRAM memory array. Our simulation results demonstrate a 5us retention time coupled with a 99.5% success rate for computing the logic gates. By incorporating processing-in-memory (PIM) functionality into GC-eDRAM, our approach enhances memory and compute densities, lowers power consumption, and improves overall performance for data-intensive applications."
2506.23405,"In contemporary general-purpose graphics processing units (GPGPUs), the continued increase in raw arithmetic throughput is constrained by the capabilities of the register file (single-cycle) and last-level cache (high bandwidth), which require the delivery of operands at a cadence demanded by wide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity, density, or bandwidth of these memories can unlock substantial performance gains; however, the recent stagnation of SRAM bit-cell scaling leads to inequivalent losses in compute density.To address the challenges posed by SRAM's scaling and leakage power consumption, this paper explores the potential CMOS+X integration of amorphous oxide semiconductor (AOS) transistors in capacitive, persistent memory topologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in multi-ported and high-bandwidth banked GPGPU memories. A detailed study of the density and energy tradeoffs of back-end-of-line (BEOL) integrated memories utilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while accounting for the macro-level limitations of integrating AOS candidate structures proposed by the device community (an aspect often overlooked in prior work). By exploiting the short lifetime of register operands, we propose a multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of the footprint of SRAM with over 70% lower standby power, enabling enhancements to compute capacity, such as larger warp sizes or processor counts. Benchmarks run on a validated NVIDIA Ampere-class GPU model, using a modified version of Accel-Sim, demonstrate improvements of up to 5.2x the performance per watt and an average 8% higher geometric mean instruction per cycle (IPC) on various compute- and memory-bound tasks."
2506.23826,"Human Digital Twins (HDTs) have traditionally been conceptualized as data-driven models designed to support decision-making across various domains. However, recent advancements in conversational AI open new possibilities for HDTs to function as authentic, interactive digital counterparts of individuals. This paper introduces a novel HDT system architecture that integrates large language models with dynamically updated personal data, enabling it to mirror an individual's conversational style, memories, and behaviors. To achieve this, our approach implements context-aware memory retrieval, neural plasticity-inspired consolidation, and adaptive learning mechanisms, creating a more natural and evolving digital persona. The resulting system does not only replicate an individual's unique conversational style depending on who they are speaking with, but also enriches responses with dynamically captured personal experiences, opinions, and memories. While this marks a significant step toward developing authentic virtual counterparts, it also raises critical ethical concerns regarding privacy, accountability, and the long-term implications of persistent digital identities. This study contributes to the field of HDTs by describing our novel system architecture, demonstrating its capabilities, and discussing future directions and emerging challenges to ensure the responsible and ethical development of HDTs."
2507.00306,"Estimating Origin-Destination (OD) travel demand is vital for effective urban planning and traffic management. Developing universally applicable OD estimation methodologies is significantly challenged by the pervasive scarcity of high-fidelity traffic data and the difficulty in obtaining city-specific prior OD estimates (or seed ODs), which are often prerequisite for traditional approaches. Our proposed method directly estimates OD travel demand by systematically leveraging aggregated, anonymized statistics from Google Maps Traffic Trends, obviating the need for conventional census or city-provided OD data. The OD demand is estimated by formulating a single-level, one-dimensional, continuous nonlinear optimization problem with nonlinear equality and bound constraints to replicate highway path travel times. The method achieves efficiency and scalability by employing a differentiable analytical macroscopic network model. This model by design is computationally lightweight, distinguished by its parsimonious parameterization that requires minimal calibration effort and its capacity for instantaneous evaluation. These attributes ensure the method's broad applicability and practical utility across diverse cities globally. Using segment sensor counts from Los Angeles and San Diego highway networks, we validate our proposed approach, demonstrating a two-thirds to three-quarters improvement in the fit to segment count data over a baseline. Beyond validation, we establish the method's scalability and robust performance in replicating path travel times across diverse highway networks, including Seattle, Orlando, Denver, Philadelphia, and Boston. In these expanded evaluations, our method not only aligns with simulation-based benchmarks but also achieves an average 13% improvement in it's ability to fit travel time data compared to the baseline during afternoon peak hours."
2507.00444,"Analog circuit design consists of the pre-layout and layout phases. Among them, the pre-layout phase directly decides the final circuit performance, but heavily depends on experienced engineers to do manual design according to specific application scenarios. To overcome these challenges and automate the analog circuit pre-layout design phase, we introduce DiffCkt: a diffusion model-based hybrid neural network framework for the automatic transistor-level generation of analog circuits, which can directly generate corresponding circuit structures and device parameters tailored to specific performance requirements. To more accurately quantify the efficiency of circuits generated by DiffCkt, we introduce the Circuit Generation Efficiency Index (CGEI), which is determined by both the figure of merit (FOM) of a single generated circuit and the time consumed. Compared with relative research, DiffCkt has improved CGEI by a factor of $2.21 \sim 8365\times$, reaching a state-of-the-art (SOTA) level. In conclusion, this work shows that the diffusion model has the remarkable ability to learn and generate analog circuit structures and device parameters, providing a revolutionary method for automating the pre-layout design of analog circuits. The circuit dataset will be open source, its preview version is available atthis https URL."
2507.0071,"Unmanned aerial vehicles (UAVs) are recognized as a promising candidate for the multi-access edge computing (MEC) in the future sixth generation communication networks. However, the aerial eavesdropping UAVs (EUAVs) pose a significant security threat to the data offloading. In this paper, we investigate a robust MEC scenario with multiple service UAVs (SUAVs) towards the potential eavesdropping from the EUAV, in which the random parameters such as task complexities are considered in the practical applications. In detail, the problem is formulated to optimize the deployment positions of SUAVs, the connection relationships between GUs and SUAVs, and the offloading ratios. With the uncertain task complexities, the corresponding chance constraints are constructed under the uncertainty set, which is tricky to deal with. Therefore, we first optimize the pre-deployment of SUAVs by the K-means algorithm. Then, the distributionally robust optimization method is employed, and the conditional value at risk is utilized to transform the chance constraints into convex forms, which can be solved via convex toolkits. Finally, the simulation results show that with the consideration of uncertainties, just 5% more energy is consumed compared with the ideal circumstance, which verifies the robustness of the proposed algorithms."
2507.01429,"Deep neural networks generate and process large volumes of data, posing challenges for low-resource embedded systems. In-memory computing has been demonstrated as an efficient computing infrastructure and shows promise for embedded AI applications. Among newly-researched memory technologies, racetrack memory is a non-volatile technology that allows high data density fabrication, making it a good fit for in-memory computing. However, integrating in-memory arithmetic circuits with memory cells affects both the memory density and power efficiency. It remains challenging to build efficient in-memory arithmetic circuits on racetrack memory within area and energy constraints. To this end, we present an efficient in-memory convolutional neural network (CNN) accelerator optimized for use with racetrack memory. We design a series of fundamental arithmetic circuits as in-memory computing cells suited for multiply-and-accumulate operations. Moreover, we explore the design space of racetrack memory based systems and CNN model architectures, employing co-design to improve the efficiency and performance of performing CNN inference in racetrack memory while maintaining model accuracy. Our designed circuits and model-system co-optimization strategies achieve a small memory bank area with significant improvements in energy and performance for racetrack memory based embedded systems."
2507.0354,"Quantum computers use quantum mechanical phenomena to perform conventionally intractable calculations for specific problems. Despite being universal machines, quantum computers are not expected to replace classical computers, but rather, to complement them and form hybrid systems. This makes integrating quantum computers into high performance computing (HPC) systems an increasingly relevant topic. We present a structured literature review on the integration aspect. We methodologically search literature databases and manually evaluate 107 publications. These publications are divided into seven categories that describe the state of the art in each category. After a brief quantitative analysis of the literature, this survey deals with the hardware architecture of hybrid quantum-classical systems, as well as the software stack. We observe the development of a wide range of tools enabling hybrid systems and emphasize the need for future standardization of interfaces and methods to foster synergy."
2507.04171,"The second annual NSF, OAC CSSI, CyberTraining and related programs PI meeting was held August 12 to 13 in Charlotte, NC, with participation from PIs or representatives of all major awards. Keynotes, panels, breakouts, and poster sessions allowed PIs to engage with each other, NSF staff, and invited experts. The 286 attendees represented 292 awards across CSSI, CyberTraining, OAC Core, CIP, SCIPE CDSE, and related programs, and presented over 250 posters. This report documents the meetings structure, findings, and recommendations, offering a snapshot of current community perspectives on cyberinfrastructure. A key takeaway is a vibrant, engaged community advancing science through CI. AI-driven research modalities complement established HPC and data centric tools. Workforce development efforts align well with the CSSI community."
2507.04818,"The widespread deployment of embedded systems in critical infrastructures, interconnected edge devices like autonomous drones, and smart industrial systems requires robust security measures. Compromised systems increase the risks of operational failures, data breaches, and -- in safety-critical environments -- potential physical harm to people. Despite these risks, current security measures are often insufficient to fully address the attack surfaces of embedded devices. CHERI provides strong security from the hardware level by enabling fine-grained compartmentalization and memory protection, which can reduce the attack surface and improve the reliability of such devices. In this work, we explore the potential of CHERI to compartmentalize one of the most critical and targeted components of interconnected systems: their network stack. Our case study examines the trade-offs of isolating applications, TCP/IP libraries, and network drivers on a CheriBSD system deployed on the Arm Morello platform. Our results suggest that CHERI has the potential to enhance security while maintaining performance in embedded-like environments."
2507.05523,"With a growing interest in securing user data within the internet-of-things (IoT), embedded encryption has become of paramount importance, requiring light-weight high-quality Random Number Generators (RNGs). Emerging stochastic device technologies produce random numbers from stochastic physical processes at high quality, however, their generated random number streams are adversely affected by process and supply voltage variations, which can lead to bias in the generated streams. In this work, we present an adaptive variation-resilient RNG capable of extracting unbiased encryption-grade random number streams from physically driven entropy sources, for embedded cryptography applications. As a proof of concept, we employ a stochastic magnetic tunnel junction (sMTJ) device as an entropy source. The impact of variations in the sMTJ is mitigated by employing an adaptive digitizer with an adaptive voltage reference that dynamically tracks any stochastic signal drift or deviation, leading to unbiased random bit stream generation. The generated unbiased bit streams, due to their higher entropy, then only need to undergo simplified post-processing. Statistical randomness tests based on the National Institute of Standards and Technology (NIST) test suite are conducted on bit streams obtained using simulations and FPGA entropy source emulation experiments, validating encryption-grade randomness at a significantly reduced hardware cost, and across a wide range of process-induced device variations and supply voltage fluctuations."
2507.06063,"Physical reservoir computing is an innovative idea for using physical phenomena as computational resources. Recent research has revealed that information processing techniques can improve the performance, but for practical applications, it is equally important to study the level of performance with a simple design that is easy to construct experimentally. We focus on a reservoir composed of independent hysteretic systems as a model suitable for the practical implementation of physical reservoir computing. In this paper, we discuss the appropriate design of this reservoir, its performance, and its limitations. This research will serve as a practical guideline for constructing hysteresis-based reservoirs."
2507.06156,"Blockchain bridges have become essential infrastructure for enabling interoperability across different blockchain networks, with more than $24B monthly bridge transaction volume. However, their growing adoption has been accompanied by a disproportionate rise in security breaches, making them the single largest source of financial loss in Web3. For cross-chain ecosystems to be robust and sustainable, it is essential to understand and address these vulnerabilities. In this study, we present a comprehensive systematization of blockchain bridge design and security. We define three bridge security priors, formalize the architectural structure of 13 prominent bridges, and identify 23 attack vectors grounded in real-world blockchain exploits. Using this foundation, we evaluate 43 representative attack scenarios and introduce a layered threat model that captures security failures across source chain, off-chain, and destination chain components.Our analysis at the static code and transaction network levels reveals recurring design flaws, particularly in access control, validator trust assumptions, and verification logic, and identifies key patterns in adversarial behavior based on transaction-level traces. To support future development, we propose a decision framework for bridge architecture design, along with defense mechanisms such as layered validation and circuit breakers. This work provides a data-driven foundation for evaluating bridge security and lays the groundwork for standardizing resilient cross-chain infrastructure."
2507.06981,"This paper presents a reinforcement learning (RL) based approach to improve the physical layer security (PLS) of an underlay cognitive radio network (CRN) over cascaded channels. These channels are utilized in highly mobile networks such as cognitive vehicular networks (CVN). In addition, an eavesdropper aims to intercept the communications between secondary users (SUs). The SU receiver has full-duplex and energy harvesting capabilities to generate jamming signals to confound the eavesdropper and enhance security. Moreover, the SU transmitter extracts energy from ambient radio frequency signals in order to power subsequent transmissions to its intended receiver. To optimize the privacy and reliability of the SUs in a CVN, a deep Q-network (DQN) strategy is utilized where multiple DQN agents are required such that an agent is assigned at each SU transmitter. The objective for the SUs is to determine the optimal transmission power and decide whether to collect energy or transmit messages during each time period in order to maximize their secrecy rate. Thereafter, we propose a DQN approach to maximize the throughput of the SUs while respecting the interference threshold acceptable at the receiver of the primary user. According to our findings, our strategy outperforms two other baseline strategies in terms of security and reliability."
2507.06983,"Cognitive radio networks (CRNs) are acknowledged for their ability to tackle the issue of spectrum under-utilization. In the realm of CRNs, this paper investigates the energy efficiency issue and addresses the critical challenge of optimizing system reliability for overlay CRN access mode. Randomly dispersed secondary users (SUs) serving as relays for primary users (PUs) are considered, in which one of these relays is designated to harvest energy through the time switching-energy harvesting (EH) protocol. Moreover, this relay amplifies-and-forwards (AF) the PU's messages and broadcasts them along with its own across cascaded $\kappa$-$\mu$ fading channels. The power splitting protocol is another EH approach utilized by the SU and PU receivers to enhance the amount of energy in their storage devices. In addition, the SU transmitters and the SU receiver are deployed with multiple antennas for reception and apply the maximal ratio combining approach. The outage probability is utilized to assess both networks' reliability. Then, an energy efficiency evaluation is performed to determine the effectiveness of EH on the system. Finally, an optimization problem is provided with the goal of maximizing the data rate of the SUs by optimizing the time switching and the power allocation parameters of the SU relay."
2507.09067,"The emergence of quantum computing presents profound challenges to existing cryptographic infrastructures, whilst the development of central bank digital currencies (CBDCs) has raised concerns regarding privacy preservation and excessive centralisation in digital payment systems. This paper proposes the Quantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital currency architecture that incorporates National Institute of Standards and Technology (NIST)-standardised post-quantum cryptography (PQC) with hash-based zero-knowledge proofs to ensure user sovereignty, scalability, and transaction confidentiality. Key contributions include adaptations of ephemeral proof chains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS) consensus to promote equitable participation, and a novel zero-knowledge proof-based mechanism for privacy-preserving selective disclosure. QRPL aims to address critical shortcomings in prevailing CBDC designs, including risks of pervasive surveillance, with a 10-20 second block time to balance security and throughput in future monetary systems. While conceptual, empirical prototypes are planned. Future work includes prototype development to validate these models empirically."
2507.10463,"Escalating artificial intelligence (AI) demands expose a critical ""compute crisis"" characterized by unsustainable energy consumption, prohibitive training costs, and the approaching limits of conventional CMOS scaling. Physics-based Application-Specific Integrated Circuits (ASICs) present a transformative paradigm by directly harnessing intrinsic physical dynamics for computation rather than expending resources to enforce idealized digital abstractions. By relaxing the constraints needed for traditional ASICs, like enforced statelessness, unidirectionality, determinism, and synchronization, these devices aim to operate as exact realizations of physical processes, offering substantial gains in energy efficiency and computational throughput. This approach enables novel co-design strategies, aligning algorithmic requirements with the inherent computational primitives of physical systems. Physics-based ASICs could accelerate critical AI applications like diffusion models, sampling, optimization, and neural network inference as well as traditional computational workloads like scientific simulation of materials and molecules. Ultimately, this vision points towards a future of heterogeneous, highly-specialized computing platforms capable of overcoming current scaling bottlenecks and unlocking new frontiers in computational power and efficiency."
2507.11134,"The growing demand for edge computing and AI drives research into analog in-memory computing using memristors, which overcome data movement bottlenecks by computing directly within memory. However, device failures and variations critically limit analog systems' precision and reliability. Existing fault-tolerance techniques, such as redundancy and retraining, are often inadequate for high-precision applications or scenarios requiring fixed matrices and privacy preservation. Here, we introduce and experimentally demonstrate a fault-free matrix representation where target matrices are decomposed into products of two adjustable sub-matrices programmed onto analog hardware. This indirect, adaptive representation enables mathematical optimization to bypass faulty devices and eliminate differential pairs, significantly enhancing computational density. Our memristor-based system achieved >99.999% cosine similarity for a Discrete Fourier Transform matrix despite 39% device fault rate, a fidelity unattainable with conventional direct representation, which fails with single device faults (0.01% rate). We demonstrated 56-fold bit-error-rate reduction in wireless communication and >196% density with 179% energy efficiency improvements compared to state-of-the-art techniques. This method, validated on memristors, applies broadly to emerging memories and non-electrical computing substrates, showing that device yield is no longer the primary bottleneck in analog computing hardware."
2507.12373,"The energy sector is experiencing rapid transformation due to increasing renewable energy integration, decentralisation of power systems, and a heightened focus on efficiency and sustainability. With energy demand becoming increasingly dynamic and generation sources more variable, advanced forecasting and optimisation strategies are crucial for maintaining grid stability, cost-effectiveness, and environmental sustainability. This paper explores emerging paradigms in energy forecasting and management, emphasizing four critical domains: Energy Demand Forecasting integrated with Weather Data, Building Energy Optimisation, Heat Network Optimisation, and Energy Management System (EMS) Optimisation within a System of Systems (SoS) framework. Leveraging machine learning techniques and Model Predictive Control (MPC), the study demonstrates substantial enhancements in energy efficiency across scales -- from individual buildings to complex interconnected energy networks. Weather-informed demand forecasting significantly improves grid resilience and resource allocation strategies. Smart building optimisation integrates predictive analytics to substantially reduce energy consumption without compromising occupant comfort. Optimising CHP-based heat networks achieves cost and carbon savings while adhering to operational and asset constraints. At the systems level, sophisticated EMS optimisation ensures coordinated control of distributed resources, storage solutions, and demand-side flexibility. Through real-world case studies we highlight the potential of AI-driven automation and integrated control solutions in facilitating a resilient, efficient, and sustainable energy future."
2507.12626,"We contribute to the mathematical theory of the design of low temperature Ising machines, a type of experimental probabilistic computing device implementing the Ising model. Encoding the output of a function in the ground state of a physical system allows efficient and distributed computation, but the design of the energy function is a difficult puzzle. We introduce a diagrammatic device that allows us to visualize the decision boundaries for Ising circuits. It is then used to prove two results: (1) Ising circuits are a generalization of 1-NN classifiers with a certain special structure, and (2) Elimination of local minima in the energy landscape can be formulated as a linear programming problem."
2507.14713,"As drones increasingly deliver packages in neighborhoods, concerns about collisions arise. One solution is to share flight paths within a specific zip code, but this compromises business privacy by revealing delivery routes. For example, it could disclose which stores send packages to certain addresses. To avoid exposing path information, we propose using homomorphic encryption-based comparison to compute path intersections. This allows drones to identify potential collisions without revealing path and destination details, allowing them to adjust altitude to avoid crashes. We implemented and tested our approach on resource-limited virtual machines to mimic the computational power of drones. Our results demonstrate that our method is significantly faster and requires less network communication compared to a garbled circuit-based approach. We also provide a security analysis of the approach against potential attacks."
2507.15146,"The design of medical systems for remote, resource-limited environments faces persistent challenges due to poor interoperability, lack of offline support, and dependency on costly infrastructure. Many existing digital health solutions neglect these constraints, limiting their effectiveness for frontline health workers in underserved regions. This paper presents a portable, edge-enabled Electronic Health Record platform optimized for offline-first operation, secure patient data management, and modular diagnostic integration. Running on small-form factor embedded devices, it provides AES-256 encrypted local storage with optional cloud synchronization for interoperability. As a use case, we integrated a non-invasive anemia screening module leveraging fingernail pallor analysis. Trained on 250 patient cases (27\% anemia prevalence) with KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8, reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5 at 0.995. The system emphasizes low-cost deployment, modularity, and data privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health adoption in disconnected settings. Our work demonstrates a scalable approach to enhance portable health information systems and support frontline healthcare in underserved regions."
2507.15483,"The reawakened era of lunar exploration is defined by a strategic shift from temporary visits to a sustained international and commercial presence, resulting in an unprecedented demand for a robust and continuously available communication infrastructure. The conventional direct-to-Earth communication architecture relies on limited and oversubscribed deep space networks, which are further challenged by the radiative environment and insufficient visibility in certain areas of the cislunar domain. We address these issues by proposing a foundational move toward inter-domain space network cooperation by introducing architectures based on near space networks. They can directly service lunar surface users or, via cislunar relays, by forming a resilient and multi-layered communication backbone. First, we establish a unified link analysis framework incorporating frequently disregarded environmental factors, such as the Moon's variable illumination, to provide a high-fidelity performance evaluation. Second, we assess architectures' reliability based on the outage risk, essential for quantifying the operational robustness of communication links. Finally, to manage the inherent dynamism of architectures, we propose an inter-domain space digital twin$-$a dynamic decision-making engine that performs real-time analysis to autonomously select the best communication path, ensuring high and stable reliability while simultaneously optimizing power consumption. Overall, our paper provides a holistic architectural and conceptual management framework, emphasizing the necessity of lunar communications to support a permanent human and economic foothold on the Moon."
2507.1586,"In this paper, using 3D Technology Computer-Aided-Design (TCAD) simulations, we show that it is possible to design a static random-access memory (SRAM) using gate-all-around field-effect-transistor (GAA-FET) technology so that it is immune to single alpha particle radiation error. In other words, with the design, there will be no single-event upset (SEU) due to alpha particles. We first use ab initio calculations in PHITS to show that there is a maximum linear energy transfer (LET), LETmax, for the alpha particle in Si and Si$_x$Ge$_{1-x}$. Based on that, by designing a sub-7nm GAA-FET-based SRAM with bottom dielectric isolation (BDI), we show that the SRAM does not flip even if the particle strike is in the worst-case scenario."
2507.16077,"Network Slicing (NS) realization requires AI-native orchestration architectures to efficiently and intelligently handle heterogeneous user requirements. To achieve this, network slicing is evolving towards a more user-centric digital transformation, focusing on architectures that incorporate native intelligence to enable self-managed connectivity in an integrated and isolated manner. However, these initiatives face the challenge of validating their results in production environments, particularly those utilizing ML-enabled orchestration, as they are often tested in local networks or laboratory simulations. This paper proposes a large-scale validation method using a network slicing prediction model to forecast latency using Deep Neural Networks (DNNs) and basic ML algorithms embedded within an NS architecture, evaluated in real large-scale production testbeds. It measures and compares the performance of different DNNs and ML algorithms, considering a distributed database application deployed as a network slice over two large-scale production testbeds. The investigation highlights how AI-based prediction models can enhance network slicing orchestration architectures and presents a seamless, production-ready validation method as an alternative to fully controlled simulations or laboratory setups."
2507.16584,"To increase efficiency in automotive manufacturing, newly produced vehicles can move autonomously from the production line to the distribution area. This requires an optimal placement of sensors to ensure full coverage while minimizing the number of sensors used. The underlying optimization problem poses a computational challenge due to its large-scale nature. Currently, classical solvers rely on heuristics, often yielding non-optimal solutions for large instances, resulting in suboptimal sensor distributions and increased operational costs.We explore quantum computing methods that may outperform classical heuristics in the future. We implemented quantum annealing with D-Wave, transforming the problem into a quadratic unconstrained binary optimization formulation with one-hot and binary encoding. Hyperparameters like the penalty terms and the annealing time are optimized and the results are compared with default parameter settings.Our results demonstrate that quantum annealing is capable of solving instances derived from real-world scenarios. Through the use of decomposition techniques, we are able to scale the problem size further, bringing it closer to practical, industrial applicability. Through this work, we provide key insights into the importance of quantum annealing parametrization, demonstrating how quantum computing could contribute to cost-efficient, large-scale optimization problems once the hardware matures."
2507.18487,"In this conference contribution, we present some initial results on switching memristive devices exhibiting fractional-order behavior using current pulses. In our model, it is assumed that the evolution of a state variable follows a fractional-order differential equation involving a Caputo-type derivative. A study of Joule losses demonstrates that the best switching strategy minimizing these losses depends on the fractional derivative's order and the power exponent in the equation of motion. It is found that when the order of the fractional derivative exceeds half of the power exponent, the best approach is to employ a wide pulse. Conversely, when this condition is not met, Joule losses are minimized by applying a zero current followed by a narrow current pulse of the highest allowable amplitude. These findings are explored further in the context of multi-pulse control. Our research lays the foundation for the advancement of the next generation of energy-efficient neuromorphic computing architectures that more closely mimic their biological counterparts."
2507.19662,"Efficient wideband spectrum sensing requires rapid evaluation and re-evaluation of signal presence and type across multiple subchannels. These tasks involve multiple hypothesis testing, where each hypothesis is implemented as a decision tree workflow containing compute-intensive kernels, including FFT, matrix operations, and signal-specific analyses. Given dynamic nature of the spectrum environment, ability to quickly switch between hypotheses is essential for maintaining low-latency, high-throughput operation. This work assumes a coarse-grained reconfigurable architecture consisting of an array of processing elements (PEs), each equipped with a local instruction memory (IMEM) capable of storing and executing kernels used in spectrum sensing applications. We propose a planner tool that efficiently maps hypothesis workflows onto this architecture to enable fast runtime context switching with minimal overhead. The planner performs two key tasks: clustering temporally non-overlapping kernels to share IMEM resources within a PE sub-array, and placing these clusters onto hardware to ensure efficient scheduling and data movement. By preloading kernels that are not simultaneously active into same IMEM, our tool enables low-latency reconfiguration without runtime conflicts. It models the planning process as a multi-objective optimization, balancing trade-offs among context switch overhead, scheduling latency, and dataflow efficiency. We evaluate the proposed tool in simulated spectrum sensing scenario with 48 concurrent subchannels. Results show that our approach reduces off-chip binary fetches by 207.81x, lowers average switching time by 98.24x, and improves per-subband execution time by 132.92x over baseline without preloading. These improvements demonstrate that intelligent planning is critical for adapting to fast-changing spectrum environments in next-generation radio frequency systems."
2507.19739,"The augmentation of Internet of Things (IoT) devices transformed both automation and connectivity but revealed major security vulnerabilities in networks. We address these challenges by designing a robust intrusion detection system (IDS) to detect complex attacks by learning patterns from the NF-ToN-IoT v2 dataset. Intrusion detection has a realistic testbed through the dataset's rich and high-dimensional features. We combine distributed preprocessing to manage the dataset size with Fast Gradient Sign Method (FGSM) adversarial attacks to mimic actual attack scenarios and XGBoost model adversarial training for improved system robustness. Our system achieves 95.3% accuracy on clean data and 94.5% accuracy on adversarial data to show its effectiveness against complex threats. Adversarial training demonstrates its potential to strengthen IDS against evolving cyber threats and sets the foundation for future studies. Real-time IoT environments represent a future deployment opportunity for these systems, while extensions to detect emerging threats and zero-day vulnerabilities would enhance their utility."
2507.20193,"Neuromorphic architectures, which incorporate parallel and in-memory processing, are crucial for accelerating artificial neural network (ANN) computations. This work presents a novel memristor-based multi-layer neural network (memristive MLNN) architecture and an efficient in-situ training algorithm. The proposed design performs matrix-vector multiplications, outer products, and weight updates in constant time $\mathcal{O}(1)$, leveraging the inherent parallelism of memristive crossbars. Each synapse is realized using a single memristor, eliminating the need for transistors, and offering enhanced area and energy efficiency. The architecture is evaluated through LTspice simulations on the IRIS, NASA Asteroid, and Breast Cancer Wisconsin datasets, achieving classification accuracies of 98.22\%, 90.43\%, and 98.59\%, respectively. Robustness is assessed by introducing stuck-at-conducting-state faults in randomly selected memristors. The effects of nonlinearity in memristor conductance and a 10\% device variation are also analyzed. The simulation results establish that the network's performance is not affected significantly by faulty memristors, non-linearity, and device variation."
2507.20998,"Memristor-based Spiking Neural Networks (SNNs) with temporal spike encoding enable ultra-low-energy computation, making them ideal for battery-powered intelligent devices. This paper presents a circuit-level memristive spiking neural network (SNN) architecture trained using a proposed novel supervised in-situ learning algorithm inspired by spike-timing-dependent plasticity (STDP). The proposed architecture efficiently implements lateral inhibition and the refractory period, eliminating the need for external microcontrollers or ancillary control hardware. All synapses of the winning neurons are updated in parallel, enhancing training efficiency. The modular design ensures scalability with respect to input data dimensions and output class count. The SNN is evaluated in LTspice for pattern recognition (using 5x3 binary images) and classification tasks using the Iris and Breast Cancer Wisconsin (BCW) datasets. During testing, the system achieved perfect pattern recognition and high classification accuracies of 99.11\% (Iris) and 97.9\% (BCW). Additionally, it has demonstrated robustness, maintaining an average recognition rate of 93.4\% under 20\% input noise. The impact of stuck-at-conductance faults and memristor device variations was also analyzed."
2507.22301,"Photonic Integrated Circuits (PICs) offer tremendous advantages in bandwidth, parallelism, and energy efficiency, making them essential for emerging applications in artificial intelligence (AI), high-performance computing (HPC), sensing, and communications. However, the design of modern PICs, which now integrate hundreds to thousands of components, remains largely manual, resulting in inefficiency, poor scalability, and susceptibility to errors. To address these challenges, we propose PoLaRIS, a comprehensive Intelligent Electronic-Photonic Design Automation (EPDA) framework that spans both device-level synthesis and system-level physical layout. PoLaRIS combines a robust, fabrication-aware inverse design engine with a routing-informed placement and curvy-aware detailed router, enabling the automated generation of design rule violation (DRV)-free and performance-optimized layouts. By unifying physics-driven optimization with machine learning and domain-specific algorithms, PoLaRIS significantly accelerates PIC development, lowers design barriers, and lays the groundwork for scalable photonic system design automation."
2507.22511,"Green Wave provides practical and advanced solutions to improve traffic efficiency and safety through network coordination. Nevertheless, the complete potential of Green Wave systems has yet to be explored. Utilizing emerging technologies and advanced algorithms, such as AI or V2X, would aid in achieving more robust traffic management strategies, especially when integrated with Green Wave. This work comprehensively surveys existing traffic control strategies that enable Green Waves and analyzes their impact on future traffic management systems and urban infrastructure. Understanding previous research on traffic management and its effect on traffic efficiency and safety helps explore the integration of Green Wave solutions with smart city initiatives for effective traffic signal coordination. This paper also discusses the advantages of using Green Wave strategies for emission reduction and considers road safety issues for vulnerable road users, such as pedestrians and cyclists. Finally, the existing challenges and research gaps in building robust and successful Green Wave systems are discussed to articulate explicitly the future requirement of sustainable urban transport."
2507.23419,"Monitoring respiratory health with the use of channel state information (CSI) has shown promising results. Many existing methods focus on monitoring only the respiratory rate, while others focus on monitoring the motion of the chest as a patient breathes, which is referred to as the respiratory waveform. This paper presents WiRM, a two-staged approach to contactless respiration monitoring. In the first stage, WiRM improves upon existing respiratory rate estimation techniques by using conjugate multiplication for phase sanitisation and adaptive multi-trace carving (AMTC) for tracing how the respiratory rate changes over time. When compared against three state-of-the-art methods, WiRM has achieved an average reduction of $38\%$ in respiratory rate root mean squared error (RMSE). In the second stage, WiRM uses this improved respiratory rate estimate to inform the decomposition and selection of the respiratory waveform from the CSI data. Remarkably, WiRM delivers a $178.3\%$ improvement in average absolute correlation with the ground truth respiratory waveform. Within the literature, it is difficult to compare the robustness of existing algorithms in noisy environments. In this paper, we develop a purpose-built simulation toolkit to evaluate the robustness of respiration monitoring solutions under various noise conditions, including thermal, multiplicative, and phase noise. Our results show that WiRM demonstrates improved or comparable resilience to these common noise sources."
2507.23618,"Conventional quantum error correction (QEC) decoders such as Minimum-Weight Perfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast decoding, respectively, but both suffer from high topological complexity. In contrast, Ising model-based decoders reduce topological complexity but demand considerable decoding time. We propose the Symmetric One-Hot Matching Elector (SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic Unconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO (OHQ). Each variable in the QUBO represents whether a given pair of flipped syndromes is matched, while the error probabilities between the pair are encoded as interaction coefficients (weight). Constraints ensure that each flipped syndrome is matched exactly once. Valid solutions of OHQ correspond to self-inverse permutation matrices, characterized by symmetric one-hot encoding. To solve the OHQ efficiently, SOME reformulates the decoding task as the construction of permutation matrices that minimize the total weight. It initializes each candidate matrix from one of the minimum-weight syndrome pairs, then iteratively appends additional pairs in ascending order of weight, and finally selects the permutation matrix with the lowest total energy. SOME achieves up to a 99.9x reduction in variable count and reduces decoding times from milliseconds to microseconds on a single-threaded commodity CPU. OHQ also maintains performance up to a 10.5% physical error rate, surpassing the highest known threshold of MWPM@."
2508.00295,"The growing demand for ultra low power computing and the emergence of quantum technologies have intensified interest in cryogenic electronics, particularly superconducting devices. Despite their promise, current controlled superconducting components face fundamental challenges in cascadability, limiting their effectiveness in complex logic architectures. To overcome this, recent efforts have focused on developing gate tunable superconducting devices, such as Josephson junction field effect transistors (JJFETs). However, achieving robust control and sufficient supercurrent gain, both critical for transistor-like performance in logic circuits remains a key challenge. A recent advancement in JJFET design, based on InAs and GaSb heterostructures, demonstrates enhanced gain and favorable device characteristics suitable for circuit integration. Building on this innovation, we propose and analyze fundamental voltage controlled logic topologies using the quantum enhanced JJFET. We develop a Verilog A based circuit compatible compact model of the quantum enhanced JJFET which accurately captures the experimentally observed device characteristics. To ensure cascadability, our logic circuits incorporate the multilayered Heater Nanocryotron (nTron), a superconducting nanowire-based thermal switch. Through simulation based analysis, we demonstrate the successful implementation of fundamental logic gates, including NOT, NAND, and NOR. Furthermore, we design a 3 input majority gate, which plays a pivotal role in quantum and reversible computing due to its universality. Finally, to demonstrate the cascadability of our proposed logic topology, we demonstrate the operation of a 2 input XOR gate based on our designed JJFET based NOT, NAND, and NOR gate."
2508.00825,"As a step in the architectural design of a quantum processing or sensing system with control and signaling, an attempt is made at putting in parallel functional properties of the random flows between neurons through electrical synapses, and quantum particle flows inside a quantum processing system mimicking biological processes. Based on a simplified dynamic electrical synapse model, a quantum synapse circuit design is proposed. This is extended to the case of bidirectional flows through a synapse, highlighting the possible role of quantum synapse circuits as highly parallel controlled interfaces crucial in sensing and sensor fusion systems. A short status of the quantum simulation is provided."
2508.00832,"The advent of quantum computing poses a significant threat to the foundational cryptographic algorithms that secure modern digital communications. Protocols such as HTTPS, digital certificates, and public key infrastructures (PKIs) heavily rely on cryptographic primitives like RSA, ECC, and Diffie-Hellman, which are vulnerable to quantum attacks -- most notably Shor's algorithm. This paper presents a comprehensive comparative analysis between classical cryptographic algorithms currently in widespread use and emerging post-quantum cryptographic schemes designed to withstand quantum adversaries. We review the cryptographic mechanisms underpinning modern internet security, outline the mathematical foundations of quantum attacks, and evaluate the security, performance, and implementation feasibility of quantum-resistant alternatives such as Kyber, Dilithium, and Falcon. Additionally, we assess the hybrid approaches currently being explored by institutions and tech companies to enable a smooth transition to post-quantum cryptography. By providing an in-depth comparison, this study aims to guide researchers, developers, and policymakers in understanding the critical implications of quantum computing on cryptographic infrastructures and the necessary steps for securing communications in the quantum era."
2508.00837,"Protein structure prediction is a core challenge in computational biology, particularly for fragments within ligand-binding regions, where accurate modeling is still difficult. Quantum computing offers a novel first-principles modeling paradigm, but its application is currently limited by hardware constraints, high computational cost, and the lack of a standardized benchmarking dataset. In this work, we present QDockBank-the first large-scale protein fragment structure dataset generated entirely using utility-level quantum computers, specifically designed for protein-ligand docking tasks. QDockBank comprises 55 protein fragments extracted from ligand-binding pockets. The dataset was generated through tens of hours of execution on superconducting quantum processors, making it the first quantum-based protein structure dataset with a total computational cost exceeding one million USD. Experimental evaluations demonstrate that structures predicted by QDockBank outperform those predicted by AlphaFold2 and AlphaFold3 in terms of both RMSD and docking affinity scores. QDockBank serves as a new benchmark for evaluating quantum-based protein structure prediction."
2508.01056,"U.S. national security customers have begun to utilize large language models, including enterprise versions of ``off-the-shelf'' models (e.g., ChatGPT) familiar to the public. This uptake will likely accelerate. However, recent studies suggest that off-the-shelf large language models frequently suggest escalatory actions when prompted with geopolitical or strategic scenarios. We demonstrate two simple, non-technical interventions to control these tendencies. Introducing these interventions into the experimental wargame design of a recent study, we substantially reduce escalation throughout the game. Calls to restrict the use of large language models in national security applications are thus premature. The U.S. government is already, and will continue, employing large language models for scenario planning and suggesting courses of action. Rather than warning against such applications, this study acknowledges the imminent adoption of large language models, and provides actionable measures to align them with national security goals, including escalation management."
2508.01205,"Semantic communication with joint semantic-channel coding robustly transmits diverse data modalities but faces challenges in mitigating semantic information loss due to packet drops in packet-based systems. Under current protocols, packets with errors are discarded, preventing the receiver from utilizing erroneous semantic data for robust decoding. To address this issue, a packet-loss-resistant MoE Swin Transformer-based Video Semantic Communication (MSTVSC) system is proposed in this paper. Semantic vectors are encoded by MSTVSC and transmitted through upper-layer protocol packetization. To investigate the impact of the packetization, a theoretical analysis of the packetization strategy is provided. To mitigate the semantic loss caused by packet loss, a 3D CNN at the receiver recovers missing information using un-lost semantic data and an packet-loss mask matrix. Semantic-level interleaving is employed to reduce concentrated semantic loss from packet drops. To improve compression, a common-individual decomposition approach is adopted, with downsampling applied to individual information to minimize redundancy. The model is lightweighted for practical deployment. Extensive simulations and comparisons demonstrate strong performance, achieving an MS-SSIM greater than 0.6 and a PSNR exceeding 20 dB at a 90% packet loss rate."
2508.01958,"In this paper, we present a brief review and introduction to Quadratic Unconstrained D-ary Optimization (QUDO), Tensor Quadratic Unconstrained D-ary Optimization (T-QUDO) and Higher-Order Unconstrained Binary Optimization (HOBO) formulations for combinatorial optimization problems. We also show their equivalences. To help their understanding, we make some examples for the knapsack problem, traveling salesman problem and different combinatorial games. The games chosen to exemplify are: Hashiwokakero, N-Queens, Kakuro, Inshi no heya, and Peg Solitaire. Although some of these games have already been formulated in a QUBO formulation, we are going to approach them with more general formulations, allowing their execution in new quantum or quantum-inspired optimization algorithms. This can be an easier way to introduce these more complicated formulations for harder problems."
2508.02284,"Advances in nanosheet technologies have significantly increased power densities, exacerbating thermal management challenges in 2.5D/3D chiplet-based Systems-in-Package (SiP). While traditional thermal analyses often employ uniform power maps to simplify computational complexity, this practice neglects localized heating effects, leading to inaccuracies in thermal estimations, especially when comparing power delivery networks (PDN) in 3D integration. This work examines the thermal impact of non-uniform power distributions on SiPs utilizing frontside (FSPDN) and backside (BSPDN) power delivery approaches. Using high-resolution thermal simulations with non-uniform power maps at resolutions down to 5 micrometers, we demonstrate that uniform power assumptions substantially underestimate peak temperatures and fail to reveal critical thermal differences between BSPDN and FSPDN configurations in 3D scenarios. Our results highlight that BSPDN configurations in 3D, although beneficial in simplified uniform scenarios, exhibit pronounced thermal penalties under realistic, localized workloads due to limited lateral heat spreading. These findings emphasize the necessity of adopting fine-grained, workload-aware power maps in early-stage thermal modeling to enable accurate PDN assessment and informed thermal-aware design decisions in advanced nanosheet-based 3D SiP."
2508.03521,"This study examines the behavioral and environmental implications of shared autonomous micro-mobility systems, focusing on autonomous bicycles and their integration with transit in the U.S. While prior research has addressed operational and lifecycle aspects, a critical gap remains in understanding which modes these services are likely to substitute, who is most inclined to adopt them, and how service attributes influence user decisions. We design a context-aware stated preference survey grounded in real-world trips and estimate discrete choice models, including a hybrid model incorporating latent attitudes. Findings indicate that adoption, mode shift, and environmental impacts are highly sensitive to service design. Scenarios with minimal wait and cost yield high adoption but increase emissions, while moderate waits are more likely to reduce impacts. Adoption likelihood varies with demographic characteristics, and outcomes depend on city type, context, and infrastructure assumptions. These insights can inform the development of more sustainable and equitable mobility systems."
2508.04878,"Oscillators designed to function at cryogenic temperatures play a critical role in superconducting electronics and quantum computing by providing stable, low noise signals with minimal energythis http URLwe present a comprehensive numerical study of injection locking and mutual coupling dynamics in superconducting nanowire based cryogenicthis http URLthe design space of standalone ScNW based oscillator, we investigate two critical mechanisms that govern frequency synchronization and signal coordination in cryogenic computingthis http URL, an injection locking induced by an external AC signal with a frequency near the oscillators natural frequency, and second, the mutual coupling dynamics between two ScNW oscillators under varying couplingthis http URLidentify key design parameters such as shunt resistance, nanowire inductance, and coupling strength that govern the lockingthis http URL, we examine how the amplitude of the injected signal affects the amplitude of the locked oscillation, offering valuable insights for power aware oscillatorthis http URL, we analyze mutual synchronization between coupled ScNW oscillators using capacitive and resistive couplingthis http URLresults reveal that the phase difference between oscillators can be precisely controlled by tuning the coupling strength, enabling programmable phase encoded informationthis http URLfindings could enable building ScNW based oscillatory neural networks, synchronized cryogenic logic blocks, and on chip cryogenic resonator arrays."
2508.05014,"We develop a computing framework that leverages wave propagation within an interconnected network, where nodes and edges possess wave manipulation capabilities, such as frequency mixing or time delay. This computing paradigm can not only achieve intrinsic parallelism like existing works by the exploration of an exponential number of possibilities simultaneously with very small number of hardware units, but also extend this unique characteristic to a multidimensional space including spatial, temporal and frequency domains, making it particularly effective for addressing NP-hard problems. The proposed architecture has been validated through SPICE simulations, demonstrating its potential capability in solving several NP-hard problems, such as the Number Partitioning Problem, the 0/1 Knapsack Problem, and the Traveling Salesman Problem."
2508.05963,"Visual neuroprostheses are commonly framed as technologies to restore natural sight to people who are blind. In practice, they create a novel mode of perception shaped by sparse, distorted, and unstable input. They resemble early extended reality (XR) headsets more than natural vision, streaming video from a head-mounted camera to a neural ""display"" with under 1000 pixels, limited field of view, low refresh rates, and nonlinear spatial mappings. No amount of resolution alone will make this experience natural. This paper proposes a reframing: bionic vision as neuroadaptive XR. Rather than replicating natural sight, the goal is to co-adapt brain and device through a bidirectional interface that responds to neural constraints, behavioral goals, and cognitive state. By comparing traditional XR, current implants, and proposed neuroadaptive systems, it introduces a new design space for inclusive, brain-aware computing. It concludes with research provocations spanning encoding, evaluation, learning, and ethics, and invites the XR community to help shape the future of sensory augmentation."
2508.05999,"This study examines how AI code assistants shape novice programmers experiences during a two-part exam in an introductory programming course. In the first part, students completed a programming task with access to AI support; in the second, they extended their solutions without AI. We collected Likert-scale and open-ended responses from 20 students to evaluate their perceptions and challenges. Findings suggest that AI tools were perceived as helpful for understanding code and increasing confidence, particularly during initial development. However, students reported difficulties transferring knowledge to unaided tasks, revealing possible overreliance and gaps in conceptual understanding. These insights highlight the need for pedagogical strategies that integrate AI meaningfully while reinforcing foundational programming skills."
2508.06799,"Digital Twins (DTs) offer powerful tools for managing complex infrastructure systems, but their effectiveness is often limited by challenges in integrating unstructured knowledge. Recent advances in Large Language Models (LLMs) bring new potential to address this gap, with strong abilities in extracting and organizing diverse textual information. We therefore propose LSDTs (LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract planning knowledge from unstructured documents like environmental regulations and technical guidelines, and organize it into a formal ontology. This ontology forms a semantic layer that powers a digital twin-a virtual model of the physical system-allowing it to simulate realistic, regulation-aware planning scenarios. We evaluate LSDTs through a case study of offshore wind farm planning in Maryland, including its application during Hurricane Sandy. Results demonstrate that LSDTs support interpretable, regulation-aware layout optimization, enable high-fidelity simulation, and enhance adaptability in infrastructure planning. This work shows the potential of combining generative AI with digital twins to support complex, knowledge-driven planning tasks."
2508.07573,"The advance of direct satellite-to-device communication has positioned mega-satellite constellations as a cornerstone of 6G wireless communication, enabling seamless global connectivity even in remote and underserved areas. However, spectrum scarcity and capacity constraints imposed by the Shannon's classical information theory remain significant challenges for supporting the massive data demands of multimedia-rich wireless applications. Generative Semantic Communication (GSC), powered by artificial intelligence-based generative foundation models, represents a paradigm shift from transmitting raw data to exchanging semantic meaning. GSC can not only reduce bandwidth consumption, but also enhance key semantic features in multimedia content, thereby offering a promising solution to overcome the limitations of traditional satellite communication systems. This article investigates the integration of GSC into mega-satellite constellations from a networking perspective. We propose a GSC-empowered satellite networking architecture and identify key enabling technologies, focusing on GSC-empowered network modeling and GSC-aware networking strategies. We construct a discrete temporal graph to model semantic encoders and decoders, distinct knowledge bases, and resource variations in mega-satellite networks. Based on this framework, we develop model deployment for semantic encoders and decoders and GSC-compatible routing schemes, and then present performance evaluations. Finally, we outline future research directions for advancing GSC-empowered satellite networks."
2508.08973,"With the broad recent research on ferroelectric hafnium oxide for non-volatile memory technology, depolarization effects in HfO2-based ferroelectric devices gained a lot of interest. Understanding the physical mechanisms regulating the retention of these devices provides an excellent opportunity for device optimization both towards non-volatile memory applications and towards real-time signal processing applications in which controlled time constants are of paramount importance. Indeed, we argue that ferroelectric devices, particularly HfO2-based, are an elegant solution to realize possibly arbitrary time constants in a single scaled memory device, which paves the way for temporal and brain-inspired computing in hardware. Here we present a ferroelectric capacitor stack realizing volatile memory due to its unique interface configuration. We provide electrical characterization of the device to motivate its use for realizing time constants in hardware, followed by an investigation of the electronic mechanisms and their possible relation to the observed retention times to facilitate further modeling of the retention process in HfO2-based ferroelectric capacitors. In the presented device, internal electric fields stabilize one polarization of the ferroelectric film, opening the possibility for unipolar operation with millisecond retention for the unstable polarization state. We show a dependence of the retention on both the polarization as well as the electrical stimuli, allowing us to exploit a range of time scales in a single device. Further, the intentionally defective interface in the presented material stack allows an insight into the interplay between retention loss in HfO2-based ferroelectric devices and the internal bias field, which we relate to the interface composition and the role of oxygen vacancies as a possible source of the internal bias fields."
2508.11246,"Retrieval-Augmented Generation (RAG) enhances language models by combining retrieval with generation. However, its current workflow remains largely text-centric, limiting its applicability in geoscience. Many geoscientific tasks are inherently evidence-hungry. Typical examples involve imputing missing observations using analog scenes, retrieving equations and parameters to calibrate models, geolocating field photos based on visual cues, or surfacing historical case studies to support policy analyses. A simple ``retrieve-then-generate'' pipeline is insufficient for these needs. We envision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular retrieve $\rightarrow$ reason $\rightarrow$ generate $\rightarrow$ verify loop. Geo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth data; (ii) reasoning under physical and domain constraints; (iii) generation of science-grade artifacts; and (iv) verification of generated hypotheses against numerical models, ground measurements, and expert assessments. This shift opens new opportunities for more trustworthy and transparent geoscience workflows."
2508.11395,"The global financial system stands at an inflection point. Stablecoins represent the most significant evolution in banking since the abandonment of the gold standard, positioned to enable ""Banking 2.0"" by seamlessly integrating cryptocurrency innovation with traditional finance infrastructure. This transformation rivals artificial intelligence as the next major disruptor in the financial sector. Modern fiat currencies derive value entirely from institutional trust rather than physical backing, creating vulnerabilities that stablecoins address through enhanced stability, reduced fraud risk, and unified global transactions that transcend national boundaries. Recent developments demonstrate accelerating institutional adoption: landmark U.S. legislation including the GENIUS Act of 2025, strategic industry pivots from major players like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive ""Pay with Crypto"" service. Widespread stablecoin implementation addresses critical macroeconomic imbalances, particularly the inflation-productivity gap plaguing modern monetary systems, through more robust and diversified backing mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency gains, paving the way for a more interconnected international financial system. This whitepaper comprehensively explores how stablecoins are poised to reshape banking, supported by real-world examples, current market data, and analysis of their transformative potential."
2508.11423,"Living systems exhibit a range of fundamental characteristics: they are active, self-referential, self-modifying systems. This paper explores how these characteristics create challenges for conventional scientific approaches and why they require new theoretical and formal frameworks. We introduce a distinction between 'natural time', the continuing present of physical processes, and 'representational time', with its framework of past, present and future that emerges with life itself. Representational time enables memory, learning and prediction, functions of living systems essential for their survival. Through examples from evolution, embryogenesis and metamorphosis we show how living systems navigate the apparent contradictions arising from self-reference as natural time unwinds self-referential loops into developmental spirals. Conventional mathematical and computational formalisms struggle to model self-referential and self-modifying systems without running into paradox. We identify promising new directions for modelling self-referential systems, including domain theory, co-algebra, genetic programming, and self-modifying algorithms. There are broad implications for biology, cognitive science and social sciences, because self-reference and self-modification are not problems to be avoided but core features of living systems that must be modelled to understand life's open-ended creativity."
2508.11451,"Compute-Near-Memory (CNM) systems offer a promising approach to mitigate the von Neumann bottleneck by bringing computational units closer to data. However, optimizing for these architectures remains challenging due to their unique hardware and programming models. Existing CNM compilers often rely on manual programmer annotations for offloading and optimizations. Automating these decisions by exploring the optimization space, common in CPU/GPU systems, is difficult for CNMs as constructing and navigating the transformation space is tedious and time consuming. This is particularly the case during system-level design, where evaluation requires time-consuming simulations. To address this, we present CoMoNM, a generic cost modeling framework for CNM systems for execution time estimation in milliseconds. It takes a high-level, hardware-agnostic application representation, target system specifications, and a mapping specification as input and estimates the execution time for the given application on the target CNM system. We show how CoMoNM can be seamlessly integrated into state-of-the-art CNM compilers, providing improved offloading decisions. Evaluation on established benchmarks for CNM shows estimation errors within 7.80% and 2.99%, when compared to the real UPMEM CNM system and Samsung's HBM-PIM simulator. Notably, CoMoNM delivers estimates seven orders of magnitude faster compared to the UPMEM and HBM-PIM simulators."
2508.13637,"Efficient task offloading is crucial for reducing latency and ensuring timely decision-making in intelligent transportation systems within the rapidly evolving Internet of Vehicles (IoV) landscape. This paper introduces a novel Quantum-Inspired Artificial Bee Colony (QABC) algorithm specifically designed for latency-sensitive task offloading involving cloud servers, Roadside Units (RSUs), and vehicular nodes. By incorporating principles from quantum computing, such as quantum state evolution and probabilistic encoding, QABC enhances the classical Artificial Bee Colony (ABC) algorithm's ability to avoid local optima and explore high-dimensional solution spaces. This research highlights the potential of quantum-inspired heuristics to optimize real-time offloading strategies in future vehicular networks."
2508.15391,"We introduce a micro-velocity framework for analysing the on-chain circulation of Lidos liquid-staking tokens, stETH, and its wrapped ERC-20 form, wstETH. By reconstructing full transfer and share-based accounting histories, we compute address-level velocities and decompose them into behavioural components. Despite their growing importance, the micro-level monetary dynamics of LSTs remain largely unexplored. Our data reveal persistently high velocity for both tokens, reflecting intensive reuse within DeFi. Yet activity is highly concentrated: a small cohort of large addresses, likely institutional accounts, are responsible for most turnover, while the rest of the users remain largely passive. We also observe a gradual transition in user behavior, characterized by a shift toward wstETH, the non-rebasing variant of stETH. This shift appears to align with DeFi composability trends, as wstETH is more frequently deployed across protocols such as AAVE, Spark, Balancer, and SkyMoney.To make the study fully reproducible, we release (i) an open-source pipeline that indexes event logs and historical contract state, and (ii) two public datasets containing every Transfer and TransferShares record for stETH and wstETH through 2024-11-08. This is the first large-scale empirical characterisation of liquid-staking token circulation. Our approach offers a scalable template for monitoring staking asset flows and provides new, open-access resources to the research community."
2508.15542,"Quantum simulators are essential tools for developing and testing quantum algorithms. However, the high-frequency traversal characteristic of quantum simulators represents an unprecedented demand in the history of IT, and existing distributed technologies is unable to meet this requirement, resulting in a single-node bottleneck of quantum simulator. To overcome this limitation, this paper introduces a novel Distributed Shared Layered Storage Quantum Simulator (DSLSQS). By leveraging an innovative distributed architecture in which multiple computational nodes share data storage directly, together with De-TCP/IP networking technology, DSLSQS effectively eliminates East-West data flow in distributed systems. This approach mitigates the bottleneck of distributed quantum simulation clusters and enhances the scalability. Moreover, the system employs layered storage technology, which reduces usage of expensive high-performance memory and substantially lowers simulation costs. Furthermore, this paper systematically analyzes the performance and cost constraints of distributed quantum simulator cluster, identifying distributed networking as the primary performance bottleneck and highlighting that minimizing storage costs is crucial to reducing the total cost. Finally, experimental evaluations with a 27-qubit simulation confirm the successful implementation of layered storage within the quantum simulator. DSLSQS significantly enhances simulation efficiency, yielding a performance improvement of over 350% compared to existing distributed technologies. These results underscore the superior performance and scalability of the proposed architecture in managing complex quantum computing tasks. This paper provides crucial insights for the practical deployment of quantum computing and presents an effective framework for the development of distributed quantum simulation clusters."
2508.15545,"In response to the challenges in large-scale quantum state simulation on classical computing platforms, including memory limits, frequent disk I/O, and high computational complexity, this study builds upon a previously proposed hierarchical storage-based quantum simulation system and introduces an optimization framework, the Quantum Vector Optimization Framework (QVecOpt). QVecOpt integrates four strategies: amplitude pairing, cache optimization, block storage optimization, and parallel optimization. These collectively enhance state vector storage and computational scheduling. The amplitude pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache optimization pre-allocates buffers and loads only required data, cutting disk I/O. Block storage optimization partitions the state vector for on-demand loading and local updates, reducing redundant access. Parallel optimization distributes the state vector across nodes for collaborative computation, achieving near-linear speedup. Complexity analysis shows that, compared with hierarchical storage simulation, the method reduces state vector traversals for single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and $O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold, breaking the memory bottleneck of existing tools and enabling high-bit quantum circuit simulations beyond traditional methods. This work provides an efficient, scalable solution for classical simulation of large-scale quantum computation with significant academic and practical value."
2508.1562,"In many cases, the behavior of physical memristive devices can be relatively well captured by using a single internal state variable. This study investigates the low-power control of first-order memristive devices to derive the most energy-efficient protocols for programming their resistances. A unique yet general approach to optimizing the switching transitions in devices of this kind is introduced. For pedagogical purposes, without loss of generality, the proposed control paradigm is applied to a couple of differential algebraic equation sets for voltage-controlled devices, specifically Kvatinsky's Voltage ThrEshold Adaptive Memristor mathematical description and Miranda's and Sune's dynamic balance model. It is demonstrated that, depending upon intrinsic physical properties of the device, captured in the model formulas and parameter setting, and upon constraints on programming time and voltages, the optimal protocol for either of the two switching scenarios may require the application of a single square voltage pulse of height set to a certain level within the admissible range across a fraction or entire given programming time interval, or of some more involved voltage stimulus of unique polarity, including analogue continuous waveforms that can be approximated by trains of square voltage pulses of different heights, over the entire programming time interval. The practical implications of these research findings are significant, as the development of energy-efficient protocols to program memristive devices, resolving the so-called voltage-time dilemma in the device physics community, is a subject under intensive and extensive studies across the academic community and industry."
2508.15733,"Realisation of significant advances in capabilities of sensors, computing, timing, and communication enabled by quantum technologies is dependent on engineering highly complex systems that integrate quantum devices into existing classical infrastructure. A systems engineering approach is considered to address the growing need for quantum-secure telecommunications that overcome the threat to encryption caused by maturing quantum computation. This work explores a range of existing and future quantum communication networks, specifically quantum key distribution network proposals, to model and demonstrate the evolution of quantum key distribution network architectures. Leveraging Orthogonal Variability Modelling and Systems Modelling Language as candidate modelling languages, the study creates traceable artefacts to promote modular architectures that are reusable for future studies. We propose a variability-driven framework for managing fast-evolving network architectures with respect to increasing stakeholder expectations. The result contributes to the systematic development of viable quantum key distribution networks and supports the investigation of similar integration challenges relevant to the broader context of quantum systems engineering."
2508.16011,"Processing-In-Memory (PIM) architectures offer a promising approach to accelerate Graph Neural Network (GNN) training and inference. However, various PIM devices such as ReRAM, FeFET, PCM, MRAM, and SRAM exist, with each device offering unique trade-offs in terms of power, latency, area, and non-idealities. A heterogeneous manycore architecture enabled by 3D integration can combine multiple PIM devices on a single platform, to enable energy-efficient and high-performance GNN training. In this work, we propose a 3D heterogeneous PIM-based accelerator for GNN training referred to as HePGA. We leverage the unique characteristics of GNN layers and associated computing kernels to optimize their mapping on to different PIM devices as well as planar tiers. Our experimental analysis shows that HePGA outperforms existing PIM-based architectures by up to 3.8x and 6.8x in energy-efficiency (TOPS/W) and compute efficiency (TOPS/mm2) respectively, without sacrificing the GNN prediction accuracy. Finally, we demonstrate the applicability of HePGA to accelerate inferencing of emerging transformer models."
2508.162,"Flow-guided Localization (FGL) enables the identification of spatial regions within the human body that contain an event of diagnostic interest. FGL does that by leveraging the passive movement of energy-constrained nanodevices circulating through the bloodstream. Existing FGL solutions rely on graph models with fixed topologies or handcrafted features, which limit their adaptability to anatomical variability and hinder scalability. In this work, we explore the use of Set Transformer architectures to address these limitations. Our formulation treats nanodevices' circulation time reports as unordered sets, enabling permutation-invariant, variable-length input processing without relying on spatial priors. To improve robustness under data scarcity and class imbalance, we integrate synthetic data generation via deep generative models, including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicate realistic circulation time distributions conditioned on vascular region labels, and are used to augment the training data. Our results show that the Set Transformer achieves comparable classification accuracy compared to Graph Neural Networks (GNN) baselines, while simultaneously providing by-design improved generalization to anatomical variability. The findings highlight the potential of permutation-invariant models and synthetic augmentation for robust and scalable nanoscale localization."
2508.16236,"Understanding the nature of information storage on memristors is vital to enable their use in novel data storage and neuromorphic applications. One key consideration in information storage is the energy cost of storage and what impact the available energy has on the information capacity of the devices. In this paper, we propose and study an energy-information trade-off for a particular kind of memristive device - Self-Directed Channel (SDC) memristors. We perform experiments to model the energy required to set the devices into various states, as well as assessing the stability of these states over time. Based on these results, we employ a generative modelling approach, using a conditional Generative Adversarial Network (cGAN) to characterise the storage conditional distribution, allowing us to estimate energy-information curves for a range of storage delays, showing the graceful trade-off between energy consumed and the effective capacity of the devices."
2508.16933,"Phase Frequency Detectors (PFDs) are essential components in Phase-Locked Loop (PLL) and Delay-Locked Loop (DLL) systems, responsible for comparing phase and frequency differences and generating up/down signals to regulate charge pumps and/or, consequently, Voltage-Controlled Oscillators (VCOs). Conventional PFD designs often suffer from significant dead zones and blind zones, which degrade phase detection accuracy and increase jitter in high-speed applications. This paper addresses PFD design challenges and presents a novel low-power True Single-Phase Clock (TSPC)-based PFD. The proposed design eliminates the blind zone entirely while achieving a minimal dead zone of 40 ps. The proposed PFD, implemented using TSMC 28 nm technology, demonstrates a low-power consumption of 4.41 uW at 3 GHz input frequency with a layout area of $10.42\mu m^2$."
2508.17896,"We present the Steiner Traveling Salesman Problem with Time Windows and Pickup and Delivery, an advanced and practical extension of classical routing models. This variant integrates the characteristics of the Steiner Traveling Salesman Problem with time-window constraints, pickup and delivery operations and vehicle capacity limitations. These features closely mirror the complexities of contemporary logistics challenges, including last-mile distribution, reverse logistics and on-demand service scenarios. To tackle the inherent computational difficulties of this NP-hard problem, we propose two specialized mathematical formulations: an arc-based model and a node-oriented model, each designed to capture distinct structural aspects of the problem. Both models are implemented on D-Wave's LeapCQMHybrid platform, which combines quantum and classical techniques for solving constrained optimization tasks. We further introduce a preprocessing reduction method that eliminates redundant arcs, significantly enhancing computational performance and scalability. Experimental results demonstrate that hybrid quantum approaches are capable of solving problem instances of realistic size, underscoring their potential as a transformative tool for next-generation routing optimization."
2508.1825,"This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM."
2509.00979,"Urban noise pollution poses a significant threat to public health, yet existing monitoring infrastructures offer limited spatial coverage and adaptability. This paper presents a scalable, low-cost, IoT-based, real-time environmental noise monitoring solution using mobile nodes (sensor nodes on a moving vehicle). The system utilizes a low-cost sound sensor integrated with GPS-enabled modules to collect geotagged noise data at one-second intervals. The sound nodes are calibrated against a reference sound level meter in a laboratory setting to ensure accuracy using various machine learning (ML) algorithms, such as Simple Linear Regression (SLR), Multiple Linear Regression (MLR), Polynomial Regression (PR), Segmented Regression (SR), Support Vector Regression (SVR), Decision Tree (DT), and Random Forest Regression (RFR). While laboratory calibration demonstrates high accuracy, it is shown that the performance of the nodes degrades during data collection in a moving vehicle. To address this, it is demonstrated that the calibration must be performed on the IoT-based node based on the data collected in a moving environment along with the reference device. Among the employed ML models, RFR achieved the best performance with an R2 of 0.937 and RMSE of 1.09 for mobile calibration. The system was deployed in Hyderabad, India, through three measurement campaigns across 27 days, capturing 436,420 data points. Results highlight temporal and spatial noise variations across weekdays, weekends, and during Diwali. Incorporating vehicular velocity into the calibration significantly improves accuracy. The proposed system demonstrates the potential for widespread deployment of IoT-based noise sensing networks in smart cities, enabling effective noise pollution management and urban planning."
2509.01448,"This paper describes the novel use of low-cost, 5-axis, multi-material additive manufacturing to fabricate functional, complex conformal antennas. Using a customised open source 5-axis desktop printer incorporating conductive filaments, conformal S-band patch and Ultra-Wide Band antennas were fabricated and compared against planar-printed counterparts and electromagnetic simulations. Results show the potential of the approach for superior impedance matching, reduced fabrication time, and cost savings; highlighting the applicability of multi-axis multi-material prototyping of antennas with complex geometries."
2509.05116,"To advance the development of assistive and rehabilitation robots, it is essential to conduct experiments early in the design cycle. However, testing early prototypes directly with users can pose safety risks. To address this, we explore the use of condition-specific simulation suits worn by healthy participants in controlled environments as a means to study gait changes associated with various impairments and support rapid prototyping. This paper presents a study analyzing the impact of a hemiplegia simulation suit on gait. We collected biomechanical data using a Vicon motion capture system and Delsys Trigno EMG and IMU sensors under four walking conditions: with and without a rollator, and with and without the simulation suit. The gait data was integrated into a digital twin model, enabling machine learning analyses to detect the use of the simulation suit and rollator, identify turning behavior, and evaluate how the suit affects gait over time. Our findings show that the simulation suit significantly alters movement and muscle activation patterns, prompting users to compensate with more abrupt motions. We also identify key features and sensor modalities that are most informative for accurately capturing gait dynamics and modeling human-rollator interaction within the digital twin framework."
2509.05532,"Despite numerous proposed designs for superconducting neural networks (SNNs), most have overlooked practical fabrication constraints, leading to implementations limited to only a few neurons or synapses. Current superconducting technologies, such as MIT LL SFQ5ee, impose severe limitations on chip area, routing, and input/output pin counts (e.g., 5x5 mm^2 chip with 40 pins), drastically restricting network size and complexity. These hardware constraints necessitate a comprehensive framework to tailor network designs for physical realizability while minimizing accuracy loss. This paper introduces SuperSNN, a comprehensive framework for the implementation of full superconducting SNNs on a chip within these constraints. The key technical contributions include: (1) A hardware-aware training methodology for SNNs, utilizing off-chip pruning and weight quantization for energy-efficient superconducting implementations. (2) Design and layout of an inference SNN chip that incorporates novel high fan-in neurons and custom superconducting cells. (3) An optimized locally synchronous, globally synchronous (LAGS) clock distribution scheme for robust circuit implementation and management of data transfer delays in SFQ SNNs. The main results and findings demonstrate the effectiveness of the framework: (1) The complete network achieved 96.47% accuracy on the full MNIST dataset after quantization and pruning. (2) The fabricated SuperSNN chip successfully classified a reduced set of digits (2, 3, and 4) with 80.07% accuracy, reaching a maximum of 86.2% accuracy for digits 0, 1, and 2. (3) The chip operates at an ultra-high 3.02 GHz clock frequency. (4) It occupies a compact area of 3.4 x 3.9 mm^2, incorporates 5,822 Josephson Junctions, consumes 2.15 mW static power, and has an exceptionally low energy cost of 6.55 fJ (or 1.31e-6 nJ) per inference."
2509.07338,"Sketch-based monitoring in SDN often suffers from tightly coupled pipeline and memory constraints, limiting algorithmic flexibility and reducing accuracy. We propose PSketch, the first in-kernel priority-aware sketching framework implemented with eBPF. It ensures lossless tracking of high-priority flows via a hash-based table and approximates top-k elephant flows using a sketch pipe. PSketch supports both TCP and UDP and enables in-kernel retransmission tracking with minimal overhead. Unlike SDN-based approaches, it runs on commodity Linux systems, removing hardware dependencies. We perform evaluation on 10 Gbps CAIDA traces. Results show that PSketch achieves 96.0% top-k detection accuracy, 96.4% retransmission recall, and only 0.7% throughput degradation."
2509.07911,"Molecular communication (MC) provides a quantitative framework for analyzing information transfer within biological systems. This paper introduces a novel and comprehensive MC framework for the gut-brain axis (GBA) as a system of six coupled, nonlinear delay differential equations (DDEs). The proposed model defines a bidirectional feedback loop with a gut-to-brain inflammatory channel and a brain-to-gut neuroendocrine channel. Under prolonged stress, this feedback loop becomes self-perpetuating and drives the system into a pathological state. We evaluate the end-to-end channel across varying conditions using time-domain simulations, small-signal frequency-domain characterization, and an information-theoretic capacity analysis. At homeostasis, the system maintains stable circadian dynamics with higher information throughput, whereas sustained stress drives a shift to dysregulated hypercortisolism. In this pathological state, spectral efficiency decreases due to a narrowed effective bandwidth and a lower passband gain driven by neuroendocrine delays and saturating cytokine-hormone kinetics. These results quantify the impact of these signaling mechanisms on stability and information processing, elucidating the transition from healthy circadian rhythms to a persistent pathological state of hypercortisolism."
2509.0808,"Quantum computing has emerged as a promising alternative for solving combinatorial optimization problems. The standard approach for encoding optimization problems on quantum processing units (QPUs) involves transforming them into their Quadratic Unconstrained Binary Optimization (QUBO) representation. However, encoding constraints of optimization problems, particularly inequality constraints, into QUBO requires additional variables, which results in more qubits. Considering the limited availability of qubits in NISQ machines, existing encoding methods fail to scale due to their reliance on large numbers of qubits. We propose a generalized exponential penalty framework for QUBO inequality constraints inspired by a class of exponential functions, which we call exponential penalization. This paper presents an encoding strategy for inequality constraints in combinatorial optimization problems, inspired by a class of exponential functions, which we call exponential penalization. The initial idea of using exponential penalties for QUBO formulation was introduced by Montanez-Barrera et al. by applying a specific exponential function to reduce qubit requirements. In this work, we extend that approach by conducting a comprehensive study on a broader class of exponential functions, analyzing their theoretical properties and empirical performance. Our experimental results demonstrate that an exponential penalization achieves 57%, 83% qubit number reduction for Bin Packing Problem (BPP) and Traveling Salesman Problem (TSP), respectively. And we demonstrate comparable solution quality to classical with a probability of 6% and 21% accuracy for BPP with 8 and TSP with 12 qubits, respectively."
2509.08652,"Six-dimensional movable antenna (6DMA) technology has been proposed to enhance the performance of Integrated Sensing and Communication (ISAC) systems. However, within 6DMA-related research, studies on the ISAC system based on rotatable array (RA) remains relatively limited. Given the significant advantages of hybrid beamforming technology in balancing system performance and hardware complexity, this paper focuses on a channel model that accounts for the efficiency of the antenna radiation pattern and studies the sub-connected hybrid beamforming design for multi-user RA-aided ISAC systems. Aiming at the non-convex nature with coupled variables in this problem, this paper transforms the complex fractional objective function using the Fractional Programming (FP) method, and then proposes an algorithm based on the Alternating Optimization (AO) framework, which achieves optimization by alternately solving five subproblems. For the analog beamforming optimization subproblem, we adopt Singular Value Decomposition (SVD) method to transform the objective function, thereby deriving the closed-form update expression for the analog beamforming matrix. For the antenna rotation optimization subproblem, we derive the closed-form derivative expression of the array rotation angle and propose a two-stage Gradient Ascent (GA) based method to optimize the antenna rotation angle. Extensive simulation results demonstrate the effectiveness of the proposed RA-aided hybrid beamforming design method. It not only significantly improves the overall system performance while reducing hardware costs, but also achieves performance comparable to that of the fully-digital beamforming design with fixed-position antennas (FPA) under specific parameter configurations."
2509.10936,"The Metaverse emerges by integrating highly-distributed, complex, and interconnecting technologies. These technologies need to be formally verified and evaluated through formal modelling before executing them in real-world applications, in order to avoid negative impacts on the real world due to failure of the Metaverse technologies. However, the formal modelling of Metaverse technologies is challenging due to its highly complex nature. Therefore, a comprehensive formal verification of the Metaverse technologies is needed for its realization in multiple potential areas. In this study, a framework is proposed for the formal modelling of Metaverse technologies, which allows holistic insights for all applications of Metaverse technologies. By utilizing the proposed framework, Metaverse applications of any complexity can be modeled. The working of the proposed framework is illustrated by modelling a case study of an Air Traffic Control system. In the proposed framework, we utilize hierarchical colored Petri nets for formal modelling of behavior of the air traffic control system. The correctness of air traffic control system properties, such as liveness, reachability, and boundedness, is verified in the proposed framework. The results of the case study reveal that the proposed framework can be used as a template for mathematical verification of challenging and complex Metaverse applications. The results also show that formal modelling provides an effective tool for identifying flaws in the early phases of the design of Metaverse applications. The implication of using formal verification is that it can increase confidence about the correctness of the Metaverse applications."
2509.11046,"Protein-ligand binding affinity is critical in drug discovery, but experimentally determining it is time-consuming and expensive. Artificial intelligence (AI) has been used to predict binding affinity, significantly accelerating this process. However, the high-performance requirements and vast datasets involved in affinity prediction demand increasingly large AI models, requiring substantial computational resources and training time. Quantum machine learning has emerged as a promising solution to these challenges. In particular, hybrid quantum-classical models can reduce the number of parameters while maintaining or improving performance compared to classical counterparts. Despite these advantages, challenges persist: why hybrid quantum models achieve these benefits, whether quantum neural networks (QNNs) can replace classical neural networks, and whether such models are feasible on noisy intermediate-scale quantum (NISQ) devices. This study addresses these challenges by proposing a hybrid quantum neural network (HQNN) that empirically demonstrates the capability to approximate non-linear functions in the latent feature space derived from classical embedding. The primary goal of this study is to achieve a parameter-efficient model in binding affinity prediction while ensuring feasibility on NISQ devices. Numerical results indicate that HQNN achieves comparable or superior performance and parameter efficiency compared to classical neural networks, underscoring its potential as a viable replacement. This study highlights the potential of hybrid QML in computational drug discovery, offering insights into its applicability and advantages in addressing the computational challenges of protein-ligand binding affinity prediction."
2509.11767,"Wireless techniques for monitoring human vital signs, such as heart and breathing rates, offer a promising solution in the context of joint communication and sensing (JCAS) with applications in medicine, sports, safety, security, and even the military. This paper reports experimental results obtained at the Fraunhofer Institute for Integrated Circuits in Ilmenau, demonstrating the effectiveness of an indoor orthogonal frequency-division multiplexing (OFDM) JCAS system for detecting human heart and breathing rates. The system operated in a bistatic configuration at an FR2 frequency of 26.5 GHz with a variable bandwidth of up to 1 GHz. Measurements were taken under various scenarios, including a subject lying down, sitting, or walking, in both line-of-sight and non-line-of-sight conditions, and with one or two subjects present simultaneously. The results indicate that while vital sign detection is generally feasible, its effectiveness is influenced by several factors, such as the subjects clothing, activity, as well as the distance and angle relative to the sensing system. In addition, no significant influence of bandwidth was detected since the vital signs information is encoded in the phase of the signal."
2509.11845,"Ride-sourcing platforms such as Uber and Lyft are prime examples of the gig economy, recruiting drivers as independent contractors, thereby avoiding legal and fiscal obligations. Although platforms offer flexibility in choosing work shifts and areas, many drivers experience low income and poor working conditions, leading to widespread strikes and protests. Minimum wage regulation is adopted to improve drivers welfare. However, the impacts of this regulation on drivers as well as on travelers and platforms, remain largely unknown. While ride-sourcing platforms do not disclose the relevant data, state-of-the-art models fail to explain the effects of minimum wage regulation on market dynamics. In this study, we assess the effectiveness and implications of minimum wage regulation in ride-sourcing markets while simulating the detailed dynamics of ride-sourcing markets under varying regulation intensities, both with and without the so-called platform lockout strategy. Our findings reveal that minimum wage regulation impacts substantially drivers income, and may lead to higher fares for travelers and threaten platforms survival. When platforms adopt a lockout strategy, their profitability significantly improves and drivers earn more, although many others lose their jobs, and service level for travelers consequently declines."
2509.12083,"Neutral atom quantum computing's great scaling potential has resulted in it emerging as a popular modality in recent years. For state preparation, atoms are loaded stochastically and have to be detected and rearranged at runtime to create a predetermined initial configuration for circuit execution. Such rearrangement schemes either suffer from low parallelizability for acousto-optic deflector (AOD)-based approaches or are comparatively slow in case of spatial light modulators (SLMs). In our work, we introduce an algorithm that can improve the parallelizability of the former. Since the transfer of atoms from static SLM traps to AOD-generated movable traps is detrimental both in terms of atom loss rates and execution time, our approach is based on highly-parallel composite moves where many atoms are picked up simultaneously and maneuvered into target positions that may be comparatively distant. We see that our algorithm outperforms its alternatives for near-term devices with up to around 1000 qubits and has the potential to scale up to several thousand with further optimizations."
2509.12657,"Space-Air-Ground-Integrated Networks (SAGIN) enable seamless data connectivity for applications such as smart transport, healthcare, smart cities, and disaster response through the coordinated use of low-earth orbit (LEO) satellites, base stations mounted with uncrewed aerial vehicles (UAV), and terrestrial infrastructure. This paper provides a detailed analysis of resource management frameworks, reviews the literature, and evaluates key methods such as alternating optimization (AO), damped iterative water filling (DIWF), and genetic algorithms (GA) for resource allocation. MATLAB simulation results benchmark these algorithms across 10,000 trials, demonstrating robust, fair, and low-latency resource allocation. In addition, this paper also analyzes strategies for user association with terrestrial and aerial base stations during emergencies and network overloads. The main contributions include a comparative assessment of resource allocation strategies in SAGIN and an in-depth analysis of user association policies for emergency scenarios. The study provides guidance for designing resilient and efficient next-generation networks. Potential future research directions include investigating satellite handover and multi-domain orchestration for SAGIN deployments."
2509.1356,"Efficiently optimizing Nondeterministic Polynomial time (NP) problems in polynomial time has profound implications in many domains. CMOS oscillator networks have been shown to be effective and efficient in approximating certain NP-hard problems such as minimization of Potts Hamiltonian, and computational complexity theory guarantees that any NP problem can be reduced to it. In this paper, we formulate a variety of NP problems using first-order and multi-phase Potts Hamiltonian. We also propose a 3-state asymmetrically weighted oscillator optimizer design to optimize the problems. Building on existing knowledge in CMOS design, our proposed algorithms offer a promising pathway for large-scale optimization of NP problems."
2509.13964,"The recent co-optimization of memristive technologies and programming algorithms enabled neural networks training with in-memory computing systems. In this context, novel analog filamentary conductive-metal-oxide (CMO)/HfOx redox-based resistive switching memory (ReRAM) represents a key technology. Despite device performance enhancements reported in literature, the underlying mechanism behind resistive switching is not fully understood. This work presents the first physics-based analytical model of the current transport and of the resistive switching in these devices. As a case study, analog TaOx/HfOx ReRAM devices are considered. The current transport is explained by a trap-to-trap tunneling process, and the resistive switching by a modulation of the defect density within the sub-band of the TaOx that behaves as electric field and temperature confinement layer. The local temperature and electric field distributions are derived from the solution of the electric and heat transport equations in a 3D finite element ReRAM model. The intermediate resistive states are described as a gradual modulation of the TaOx defect density, which results in a variation of its electrical conductivity. The drift-dynamics of ions during the resistive switching is analytically described, allowing the estimation of defect migration energies in the TaOx layer. Moreover, the role of the electro-thermal properties of the CMO layer is unveiled. The proposed analytical model accurately describes the experimental switching characteristic of analog TaOx/HfOx ReRAM devices, increasing the physical understanding and providing the equations necessary for circuit simulations incorporating this technology."
2509.1454,"Wearable devices are revolutionizing personal technology, but their usability is often hindered by frequent charging due to high power consumption. This paper introduces Distributed Neural Networks (DistNN), a framework that distributes neural network computations between resource-constrained wearable nodes and resource-rich hubs to reduce energy at the node without sacrificing performance. We define a Figure of Merit (FoM) to select the optimal split point that minimizes node-side energy. A custom hardware design using low-precision fixed-point arithmetic achieves ultra-low power while maintaining accuracy. The proposed system is ~1000x more energy efficient than a GPU and averages 11x lower power than recent machine learning (ML) ASICs at 30 fps. Evaluated with CNNs and autoencoders, DistNN attains an SSIM of 0.90 for image reconstruction and 0.89 for denoising, enabling scalable, energy-efficient, real-time wearable applications."
2509.14883,"The unmanned aerial vehicle (UAV) based multi-access edge computing (MEC) appears as a popular paradigm to reduce task processing latency. However, the secure offloading is an important issue when occurring aerial eavesdropping. Besides, the potential uncertainties in practical applications and flexible trajectory optimizations of UAVs pose formidable challenges for realizing robust offloading. In this paper, we consider the aerial secure MEC network including ground users, service unmanned aerial vehicles (S-UAVs) integrated with edge servers, and malicious UAVs overhearing transmission links. To deal with the task computation complexities, which are characterized as uncertainties, a robust problem is formulated with chance constraints. The energy cost is minimized by optimizing the connections, trajectories of S-UAVs and offloading ratios. Then, the proposed non-linear problem is tackled via the distributionally robust optimization and conditional value-at-risk mechanism, which is further transformed into the second order cone programming forms. Moreover, we decouple the reformulated problem and design the successive convex approximation for S-UAV trajectories. The global algorithm is designed to solve the sub-problems in a block coordinate decent manner. Finally, extensive simulations and numerical analyses are conducted to verify the robustness of the proposed algorithms, with just 2\% more energy cost compared with the ideal circumstance."
2509.15461,"Digital Engineering (DE) transformation represents a paradigm shift in systems engineering (SE), aiming to integrate diverse analytical models and digital artifacts into an authoritative source of truth for improved traceability and more efficient system lifecycle management. Despite institutional support, many DE initiatives underperform or fail to realize their intended benefits. We argue that this often results from a limited understanding of the social and technical barriers, and particularly how their interplay shapes transformation outcomes. To address this gap, we document barriers identified in the literature and grounded in sociotechnical systems theory, organized into six dimensions: people, processes, culture, goals, infrastructure, and technology. We then map these barriers to the U.S. Department of Defense's DE policy goals. Our analysis shows that technological investments alone are insufficient, as failures frequently arise from social factors such as workforce readiness, leadership support, and cultural alignment. The mapping also demonstrates that sociotechnical barriers often cascade across dimensions, making their impact on policy goals difficult to trace and complicating implementation. These insights carry practical implications: managers may use the mapping as a diagnostic tool to identify risks and prioritize resources; policymakers may complement strategic mandates with sustained investments and long-term change management; and engineers may view DE not as a threat to job security but as an opportunity for more effective collaboration."
2509.16205,"We presentthis http URL, a compact (approx. 200 lines) Python script that automates the collection of key compiler metrics, i.e., gate depth, two-qubit-gate count, wall-clock compilation time, and memory footprint, across multiple open-source quantum circuit transpilers. The suite ships with six didactic circuits (3 to 8 qubits) implementing fundamental quantum algorithms and supports Qiskit, tket, Cirq, and the Qiskit-Braket provider; in this paper we showcase results for Qiskit 0.46 and Braket 1.16. The entire run completes in under three minutes on a laptop, emits a single CSV plus publisheable plot, and reproduces the figure here with one command. We release the code under the MIT licence to serve as a quick-start regression harness for NISQ compiler research."
2509.16213,"Neuromorphic computing promises brain-like efficiency, yet today's multi-chip systems scale over PCBs and incur orders-of-magnitude penalties in bandwidth, latency, and energy, undermining biological algorithms and system efficiency. We present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip interconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets on a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based asynchronous wafer fabric with hierarchical time-step synchronization provide low-latency, coherent operation across the wafer. Each chiplet implements 2.35 M neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses perthis http URL333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9 pJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by a holistic chiplet-interposer co-design flow (including an in-house interposer-bump planner with early SI/PI and electro-thermal closure) and a warpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin connections, enabling robust, demountable wafer-to-board integration. Measurements confirm 10 mV supply droop and a uniform thermal profile (34-36 C) under ~100 W. Application studies demonstrate whole-brain simulations: two zebrafish brains per chiplet with high connectivity fidelity (Spearman r = 0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our knowledge, DarwinWafer represents a pioneering demonstration of wafer-scale neuromorphic computing, establishing a viable and scalable path toward large-scale, brain-like computation on silicon by replacing PCB-level interconnects with high-density, on-wafer integration."
2509.16497,"Souper is a powerful enumerative superoptimizer that enhances the runtime performance of programs by optimizing LLVM intermediate representation (IR) code. However, its verification process, which relies on a computationally expensive SMT solver to validate optimization candidates, must explore a large search space. This large search space makes the verification process particularly expensive, increasing the burden to incorporate Souper into compilation tools. We propose PrediPrune, a stochastic candidate pruning strategy that effectively reduces the number of invalid candidates passed to the SMT solver. By utilizing machine learning techniques to predict the validity of candidates based on features extracted from the code, PrediPrune prunes unlikely candidates early, decreasing the verification workload. When combined with the state-of-the-art approach (Dataflow), PrediPrune decreases compilation time by 51% compared to the Baseline and by 12% compared to using only Dataflow, emphasizing the effectiveness of the combined approach that integrates a purely ML-based method (PrediPrune) with a purely non-ML based (Dataflow) method. Additionally, PrediPrune offers a flexible interface to trade-off compilation time and optimization opportunities, allowing end users to adjust the balance according to their needs."
2509.16676,"The emergence of agentic Artificial Intelligence (AI), which can operate autonomously, demonstrate goal-directed behavior, and adaptively learn, indicates the onset of a massive change in today's computing infrastructure. This study investigates how agentic AI models' multiple characteristics may impact the architecture, governance, and operation under which computing environments function. Agentic AI has the potential to reduce reliance on extremely large (public) cloud environments due to resource efficiency, especially with processing and/or storage. The aforementioned characteristics provide us with an opportunity to canvas the likelihood of strategic migration in computing infrastructures away from massive public cloud services, towards more locally distributed architectures: edge computing and on-premises computing infrastructures. Many of these likely migrations will be spurred by factors like on-premises processing needs, diminished data consumption footprints, and cost savings. This study examines how a solution for implementing AI's autonomy could result in a re-architecture of the systems and model a departure from today's governance models to help us manage these increasingly autonomous agents, and an operational overhaul of processes over a very diverse computing systems landscape that bring together computing via cloud, edge, and on-premises computing solutions. To enable us to explore these intertwined decisions, it will be fundamentally important to understand how to best position agentic AI, and to navigate the future state of computing infrastructures."
2509.16723,"6G wireless communication networks are expected to use extremely large-scale antenna arrays (ELAAs) to support higher throughput, massive connectivity, and improved system performance. ELAAs would fundamentally alter wave characteristics, transforming them from plane waves into spherical waves, thereby operating in the near field. Near-field communications (NFC) offer unique advantages to enhance system performance, but also present significant challenges in channel modeling, computational complexity, and beamforming design. The use of machine learning (ML) is emerging as a powerful approach to tackle such challenges and has the capabilities to enable intelligent, secure, and efficient 6G wireless communications. In this survey, we discuss ML-driven approaches for NFC. We first outline the fundamental concepts of NFC and ML. We then discuss ML applications in channel estimation, beamforming design, and security enhancement. We also highlight key challenges (e.g., data privacy and computational overhead). Finally, we discuss open issues and future directions to emphasize the role of advanced ML techniques in near-field system design."
2509.17227,"The Internet of Bio-Nano Things (IoBNT) promises to revolutionize healthcare by interfacing the cyber domain with the living systems at unprecedented resolution. Realizing this vision hinges on the development of Bio-Nano Things (BNTs), i.e., functional nodes capable of sensing, actuation, and communications within biological environments. Existing BNT architectures, e.g., nanomaterial-based, biosynthetic, and passive molecular agents, face significant limitations, including toxicity, lack of autonomy, or the safety and metabolic burdens associated with genetic modification. This paper posits a fourth paradigm: the transient hijacking of living cells via non-genetic cell surface engineering (NG-CSE) to enable living BNTs. NGCSE allows for the precise, reversible functionalization of cell membranes with synthetic molecular machinery, reprogramming cellular functions and interactions without altering the genome. It uniquely combines the inherent biocompatibility and agency of living cells with the programmability enabled by nanotechnology, mitigating the risks of genetic engineering. We critically review the toolbox of NG-CSE and explore the opportunities it unlocks for IoBNT, including programmable cell-cell communication, dynamic network topologies, and improved bio-cyber interfacing. Moreover, we propose novel IoBNT architectures that leverage these capabilities, such as circulating sentinel networks exploiting cellular agency for continuous liquid biopsy, and rationally designed, in vitro biocomputers exploiting interkingdom interactions. We also outline the critical challenges in modeling and exploiting cellular agency with NG-CSE, providing a roadmap for the effective utilization of NG-CSE-enabled living BNTs within IoBNT."
2509.1729,"Machine-generated proofs are poised to reach large-scale, human-unreadable artifacts. They foreshadow what we call the Fourth Mathematical Crisis. This crisis crystallizes around three fundamental tensions: trusting proofs that no human can inspect, understanding results that no one can fully read, and verifying systems that themselves resist verification. As a minimal yet principled response, we propose the Human Understandability (HU) meta-axiom, which requires that every proof admits at least one projection that is resource-bounded, divergence-measured, and acceptable to a verifier. Confronting these questions opens a timely research agenda and points toward new directions in scalable reasoning, interpretable inference, and epistemic trust for the era of machine-scale mathematics."
2509.17324,"Variational Quantum Algorithms (VQAs) are widely used in the noisy intermediate-scale quantum (NISQ) era, but their trainability and performance depend critically on initialization parameters that shape the optimization landscape. Existing machine learning-based initializers achieve state-of-the-art results yet remain constrained to single-task domains and small datasets of only hundreds of samples. We address these limitations by reformulating VQA parameter initialization as a generative modeling problem and introducing DiffQ, a parameter initializer based on the Denoising Diffusion Probabilistic Model (DDPM). To support robust training and evaluation, we construct a dataset of 15,085 instances spanning three domains and five representative tasks. Experiments demonstrate that DiffQ surpasses baselines, reducing initial loss by up to 8.95 and convergence steps by up to 23.4%."
2509.17533,"The deployment of machine learning (ML) models on microcontrollers (MCUs) is constrained by strict energy, latency, and memory requirements, particularly in battery-operated and real-time edge devices. While software-level optimizations such as quantization and pruning reduce model size and computation, hardware acceleration has emerged as a decisive enabler for efficient embedded inference. This paper evaluates the impact of Neural Processing Units (NPUs) on MCU-based ML execution, using the ARM Cortex-M55 core combined with the Ethos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a representative platform. A rigorous measurement methodology was employed, incorporating per-inference net energy accounting via GPIO-triggered high-resolution digital multimeter synchronization and idle-state subtraction, ensuring accurate attribution of energy costs. Experimental results across six representative ML models -including MiniResNet, MobileNetV2, FD-MobileNet, MNIST, TinyYolo, and SSD-MobileNet-demonstrate substantial efficiency gains when inference is offloaded to the NPU. For moderate to large networks, latency improvements ranged from 7x to over 125x, with per-inference net energy reductions up to 143x. Notably, the NPU enabled execution of models unsupported on CPU-only paths, such as SSD-MobileNet, highlighting its functional as well as efficiency advantages. These findings establish NPUs as a cornerstone of energy-aware embedded AI, enabling real-time, power-constrained ML inference at the MCU level."
2509.17963,"This work presents a novel approach to configure 2T-nC ferroelectric RAM (FeRAM) for performing single cell logic-in-memory operations, highlighting its advantages in energy-efficient computation over conventional DRAM-based approaches. Unlike conventional 1T-1C dynamic RAM (DRAM), which incurs refresh overhead, 2T-nC FeRAM offers a promising alternative as a non-volatile memory solution with low energy consumption. Our key findings include the potential of quasi-nondestructive readout (QNRO) sensing in 2T-nC FeRAM for logic-in-memory (LiM) applications, demonstrating its inherent capability to perform inverting logic without requiring external modifications, a feature absent in traditional 1T-1C DRAM. We successfully implement the MINORITY function within a single cell of 2T-nC FeRAM, enabling universal NAND and NOR logic, validated through SPICE simulations and experimental data. Additionally, the research investigates the feasibility of 3D integration with 2T-nC FeRAM, showing substantial improvements in storage and computational density, facilitating bulk-bitwise computation. Our evaluation of eight real-world, data-intensive applications reveals that 2T-nC FeRAM achieves 2x higher performance and 2.5x lower energy consumption compared to DRAM. Furthermore, the thermal stability of stacked 2T-nC FeRAM is validated, confirming its reliable operation when integrated on a compute die. These findings emphasize the advantages of 2T-nC FeRAM for LiM, offering superior performance and energy efficiency over conventional DRAM."
2509.18121,"The increasing deployment of wearable sensors and implantable devices is shifting AI processing demands to the extreme edge, necessitating ultra-low power for continuous operation. Inspired by the brain, emerging memristive devices promise to accelerate neural network training by eliminating costly data transfers between compute and memory. Though, balancing performance and energy efficiency remains a challenge. We investigate ferroelectric synaptic devices based on HfO2/ZrO2 superlattices and feed their experimentally measured weight updates into hardware-aware neural network simulations. Across pulse widths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require more training epochs while still reducing total energy without sacrificing accuracy. Classification accuracy using plain stochastic gradient descent (SGD) is diminished compared to mixed-precision SGD. We analyze the causes and propose a ``symmetry point shifting'' technique, addressing asymmetric updates and restoring accuracy. These results highlight a trade-off among accuracy, convergence speed, and energy use, showing that short-pulse programming with tailored training significantly enhances on-chip learning efficiency."
2509.18143,"Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits offer the potential for highly energy-efficient Artificial Neural Network (ANN) computation in full custom analog IC designs. The efficient mapping of Artificial Neuron (AN) abstract weights, extracted from the software-trained ANNs, onto physical ACN capacitance values has, however, yet to be fully researched. In this paper, we explore the unexpected hidden complexities, challenges and properties of the mapping, as well as, the ramifications for IC designers in terms accuracy, design and implementation. We propose an optimal, AN to ACN methodology, that promotes smaller chip sizes and improved overall classification accuracy, necessary for successful practical deployment. Using TensorFlow and Larq software frameworks, we train three different ANN networks and map their weights into the energy-efficient DTSC ACN capacitance value domain to demonstrate 100% functional equivalency. Finally, we delve into the impact of weight quantization on ACN performance using novel metrics related to practical IC considerations, such as IC floor space and comparator decision-making efficacy."
2509.18679,"In the current era of quantum computing, minimizing noise is essential for reliably executing quantum circuits on hardware. A key factor affecting circuit performance is the mapping of the abstract quantum circuit to the physical layout of the quantum hardware. This mapping can significantly influence output quality, especially since hardware noise profiles are non-uniform and dynamic. Existing solutions such as Mapomatic and Just-In-Time (JIT) Transpilation attempt to address this issue but are limited either by relying on stale calibration data or high hardware usage, respectively. In this article, we propose Quality Indicator Circuits (QICs) as a lightweight, real-time method for assessing layout quality. A QIC is a small probe circuit that is designed to retain the basic structure of the user's circuit and whose ideal noiseless outcome is known. It is used to evaluate which region of the quantum hardware is best suited for executing the circuit of interest. We first propose a basic method where a QIC is executed for each isomorphic layout to detect the best among them. Although this requires several targeted circuit executions, we show that it still, in most cases, reduces the execution overheads as compared with JIT. To reduce the overheads further, we propose the union of multiple layouts with a Union QIC approach that has no overlaps, and a Distortion Threshold based approach allowing some overlap. Our results show that these outperform Mapomatic in the quality of layout selection while reducing the hardware overhead of JIT by 79 percent on average. This makes our proposed method lightweight and reliable, and a viable technique for layout selection in near-term quantum devices."
2509.18906,"This paper introduces a novel framework for Edge Inference (EI) that bypasses the conventional practice of treating the wireless channel as noise. We utilize Stacked Intelligent Metasurfaces (SIMs) to control wireless propagation, enabling the channel itself to perform over-the-air computation. This eliminates the need for symbol estimation at the receiver, significantly reducing computational and communication overhead. Our approach models the transmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN) where the response of the SIM elements are trainable parameters. To address channel variability, we incorporate a dedicated DNN module responsible for dynamically adjusting transmission power leveraging user location information. Our performance evaluations showcase that the proposed metasurfaces-integrated DNN framework with deep SIM architectures are capable of balancing classification accuracy and power consumption under diverse scenarios, offering significant energy efficiency improvements."
2509.19257,"Transparency and security are essential in our voting system, and voting machines. This paper describes an implementation of a stateless, transparent voting machine (STVM). The STVM is a ballot marking device (BMD) that uses a transparent, interactive printing interface where voters can verify their paper ballots as they fill out the ballot. The transparent interface turns the paper ballot into an interactive interface. In this architecture, stateless describes the machine's boot sequence, where no information is stored or passed forward between reboots. The machine does not have a hard drive. Instead, it boots and runs from read-only media. This STVM design utilizes a Blu-ray Disc ROM (BD-R) to boot the voting software. This system's statelessness and the transparent interactive printing interface make this design the most secure BMD for voting. Unlike other voting methods, this system incorporates high usability, accessibility, and security for all voters. The STVM uses an open-source voting system that has a universally designed interface, making the system accessible for all voters independent of their ability or disability. This system can make voting safer by simultaneously addressing the issue of voters noticing a vote flip and making it difficult for a hack to persist or go unmitigated."
2509.19642,"The ability to process and act on data in real time is increasingly critical for applications ranging from autonomous vehicles, three-dimensional environmental sensing and remote robotics. However, the deployment of deep neural networks (DNNs) in edge devices is hindered by the lack of energy-efficient scalable computing hardware. Here, we introduce a fanout spatial time-of-flight optical neural network (FAST-ONN) that calculates billions of convolutions per second with ultralow latency and power consumption. This is enabled by the combination of high-speed dense arrays of vertical-cavity surface-emitting lasers (VCSELs) for input modulation with spatial light modulators of high pixel counts for in-memory weighting. In a three-dimensional optical system, parallel differential readout allows signed weight values accurate inference in a single shot. The performance is benchmarked with feature extraction in You-Only-Look-Once (YOLO) for convolution at 100 million frames per second (MFPS), and in-system backward propagation training with photonic reprogrammability. The VCSEL transmitters are implementable in any free-space optical computing systems to improve the clockrate to over gigahertz. The high scalability in device counts and channel parallelism enables a new avenue to scale up free space computing hardware."
2509.21598,"Current biocomputing approaches predominantly rely on engineered circuits with fixed logic, offering limited stability and reliability under diverse environmental conditions. Here, we use the GRNN framework introduced in our previous work to transform bacterial gene expression dynamics into a biocomputing library of mathematical solvers. We introduce a sub-GRNN search algorithm that identifies functional subnetworks tailored to specific mathematical calculation and classification tasks by evaluating gene expression patterns across chemically encoded input conditions. Tasks include identifying Fibonacci numbers, prime numbers, multiplication, and Collatz step counts. The identified problem-specific sub-GRNNs are then assessed using gene-wise and collective perturbation, as well as Lyapunov-based stability analysis, to evaluate robustness and reliability. Our results demonstrate that native transcriptional machinery can be harnessed to perform diverse mathematical calculation and classification tasks, while maintaining computing stability and reliability."
2509.21622,"Quantum machine learning (QML) promises significant speedups, particularly when operating on quantum datasets. However, its progress is hindered by the scarcity of suitable training data. Existing synthetic data generation methods fall short in capturing essential entanglement properties, limiting their utility for QML. To address this, we introduce QMill, a low-depth quantum data generation framework that produces entangled, high-quality samples emulating diverse classical and quantum distributions, enabling more effective development and evaluation of QML models in representative-data settings."
2509.2268,"AI training creates synchronized, step-dominant surges with millisecond edges that destabilize constant-power loads (Choukse et al., 2025;arXiv:2508.14318). We propose a physics-anchored row-scale $\pm 400$ Vdc architecture that makes Computational Continuity a structural property. DRUs supply fast energy via controlled droop; SSTs regulate average power with bounded ramps and no reverse power flow and no high-frequency export at the PCC; import is subjected to a bounded dP/dt envelope; film capacitance and clamps absorb the first edge. The contract is explicit: $\pm 1\%$ steady-band, $\leq 2\%$ transient deviation, $\leq 3$ ms recovery, $\geq 45^{\circ}$ margin, reserve floors intact, yields spine and lowest branches. Recharge is valley-following (admitted only below Avg with MW headroom; $\leq 5$ kW/s per row ramps). Protection is time-graded (branch $\mu$s, row ms, MW seconds). Scaling preserves invariants from row to pod/hall/campus without retuning. Conformance is by waveform evidence (microsecond branch clears, $2\%/50$ ms holds, FLISR with no reverse power flow and no high-frequency export at the PCC). The result is not tuning but a contract for continuity."
2509.22767,"Modern computers perform pre-defined operations using static memory components, whereas biological systems learn through inherently dynamic, time-dependent processes in synapses and neurons. The biological learning process also relies on global signals - neuromodulators - who influence many synapses at once depending on their dynamic, internal state. In this study, using optical radiation as a global neuromodulatory signal, we investigate nanoscale SrTiO3 (STO) memristors that can act as solid-state synapses. Via diverse sets of measurements, we demonstrate that the memristor's photoresponse depends on the electrical conductance state, following a well-defined square root relation. Additionally, we show that the conductance decays after photoexcitation with time constants in the range of 1 - 10 s and that this effect can be reliably controlled using an electrical bias. These properties in combination with our device's low power operation (< 1pJ per optical pulse) and small measurement variability may pave the way for space- and energy-efficient implementations of complex biological learning processes in electro-optical hardware."
2509.23463,"In the realm of programmable photonic integrated circuits (PICs), precise wire length control is crucial for the performance of on-chip programmable components such as optical ring resonators, Mach-Zehnder interferometers, and optical true time-delay lines. Unlike conventional routing algorithms that prioritize shortest-path solutions, these photonic components require exact-length routing to maintain the desired optical properties.To address these challenges, this paper presents different length-matching routing strategies to find exact-length paths while balancing search space and runtime efficiently. We propose a novel admissible heuristic estimator and a pruning method, designed to enhance the accuracy and efficiency of the search process. The algorithms are derived from the Best-First search with modified evaluation functions. For two-pin length-matching routing, we formally prove that the proposed algorithms are complete under monotonic heuristics. For multi-pin length-matching challenges, we introduce a pin-ordering mechanism based on detour margins to reduce the likelihood of prematurely blocking feasible routes. Through evaluations on various length-matching benchmarks, we analyze runtime and heuristic performance, demonstrating the effectiveness of the proposed approaches across different layout scenarios."
2509.24992,"We present a bio-hybrid environmental sensor system that integrates natural plants and embedded deep learning for real-time, on-device detection of temperature and ozone level changes. Our system, based on the low-power PhytoNode platform, records electric differential potential signals from Hedera helix and processes them onboard using an embedded deep learning model. We demonstrate that our sensing device detects changes in temperature and ozone with good sensitivity of up to 0.98. Daily and inter-plant variability, as well as limited precision, could be mitigated by incorporating additional training data, which is readily integrable in our data-driven framework. Our approach also has potential to scale to new environmental factors and plant species. By integrating embedded deep learning onboard our biological sensing device, we offer a new, low-power solution for continuous environmental monitoring and potentially other fields of application."
2509.25057,"Microorganisms employ sophisticated mechanisms for intercellular communication and environmental sensing, with quorum sensing serving as a fundamental regulatory process. Dysregulation of quorum sensing has been implicated in various diseases. While most theoretical studies focus on mathematical modeling of quorum sensing dynamics, the communication-theoretic aspects remain less explored. In this study, we investigate the information processing capabilities of quorum sensing systems using a stochastic differential equation framework that links intracellular gene regulation to extracellular autoinducer dynamics. We quantify mutual information as a measure of signaling efficiency and information fidelity in two major bacterial phyla of the gut microbiota: Firmicutes and Bacteroidetes."
2510.01673,"Photonic computing has emerged as a promising substrate for accelerating the dense linear-algebra operations at the heart of AI, yet adoption for large Transformer models remains in its infancy. We identify two bottlenecks: (1) costly electro--optic conversions and data-movement overheads that erode energy efficiency as model sizes scale; (2) a mismatch between limited on-chip photonic resources and Transformer scale, which forces frequent reuse of photonic tensor cores and dilutes throughput gains. To address these challenges, we introduce a hardware--software co-design framework. First, we propose \texttt{Lighten}, a PTC-aware compression flow that post-hoc decomposes each Transformer weight matrix into a low-rank component plus a structured-sparse component aligned to photonic tensor-core granularity, without lengthy retraining. Second, we present \texttt{ENLighten}, a reconfigurable photonic accelerator with dynamically adaptive tensor cores, driven by broadband light redistribution, enabling fine-grained sparsity support and full power gating of inactive parts. On ImageNet, \texttt{Lighten} prunes a Base-scale Vision Transformer by 50\% with $\approx$1\% accuracy drop after only 3 epochs (about 1 hour) of fine-tuning. Deployed on \texttt{ENLighten}, it achieves a $2.5\times$ improvement in energy--delay product over the state-of-the-art photonic Transformer accelerator."
2510.02417,"DNA is a promising medium for digital information storage for its exceptional density and durability. While prior studies advanced coding theory, workflow design, and simulation tools, challenges such as synthesis costs, sequencing errors, and biological constraints (GC-content imbalance, homopolymers) limit practical deployment. To address this, our framework draws from quantum parallelism concepts to enhance encoding diversity and resilience, integrating biologically informed constraints with deep learning to enhance error mitigation in DNA storage. NeuroDNAAI encodes binary data streams into symbolic DNA sequences, transmits them through a noisy channel with substitutions, insertions, and deletions, and reconstructs them with high fidelity. Our results show that traditional prompting or rule-based schemes fail to adapt effectively to realistic noise, whereas NeuroDNAAI achieves superior accuracy. Experiments on benchmark datasets demonstrate low bit error rates for both text and images. By unifying theory, workflow, and simulation into one pipeline, NeuroDNAAI enables scalable, biologically valid archival DNA storage"
2510.03126,"Advances in fabrication technology have enabled modularizing electronic components at the micro- or nano-scale and composing these modules on demand into larger circuits. Micromodular and nanomodular electronics (ME and NE) open a new design space in electronics, promising a degree of flexibility, extensibility, and accessibility far superior to traditional monolithic methods. ME/NE leverage a multi-stage process of initial imprecise component deposition, followed by precise wire printing to compose them into a circuit. Due to imperfections in deposition, each circuit instance has a unique layout with its own component placement and wire routing solutions, putting the design automation process on the critical path. Moreover, high-performance nanomodular components enable the synthesis of larger heterogeneous circuits than traditional printed electronics, requiring more scalable algorithms. ME/NE thus introduce a tradeoff between the time-to-solution for placement/routing algorithms and the resulting total wire length, with the latter dictating circuit printing time. We explore this tradeoff by adapting standard partitioning, floorplanning, placement, and routing algorithms to the unique characteristics of ME/NE. Our evaluations demonstrate significant optimization headroom in different dimensions. For example, our tunable algorithms can deliver a $108\times$ improvement in end-to-end manufacturing time at the cost of $21\%$ increase in total wire length. Conversely, circuit quality/performance can be prioritized at the cost of increased manufacturing time, highlighting the value of the ability to dynamically navigate the tradeoff space according to the primary optimization metric."
2510.03956,"Adiabatic Quantum-Flux-Parametron (AQFP) logic is a promising emerging superconducting technology for ultra-low power digital circuits, offering orders of magnitude lower power consumption than CMOS. However, AQFP scalability is challenged by excessive buffer overhead due to path balancing technology constraints. Addressing this, recent AQFP works have proposed design solutions to reduce path balancing overhead using phase-skipping and phase-alignment. Phase-skipping is a circuit-level technique that allows data transfer between AQFP gates clocked with non-consecutive clock phases. In contrast, phase-alignment is an architectural approach involving repeating input patterns to allow data transfer between AQFP gates across multiples of full clock cycles. While both techniques individually mitigate the area overhead of path-balancing, they have not yet been jointly explored. In this work, we present the first clock phase scheduling algorithm that combines phase-skipping and phase-alignment. We first present a minimum area method that on average, achieves a 25% area reduction compared to phase-skipping alone and a 11% reduction compared to phase-alignment. We then extend the method to enforce a target throughput, enabling efficient area-performance trade-offs. With our throughput constrained optimization, we achieve on average 6.8% area savings with a 2.62x increased throughput compared to the state-of-the-art phase-aligned method."
2510.04535,"We propose to revisit the functional scaling paradigm by capitalizing on two recent developments in advanced chip manufacturing, namely 3D wafer bonding and backside processing. This approach leads to the proposal of the CMOS 2.0 platform. The main idea is to shift the CMOS roadmap from geometric scaling to fine-grain heterogeneous 3D stacking of specialized active device layers to achieve the ultimate Power-Performance-Area and Cost gains expected from future technology generations. However, the efficient utilization of such a platform requires devising architectures that can optimally map onto this technology, as well as the EDA infrastructure that supports it. We also discuss reliability concerns and eventual mitigation approaches. This paper provides pointers into the major disruptions we expect in the design of systems in CMOS 2.0 moving forward."
2510.06413,"Variational quantum algorithms provide a direct, physics-based approach to protein structure prediction, but their accuracy is limited by the coarse resolution of the energy landscapes generated on current noisy devices. We propose a hybrid framework that combines quantum computation with deep learning, formulating structure prediction as a problem of energy fusion. Candidate conformations are obtained through the Variational Quantum Eigensolver (VQE) executed on IBM's 127-qubit superconducting processor, which defines a global yet low-resolution quantum energy surface. To refine these basins, secondary structure probabilities and dihedral angle distributions predicted by the NSP3 neural network are incorporated as statistical potentials. These additional terms sharpen the valleys of the quantum landscape, resulting in a fused energy function that enhances effective resolution and better distinguishes native-like structures. Evaluation on 375 conformations from 75 protein fragments shows consistent improvements over AlphaFold3, ColabFold, and quantum-only predictions, achieving a mean RMSD of 4.9  with statistical significance (p < 0.001). The findings demonstrate that energy fusion offers a systematic method for combining data-driven models with quantum algorithms, improving the practical applicability of near-term quantum computing to molecular and structural biology."
2510.06608,"ProtoSpace is a custom JPL-built platform to help scientists and engineers visualize their CAD models collaboratively in augmented reality (AR) and on the web in 3D. In addition to this main use case, ProtoSpace has been used throughout the entire spacecraft mission lifecycle and beyond: ventilator design and assembly; providing AR-based instructions to astronauts in-training; educating the next generation on the process of spacecraft design; etc. ProtoSpace has been used for a decade by NASA missions-including Mars Perseverance, Europa Clipper, NISAR, SPHEREx, CAL, and Mars Sample Return-to reduce cost and risk by helping engineers and scientists fix problems earlier through reducing miscommunication and helping people understand the spatial context of their spacecraft in the appropriate physical context more quickly. This paper will explore how ProtoSpace came to be, define the system architecture and overview-including HoloLens and 3D web clients, the ProtoSpace server, and the CAD model optimizer-and dive into the use cases, spin-offs, and lessons learned that led to 10 years of success at NASA's Jet Propulsion Laboratory."
2510.07009,"We present a low-latency tele-immersive entertainment system that streams 3D point clouds and performers' footstep vibrations, creating the sense that the stage is present. Moving performers and their surroundings are captured as dynamic point clouds under rapidly changing lighting, then processed, transmitted, and rendered within a total latency of less than 100 ms. Under high ambient noise, footstep vibrations are sensed by wearable accelerometers. Real-time visual and haptic streams are delivered to a remote venue, where a large 3D LED wall and a vibration-efficient haptic floor envelop dozens of spectators. A public trial at Expo 2025 linked sites 20 km apart: visitors watched a live dance show and conversed with performers without noticeable delay."
2510.07079,"We present a blueprint for a quantum middle layer that supports applications across various quantum technologies. Inspired by concepts and abstractions from HPC libraries and middleware, our design is backend-neutral and context-aware. A program only needs to specify its intent once as typed data and operator descriptors. It declares what the quantum registers mean and which logical transformations are required, without committing to gates, pulses, continuous-variable routines, or anneal backend. Such execution details are carried separately in a context descriptor and can change per backend without modifying the intent artifacts.We develop a proof of concept implementation that uses JSON files for the descriptors and two backends: a gate-model path realized with IBM Qiskit Aer simulator and an annealing path realized with D-Wave Ocean's simulated annealer. On a Max-Cut problem instance, the same typed problem runs on both backends by varying only the operator formulation (Quantum Approximated Optimization Algorithm formulation vs. Ising Hamiltonian formulation) and the context. The proposed middle layer concepts are characterized by portability, composability, and its minimal core can evolve with hardware capabilities."
2510.07116,"Neurotechnologies are transforming how we measure, interpret, and modulate brain-body interactions, integrating real-time sensing, computation, and stimulation to enable precise physiological control. They hold transformative potential across clinical and non-clinical domains, from treating disorders to enhancing cognition and performance. Realizing this potential requires navigating complex, interdisciplinary challenges spanning neuroscience, materials science, device engineering, signal processing, computational modelling, and regulatory and ethical frameworks. This Perspective presents a strategic roadmap for neurotechnology development, created by early-career researchers, highlighting their role at the intersection of disciplines and their capacity to bridge traditional silos. We identify five cross-cutting trade-offs that constrain progress across functionality, scalability, adaptability, and translatability, and illustrate how technical domains influence their resolution. Rather than a domain-specific review, we focus on shared challenges and strategic opportunities that transcend disciplines. We propose a unified framework for collaborative innovation and education, highlight ethical and regulatory priorities, and outline a timeline for overcoming key bottlenecks. By aligning technical development with translational and societal needs, this roadmap aims to accelerate equitable, effective, and future-ready adaptive neurotechnologies, guiding coordinated efforts across the global research and innovation community."
2510.07427,"Research into optical spiking neural networks (SNNs) has primarily focused on spiking devices, networks of excitable lasers or numerical modelling of large architectures, often overlooking key constraints such as limited optical power, crosstalk and footprint. We introduce SEPhIA, a photonic-electronic, multi-tiled SNN architecture emphasizing implementation feasibility and realistic scaling. SEPhIA leverages microring resonator modulators (MRMs) and multi-wavelength sources to achieve effective sub-one-laser-per-spiking neuron efficiency. We validate SEPhIA at both device and architecture levels by time-domain co-simulating excitable CMOS-MRR coupled circuits and by devising a physics-aware, trainable optoelectronic SNN model, with both approaches utilizing experimentally derived device parameters. The multi-layer optoelectronic SNN achieves classification accuracies over 90% on a four-class spike-encoded dataset, closely comparable to software models. A design space study further quantifies how photonic device parameters impact SNN performance under constrained signal-to-noise conditions. SEPhIA offers a scalable, expressive, physically grounded solution for neuromorphic photonic computing, capable of addressing spike-encoded tasks."
2510.08257,"In-memory computing technology is used extensively in artificial intelligence devices due to lower power consumption and fast calculation of matrix-based functions. The development of such a device and its integration in a system takes a significant amount of time and requires the use of a real-time emulation environment, where various system aspects are analyzed, microcode is tested, and applications are deployed, even before the real chip is available. In this work, we present the architecture, the software development tools, and experimental results of a distributed and expandable emulation system for rapid prototyping of integrated circuits based on in-memory computing technologies. Presented experimental results demonstrate the usefulness of the proposed emulator."
2510.08731,"Large Language Models (LLMs) demonstrate substantial accuracy gains when augmented with reasoning modes such as chain-of-thought and inference-time scaling. However, reasoning also incurs significant costs in inference latency and token usage, with environmental and financial impacts, which are unnecessary for many simple prompts. We present a semantic router that classifies queries based on their reasoning requirements and selectively applies reasoning only when beneficial. Our approach achieves a 10.2 percentage point improvement in accuracy on the MMLU-Pro benchmark while reducing response latency by 47.1% and token consumption by 48.5% compared to direct inference with vLLM. These results demonstrate that semantic routing offers an effective mechanism for striking a balance between accuracy and efficiency in open-source LLM serving systems"
2510.08891,"Interprofessional education has long relied on case studies and the use of standardized patients to support teamwork, communication, and related collaborative competencies among healthcare professionals. However, traditional approaches are often limited by cost, scalability, and inability to mimic the dynamic complexity of real-world clinical scenarios. To address these challenges, we designed and developed AIMS (AI-Enhanced Immersive Multidisciplinary Simulations), a virtual simulation that integrates a large language model (Gemini-2.5-Flash), a Unity-based virtual environment engine, and a character creation pipeline to support synchronized, multimodal interactions between the user and the virtual patient. AIMS was designed to enhance collaborative clinical reasoning and health promotion competencies among students from pharmacy, medicine, nursing, and social work. A formal usability testing session was conducted which participants assumed professional roles on a healthcare team and engaged in a mix of scripted and unscripted conversations. Participants explored the patient's symptoms, social context, and care needs. Usability issues were identified (e.g., audio routing, response latency) and used to guide subsequent refinements. Findings in general suggest that AIMS supports realistic, profession-specific and contextually appropriate conversations. We discussed both technical and pedagogical innovations of AIMS and concluded with future directions."
2510.11727,"Neuromorphic computing hardware enables edge computing and can be implemented in flexible electronics for novel applications. Metal oxide materials are promising candidates for fabricating flexible neuromorphic electronics, but suffer from processing constraints due to the incompatibilities between oxides and polymer substrates. In this work, we use photonic curing to fabricate flexible metal-insulator-metal capacitors with solution-processible aluminum oxide dielectric tailored for neuromorphic applications. Because photonic curing outcomes depend on many input parameters, identifying an optimal processing condition through a traditional grid-search approach is unfeasible. Here, we apply multi-objective Bayesian optimization (MOBO) to determine photonic curing conditions that optimize the trade-off between desired electrical properties of large capacitance-frequency dispersion and low leakage current. Furthermore, we develop a human-in-the-loop (HITL) framework for incorporating failed experiments into the MOBO machine learning workflow, demonstrating that this framework accelerates optimization by reducing the number of experimental rounds required. Once optimization is concluded, we analyze different Pareto-optimal conditions to tune the dielectrics properties and provide insight into the importance of different inputs through Shapley Additive exPlanations analysis. The demonstrated framework of combining MOBO with HITL feedback can be adapted to a wide range of multi-objective experimental problems that have interconnected inputs and high experimental failure rates to generate usable results for machine learning models."
2510.1173,"In this study, we demonstrate the first realization of wireless strain and temperature sensing within 3D-printed metallic structures using standard electromagnetic inspection hardware. This establishes a path toward need-based parts maintenance driven by accurate damage assessments instead of relying on regularly scheduled maintenance teardowns, extending the service intervals of structures operating in harsh environments. To this end, we encapsulate magnetoelastic and thermomagnetic materials inside microtubes and embed the sensing elements during additive manufacturing. Mechanical and thermal stimuli affect the magnetic permeability of the embedded materials, which modulates the impedance of a coil placed on or near the surface of the printed part. We demonstrate strain sensing accurate to +/-27x10-6 over at least a 6x10-4 strain range, and temperature sensing accurate to +/-0.75oC over a 70oC range, both to a 95% confidence interval. We highlight these sensors' capabilities by detecting the onset of plasticity and fatigue-driven crack growth thousands of cycles before critical failure. This extends non-destructive eddy-current damage detection to accurate, real-time strain and temperature monitoring within metallic structures."
2510.12278,"We address a novel staff allocation problem that arises in the organization of collaborators among multiple school sites and educational levels. The problem emerges from a real case study in a public school in Calabria, Italy, where staff members must be distributed across kindergartens, primary, and secondary schools under constraints of availability, competencies, and fairness. To tackle this problem, we develop an optimization model and investigate a solution approach based on quantum annealing. Our computational experiments on real-world data show that quantum annealing is capable of producing balanced assignments in short runtimes. These results provide evidence of the practical applicability of quantum optimization methods in educational scheduling and, more broadly, in complex resource allocation tasks."
2510.1412,"Memristive crossbar arrays (MCA) are emerging as efficient building blocks for in-memory computing and neuromorphic hardware due to their high density and parallel analog matrix-vector multiplication capabilities. However, the physical properties of their nonvolatile memory elements introduce new attack surfaces, particularly under fault injection scenarios. This work explores Laser Fault Injection as a means of inducing analog perturbations in MCA-based architectures. We present a detailed threat model in which adversaries target memristive cells to subtly alter their physical properties or outputs using laser beams. Through HSPICE simulations of a large MCA on 45 nm CMOS tech. node, we show how laser-induced photocurrent manifests in output current distributions, enabling differential fault analysis to infer internal weights with up to 99.7% accuracy, replicate the model, and compromise computational integrity through targeted weight alterations by approximately 143%."
2510.15935,"Multiple-input multiple-output (MIMO) is critical for 6G communication, offering improved spectral efficiency and reliability. However, conventional fully digital designs face significant challenges due to high hardware complexity and power consumption. Low-bit MIMO architectures, such as those employing b-bit quantized phase shifters, provide a cost-effective alternative but introduce NP-hard combinatorial problems in the pre- and post-coding design. This paper explores the use of the Quantum Approximate Optimization Algorithm (QAOA) and alternating optimization to address the problem of b-bit quantized phase shifters both at the transmitter and the receiver. We demonstrate that the structure of this quantized beamforming problem aligns naturally with hybrid-classical methods like QAOA, as the phase shifts used in beamforming can be directly mapped to rotation gates in a quantum circuit. Notably, this paper is the first to show that theoretical connection. Then, the Hamiltonian derivation analysis for the b-bit case is presented, which could have applications in different fields, such as integrated sensing and communication, and emerging quantum algorithms such as quantum machine learning. In addition, a warm-start QAOA approach is studied which improves computational efficiency. Numerical results highlight the effectiveness of the proposed methods in achieving an improved quantized beamforming gain over their classical optimization benchmarks from the literature."
2510.1753,"This perspective analyzes the intricate interplay among neuroscience, Brain-Inspired Intelligence (BII), and Brain-Inspired Navigation (BIN), revealing a current lack of cooperative relationship between Brain-Computer Interfaces (BCIs) and BIN fields. We advocate for the integration of neuromorphic-empowered BCI into BIN, thereby bolstering the unmanned systems' reliable navigation in demanding missions, such as deep space exploration, etc. We highlight that machine intelligence, reinforced by brain-inspired artificial consciousness, can extend human intelligence, with human intelligence mediated by neuromorphic-enabled BCI acting as a safeguard in case machine intelligence failures. This study also discusses the potentials of the proposed approach to enhance unmanned systems' capabilities and facilitate the diagnostics of spatial cognition disorders, while considering associated ethical and security concerns."
2510.17688,"Data scarcity and sparsity in bio-manufacturing poses challenges for accurate modeldevelopment, process monitoring, and optimization. We aim to replicate and capturethe complex dynamics of industrial bioprocesses by proposing the use of a QuantumWasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP) togenerate synthetic time series data for industrially relevant processes. Thegenerator within our GAN is comprised of a Parameterized Quantum Circuit (PQC). Thismethodology offers potential advantages in process monitoring, modeling,forecasting, and optimization, enabling more efficient bioprocess management byreducing the dependence on scarce experimental data. Our results demonstrateacceptable performance in capturing the temporal dynamics of real bioprocess data.We focus on Optical Density, a key measurement for Dry Biomass estimation. The datagenerated showed high fidelity to the actual historical experimental data. Thisintersection of quantum computing and machine learning has opened new frontiers indata analysis and generation, particularly in computationally intensive fields, foruse cases such as increasing prediction accuracy for soft sensor design or for usein predictive control."
2511.00316,"Low-power multicore platforms are suitable for running data-intensive tasks in parallel, but they are highly inefficient for computing on intermittent power. In this work, we present PEARL (PowEr And eneRgy-aware MuLticore Intermittent Computing), a novel systems support that can make existing multicore microcontroller (MCU) platforms suitable for efficient intermittent computing. PEARL achieves this by leveraging only a three-threshold voltage tracking circuit and an external fast non-volatile memory, which multicore MCUs can smoothly interface. PEARL software runtime manages these components and performs energy- and power-aware adaptation of the multicore configuration to introduce minimal backup overheads and boost performance. Our evaluation shows that PEARL outperforms the state-of-the-art solutions by up to 30x and consumes up to 32x less energy."
2511.01459,"This paper presents an optimized Joint Radar-Communication (JRC) system utilizing multiple Unmanned Aerial Vehicles (UAVs) to simultaneously achieve sensing and communication objectives. By leveraging UAVs equipped with dual radar and communication capabilities, the proposed framework aims to maximize radar sensing performance across all UAVs in challenging environments. The proposed approach focuses on formulating and solving a UAV positioning and power allocation problem to optimize multi-UAV sensing and communications performance over multiple targets within designated zones. Due to the NP-hard and combinatorial nature of the problem, we propose a Distributed JRC-based (DJRC) solution. This solution employs an efficient reward for potential actions and consistently selects the best action that maximizes the reward while ensuring both communications and sensing performance. Simulation results demonstrate significant performance improvements of the proposed solution over state-of-the-art radar- or communication-centric trajectory planning methods, with polynomial complexity dependent on the number of UAVs and linear dependence on the iteration count."
2511.02949,"The 6G wireless networks impose extremely high requirements on physical layer secure communication. However, the existing solutions usually can only achieve one-dimensional physical layer security (PLS) in the angle dimension, and cannot achieve PLS in the range dimension. In this paper, we propose the NF-SecRIS system, the first range-angle-dependent (2D) PLS near-field communication system based on ultra-large-scale reconfigurable intelligent surface (RIS). We propose the secure location modulation scheme to synthesize the near-field spatial-temporal coding pattern of RIS with extremely low complexity. It ensures that only legitimate user can receive the raw constellations, while potential eavesdroppers at other ranges or angles can only receive the obfuscated constellations. NF-SecRIS operates without requiring synchronization with either transmitter or receiver. We implement a prototype of NF-SecRIS and conduct comprehensive experiments with multiple modulation schemes. The results show that the bit error rate (BER) of legitimate user is below 10^{-4}, while eavesdroppers at other ranges or angles suffer from BER exceeding 40%. It validates the implementation of 2D PLS in near-field communications."
2511.03119,"Noisy quantum devices demand error-mitigation techniques to be accurate yet simple and efficient in terms of number of shots and processing time. Many established approaches (e.g., extrapolation and quasi-probability cancellation) impose substantial execution or calibration overheads, while existing learning-based methods have difficulty scaling to large and deep circuits. In this research, we introduce QAGT-MLP: an attention-based graph transformer tailored for small- and large-scale quantum error mitigation (QEM). QAGT-MLP encodes each quantum circuit as a graph whose nodes represent gate instances and whose edges capture qubit connectivity and causal adjacency. A dual-path attention module extracts features around measured qubits at two scales or contexts: 1) graph-wide global structural context; and 2) fine-grained local lightcone context. These learned representations are concatenated with circuit-level descriptor features and the circuit noisy expected values, then they are passed to a lightweight MLP to predict the noise-mitigated values. On large-scale 100-qubit Trotterized 1D Transverse-Field Ising Models -- TFIM circuits -- the proposed QAGT-MLP outperformed state-of-the-art learning baselines in terms of mean error and error variability, demonstrating strong validity and applicability in real-world QEM scenarios under matched shot budgets. By using attention to fuse global structures with local lightcone neighborhoods, QAGT-MLP achieves high mitigation quality without the increasing noise scaling or resource demand required by classical QEM pipelines, while still offering a scalable and practical path to QEM in modern and future quantum workloads."
2511.03706,"Air quality monitoring is central to environmental sustainability and public health, yet traditional systems remain difficult for non-expert users to interpret due to complex visualizations, limited interactivity, and high deployment costs. Recent advances in Large Language Models (LLMs) offer new opportunities to make sensor data more accessible, but their tendency to produce hallucinations limits reliability in safety-critical domains. To address these challenges, we present an LLM-enhanced Air Monitoring Interface (AMI) that integrates real-time sensor data with a conversational interface via the Model Context Protocol (MCP). Our system grounds LLM outputs in live environmental data, enabling accurate, context-aware responses while reducing hallucination risk. The architecture combines a Django-based backend, a responsive user dashboard, and a secure MCP server that exposes system functions as discoverable tools, allowing the LLM to act as an active operator rather than a passive responder. Expert evaluation demonstrated high factual accuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a scale of 5, supported by inter-rater reliability analysis. These results highlight the potential of combining LLMs with standardized tool protocols to create reliable, secure, and user-friendly interfaces for real-time environmental monitoring."
2511.03747,"Memristive crossbars enable in-memory multiply-accumulate and local plasticity learning, offering a path to energy-efficient edge AI. To this end, we present Open-MENA (Open Memristor-in-Memory Accelerator), which, to our knowledge, is the first fully open memristor interfacing system integrating (i) a reproducible hardware interface for memristor crossbars with mixed-signal read-program-verify loops; (ii) a firmware-software stack with high-level APIs for inference and on-device learning; and (iii) a Voltage-Incremental Proportional-Integral (VIPI) method to program pre-trained weights into analog conductances, followed by chip-in-the-loop fine-tuning to mitigate device non-idealities. OpenMENA is validated on digit recognition, demonstrating the flow from weight transfer to on-device adaptation, and on a real-world robot obstacle-avoidance task, where the memristor-based model learns to map localization inputs to motor commands. OpenMENA is released as open source to democratize memristor-enabled edge-AI research."
2511.04136,"The recent rapid deployment of datacenter infrastructures for performing large language models (LLMs) and related artificial intelligence (AI) applications in the clouds is predicted to incur an exponentially growing energy consumption in the near-term future. In this paper, we propose and analyze the implementation of the transformer model, which is the cornerstone of the modern LLMs, with novel large-scale optoelectronic neurons (OENs) constructed over the commercially available complementary metal-oxide-semiconductor (CMOS) image sensor (CIS) platform. With all of the required optoelectronic devices and electronic circuits integrated in a chiplet only about 2 cm by 3 cm in size, 175 billon parameters in the case of GPT-3 are shown to perform inference at an unprecedented speed of 12.6 POPS using only a 40 nm CMOS process node, along with a high power efficiency of 74 TOPS/W and a high area efficiency of 19 TOPS/mm2, both surpassing the related digital electronics by roughly two orders of magnitude. The influence of the quantization formats and the hardware induced errors are numerically investigated, and are shown to have a minimal impact. Our study presents a new yet practical path toward analog neural processing units (NPUs) to complement existing digital processing units."
2511.04928,"Phase Change Memory (PCM) has rapidly progressed and surpassed Dynamic Random-Access Memory (DRAM) in terms of scalability and standby energy efficiency. Altering a PCM cell's state during writes demands substantial energy, posing a significant challenge to PCM's role as the primary main memory. Prior research has explored methods to reduce write energy consumption, including the elimination of redundant writes, minimizing cell writes, and employing compact row buffers for filtering PCM main memory accesses. However, these techniques had certain drawbacks like bit-wise comparison of the stored values, preemptive updates increasing write cycles, and poor endurance. In this paper, we propose WIRE, a new coding mechanism through which most write operations force a maximum of one-bit flip. In this coding-based data storage method, we look at the frequent value stack and assign a code word to the most frequent values such that they have a hamming distance of one. In most of the write accesses, writing a value needs one or fewer bit flips which can save considerable write energy. This technique can be augmented with a wear-leveling mechanism at the block level, and rotating the difference bit in the assigned codes, increasing the lifetime of the PCM array at a low cost. Using a full-system evaluation of our method and comparing it to the existing mechanisms, our experimental results for multi-threaded and multi-programmed workloads revealed considerable improvement in lifetime and write energy as well as bit flip reduction."
2511.07245,"This paper presents a Markov-based system model for microfluidic molecular communication (MC) channels. By discretizing the advection-diffusion dynamics, the proposed model establishes a physically consistent state-space formulation. The transition matrix explicitly captures diffusion, advective flow, reversible binding, and flow-out effects. The resulting discrete-time formulation enables analytical characterization of both transient and equilibrium responses through a linear system representation. Numerical results verify that the proposed framework accurately reproduces channel behaviors across a wide range of flow conditions, providing a tractable basis for the design and analysis of MC systems in microfluidic environments."
