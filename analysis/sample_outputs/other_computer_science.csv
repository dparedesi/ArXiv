paper_id,abstract
2501.00002,"In this paper we present a QUBO formulation for the Takuzu game (or Binairo), for the most recent LinkedIn game, Tango, and for its generalizations. We optimize the number of variables needed to solve the combinatorial problem, making it suitable to be solved by quantum devices with fewer resources."
2501.14754,"The dynamic environment context necessitates harnessing digital technologies, including artificial intelligence and the Internet of Things, to supply high-resolution, real-time meteorological data to support agricultural decision-making and improve overall farm productivity and sustainability. This study investigates the potential application of various AI-powered, IoT-based, low-cost platforms for local weather forecasting to enable smart farming. Despite the increasing demand for this topic, a few promising studies have explored this area. This paper developed a conceptual research framework based on a systematic review of relevant literature and employed a case study method to validate the framework. The framework comprised five key components: the Data Acquisition Layer, Data Storage Layer, Data Processing Layer, Application Layer, and Decision-Making Layer. This paper contributes to the literature by exploring the integration of AI-ML and IoT techniques for weather prediction tasks to support agriculture, and the incorporation of IoT technologies that provide real-time, high-resolution meteorological data, representing a step forward. Furthermore, this paper discusses key research gaps, such as the significant obstacles impeding the adoption of AI in agriculture and local weather forecasting, including the lack of straightforward solutions and the lack of digital skills among farmers, particularly those in rural areas. Further empirical research is needed to enhance the existing frameworks and address these challenges."
2502.02606,"As computing power advances, the environmental cost of semiconductor manufacturing and operation has become a critical concern. However, current sustainability metrics fail to quantify carbon emissions at the transistor level, the fundamental building block of modern processors. This paper introduces a Carbon Per Transistor (CPT) formula -- a novel approach and green implementation metric to measuring the CO$_2$ footprint of semiconductor chips from fabrication to end-of-life. By integrating emissions from silicon crystal growth, wafer production, chip manufacturing, and operational power dissipation, the CPT formula provides a scientifically rigorous benchmark for evaluating the sustainability of computing hardware. Using real-world data from Intel Core i9-13900K, AMD Ryzen 9 7950X, and Apple M1/M2/M3 processors, we reveal a startling insight-manufacturing emissions dominate, contributing 60-125 kg CO$_2$ per CPU, far exceeding operational emissions over a typical device lifespan. Notably, Apple's high-transistor-count M-series chips, despite their energy efficiency, exhibit a significantly larger carbon footprint than traditional processors due to extensive fabrication impact. This research establishes a critical reference point for green computing initiatives, enabling industry leaders and researchers to make data-driven decisions in reducing semiconductor-related emissions and get correct estimates for the green factor of the information technology process. The proposed formula paves the way for carbon-aware chip design, regulatory standards, and future innovations in sustainable computing."
2502.02626,"Open-source projects require outreach material to grow their community, secure funds, and strengthen their influence. Numbers, specifications, and facts alone are intangible to uninvolved people; using a clear brand and appealing visual material is thus ample to reach a broad audience. This is especially true for application-specific integrated circuits (ASICs) during the early stages of the development cycle without running prototype systems. This work presents ArtistIC, an open-source framework to brand ASICs with top-metal art and to render GDSII layouts with ultra-high fidelity reaching render densities below 25 nm/px and gigapixels-scale resolutions."
2502.11199,"Although the methodology of Design Science Research (DSR) is playing an increasingly important role with the emergence of the ""sciences of the artificial"", the validity of the resulting artifacts is occasionally questioned. This paper compares three influential DSR frameworks to assess their support for artifact validity. Using five essential validity types (instrument validity, technical validity, design validity, purpose validity and generalization), the qualitative analysis reveals that while purpose validity is explicitly emphasized, instrument and design validity remain the least developed. Their implicit treatment in all frameworks poses a risk of overlooked validation, and the absence of mandatory instrument validity can lead to invalid artifacts, threatening research credibility. Beyond these findings, the paper contributes (a) a comparative overview of each framework's strengths and weaknesses and (b) a revised DSR framework incorporating all five validity types with definitions and examples. This ensures systematic artifact evaluation and improvement, reinforcing the rigor of DSR."
2502.14012,"Considering that the physical design of printed circuit board (PCB) follows the principle of modularized design, this paper proposes an automatic placement algorithm for functional modules. We first model the placement problem as a mixed-variable optimization problem, and then, developed tailored algorithms of global placement and legalization for the top-layer centralized placement subproblem and the bottom-layer pin-oriented placement subproblem. Numerical comparison demonstrates that the proposed mixed-variable optimization scheme can get optimized total wirelength of placement. Meanwhile, experimental results on several industrial PCB cases show that the developed centralized strategies can well accommodate the requirement of top-layer placement, and the pin-oriented global placement based on bin clustering contributes to optimized placement results meeting the requirement of pin-oriented design."
2502.17502,"With the application of advanced science and technology in the military field, modern warfare has developed into a confrontation between systems. The combat system-of-systems (CSoS) has numerous nodes, multiple attributes and complex interactions, and its research and analysis are facing great difficulties. Electromagnetic space is an important dimension of modern warfare. Modeling and analyzing the CSoS from this perspective is of great significance to studying modern warfare and can provide a reference for the research of electromagnetic warfare. In this study, the types of nodes and relationships in the complex electromagnetic space of CSoS are first divided, the important attributes of the combat nodes are extracted, and the relationship weights are normalized to establish a networked model. On this basis, the calculation method of CSoS combat effectiveness based on the combat cycle is proposed, and then the identification and sorting of key nodes can be realized by the node deletion method. Finally, by constructing an instance of aircraft carrier fleet confrontation, the feasibility of this method has been verified, and the experimental results have been compared with classical algorithms to demonstrate the advanced nature of this method."
2503.07268,"Routing is a crucial step in the VLSI design flow. With the advancement of manufacturing technologies, more constraints have emerged in design rules, particularly regarding obstacles during routing, leading to increased routing complexity. Unfortunately, many global routers struggle to efficiently generate obstacle-free solutions due to the lack of scalable obstacle-avoiding tree generation methods and the capability of handling modern designs with complex obstacles and nets. In this work, we propose an efficient obstacle-aware global routing flow for VLSI designs with obstacles. The flow includes a rule-based obstacle-avoiding rectilinear Steiner minimal tree (OARSMT) algorithm during the tree generation phase. This algorithm is both scalable and fast to provide tree topologies avoiding obstacles in the early stage globally. With its guidance, OARSMT-guided and obstacle-aware sparse maze routing are proposed in the later stages to minimize obstacle violations further and reduce overflow costs. Compared to advanced methods on the benchmark with obstacles, our approach successfully eliminates obstacle violations, and reduces wirelength and overflow cost, while sacrificing only a limited number of via counts and runtime overhead."
2503.11653,"Virtual archaeology has significantly evolved over the last few decades through advancements in data acquisition and representation; for example, by improved data recording technologies and virtual reality devices. Immersive environments provide novel ways to present historical events or objects with high visual quality for both the general public and researchers. Here, we examine how the emerging field of immersive analytics can contribute to enhancing the understanding and exploration of archaeological data, and we explore the junction of virtual archaeology and immersive analytics. We discuss a selection of features already used by the community and examine how optimizing these can facilitate the discourse on cultural heritage objects. As a basis for discussion, we introduce and utilize three digital reconstruction interpretations of the mausoleum of the late Roman Emperor Maxentius in Rome, which are based on prior scientific work and a typological framework. Based on our work, we advocate for the value of combining historical and computer science expertise to optimize immersive environments for virtual reconstructions, thereby facilitating a deeper understanding and interactive exploration of archaeological data."
2503.21709,"This paper introduces a novel framework that combines traditional centrality measures with eigenvalue spectra and diffusion processes for a more comprehensive analysis of complex networks. While centrality measures such as degree, closeness, and betweenness have been commonly used to assess nodal importance, they provide limited insight into dynamic network behaviors. By incorporating eigenvalue analysis, which evaluates network robustness and connectivity through spectral properties, and diffusion processes that model information flow, this framework offers a deeper understanding of how networks function under dynamic conditions. Applied to synthetic networks, the approach identifies key nodes not only by centrality but also by their role in diffusion dynamics and vulnerability points, offering a multi-dimensional view that traditional methods alone cannot. This integrated analysis enables a more precise identification of critical nodes and potential weaknesses, with implications for improving network resilience in fields ranging from epidemiology to cybersecurity. Keywords: Centrality measures, eigenvalue spectra, diffusion processes, network analysis, network robustness, information flow, synthetic networks."
2504.03737,"The management of chronic heart failure presents significant challenges in modern healthcare, requiring continuous monitoring, early detection of exacerbations, and personalized treatment strategies. This paper presents the preliminary results of the PrediHealth research project conducted in this context. Specifically, it aims to address the challenges above by integrating telemedicine, mobile health solutions, and predictive analytics into a unified digital healthcare platform. We leveraged a web-based IoT platform, a telemonitoring kit with medical devices and environmental sensors, and AI-driven predictive models to support clinical decision-making. The project follows a structured methodology comprising research on emerging CPS/IoT technologies, system prototyping, predictive model development, and empirical validation."
2504.0378,"Many organisational problems are addressed through systemic change and re-engineering of existing Information Systems rather than radical new design. In the face of widespread IT project failure, devising effective ways to tackle this type of change remains an open challenge. This work discusses the motivation, theoretical foundation, characteristics and evaluation of a novel framework - referred to as POE-$\Delta$, which is rooted in design and engineering and is aimed at providing systematic support for representing, structuring and exploring change problems of a socio-technical nature, including implementing their solutions when they exist. We generalise an existing framework of greenfield design as problem solving for application to change problems. From a theoretical perspective,POE-$\Delta$ is a strict extension to its parent framework, allowing the seamless integration of greenfield and brownfield design to tackle change problems.A Design Science Research methodology was applied over a decade to define and evaluate POE-$\Delta$, with significant case study research conducted to evaluate the framework in its application to real-world change problems of varying criticality and complexity. The results show that POE-$\Delta$ exhibits desirable characteristics of a design approach to organisational change and can bring tangible benefits when applied in practice as a holistic and systematic approach to change in socio-technical contexts."
2504.03808,"With the advent of the post-Moore era, the 2.5-D advanced package is a promising solution to sustain the development of very large-scale integrated circuits. However, the thermal placement of chiplet, due to the high complexity of thermal simulation, is very challenging. In this paper, a surrogate-assisted simulated annealing algorithm is proposed to simultaneously minimize both the wirelength and the maximum temperature of integrated chips. To alleviate the computational cost of thermal simulation, a radial basis function network is introduced to approximate the thermal field, assisted by which the simulated annealing algorithm converges to the better placement in less time. Numerical results demonstrate that the surrogate-assisted simulated annealing algorithm is competitive to the state-of-the-art thermal placement algorithms of chiplet, suggesting its potential application in the agile design of 2.5D package chip."
2504.09029,"The Kullback-Leibler (KL) divergence is a foundational measure for comparing probability distributions. Yet in multivariate settings, its single value often obscures the underlying reasons for divergence, conflating mismatches in individual variable distributions (marginals) with effects arising from statistical dependencies. We derive an algebraically exact, additive, and hierarchical decomposition of the KL divergence between a joint distribution P(X1,...,Xn) and a standard product reference distribution Q(X1,...,Xn) = product_i q(Xi), where variables are assumed independent and identically distributed according to a common reference q. The total divergence precisely splits into two primary components: (1) the summed divergence of each marginal distribution Pi(Xi) from the common reference q(Xi), quantifying marginal deviations; and (2) the total correlation (or multi-information), capturing the total statistical dependency among variables. Leveraging Mobius inversion on the subset lattice, we further decompose this total correlation term into a hierarchy of signed contributions from distinct pairwise, triplet, and higher-order statistical interactions, expressed using standard Shannon information quantities. This decomposition provides an algebraically complete and interpretable breakdown of KL divergence using established information measures, requiring no approximations or model assumptions. Numerical validation using hypergeometric sampling confirms the decomposition's exactness to machine precision across diverse system configurations."
2504.10667,"Optimal statistical decisions should transcend the language used to describe them. Yet, how do we guarantee that the choice of coordinates - the parameterisation of an optimisation problem - does not subtly dictate the solution? This paper reveals a fundamental geometric invariance principle. We first analyse the optimal combination of two asymptotically normal estimators under a strictly convex trace-AMSE risk. While methods for finding optimal weights are known, we prove that the resulting optimal estimator is invariant under direct affine reparameterisations of the weighting scheme. This exemplifies a broader principle we term meta-equivariance: the unique minimiser of any strictly convex, differentiable scalar objective over a matrix space transforms covariantly under any invertible affine reparameterisation of that space. Distinct from classical statistical equivariance tied to data symmetries, meta-equivariance arises from the immutable geometry of convex optimisation itself. It guarantees that optimality, in these settings, is not an artefact of representation but an intrinsic, coordinate-free truth."
2504.13423,"Symmetric alpha-stable (S alpha S) distributions with alpha<2 lack finite classical Fisher information. Building on Johnson's framework, we define Mixed Fractional Information (MFI) via the initial rate of relative entropy dissipation during interpolation between S alpha S laws with differing scales, v and s. We demonstrate two equivalent formulations for MFI in this specific S alpha S-to-S alpha S setting. The first involves the derivative D'(v) of the relative entropy between the two S alpha S densities. The second uses an integral expectation E_gv[u(x,0) (pF_v(x) - pF_s(x))] involving the difference between Fisher scores (pF_v, pF_s) and a specific MMSE-related score function u(x,0) derived from the interpolation dynamics. Our central contribution is a rigorous proof of the consistency identity: D'(v) = (1/(alpha v)) E_gv[X (pF_v(X) - pF_s(X))]. This identity mathematically validates the equivalence of the two MFI formulations for S alpha S inputs, establishing MFI's internal coherence and directly linking entropy dissipation rates to score function differences. We further establish MFI's non-negativity (zero if and only if v=s), derive its closed-form expression for the Cauchy case (alpha=1), and numerically validate the consistency identity. MFI provides a finite, coherent, and computable information-theoretic measure for comparing S alpha S distributions where classical Fisher information fails, connecting entropy dynamics to score functions and estimation concepts. This work lays a foundation for exploring potential fractional I-MMSE relations and new functional inequalities tailored to heavy-tailed systems."
2504.20174,"Movement data is prevalent across various applications and scientific fields, often characterized by its massive scale and complexity. Exploratory Data Analysis (EDA) plays a crucial role in summarizing and describing such data, enabling researchers to generate insights and support scientific hypotheses. Despite its importance, traditional EDA practices face limitations when applied to high-dimensional, unlabeled movement data. The complexity and multi-faceted nature of this type of data require more advanced methods that go beyond the capabilities of current EDA techniques. This study addresses the gap in current EDA practices by proposing a novel approach that leverages movement variable taxonomies and outlier detection. We hypothesize that organizing movement features into a taxonomy, and applying anomaly detection to combinations of taxonomic nodes, can reveal meaningful patterns and lead to more interpretable descriptions of the data. To test this hypothesis, we introduce TUMD, a new method that integrates movement taxonomies with outlier detection to enhance data analysis and interpretation. TUMD was evaluated across four diverse datasets of moving objects using fixed parameter values. Its effectiveness was assessed through two passes: the first pass categorized the majority of movement patterns as Kinematic, Geometric, or Hybrid for all datasets, while the second pass refined these behaviors into more specific categories such as Speed, Acceleration, or Indentation. TUMD met the effectiveness criteria in three datasets, demonstrating its ability to describe and refine movement behaviors. The results confirmed our hypothesis, showing that the combination of movement taxonomies and anomaly detection successfully uncovers meaningful and interpretable patterns within high-dimensional, unlabeled movement data."
2504.20542,"This paper presents a digital twin-empowered real-time optimal delivery system specifically validated through a proof-of-concept (PoC) demonstration of a real-world autonomous car-sharing service. This study integrates real-time data from roadside units (RSUs) and connected and autonomous vehicles (CAVs) within a digital twin of a campus environment to address the dynamic challenges of urban traffic. The proposed system leverages the Age of Information (AoI) metric to optimize vehicle routing by maintaining data freshness and dynamically adapting to real-time traffic conditions. Experimental results from the PoC demonstrate a 22% improvement in delivery efficiency compared to conventional shortest-path methods that do not consider information freshness. Furthermore, digital twin-based simulation results demonstrate that this proposed system improves overall delivery efficiency by 12% and effectively reduces the peak average AoI by 23% compared to the conventional method, where each vehicle selects the shortest route without considering information freshness. This study confirms the practical feasibility of cooperative driving systems, highlighting their potential to enhance smart mobility solutions through scalable digital twin deployments in complex urban environments."
2505.00747,"Cooperative perception extends the perception capabilities of autonomous vehicles by enabling multi-agent information sharing via Vehicle-to-Everything (V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic ""information sensor"" characterized by limited communication, heterogeneity, mobility, and scalability. This survey provides a comprehensive review of recent advancements from the perspective of information-centric cooperative perception, focusing on three key dimensions: information representation, information fusion, and large-scale deployment. We categorize information representation into data-level, feature-level, and object-level schemes, and highlight emerging methods for reducing data volume and compressing messages under communication constraints. In information fusion, we explore techniques under both ideal and non-ideal conditions, including those addressing heterogeneity, localization errors, latency, and packet loss. Finally, we summarize system-level approaches to support scalability in dense traffic scenarios. Compared with existing surveys, this paper introduces a new perspective by treating V2X communication as an information sensor and emphasizing the challenges of deploying cooperative perception in real-world intelligent transportation systems."
2505.08034,"The proliferation of IoT in cities, combined with Digital Twins, creates a rich data foundation for Smart Cities aimed at improving urban life and operations. Generative AI (GenAI) significantly enhances this potential, moving beyond traditional AI analytics and predictions by processing multimodal content and generating novel outputs like text and simulations. Using specialized or foundational models, GenAI's natural language abilities such as Natural Language Understanding (NLU) and Natural Language Generation (NLG) can power tailored applications and unified interfaces, dramatically lowering barriers for users interacting with complex smart city systems. In this paper, we focus on GenAI applications based on conversational interfaces within the context of three critical user archetypes in a Smart City - Citizens, Operators and Planners. We identify and review GenAI models and techniques that have been proposed or deployed for various urban subsystems in the contexts of these user archetypes. We also consider how GenAI can be built on the existing data foundation of official city records, IoT data streams and Urban Digital Twins. We believe this work represents the first comprehensive summarization of GenAI techniques for Smart Cities from the lens of the critical users in a Smart City."
2505.17111,"Commuting Origin-Destination (OD) flows capture movements of people from residences to workplaces, representing the predominant form of intra-city mobility and serving as a critical reference for understanding urban dynamics and supporting sustainable policies. However, acquiring such data requires costly, time-consuming censuses. In this study, we introduce a commuting OD flow dataset for cities around the world, spanning 6 continents, 179 countries, and 1,625 cities, providing unprecedented coverage of dynamics under diverse urban environments. Specifically, we collected fine-grained demographic data, satellite imagery, and points of interest~(POIs) for each city as foundational inputs to characterize the functional roles of urban regions. Leveraging these, a deep generative model is employed to capture the complex relationships between urban geospatial features and human mobility, enabling the generation of commuting OD flows between urban regions. Comprehensively, validation shows that the spatial distributions of the generated flows closely align with real-world observations. We believe this dataset offers a valuable resource for advancing sustainable urban development research in urban science, data science, transportation engineering, and related fields."
2505.20247,"Governments around the world have increasingly adopted digital transformation (DT) initiatives to increase their strategic competitiveness in the global market. To support successful DT, governments have to introduce new governance logics and revise IT strategies to facilitate DT initiatives. In this study, we report a case study of how Enterprise Architecture (EA) concepts were introduced and translated into practices in Vietnamese government agencies over a span of 15 years. This translation process has enabled EA concepts to facilitate various DT initiatives such as e-government, digitalization, to name a few. Our findings suggest two mechanisms in the translation process: a theorization mechanism to generalize local practices into field-level abstract concepts, making them easier to spread, while a contextualization mechanism unpacks these concepts into practical, adaptable approaches, aligning EA with adopters' priorities and increasing its chances of dissemination. Furthermore, our findings illustrate how translation happened when the initial concepts are ambiguous and not-well-understood by adopters. In this situation, there is a need for widespread experiments and sense-making among pioneers before field- and organizational-level translation can occur."
2506.02033,"With growing consumer health awareness, ensuring food safety and quality throughout the supply chain is crucial, particularly for perishable goods. Contamination can occur during production, processing, or distribution, making real-time monitoring essential. This study proposes an affordable Smartphone-based food traceability system (FTS) that utilizes RFID technology and smartphone sensors. A smartphone-based RFID reader tracks products, while integrated sensors monitor temperature, humidity, and location during storage and transport. The system is assessed in the kimchi supply chain in Korea, providing real-time data to both managers and consumers. It offered comprehensive product tracking, including temperature and humidity records, ensuring transparency and safety. Compared to traditional methods, the proposed system demonstrated improved efficiency in handling large volumes of data while maintaining accurate traceability. The results highlight its potential for enhancing food safety and quality across supply chains."
2506.05698,"Simulation was launched in the 1950s, nicknamed a tool of ""last resort."" Over the years, this Operations Research (OR) method has made significant progress, and utilizing the accelerated advances in computer science (hardware and software, processing speed, and advanced information visualization capabilities) to improve simulation usability in research and practice. After overcoming the initial obstacles and the scare of outliving its usefulness in the 2000s, computer simulation has remained a popular OR tool applied in diverse industries and sectors, earning its popularity leading to the term ""simulation everywhere."" This study uses bibliographic data from research and practice literature to evaluate the evolutionary expansion in simulation, focusing on discrete-event simulation (DES). The results show asymmetrical but positive yearly literature out-put, broadened DES adoption in diverse fields, and sustained relevance as a scientific method for tackling old, new, and emerging issues. Also, DES is an essential tool in Industry 4.0 and plays a central role in digital transformation that has swept the industrial space, from manufacturing to healthcare and other sectors. With the emergence, ongoing adoption, and deployment of generative artificial intelligence (GenAI), future studies seek ways to integrate GenAI in DES to remain relevant and improve the modeling and simulation processes."
2506.13263,"The theory that all processes in the universe are computational is attractive in its promise to provide an understandable theory of everything. I want to suggest here that this pancomputationalism is not sufficiently clear on which problem it is trying to solve, and how. I propose two interpretations of pancomputationalism as a theory: I) the world is a computer and II) the world can be described as a computer. The first implies a thesis of supervenience of the physical over computation and is thus reduced ad absurdum. The second is underdetermined by the world, and thus equally unsuccessful as theory. Finally, I suggest that pancomputationalism as metaphor can be useful."
2506.17295,"The fast pace of technological growth has created a heightened need for intelligent, autonomous monitoring systems in a variety of fields, especially in environmental applications. This project shows the design process and implementation of a proper dual node (master-slave) IoT-based monitoring system using STM32F103C8T6 microcontrollers. The structure of the wireless monitoring system studies the environmental conditions in real-time and can measure parameters like temperature, humidity, soil moisture, raindrop detection and obstacle distance. The relay of information occurs between the primary master node (designated as the Green House) to the slave node (the Red House) employing the HC-05 Bluetooth module for information transmission. Each node displays the sensor data on OLED screens and a visual or auditory alert is triggered based on predetermined thresholds. A comparative analysis of STM32 (ARM Cortex-M3) and Arduino (AVR) is presented to justify the STM32 used in this work for greater processing power, less energy use, and better peripherals. Practical challenges in this project arise from power distribution and Bluetooth configuration limits. Future work will explore the transition of a Wi-Fi communication protocol and develop a mobile monitoring robot to enhance scalability of the system. Finally, this research shows that ARM based embedded systems can provide real-time environmental monitoring systems that are reliable and consume low power."
2506.23032,"One classic idea from the cybernetics literature is the Every Good Regulator Theorem (EGRT). The EGRT provides a means to identify good regulation, or the conditions under which an agent (regulator) can match the dynamical behavior of a system. We reevaluate and recast the EGRT in a modern context to provide insight into how intelligent autonomous learning systems might utilize a compressed global representation (world model). One-to-one mappings between a regulator (R) and the corresponding system (S) provide a reduced representation that preserves useful variety to match all possible outcomes of a system. The EGRT also extends to second-order cybernetics, where an internal model (M) observes the behavior of S and supervises a S-R closed loop mapping. Secondarily, we demonstrate how physical phenomena such as temporal criticality, non-normal denoising, and alternating procedural acquisition can recast behavior as statistical mechanics and yield regulatory relationships. These diverse physical systems challenge the notion of tightly-coupled good regulation when applied to non-uniform and out-of-distribution phenomena. Overall, we aim to recast the EGRT as a potential approach for developing world models that adapt and respond to a wide range of task environments."
2507.02876,"This article critically examines the foundational principles of contemporary AI methods, exploring the limitations that hinder its potential. We draw parallels between the modern AI landscape and the 20th-century Modern Synthesis in evolutionary biology, and highlight how advancements in evolutionary theory that augmented the Modern Synthesis, particularly those of Evolutionary Developmental Biology, offer insights that can inform a new design paradigm for AI. By synthesizing findings across AI and evolutionary theory, we propose a pathway to overcome existing limitations, enabling AI to achieve its aspirational goals. We also examine how this perspective transforms the idea of an AI-driven technological singularity from speculative futurism into a grounded prospect."
2507.03007,"Machine learning (ML) frameworks rely heavily on pseudorandom number generators (PRNGs) for tasks such as data shuffling, weight initialization, dropout, and optimization. Yet, the statistical quality and reproducibility of these generators-particularly when integrated into frameworks like PyTorch, TensorFlow, and NumPy-are underexplored. In this paper, we compare the statistical quality of PRNGs used in ML frameworks (Mersenne Twister, PCG, and Philox) against their original C implementations. Using the rigorous TestU01 BigCrush test suite, we evaluate 896 independent random streams for each generator. Our findings challenge claims of statistical robustness, revealing that even generators labeled ''crush-resistant'' (e.g., PCG, Philox) may fail certain statistical tests. Surprisingly, we can observe some differences in failure profiles between the native and framework-integrated versions of the same algorithm, highlighting some implementation differences that may exist."
2507.06271,"Many scientific disciplines have traditionally advanced by iterating over hypotheses using labor-intensive trial-and-error, which is a slow and expensive process. Recent advances in computing, digitalization, and machine learning have introduced tools that promise to make scientific research faster by assisting in this iterative process. However, these advances are scattered across disciplines and only loosely connected, with specific computational methods being primarily developed for narrow domain-specific applications. Virtual Laboratories are being proposed as a unified formulation to help researchers navigate this increasingly digital landscape using common AI technologies. While conceptually promising, VLs are not yet widely adopted in practice, and concrete implementations remainthis http URLpaper explains how the Virtual Laboratory concept can be implemented in practice by introducing the modular software library VAILabs, designed to support scientific discovery. VAILabs provides a flexible workbench and toolbox for a broad range of scientific domains. We outline the design principles and demonstrate a proof-of-concept by mapping three concrete research tasks from differing fields as virtual laboratory workflows."
2507.12468,"Digital Twins (DTs) are virtual representations of physical systems synchronized in real time through Internet of Things (IoT) sensors and computational models. In industrial applications, DTs enable predictive maintenance, fault diagnosis, and process optimization. This paper explores the mathematical foundations of DTs, hybrid modeling techniques, including Physics Informed Neural Networks (PINNs), and their implementation in industrial scenarios. We present key applications, computational tools, and future research directions."
2507.12487,"Single-board computers, with their wide range of external interfaces, provide a cost-effective solution for studying animals and plants in their natural habitat. With the introduction of the Raspberry Pi Zero 2 W, which provides hardware-based image and video encoders, it is now possible to extend this application area to include video surveillance capabilities. This paper demonstrates a solution that offloads video stream generation from the Central Processing Unit (CPU) to hardware-based encoders. The flow of data through an encoding application is described, followed by a method of accelerating image processing by reducing the number of memory copies. The paper concludes with an example use case demonstrating the application of this new feature in an underwater camera."
2507.22098,"System design is often taught through domain-specific solutions specific to particular domains, such as databases, operating systems, or computer architecture, each with its own methods and vocabulary. While this diversity is a strength, it can obscure cross-cutting principles that recur across domains. This paper proposes a preliminary ""periodic table"" of system design principles distilled from several domains in computer systems. The goal is a shared, concise vocabulary that helps students, researchers, and practitioners reason about structure and trade-offs, compare designs across domains, and communicate choices more clearly. For supporting materials and updates, please refer to the repository at:this https URL."
2508.01599,"Proactive safety systems that anticipate and mitigate traffic risks before incidents occur are increasingly recognized as essential for improving work zone safety. Unlike traditional reactive methods, these systems rely on real-time sensing, trajectory prediction, and intelligent infrastructure to detect potential hazards. Existing simulation-based studies often overlook, and real-world deployment studies rarely discuss the practical challenges associated with deploying such systems in operational settings, particularly those involving roadside infrastructure and multi-sensor integration and fusion. This study addresses that gap by presenting deployment insights and technical lessons learned from a real-world implementation of a multi-sensor safety system at an active bridge repair work zone along the N-2/US-75 corridor in Lincoln, Nebraska. The deployed system combines LiDAR, radar, and camera sensors with an edge computing platform to support multi-modal object tracking, trajectory fusion, and real-time analytics. Specifically, this study presents key lessons learned across three critical stages of deployment: (1) sensor selection and placement, (2) sensor calibration, system integration, and validation, and (3) sensor fusion. Additionally, we propose a predictive digital twin framework that leverages fused trajectory data for early conflict detection and real-time warning generation, enabling proactive safety interventions."
2508.06544,"Proactive safety systems aim to mitigate risks by anticipating potential conflicts between vehicles and enabling early intervention to prevent work zone-related crashes. This study presents an infrastructure-enabled proactive work zone safety warning system that leverages a Digital Twin environment, integrating real-time multi-sensor data, detailed High-Definition (HD) maps, and a historical prediction attention mechanism-based trajectory prediction model. Using a co-simulation environment that combines Simulation of Urban MObility (SUMO) and CAR Learning to Act (CARLA) simulators, along with Lanelet2 HD maps and the Historical Prediction Network (HPNet) model, we demonstrate effective trajectory prediction and early warning generation for vehicle interactions in freeway work zones. To evaluate the accuracy of predicted trajectories, we use two standard metrics: Joint Average Displacement Error (ADE) and Joint Final Displacement Error (FDE). Specifically, the infrastructure-enabled HPNet model demonstrates superior performance on the work-zone datasets generated from the co-simulation environment, achieving a minimum Joint FDE of 0.3228 meters and a minimum Joint ADE of 0.1327 meters, lower than the benchmarks on the Argoverse (minJointFDE: 1.0986 m, minJointADE: 0.7612 m) and Interaction (minJointFDE: 0.8231 m, minJointADE: 0.2548 m) datasets. In addition, our proactive safety warning generation application, utilizing vehicle bounding boxes and probabilistic conflict modeling, demonstrates its capability to issue alerts for potential vehicle conflicts."
2508.14124,"Dairy farming has great economic value in Brazil, however, during production, diseases such as mastitis can occur in animals, which can reduce productivity and, consequently, economic profitability. When mastitis is present in animals, it can cause physical and chemical changes in the milk, affecting its quality, market value and also compromising the health of the animal. MastiteApp is a tool to help producers prevent mastitis in their herds by checking the temperature taken from the four teats of the animal. To perform theanalysis, the temperature of all the animals' teats must be measured and, if there is a change in temperature, the system will display a message informing the producer of the possible presence of subclinical mastitis in their animal. The application has proven to be efficient in alerting producers to the possible presence of subclinical mastitis in the first few days of manifestation, thus initiating treatment and preventing the disease from worsening."
2509.00129,"A truly effective diagnostic system provides system engineers with valuable insights into the behavior of their machines, leveraging a rich body of (often tacit) expertise. Much of this expertise typically resides in written documentation or troubleshooting manuals, which are frequently imprecise or vaguely specified. Therefore, methods for formalizing this knowledge, such as through the use of knowledge graphs, are of particular interest. However, ensuring that the extracted knowledge (ideally in a semi-automatic way) encapsulates sufficient semantic depth for system-level diagnostics is a challenging task. In this paper, we propose a minimal format for knowledge graphs that is semantically rich enough to facilitate the synthesis of meaningful fault trees. Fault trees offer an intuitive and efficient means for systematic failure analysis, enabling engineers to assess all potential failure modes in a structured, hierarchical manner. The methodology is applied to the Lycoming O-320 engine, showing that meaningful fault trees can be synthesized from only structural and functional knowledge of the system, defined by the proposed conceptual model."
2509.07043,"We present an analytical model to evaluate the reductions in emissions resulting from geographic load shifting. This model is optimistic as it ignores issues of grid capacity, demand and curtailment. In other words, real-world reductions will be smaller than the estimates. However, even with these assumptions, the presented scenarios show that the realistic reductions from carbon-aware geographic load shifting are small, of the order of 5\%. This is not enough to compensate the growth in emissions from global data centre expansion."
2509.1163,"With the extension of China high-speed railway network, the number of railway stations is increasing. The challenge that railway companies are facing is how to reasonably plan the location of hot standby Electric Multiple Units (EMU) and determine the rescue coverage area of each hot standby EMU. It is uneconomical to configure a hot standby EMU for each station. Railway companies want to use the minimum number of hot standby EMUs to provide rescue tasks for the entire high-speed railway network. In this paper, we develop the optimization models for hot standby EMU location and coverage area, respectively. The models aim to maximize the rescue distance and minimize the number of hot standby EMUs. Wha'ts more, in order to reduce the complexity of the models, a method of merging railway lines is used to transform the ""point-to-network"" to ""point-to-point"" rescue problem. Two numerical examples are carried to demonstrate the effectiveness of the models. The developed models are linear, and we use Python 3.7 to call Gurobi 9.5.2 to solve the models. In the results, we can find that the developed models can provide an optimal and reasonable plan for the location and coverage area of hot standby EMUs."
2509.13393,"The increasing adoption of electric vehicles has spurred significant interest in Vehicle-to-Grid technology as a transformative approach to modern energy systems. This paper presents a systematic review of V2G systems, focusing on their integration challenges and potential solutions. First, the current state of V2G development is examined, highlighting its growing importance in mitigating peak demand, enhancing voltage and frequency regulation, and reinforcing grid resilience. The study underscores the pivotal role of artificial intelligence and machine learning in optimizing energy management, load forecasting, and real-time grid control. A critical analysis of cybersecurity risks reveals heightened vulnerabilities stemming from V2G's dependence on interconnected networks and real-time data exchange, prompting an exploration of advanced mitigation strategies, including federated learning, blockchain, and quantum-resistant cryptography. Furthermore, the paper reviews economic and market aspects, including business models (V2G as an aggregator or due to self-consumption), regulation (as flexibility service provider) and factors influencing user acceptance shaping V2G adoption. Data from global case studies and pilot programs offer a snapshot of how V2G has been implemented at different paces across regions. Finally, the study suggests a multi-layered framework that incorporates grid stability resilience, cybersecurity resiliency, and energy market dynamics and provides strategic recommendations to enable scalable, secure, and economically viable V2G deployment."
2509.18191,"This paper introduces CARLA (spatially Constrained Anchor-based Recursive Location Assignment), a recursive algorithm for assigning secondary or any activity locations in activity-based travel models. CARLA minimizes distance deviations while integrating location potentials, ensuring more realistic activity distributions. The algorithm decomposes trip chains into smaller subsegments, using geometric constraints and configurable heuristics to efficiently search the solution space. Compared to a state-of-the-art relaxation-discretization approach, CARLA achieves significantly lower mean deviations, even under limited runtimes. It is robust to real-world data inconsistencies, such as infeasible distances, and can flexibly adapt to various priorities, such as emphasizing location attractiveness or distance accuracy. CARLA's versatility and efficiency make it a valuable tool for improving the spatial accuracy of activity-based travel models and agent-based transport simulations. Our implementation is available atthis https URL."
2509.26163,"Raising server inlet temperatures tends to increase the server power consumption due to heightened server fan activity needed to compensate for the warmer air, and to decrease cooling infrastructure power consumption due to less intense and possibly less frequent chiller activity.Relying on primary data from two data centres of a colocation provider in Switzerland, this study succeeds in confirming and quantifying the first effect: In the upper half of the ASHRAE recommended temperature range and slightly above it, i.e., between 23 - 30 째C, the server power consumption correlated positively with the temperature, with a server room-wide temperature sensitivity of 0.35 - 0.5 %/째C.Based on the available data and due to ambivalent results, the study did not succeed in answering which effect is stronger for the entire data centre: the increase in server consumption or the savings in cooling consumption. Whether raising inlet temperatures beyond the ASHRAE upper limit (i.e., 27 째C) yields additional overall benefits thus remains an open question that would be best addressed through controlled experiments.A few pieces of evidence indicate nevertheless that the upper part of the current ASHRAE recommendation (i.e., around 25 - 27 째C) might be the current sweet spot for colocation DCs.The widely used power usage effectiveness (PUE) metric is not at all useful in this context. For practical reasons but semantically wrong, the server fan power consumption is included in the consumption of servers (and thus, of the IT infrastructure) and is thus on the wrong side of the PUE fraction. An inlet temperature raise, which will lead to increased fan consumption, will thus always yield a PUE decrease, irrespective of what happens to the overall data centre power consumption."
2510.06158,"Wearable physiological monitors are ubiquitous, and photoplethysmography (PPG) is the standard low-cost sensor for measuring cardiac activity. Metrics such as inter-beat interval (IBI) and pulse-rate variability (PRV) -- core markers of stress, anxiety, and other mental-health outcomes -- are routinely extracted from PPG, yet preprocessing remains non-standardized. Prior work has focused on removing motion artifacts; however, our preliminary analysis reveals sizeable beat-detection errors even in low-motion data, implying artifact removal alone may not guarantee accurate IBI and PRV estimation. We therefore investigate how band-pass cutoff frequencies affect beat-detection accuracy and whether optimal settings depend on specific persons and tasks observed. We demonstrate that a fixed filter produces substantial errors, whereas the best cutoffs differ markedly across individuals and contexts. Further, tuning cutoffs per person and task raised beat-location accuracy by up to 7.15% and reduced IBI and PRV errors by as much as 35 ms and 145 ms, respectively, relative to the fixed filter. These findings expose a long-overlooked limitation of fixed band-pass filters and highlight the potential of adaptive, signal-specific preprocessing to improve the accuracy and validity of PPG-based mental-health measures."
2510.15881,"This work introduces ParamRF: a Python library for efficient, parametric modelling of radio frequency (RF) circuits. Built on top of the next-generation computational library JAX, as well as the object-oriented wrapper Equinox, the framework provides an easy-to-use, declarative modelling interface, without sacrificing performance. By representing circuits as JAX PyTrees and leveraging just-in-time compilation, models are compiled as pure functions into an optimized, algebraic graph. Since the resultant functions are JAX-native, this allows computation on CPUs, GPUs, or TPUs, providing integration with a wide range of solvers. Further, thanks to JAX's automatic differentiation, gradients with respect to both frequency and circuit parameters can be calculated for any circuit model outputs. This allows for more efficient optimization, as well as exciting new analysis opportunities. We showcase ParamRF's typical use-case of fitting a model to measured data via its built-in fitting engines, which include classical optimizers like L-BFGS and SLSQP, as well as modern Bayesian samplers such as PolyChord and BlackJAX. The result is a flexible framework for frequency-domain circuit modelling, fitting and analysis."
2510.15901,"This paper introduces Direct Simplified Symbolic Analysis (DSSA), a new method for simplifying analog circuits. Unlike traditional matrix- or graph-based techniques that are often slow and memory-intensive, DSSA treats the task as a modeling problem and directly extracts the most significant transfer function terms. By combining Monte Carlo simulation with a genetic algorithm, it minimizes error between simplified symbolic and exact numeric expressions. Tests on five circuits in MATLAB show strong performance, with only 0.64 dB average and 1.36 dB maximum variation in dc-gain, along with a 6.8% average pole/zero error. These results highlight DSSA as an efficient and accurate tool for symbolic circuit analysis."
2511.01878,"In the rapidly evolving landscape of global supply chains, where digital disruptions and sustainability imperatives converge, traditional operational frameworks often struggle to adapt. This paper introduces the Design-Based Supply Chain Operations Research Model, a novel extension of the Design SCOR framework, which embeds operational research techniques to enhance decision-making, resilience, and environmental stewardship. Building on the foundational processes of DSCOR such as Design, Orchestrate, Plan, Order, Source, Transform, Fulfil, and Return DSCORM incorporates predictive analytics, simulation modelling, and optimization algorithms to address contemporary challenges like supply chain volatility and ESG (environmental, social, governance) compliance. Through a comprehensive literature synthesis and methodological approach involving case-based simulations, we explore DSCORM's hierarchical structure, performance metrics, implementation strategies, and digital modernization pathways. Results from simulated scenarios indicate potential efficiency gains of 15to25 percent, reduced carbon footprints by up to 20 percent, and improved agility in dynamic markets. Discussions delve into practical implications for industries like manufacturing and logistics, highlighting barriers such as data integration hurdles and the need for skilled workforces. By humanizing supply chain management emphasizing collaborative, adaptive strategies over rigid automation DSCORM positions itself as a blueprint for sustainable growth. Conclusions underscore its role in advancing digital transformation, with recommendations for future empirical validations in real-world settings"
