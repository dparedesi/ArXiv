paper_id,abstract
2501.03381,"Complex systems are characterized by nonlinear dynamics, multi-level interactions, and emergent collective behaviors. Traditional analyses that focus solely on pairwise interactions often oversimplify these systems, neglecting the higher-order interactions critical for understanding their full collective dynamics. Recent advances in multivariate information theory provide a principled framework for quantifying these higher-order interactions, capturing key properties such as redundancy, synergy, shared randomness, and collective constraints. However, two major challenges persist: accurately estimating joint entropies and addressing the combinatorial explosion of interacting terms. To overcome these challenges, we introduce THOI (Torch-based High-Order Interactions), a novel, accessible, and efficient Python library for computing high-order interactions in continuous-valued systems. THOI leverages the well-established Gaussian copula method for joint entropy estimation, combined with state-of-the-art batch and parallel processing techniques to optimize performance across CPU, GPU, and TPU environments. Our results demonstrate that THOI significantly outperforms existing tools in terms of speed and scalability. For larger systems, where exhaustive analysis is computationally impractical, THOI integrates optimization strategies that make higher-order interaction analysis feasible. We validate THOI accuracy using synthetic datasets with parametrically controlled interactions and further illustrate its utility by analyzing fMRI data from human subjects in wakeful resting states and under deep anesthesia. Finally, we analyzed over 900 real-world and synthetic datasets, establishing a comprehensive framework for applying higher-order interaction (HOI) analysis in complex systems."
2501.03837,"We adapt the theory of normal and special polynomials from symbolic integration to the summation setting, and then built up a general framework embracing both the usual shift case and the $q$-shift case. In the context of this general framework, we develop a unified reduction algorithm, and subsequently a creative telescoping algorithm, applicable to both hypergeometric terms and their $q$-analogues. Our algorithms allow to split up the usual shift case and the $q$-shift case only when it is really necessary, and thus instantly reveal the intrinsic differences between these two cases. Computational experiments are also provided."
2501.05318,The report is devoted to the concept of creating block-recursive matrix algorithms for computing on a supercomputer with distributed memory and dynamic decentralized control.
2501.1368,"For a polynomial dynamical system, we study the problem of computing the minimal differential equation satisfied by a chosen coordinate (in other words, projecting the system on the coordinate). This problem can be viewed as a special case of the general elimination problem for systems of differential equations and appears in applications to modeling and control.We give a bound for the Newton polytope of such minimal equation and show that our bound is sharp in ""more than half of the cases"". We further use this bound to design an algorithm for computing the minimal equation following the evaluation-interpolation paradigm. We demonstrate that our implementation of the algorithm can tackle problems which are out of reach for the state-of-the-art software for differential elimination."
2502.00503,"We consider the first order autonomous differential equation (ODE) ${\bf x}'={\bf f}({\bf x})$ where ${\bf f}: {\mathbb R}^n\to{\mathbb R}^n$ is locally Lipschitz. For ${\bf x}_0\in{\mathbb R}^n$ and $h>0$, the initial value problem (IVP) for $({\bf f},{\bf x}_0,h)$ is to determine if there is a unique solution, i.e., a function ${\bf x}:[0,h]\to{\mathbb R}^n$ that satisfies the ODE with ${\bf x}(0)={\bf x}_0$. Write ${\bf x} ={\tt IVP}_{\bf f}({\bf x}_0,h)$ for this unique solution.We pose a corresponding computational problem, called the End Enclosure Problem: given $({\bf f},B_0,h,\varepsilon_0)$ where $B_0\subseteq{\mathbb R}^n$ is a box and $\varepsilon_0>0$, to compute a pair of non-empty boxes $(\underline{B}_0,B_1)$ such that $\underline{B}_0\subseteq B_0$, width of $B_1$ is $<\varepsilon_0$, and for all ${\bf x}_0\in \underline{B}_0$, ${\bf x}={\tt IVP}_{\bf f}({\bf x}_0,h)$ exists and ${\bf x}(h)\in B_1$. We provide a algorithm for this problem. Under the assumption (promise) that for all ${\bf x}_0\in B_0$, ${\tt IVP}_{\bf f}({\bf x}_0,h)$ exists, we prove the halting of our algorithm. This is the first halting algorithm for IVP problems in such a general setting.We also introduce novel techniques for subroutines such as StepA and StepB, and a scaffold datastructure to support our End Enclosure algorithm. Among the techniques are new ways refine full- and end-enclosures based on a {\bf radical transform} combined with logarithm norms. Our preliminary implementation and experiments show considerable promise, and compare well with current algorithms."
2502.03757,"Elaborating on an approach recently proposed by Mark van Hoeij, we continue to investigate why creative telescoping occasionally fails to find the minimal-order annihilating operator of a given definite sum or integral. We offer an explanation based on the consideration of residues."
2502.04514,"The flip graph algorithm is a method for discovering new matrix multiplication schemes by following random walks on a graph. We introduce a version of the flip graph algorithm for matrix multiplication schemes that admit certain symmetries. This significantly reduces the size of the search space, allowing for more efficient exploration of the flip graph. The symmetry in the resulting schemes also facilitates the process of lifting solutions from $F_2$ to $\mathbb{Z}$. Our results are new schemes for multiplying $5\times 5$ matrices using $93$ multiplications and $6\times 6$ matrices using $153$ multiplications over arbitrary ground fields."
2502.04914,"The real type of a finite family of univariate polynomials characterizes the combined sign behavior of the polynomials over the real line. We derive an explicit formula for the number of real types subject to given degree bounds. For the special case of a single polynomial we present a closed-form expression involving Fibonacci numbers. This allows us to precisely describe the asymptotic growth of the number of real types as the degree increases, in terms of the golden ratio."
2502.05015,"For systems of polynomial equations, we study the problem of computing the Newton polytope of their eliminants. As was shown by Esterov and Khovanskii, such Newton polytopes are mixed fiber polytopes of the Newton polytopes of the input equations. We use their results in combination with mixed subdivisions to design an algorithm computing these special polytopes. We demonstrate the increase in practical performance of our algorithm compared to existing methods using tropical geometry and discuss the differences that lead to this increase in performance. We also demonstrate an application of our work to differential elimination."
2502.05357,"We present a certified algorithm that takes a smooth algebraic curve in $\mathbb{R}^n$ and computes an isotopic approximation for a generic projection of the curve into $\mathbb{R}^2$. Our algorithm is designed for curves given implicitly by the zeros of $n-1$ polynomials, but it can be partially extended to parametrically defined curves. The main challenge in correctly computing the projection is to guarantee the topological correctness of crossings in the projection. Our approach combines certified path tracking and interval arithmetic in a two-step procedure: first, we construct an approximation to the curve in $\mathbb{R}^n$, and, second, we refine the approximation until the topological correctness of the projection can be guaranteed. We provide a proof-of-concept implementation illustrating the algorithm."
2502.06264,"Flip graphs were recently introduced in order to discover new matrix multiplication methods for matrix sizes. The technique applies to other tensors as well. In this paper, we explore how it performs for polynomial multiplication."
2502.10005,"Dynamical systems with quadratic or polynomial drift exhibit complex dynamics, yet compared to nonlinear systems in general form, are often easier to analyze, simulate, control, and learn. Results going back over a century have shown that the majority of nonpolynomial nonlinear systems can be recast in polynomial form, and their degree can be reduced further to quadratic. This process of polynomialization/quadratization reveals new variables (in most cases, additional variables have to be added to achieve this) in which the system dynamics adhere to that specific form, which leads us to discover new structures of a model. This chapter summarizes the state of the art for the discovery of polynomial and quadratic representations of finite-dimensional dynamical systems. We review known existence results, discuss the two prevalent algorithms for automating the discovery process, and give examples in form of a single-layer neural network and a phenomenological model of cell signaling."
2502.11606,"In this work, we extend modular techniques for computing Gröbner bases involving rational coefficients to (two-sided) ideals in free algebras. We show that the infinite nature of Gröbner bases in this setting renders the classical approach infeasible. Therefore, we propose a new method that relies on signature-based algorithms. Using the data of signatures, we can overcome the limitations of the classical approach and obtain a practical modular algorithm. Moreover, the final verification test in this setting is both more general and more efficient than the classical one. We provide a first implementation of our modular algorithm in SageMath. Initial experiments show that the new algorithm can yield significant speedups over the non-modular approach."
2502.11787,We propose a version of the classical shape lemma for zero-dimensional ideals of a commutative multivariate polynomial ring to the noncommutative setting of zero-dimensional ideals in an algebra of differential operators.
2502.16783,"Linear algebra's main concerns are sets of vectors, linear functions, subspaces, linear systems, matrices and concepts about those, such as whether the solution of linear system exists or is unique; a set of vectors is linearly independent or spans the whole space; a linear function has a right or a left inverse; a linear function is surjective or injective; and the kernel of a matrix is trivial or the its image is full.The Invertible Matrix Theorem ties all these ideas and many others together. Many modern linear algebra books use this theorem as a guiding principle to explain many connections in linear algebra. The main idea is to separately characterize whether the linear function is surjective or injective. The proof usually uses a matrix decomposition as the key step. However, the invertible matrix theorem deals with a single linear function, a single set of vectors, a single subspace, and a single matrix.In this work, we generalize part of the invertible matrix theorem to results about a pair of linear functions, a pair of sets of vectors, a pair of subspaces, and a single linear relation. The main idea is to separately characterize the linear relation's fundamental properties -- whether it is surjective, injective, deterministic and total. Our proof uses a decomposition of a linear relation as the key step.Unfortunately, reasoning with linear relations in classical notation requires applying many rules besides shuffling quantifiers and variables around, which can obscure the symmetries in the results. Therefore, this work employs graphical linear algebra, a two-dimensional diagrammatic syntax with the fundamental rules of linear relations built-in."
2502.16975,"Though Mahler equations have been introduced nearly one century ago, the study of their solutions is still a fruitful topic for research. In particular, the Galois theory of Mahler equations has been the subject of many recent papers. Nevertheless, long is the way to a complete understanding of relations between solutions of Mahler equations. One step along this way is the study of singularities. Mahler equations with a regular singularity at 0 have rather ""nice"" solutions: they can be expressed with the help of Puiseux series and solutions of equations with constant coefficients. In a previous paper, the authors described an algorithm to determine whether an equation is regular singular at 0 or not. Exploiting information from the Frobenius method and Newton polygons, we improve this algorithm by significantly reducing its complexity, by providing some simple criterion for an equation to be regular singular at 0, and by extending its scope to equations with Puiseux coefficients."
2503.01487,"We consider linear matrix inequalities (LMIs) $A = A_0 + x_1 A_1 + ... + x_n A_n \succeq 0$ with the $A_i$'s being $m \times m$ symmetric matrices, with entries in a ring $\mathcal{R}$. When $\mathcal{R} = \mathbb{R}$, the feasibility problem consists in deciding whether the $x_i$'s can be instantiated to obtain a positive semidefinite matrix. When $\mathcal{R} = \mathbb{Q}[y_1, ... , y_t]$, the problem asks for a formula on the parameters $y_1, ..., y_t$, which describes the values of the parameters for which the specialized LMI is feasible. This problem can be solved using general quantifier elimination algorithms, with a complexity that is exponential in n. In this work, we leverage the LMI structure of the problem to design an algorithm that computes a formula $\Phi$ describing a dense subset of the feasible region of parameters, under genericity assumptions. The complexity of this algorithm is exponential in n, m and t but becomes polynomial in $n$ when $m$ and $t$ are fixed. We apply the algorithm to a parametric sum-of-squares problem and to the convergence analyses of certain first-order optimization methods, which are both known to be equivalent to the feasibility of certain parametric LMIs, hence demonstrating its practical interest."
2503.03337,"We identify a common scheme in several existing algorithms addressing computational problems on linear differential equations with polynomial coefficients. These algorithms reduce to computing a linear relation between vectors obtained as iterates of a simple differential operator known as pseudo-linear map.We focus on establishing precise degree bounds on the output of this class of algorithms. It turns out that in all known instances (least common left multiple, symmetric product,. . . ), the bounds that are derived from the linear algebra step using Cramer's rule are pessimistic. The gap with the behaviour observed in practice is often of one order of magnitude, and better bounds are sometimes known and derived from ad hoc methods and independent arguments. We propose a unified approach for proving output degree bounds for all instances of the class at once. The main technical tools come from the theory of realisations of matrices of rational functions and their determinantal denominators."
2503.07342,"In this work, we introduce a novel variant of the multivariate quadratic problem, which is at the core of one of the most promising post-quantum alternatives: multivariate cryptography. In this variant, the solution of a given multivariate quadratic system must also be regular, i.e. if it is split into multiple blocks of consecutive entries with the same fixed length, then each block has only one nonzero entry. We prove the NP-completeness of this variant and show similarities and differences with other computational problems used in cryptography. Then we analyze its hardness by reviewing the most common solvers for polynomial systems over finite fields, derive asymptotic formulas for the corresponding complexities and compare the different approaches."
2503.12275,"A semi-algebraic set is a subset of $\mathbb{R}^n$ defined by a finite collection of polynomial equations and inequalities. In this paper, we investigate the problem of determining whether two points in such a set belong to the same connected component. We focus on the case where the defining equations and inequalities are invariant under the natural action of the symmetric group and where each polynomial has degree at most \( d \), with \( d < n \) (where \( n \) denotes the number of variables). Exploiting this symmetry, we develop and analyze algorithms for two key tasks. First, we present an algorithm that determines whether the orbits of two given points are connected. Second, we provide an algorithm that decides connectivity between arbitrary points in the set. Both algorithms run in polynomial time with respect to \( n \)."
2503.12995,"Using Hahn series, one can attach to any linear Mahler equation a basis of solutions at 0 reminiscent of the solutions of linear differential equations at a regular singularity. We show that such a basis of solutions can be produced by using a variant of Frobenius method."
2503.1364,"The matrix LU factorization algorithm is a fundamental algorithm in linear algebra. We propose a generalization of the LU and LEU algorithms to accommodate the case of a commutative domain and its field of quotients. This algorithm decomposes any matrix A into a product of three matrices A=LSU, where each element of the triangular matrices L and U is a minor of matrix A. The number of non-zero elements in matrix S is equal to rank(A), and each of them is the inverse of the product of a specific pair of matrix A minors. The algorithm's complexity is equivalent to that of matrix multiplication."
2503.14264,"Deciding the positivity of a sequence defined by a linear recurrence and initial conditions is, in general, a hard problem. When the coefficients of the recurrences are constants, decidability has only been proven up to order 5. The difficulty arises when the characteristic polynomial of the recurrence has several roots of maximal modulus, called dominant roots of the recurrence. We study the positivity problem for recurrences with polynomial coefficients, focusing on sequences of Poincaré type, which are perturbations of constant-coefficient recurrences. The dominant eigenvalues of a recurrence in this class are the dominant roots of the associated constant-coefficient recurrence. Previously, we have proved the decidability of positivity for recurrences having a unique, simple, dominant eigenvalue, under a genericity assumption. The associated algorithm proves positivity by constructing a positive cone contracted by the recurrence operator. We extend this cone-based approach to a larger class of recurrences, where a contracted cone may no longer exist. The main idea is to construct a sequence of cones. Each cone in this sequence is mapped by the recurrence operator to the next. This construction can be applied to prove positivity by induction. For recurrences with several simple dominant eigenvalues, we provide a condition that ensures that these successive inclusions hold. Additionally, we demonstrate the applicability of our method through examples, including recurrences with a double dominant eigenvalue."
2503.15636,"A rational function $f(x)$ is rationally summable if there exists a rational function $g(x)$ such that $f(x)=g(x+1)-g(x)$. Detecting whether a given rational function is summable is an important and basic computational subproblem that arises in algorithms to study diverse aspects of shift difference equations. The discrete residues introduced by Chen and Singer in 2012 enjoy the obstruction-theoretic property that a rational function is summable if and only if all its discrete residues vanish. However, these discrete residues are defined in terms of the data in the complete partial fraction decomposition of the given rational function, which cannot be accessed computationally in general. We explain how to efficiently compute (a rational representation of) the discrete residues of any rational function, relying only on gcd computations, linear algebra, and a black box algorithm to compute the autodispersion set of the denominator polynomial. We also explain how to apply our algorithms to serial summability and creative telescoping problems, and how to apply these computations to compute Galois groups of difference equations."
2503.21731,"CylindricalAlgebraicDecomposition.m2 is the first implementation of Cylindrical Algebraic Decomposition (CAD) in Macaulay2. CAD decomposes space into 'cells' where input polynomials are sign-invariant. This package computes an Open CAD (full-dimensional cells only) for sets of real polynomials with rational coefficients, enabling users to solve existential problems involving strict inequalities. With the construction of a full CAD (cells of all dimensions), this tool could be extended to solve any real quantifier elimination problem. The current implementation employs the Lazard projection and introduces a new heuristic for choosing the variable ordering."
2504.12724,"We present a new algorithm for solving the reduction problem in the context of holonomic integrals, which in turn provides an approach to integration with parameters. Our method extends the Griffiths--Dwork reduction technique to holonomic systems and is implemented in Julia. While not yet outperforming creative telescoping in D-finite cases, it enhances computational capabilities within the holonomic framework. As an application, we derive a previously unattainable differential equation for the generating series of 8-regular graphs."
2504.13506,"Given a number field with absolute Galois group $\mathcal{G}$, a finite Galois module $M$, and a Selmer system $\mathcal{L}$, this article gives a method to compute Sel$_\mathcal{L}$, the Selmer group of $M$ attached to $\mathcal{L}$. First we describe an algorithm to obtain a resolution of $M$ where the morphisms are given by Hecke operators. Then we construct another group $H^1_S(\mathcal{G}, M)$ and we prove, using the properties of Hecke operators, that $H^1_S(\mathcal{G}, M)$ is a Selmer group containing Sel$_\mathcal{L}$. Then, we discuss the time complexity of this method."
2504.1499,"Quaternionic polynomials occur naturally in applications of quaternions in science and engineering, and normalization of quaternionic polynomials is a basic manipulation. Once a Groebner basis is certified for the defining ideal I of the quaternionic polynomial algebra, the normal form of a quaternionic polynomial can be computed by routine top reduction with respect to the Groebner basis. In the literature, a Groebner basis under the conjugate-alternating order of quaternionic variables was conjectured for I in 2013, but no readable and convincing proof was found.In this paper, we present the first readable certification of the conjectured Groebner basis. The certification is based on several novel techniques for reduction in free associative algebras, which enables to not only make reduction to S-polynomials more efficiently, but also reduce the number of S-polynomials needed for the certification."
2504.17268,"We consider dynamical models given by rational ODE systems. Parameter estimation is an important and challenging task of recovering parameter values from observed data. Recently, a method based on differential algebra and rational interpolation was proposed to express parameter estimation in terms of polynomial system solving. Typically, polynomial system solving is a bottleneck, hence the choice of the polynomial solver is crucial. In this contribution, we compare two polynomial system solvers applied to parameter estimation: homotopy continuation solver fromthis http URLand our new implementation of a certified solver based on rational univariate representation (RUR) and real root isolation. We show how the new RUR solver can tackle examples that are out of reach for the homotopy methods and vice versa."
2504.19243,"This paper introduces an algorithmic approach to the analysis of Jacobi stability of systems of second order ordinary differential equations (ODEs) via the Kosambi--Cartan--Chern (KCC) theory. We develop an efficient symbolic program using Maple for computing the second KCC invariant for systems of second order ODEs in arbitrary dimension. The program allows us to systematically analyze Jacobi stability of a system of second order ODEs by means of real solving and solution classification using symbolic computation. The effectiveness of the proposed approach is illustrated by a model of wound strings, a two-dimensional airfoil model with cubic nonlinearity in supersonic flow and a 3-DOF tractor seat-operator model. The computational results on Jacobi stability of these models are further verified by numerical simulations. Moreover, our algorithmic approach allows us to detect hand-guided computation errors in published papers."
2504.20003,"In difference algebra, summability arises as a basic problem upon which rests the effective solution of other more elaborate problems, such as creative telescoping problems and the computation of Galois groups of difference equations. In 2012 Chen and Singer introduced discrete residues as a theoretical obstruction to summability for rational functions with respect to the shift and $q$-dilation difference operators. Since then analogous notions of discrete residues have been defined in other difference settings relevant for applications, such as for Mahler and elliptic shift difference operators. Very recently there have been some advances in making these theoretical obstructions computable in practice."
2504.21058,"Let $m,n,d > 1$ be integers such that $n=md$. In this paper, we present an efficient change of level algorithm that takes as input $(B, \mathscr{M}, \Theta_\mathscr{M})$ a marked abelian variety of level $m$ over the base field $k$ of odd characteristic and returns $(B, \mathscr{M}^d, \Theta_{\mathscr{M}^d})$ a marked abelian variety of level $n$ at the expense of $O(m^g d^{2g})$ operations in $k$. A similar algorithm allows to compute $d$-isogenies: from $(B, \mathscr{M}, \Theta_\mathscr{M})$ a marked abelian variety of level $m$, $K\subset B[d]$ isotropic for the Weil pairing isomorphic to $(\mathbb{Z}/d\mathbb{Z})^g$ defined over $k$, the isogeny algorithm returns $(A, \mathscr{L}, \Theta_\mathscr{L})$ of level $m$ such that $A=B/K$ with $O(m^g d^g)$ operations in $k$. Our algorithms extend previous known results in the case that $d \wedge m=1$ and $d$ odd. In this paper, we lift theses restrictions. We use the same general approach as in the literature in conjunction with the notion of symmetric compatible that we introduce, study and link to previous results of Mumford. For practical computation, most of the time $m$ is $2$ or $4$ so that our algorithms allows in particular to compute $2^e$-isogenies which are important for the theory of theta functions but also for computational applications such as isogeny based cryptography."
2504.21708,"Let $\mathcal{R} = \mathbb{K}[x_1, \dots, x_n]$ be a multivariate polynomial ring over a field $\mathbb{K}$ of characteristic 0. Consider $n$ algebraically independent elements $g_1, \dots, g_n$ in $\mathcal{R}$. Let $\mathcal{S}$ denote the subring of $\mathcal{R}$ generated by $g_1, \dots, g_n$, and let $h$ be an element of $\mathcal{S}$. Then, there exists a unique element ${f} \in \mathbb{K}[u_1, \dots, u_n]$ such that $h = f(g_1, \dots, g_n)$.In this paper, we provide an algorithm for computing ${f}$, given $h$ and $g_1, \dots, g_n$. The complexity of our algorithm is linear in the size of the input, $h$ and $g_1, \dots, g_n$, and polynomial in $n$ when the degree of $f$ is fixed. Previous works are mostly known when $f$ is a symmetric polynomial and $g_1, \dots, g_n$ are elementary symmetric, homogeneous symmetric, or power symmetric polynomials."
2505.0062,"Ensuring software correctness remains a fundamental challenge in formal program verification. One promising approach relies on finding polynomial invariants for loops. Polynomial invariants are properties of a program loop that hold before and after each iteration. Generating polynomial invariants is a crucial task for loops, but it is an undecidable problem in the general case. Recently, an alternative approach to this problem has emerged, focusing on synthesizing loops from invariants. However, existing methods only synthesize affine loops without guard conditions from polynomial invariants. In this paper, we address a more general problem, allowing loops to have polynomial update maps with a given structure, inequations in the guard condition, and polynomial invariants of arbitrary form.In this paper, we use algebraic geometry tools to design and implement an algorithm that computes a finite set of polynomial equations whose solutions correspond to all loops satisfying the given polynomial invariants. In other words, we reduce the problem of synthesizing loops to finding solutions of polynomial systems within a specified subset of the complex numbers. The latter is handled in our software using an SMT solver."
2505.0073,"This paper presents a novel primality test based on the eigenvalue structure of circulant matrices constructed from roots of unity. We prove that an integer $n > 2$ is prime if and only if the minimal polynomial of the circulant matrix $C_n = W_n + W_n^2$ has exactly two irreducible factors over $\mathbb{Q}$. This characterization connects cyclotomic field theory with matrix algebra, providing both theoretical insights and practical applications. We demonstrate that the eigenvalue patterns of these matrices reveal fundamental distinctions between prime and composite numbers, leading to a deterministic primality test. Our approach leverages the relationship between primitive roots of unity, Galois theory, and the factorization of cyclotomic polynomials. We provide comprehensive experimental validation across various ranges of integers, discuss practical implementation considerations, and analyze the computational complexity of our method in comparison with established primality tests. The visual interpretation of our mathematical framework provides intuitive understanding of the algebraic structures that distinguish prime numbers. Our experimental validation demonstrates that our approach offers a deterministic alternative to existing methods, with performance characteristics reflecting its algebraic foundations."
2505.00878,"Machine-assisted methods for discovering new physical laws of nature, starting from a given background theory and data, have recently emerged, and seem to hold the promise of someday advancing our understanding of the physical world. To address these needs, we have developed SynPAT, a system for generating synthetic physical theories comprising (i) a set of consistent axioms, (ii) a symbolic expression that is a consequence of the axioms and the challenge to be discovered, and (iii) noisy data that approximately match the consequence. We also generate theories that do not correctly predict the consequence. We give a detailed description of the inner workings of SynPAT and its various capabilities. We also report on our benchmarking of several open-source symbolic regression systems using our generated theories and data."
2505.01103,We present a version of the REDUCE computer algebra system as it was in the early 1970s. We show how this historical version of REDUCE may be built and run in very modest present-day environments and outline some of its capabilities.
2505.02261,"This paper introduces a structured decoding framework for the Voynich Manuscript, based on mathematical rhythm, symbolic transformation, and glyph-level recursion. Rather than interpret symbols phonetically, this method decodes them by structural roles and spatial pacing. Using scroll-wide sequencing, the system tracks prime number grouping, Fibonacci clustering, and golden ratio alignment. These symbolic structures are validated using a ten-part chi-squared test suite and Boolean logic. The method is falsifiable and reproducible. Scroll sections like f57v, f88v, and f91r are used to demonstrate glyph flow, breath-segment patterns, and tri-dot alignment. This decoding strategy challenges assumptions about pre-phonetic manuscripts and proposes a new lens for interpreting symbolic logic."
2505.02509,"The reason why Cooley-Tukey Fast Fourier Transform (FFT) over $\mathbb{Q}$ can be efficiently implemented using complex roots of unity is that the cyclotomic extensions of the completion $\mathbb{R}$ of $\mathbb{Q}$ are at most quadratic, and that roots of unity in $\mathbb{C}$ can be evaluated quickly. In this paper, we investigate a $p$-adic analogue of this efficient FFT. A naive application of this idea--such as invoking well-known algorithms like the Cantor-Zassenhaus algorithm or Hensel's lemma for polynomials to compute roots of unity--would incur a cost quadratic in the degree of the input polynomial. This would eliminate the computational advantage of using FFT in the first place. We present a method for computing roots of unity with lower complexity than the FFT computation itself. This suggests the possibility of designing new FFT algorithms for rational numbers. As a simple application, we construct an $O(N^{1+o(1)})$-time FFT algorithm over $\mathbb{Q}_p$ for fixed $p$."
2505.0313,"Symbolic regression (SR) -- which learns symbolic equations to describe the underlying relation from input-output pairs -- is widely used for scientific discovery. However, a rich set of scientific data from the real world (e.g., particle trajectories and astrophysics) are typically unsupervised, devoid of explicit input-output pairs. In this paper, we focus on symbolic implicit equation discovery, which aims to discover the mathematical relation from unsupervised data that follows an implicit equation $f(\mathbf{x}) =0$. However, due to the dense distribution of degenerate solutions (e.g., $f(\mathbf{x})=x_i-x_i$) in the discrete search space, most existing SR approaches customized for this task fail to achieve satisfactory performance. To tackle this problem, we introduce a novel pre-training framework -- namely, Pre-trained neural symbolic model for Implicit Equation (PIE) -- to discover implicit equations from unsupervised data. The core idea is that, we formulate the implicit equation discovery on unsupervised scientific data as a translation task and utilize the prior learned from the pre-training dataset to infer non-degenerate skeletons of the underlying relation end-to-end. Extensive experiments shows that, leveraging the prior from a pre-trained language model, PIE effectively tackles the problem of degenerate solutions and significantly outperforms all the existing SR approaches. PIE shows an encouraging step towards general scientific discovery on unsupervised data."
2505.05345,These notes on creative telescoping are based on a series of lectures at the Institut Henri Poincare in November and December 2023.
2505.05896,"Moosbauer and Poole have recently shown that the multiplication of two $5\times 5$ matrices requires no more than 93 multiplications in the (possibly non-commutative) coefficient ring, and that the multiplication of two $6\times 6$ matrices requires no more than 153 multiplications. Taking these multiplication schemes as starting points, we found improved matrix multiplication schemes for various rectangular matrix formats using a flip graph search."
2505.07304,"We provide bounds on the size of polynomial differential equations obtained by executing closure properties for D-algebraic functions. While it is easy to obtain bounds on the order of these equations, it requires some more work to derive bounds on their degree. Here we give bounds that apply under some technical condition about the defining differential equations."
2505.08149,"Inequalities among symmetric functions are fundamental in various branches of mathematics, thus motivating a systematic study of their structure. Majorization has been shown to characterize inequalities among commonly used symmetric functions, except for complete homogeneous symmetric functions (shortened as CHs). In 2011, Cuttler, Greene, and Skandera posed a natural question: Can majorization also characterize inequalities among CHs? Their work demonstrated that majorization characterizes inequalities among CHs up to degree 7 and suggested exploring its validity for higher degrees. In this paper, we show that, for every degree greater than 7, majorization does not characterize inequalities among CHs."
2505.09191,"This paper demonstrates how certified computational tools can be used to address various problems in control theory. In particular, we introducethis http URL, a Julia package that implements symbolic elimination techniques, including (among others) discriminant varieties and Rational Univariate Representation, while also supporting multi-precision interval computations. We showcase its applications to key control theory problems, including identification, stability analysis, and optimization, for both parameter-dependent and parameter-free systems."
2505.10246,"We present an efficient algorithm for computing the leading monomials of a minimal Groebner basis of a generic sequence of homogeneous polynomials. Our approach bypasses costly polynomial reductions by exploiting structural properties conjectured to hold for generic sequences-specifically, that their leading monomial ideals are weakly reverse lexicographic and that their Hilbert series follow a known closed-form expression. The algorithm incrementally constructs the set of leading monomials degree by degree by comparing Hilbert functions of monomial ideals with the expected Hilbert series of the input ideal. To enhance computational efficiency, we introduce several optimization techniques that progressively narrow the search space and reduce the number of divisibility checks required at each step. We also refine the loop termination condition using degree bounds, thereby avoiding unnecessary recomputation of Hilbert series. Experimental results confirm that the proposed method substantially reduces both computation time and memory usage compared to conventional Groebner basis computations for computing the leading monomials of a minimal Groebner basis of generic sequences."
2505.21879,"In science, we are interested not only in forecasting but also in understanding how predictions are made, specifically what the interpretable underlying model looks like. Data-driven machine learning technology can significantly streamline the complex and time-consuming traditional manual process of discovering scientific laws, helping us gain insights into fundamental issues in modern science. In this work, we introduce a pre-trained symbolic foundation regressor that can effectively compress complex data with numerous interacting variables while producing interpretable physical representations. Our model has been rigorously tested on non-network symbolic regression, symbolic regression on complex networks, and the inference of network dynamics across various domains, including physics, biochemistry, ecology, and epidemiology. The results indicate a remarkable improvement in equation inference efficiency, being three times more effective than baseline approaches while maintaining accurate predictions. Furthermore, we apply our model to uncover more intuitive laws of interaction transmission from global epidemic outbreak data, achieving optimal data fitting. This model extends the application boundary of pre-trained symbolic regression models to complex networks, and we believe it provides a foundational solution for revealing the hidden mechanisms behind changes in complex phenomena, enhancing interpretability, and inspiring further scientific discoveries."
2506.01864,"We have been involved in the creation of multiple software systems for computer algebra, including Reduce, Maple, Axiom and Aldor as well as a number of smaller specialised programs. We relate observations on how the meaning of software portability has changed over time and how it continues to evolve. We describe how the systems with which we have first-hand experience have achieved portability, how the central issues have changed over time and the challenges that remain."
2506.04436,"We introduce beyond-worst-case analysis into symbolic computation. This is an extensive field which almost entirely relies on worst-case bit complexity, and we start from a basic problem in the field: isolating the real roots of univariate polynomials. This is a fundamental problem in symbolic computation and it is arguably one of the most basic problems in computational mathematics. The problem has a long history decorated with numerous ingenious algorithms and furnishes an active area of research. However, most available results in literature either focus on worst-case analysis in the bit complexity model or simply provide experimental benchmarking without any theoretical justifications of the observed results. We aim to address the discrepancy between practical performance of root isolation algorithms and prescriptions of worst-case complexity theory: We develop a smoothed analysis framework for polynomials with integer coefficients to bridge this gap. We demonstrate (quasi-)linear (expected and smoothed) complexity bounds for Descartes algorithm, that is one most well know symbolic algorithms for isolating the real roots of univariate polynomials with integer coefficients. Our results explain the surprising efficiency of Descartes solver in comparison to sophisticated algorithms that have superior worst-case complexity. We also analyse the Sturm solver, ANewDsc a symbolic-numeric algorithm that combines Descartes with Newton operator, and a symbolic algorithm for sparse polynomials."
2506.08767,"A complete reduction on a difference field is a linear operator that enables one to decompose an element of the field as the sum of a summable part and a remainder such that the given element is summable if and only if the remainder is equal to zero. In this paper, we present a complete reduction in a tower of $\Sigma^*$-extensions that turns to a new efficient framework for the parameterized telescoping problem. Special instances of such $\Sigma^*$-extensions cover iterative sums such as the harmonic numbers and generalized versions that arise, e.g., in combinatorics, computer science or particle physics. Moreover, we illustrate how these new ideas can be used to reduce the depth of the given sum and provide structural theorems that connect complete reductions to Karr's Fundamental Theorem of symbolic summation."
2506.08824,"We study an important special case of the differential elimination problem: given a polynomial parametric dynamical system $\mathbf{x}' = \mathbf{g}(\boldsymbol{\mu}, \mathbf{x})$ and a polynomial observation function $y = f(\boldsymbol{\mu}, \mathbf{x})$, find the minimal differential equation satisfied by $y$. In our previous work, for the case $y = x_1$, we established a bound on the support of such a differential equation for the non-parametric case and shown that it can be turned into an algorithm via the evaluation-interpolation approach. The main contribution of the present paper is a generalization of the aforementioned result in two directions: to allow any polynomial function $y = f(\mathbf{x})$, not just a single coordinate, and to allow $\mathbf{g}$ and $f$ depend on unknown symbolic parameters. We conduct computation experiments to evaluate the accuracy of our new bound and show that the approach allows to perform elimination for some cases out of reach for the state of the art software."
2506.09529,"This paper studies the concept and the computation of approximately vanishing ideals of a finite set of data points. By data points, we mean that the points contain some uncertainty, which is a key motivation for the approximate treatment. A careful review of the existing border basis concept for an exact treatment motivates a new adaptation of the border basis concept for an approximate treatment. In the study of approximately vanishing polynomials, the normalization of polynomials plays a vital role. So far, the most common normalization in computational commutative algebra uses the coefficient norm of a polynomial. Inspired by recent developments in machine learning, the present paper proposes and studies the use of gradient-weighted normalization. The gradient-weighted semi-norm evaluates the gradient of a polynomial at the data points. This data-driven nature of gradient-weighted normalization produces, on the one hand, better stability against perturbation and, on the other hand, very significantly, invariance of border bases with respect to scaling the data points. Neither property is achieved with coefficient normalization. In particular, we present an example of the lack of scaling invariance with respect to coefficient normalization, which can cause an approximate border basis computation to fail. This is extremely relevant because scaling of the point set is often recommended for preprocessing the data. Further, we use an existing algorithm with coefficient normalization to show that it is easily adapted to gradient-weighted normalization. The analysis of the adapted algorithm only requires tiny changes, and the time complexity remains the same. Finally, we present numerical experiments on three affine varieties to demonstrate the superior stability of our data-driven normalization over coefficient normalization. We obtain robustness to perturbations and invariance to scaling."
2506.13242,"The quest for non-commutative matrix multiplication algorithms in small dimensions has seen a lot of recent improvements recently. In particular, the number of scalar multiplications required to multiply two $4\times4$ matrices was first reduced in \cite{Fawzi:2022aa} from 49 (two recursion levels of Strassen's algorithm) to 47 but only in characteristic 2 or more recently to 48 in \cite{alphaevolve} but over complex numbers. We propose an algorithm in 48 multiplications with only rational coefficients, hence removing the complex number requirement. It was derived from the latter one, under the action of an isotropy which happen to project the algorithm on the field of rational numbers. We also produce a straight line program of this algorithm, reducing the leading constant in the complexity, as well as an alternative basis variant of it, leading to an algorithm running in $\frac{19}{16} n^{2+\frac{\log_2 3}{2}} +o\left(n^{2+\frac{log_2 3}{2}}\right)$ operations over any ring containing an inverse of 2."
2506.22113,"We explore new approaches for finding matrix multiplication algorithms in the commutative setting by adapting the flip graph technique: a method previously shown to be effective for discovering fast algorithms in the non-commutative case. While an earlier attempt to apply flip graphs to commutative algorithms saw limited success, we overcome both theoretical and practical obstacles using two strategies: one inspired by Marakov's algorithm to multiply 3x3 matrices, in which we construct a commutative tensor and approximate its rank using the standard flip graph; and a second that introduces a fully commutative variant of the flip graph defined via a quotient tensor space. We also present a hybrid method that combines the strengths of both. Across all matrix sizes up to 5x5, these methods recover the best known bounds on the number of multiplications and allow for a comparison of their efficiency and efficacy. Although no new improvements are found, our results demonstrate strong potential for these techniques at larger scales."
2507.01878,"The integrability problem of rational first-order ODEs $y^{\prime}=\frac{M(x,y)}{N(x,y)}$, where $M,N \in \mathbb{R}[x,y]$ is a long-term research focus in the area of dynamical systems, physics, etc. Although the computer algebra system such as Mathematica, Maple has developed standard algorithms to tackle its first integral expressed by Liouvillian or special function, this problem is quite difficult and the general method requires specifying a tight degree bound for the Darboux polynomial. Computing the bounded degree first integral, in general, is very expensive for a computer algebra system\cite{duarte2021efficient}\cite{cheze2020symbolic} and becomes impractical for ODE of large size. In \cite{huang2025algorithm}, we have proposed an algorithm to find the inverse of a local rational transformation $y \to \frac{A(x,y)}{B(x,y)}$ that transforms a rational ODE to a simpler and more tractable structure $y^{\prime}=\sum_{i=0}^nf_i(x)y^i$, whose integrability under linear transformation $\left\{x \to F(x),y \to P(x)y+Q(x)\right\}$ can be detected by Maple efficiently \cite{CHEBTERRAB2000204}\cite{cheb2000first}. In that paper we have also mentioned when $M(x,y),N(x,y)$ of the reducible structure are not coprime, canceling the common factors in $y$ will alter the structure which makes that algorithm fail. In this paper, we consider this issue. We conclude that with the exact tight degree bound for the polynomial $A(x,y)$ given, we have an efficient algorithm to compute such transformation and the reduced ODE for ""quite a lot of"" cases where $M,N$ are not coprime. We have also implemented this algorithm in Maple and the code is available in researchgate."
2507.06681,We describe a complete algorithm to compute millions of coefficients of classical modular forms in a few seconds. We also review operations on Euler products and illustrate our methods with a computation of triple product L-function of large conductor.
2507.11987,"Neural certificates have emerged as a powerful tool in cyber-physical systems control, providing witnesses of correctness. These certificates, such as barrier functions, often learned alongside control policies, once verified, serve as mathematical proofs of system safety. However, traditional formal verification of their defining conditions typically faces scalability challenges due to exhaustive state-space exploration. To address this challenge, we propose a lightweight runtime monitoring framework that integrates real-time verification and does not require access to the underlying control policy. Our monitor observes the system during deployment and performs on-the-fly verification of the certificate over a lookahead region to ensure safety within a finite prediction horizon. We instantiate this framework for ReLU-based control barrier functions and demonstrate its practical effectiveness in a case study. Our approach enables timely detection of safety violations and incorrect certificates with minimal overhead, providing an effective but lightweight alternative to the static verification of the certificates."
2507.12366,"Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical analysis and reasoning. Hyperdimensional Computing (HDC), a promising brain-inspired computational model, is integral to neuro-symbolic AI. Various HDC models have been proposed to represent class-instance and class-class relations, but when representing the more complex class-subclass relation, where multiple objects associate different levels of classes and subclasses, they face challenges for factorization, a crucial task for neuro-symbolic AI systems. In this article, we propose FactorHD, a novel HDC model capable of representing and factorizing the complex class-subclass relation efficiently. FactorHD features a symbolic encoding method that embeds an extra memorization clause, preserving more information for multiple objects. In addition, it employs an efficient factorization algorithm that selectively eliminates redundant classes by identifying the memorization clause of the target class. Such model significantly enhances computing efficiency and accuracy in representing and factorizing multiple objects with class-subclass relation, overcoming limitations of existing HDC models such as ""superposition catastrophe"" and ""the problem of 2"". Evaluations show that FactorHD achieves approximately 5667x speedup at a representation size of 10^9 compared to existing HDC models. When integrated with the ResNet-18 neural network, FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset."
2507.13071,"Let K be the unit-cube in Rn and f\,: K $\rightarrow$ R^n be a Morse function. We assume that the function f is given by an evaluation program $\Gamma$ in the noisy model, i.e., the evaluation program $\Gamma$ takes an extra parameter $\eta$ as input and returns an approximation that is $\eta$-close to the true value of f . In this article, we design an algorithm able to compute all local minimizers of f on K . Our algorithm takes as input $\Gamma$, $\eta$, a numerical accuracy parameter $\epsilon$ as well as some extra regularity parameters which are made explicit. Under assumptions of probabilistic nature -- related to the choice of the evaluation points used to feed $\Gamma$ --, it returns finitely many rational points of K , such that the set of balls of radius $\epsilon$ centered at these points contains and separates the set of all local minimizers of f . Our method is based on approximation theory, yielding polynomial approximants for f , combined with computer algebra techniques for solving systems of polynomial equations. We provide bit complexity estimates for our algorithm when all regularity parameters are known. Practical experiments show that our implementation of this algorithm in the Julia package Globtim can tackle examples that were not reachable until now."
2507.20267,"Proof certificates can be used to validate the correctness of algebraic derivations. However, in practice, we frequently observed that the exact same proof steps are repeated for different sets of variables, which leads to unnecessarily large proofs. To overcome this issue we extend the existing Practical Algebraic Calculus with linear combinations (LPAC) with two new proof rules that allow us to capture and reuse parts of the proof to derive a more condensed proof certificate. We integrate these rules into the proof checker Pacheck 2.0. Our experimental results demonstrate that the proposed extension helps to reduce both proof size and verification time."
2507.20889,"In 1978, Frost and Storey asserted that a bivariate polynomial matrix is equivalent to its Smith normal form if and only if the reduced minors of all orders generate the unit ideal. In this paper, we first demonstrate by constructing an example that for any given positive integer s with s >= 2, there exists a square bivariate polynomial matrix M with the degree of det(M) in y equal to s, for which the condition that reduced minors of all orders generate the unit ideal is not a sufficient condition for M to be equivalent to its Smith normal form. Subsequently, we prove that for any square bivariate polynomial matrix M where the degree of det(M) in y is at most 1, Frost and Storey's assertion holds. Using the Quillen-Suslin theorem, we further extend our consideration of M to rank-deficient and non-square cases."
2508.00505,"The Cylindrical Algebraic Decomposition (CAD) method is currently the only complete algorithm used in practice for solving real-algebraic problems. To ameliorate its doubly-exponential complexity, different exploration-guided adaptations try to avoid some of the computations. The first such adaptation named NLSAT was followed by Non-uniform CAD (NuCAD) and the Cylindrical Algebraic Covering (CAlC). Both NLSAT and CAlC have been developed and implemented in SMT solvers for satisfiability checking, and CAlC was recently also adapted for quantifier elimination. However, NuCAD was designed for quantifier elimination only, and no complete implementation existed before this work.In this paper, we present a novel variant of NuCAD for both real quantifier elimination and SMT solving, provide an implementation, and evaluate the method by experimentally comparing it to CAlC."
2508.00512,"The cylindrical algebraic decomposition (CAD) is the only complete method used in practice for solving problems like quantifier elimination or SMT solving related to real algebra, despite its doubly exponential complexity. Recent exploration-guided algorithms like NLSAT, NuCAD, and CAlC rely on CAD technology but reduce the computational effort heuristically. Single cell construction is a paradigm that is used in each of these algorithms.The central property on which the CAD algorithm is based is called delineability. Recently, we introduced a weaker notion called projective delineability which can require fewer computations to guarantee, but needs to be applied carefully. This paper adapts the single cell construction for exploiting projective delineability and reports on experimental results."
2508.0459,"Physics-Informed Neural Network (PINN) is a deep learning framework that integrates the governing equations underlying data into a loss function. In this study, we consider the problem of estimating state variables and parameters in epidemiological models governed by ordinary differential equations using PINNs. In practice, not all trajectory data corresponding to the population described by models can be measured. Learning PINNs to estimate the unmeasured state variables and epidemiological parameters using partial measurements is challenging.Accordingly, we introduce the concept of algebraic observability of the state variables. Specifically, we propose augmenting the unmeasured data based on algebraic observability analysis. The validity of the proposed method is demonstrated through numerical experiments under three scenarios in the context of epidemiological modelling. Specifically, given noisy and partial measurements, the accuracy of unmeasured states and parameter estimation of the proposed method is shown to be higher than that of the conventional methods. The proposed method is also shown to be effective in practical scenarios, such as when the data corresponding to certain variables cannot be reconstructed from the measurements."
2508.06383,"Symbolic indefinite integration in Computer Algebra Systems such as Maple involves selecting the most effective algorithm from multiple available methods. Not all methods will succeed for a given problem, and when several do, the results, though mathematically equivalent, can differ greatly in presentation complexity. Traditionally, this choice has been made with minimal consideration of the problem instance, leading to inefficiencies.We present a machine learning (ML) approach using tree-based deep learning models within a two-stage architecture: first identifying applicable methods for a given instance, then ranking them by predicted output complexity. Furthermore, we find representing mathematical expressions as tree structures significantly improves performance over sequence-based representations, and our two-stage framework outperforms alternative ML formulations.Using a diverse dataset generated by six distinct data generators, our models achieve nearly 90% accuracy in selecting the optimal method on a 70,000 example holdout test set. On an independent out-of-distribution benchmark from Maple's internal test suite, our tree transformer model maintains strong generalisation, outperforming Maple's built-in selector and prior ML approaches.These results highlight the critical role of data representation and problem framing in ML for symbolic computation, and we expect our methodology to generalise effectively to similar optimisation problems in mathematical software."
2508.09754,"As a generalization of our previous result\cite{huang2025algorithm}, this paper aims to answer the following question: Given a 2-dimensional polynomial vector field $y^{\prime}=\frac{M(x,y)}{N(x,y)}$, how to find a rational transformation $y \to \frac{A(x,y)}{B(x,y)}$ with bounded degree numerator, the inverse of which transforms this vector field into a simpler form $y^{\prime}=\sum_{i=0}^nf_i(x)y^i$. Such a structure, often known as the generalized Abel equation and has been studied in various areas, provides a deeper insight into the property of the original vector field. We have implemented an algorithm with considerable performance to tackle this problem and the code is available in \href{this https URL}{Researchgate}."
2508.16164,"In this paper, we present a probabilistic algorithm to multiply two sparse polynomials almost as efficiently as two dense univariate polynomials with a result of approximately the same size. The algorithm depends on unproven heuristics that will be made precise. Non-heuristic versions that are a constant times slower are also presented."
2509.03346,"These notes originate from a reading course held by the authors in the spring of 2024 at the Università di Genova. They provide a hands-on introduction to the F4 and FGLM algorithms. In addition to the notes, we present two implementations of the algorithms: FGLM in CoCoALib and F4 in Sage. These implementations closely follow the structure of the algorithms as described here and are intended to help readers experiment with them in practice, thereby gaining a deeper understanding."
2509.10828,"Codifying mathematical theories in a proof assistant or computer algebra system is a challenging task, of which the most difficult part is, counterintuitively, structuring definitions. This results in a steep learning curve for new users and slow progress in formalizing even undergraduate level mathematics. There are many considerations one has to make, such as level of generality, readability, and ease of use in the type system, and there are typically multiple equivalent or related definitions from which to choose. Often, a definition that is ultimately selected for formalization is settled on after a lengthy trial and error process. This process involves testing potential definitions for usability by formalizing standard theorems about them, and weeding out the definitions that are unwieldy.Inclusion of a formal definition in a centralized community-run mathematical library is typically an indication that the definition is ""good."" For this reason, in this survey, we make some observations about what makes a definition ""good,"" and examine several case studies of the refining process for definitions that have ultimately been added to the Lean Theorem Prover community-run mathematical library, mathlib. We observe that some of the difficulties are shared with the design of libraries for computer algebra systems, and give examples of related issues in that context."
2510.01436,"The symmetric product of two ordinary linear differential operators $L_1,L_2$ is an operator whose solution set contains the product $f_1f_2$ of any solution $f_1$ of $L_1$ and any solution $f_2$ of~$L_2$. It is well known how to compute the symmetric product of two given operators $L_1,L_2$. In this paper we consider the corresponding division problem: given a symmetric product $L$ and one of its factors, what can we say about the other factors?"
2510.01568,"This paper presents a novel algorithm for constructing a sum-of-squares (SOS) decomposition for positive semi-definite polynomials with rational coefficients. Unlike previous methods that typically yield SOS decompositions with floating-point coefficients, our approach ensures that all coefficients in the decomposition remain rational. This is particularly useful in formal verification and symbolic computation, where exact arithmetic is required. We introduce a stepwise reduction technique that transforms a given polynomial into a sum of ladder-like squares while preserving rationality. Experimental results demonstrate the effectiveness of our method compared to existing numerical approaches. This artical is an extension of the following Chinnese paper: HUANG Yong , ZENG Zhenbing , YANG Lu , RAO Yongsheng. An Algorithm to Represent Positive Semi-Definite Polynomials to Sum of Lader-Like Squares of Polynomials with Rational Coefficients (in Chinese). Journal of Systems Science and Mathematical Sciences, 2024, 44(5): 1241-1271this https URL"
2510.02551,"This paper presents a novel approach to finding analytical approximations for bright-soliton solutions in strongly magnetized plasmas. We leverage Physics-Informed Symbolic Regression (PISR) to discover closed-form expressions for the vector potential and number density profiles, governed by a reduced-order model derived from Maxwell-fluid equations. The PISR framework combines symbolic regression with physics-based constraints, boundary conditions, and available simulation data to guide the search for solutions. We demonstrate the effectiveness of the approach by rediscovering approximate solutions consistent with previously published numerical results, showcasing the potential of PISR for reducing simulation costs of reduced-order models in plasma physics."
2510.03103,"An efficient method is proposed for computing the structure of Jordan blocks of a matrix of integers or rational numbers by exact computation. We have given a method for computing Jordan chains of a matrix with exact computation. However, for deriving just the structure of Jordan chains, the algorithm can be reduced to increase its efficiency. We propose a modification of the algorithm for that purpose. Results of numerical experiments are given."
2510.05103,Recently Hashemi and Kapur published an algorithm [1] for Groebner basis conversion by truncating polynomials according to a source and a target monomial order. Here we present a counterexample to this algorithm.
2510.13456,"A complete reduction $\phi$ for derivatives in a differential field is a linear operator on the field over its constant subfield. The reduction enables us to decompose an element $f$ as the sum of a derivative and the remainder $\phi(f)$. A direct application of $\phi$ is that $f$ is in-field integrable if and only if $\phi(f) = 0.$In this paper, we present a complete reduction for derivatives in a primitive tower algorithmically. Typical examples for primitive towers are differential fields generated by (poly-)logarithmic functions and logarithmic integrals. Using remainders and residues, we provide a necessary and sufficient condition for an element from a primitive tower to have an elementary integral, and discuss how to construct telescopers for non-D-finite functions in some special primitive towers."
2511.05849,"Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the effective search space and accelerating training lies in symbolic equivalence: many expressions, although syntactically different, define the same function -- for example, $\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, and $2\log(x_1)+3\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates equality graphs (e-graphs) into diverse symbolic regression algorithms, including Monte Carlo Tree Search (MCTS), deep reinforcement learning (DRL), and large language models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module, enabling more efficient learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalence classes in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Under mild assumptions, we show that embedding e-graphs tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances multiple baselines across challenging benchmarks, discovering equations with lower normalized mean squared error than state-of-the-art methods. Code implementation is available at:this https URL."
