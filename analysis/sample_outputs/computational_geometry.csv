paper_id,abstract
2501.0012,"For a set $P$ of $n$ points in the plane and a value $r > 0$, the unit-disk range reporting problem is to construct a data structure so that given any query disk of radius $r$, all points of $P$ in the disk can be reported efficiently. We consider the dynamic version of the problem where point insertions and deletions of $P$ are allowed. The previous best method provides a data structure of $O(n\log n)$ space that supports $O(\log^{3+\epsilon}n)$ amortized insertion time, $O(\log^{5+\epsilon}n)$ amortized deletion time, and $O(\log^2 n/\log\log n+k)$ query time, where $\epsilon$ is an arbitrarily small positive constant and $k$ is the output size. In this paper, we improve the query time to $O(\log n+k)$ while keeping other complexities the same as before. A key ingredient of our approach is a shallow cutting algorithm for circular arcs, which may be interesting in its own right. A related problem that can also be solved by our techniques is the dynamic unit-disk range emptiness queries: Given a query unit disk, we wish to determine whether the disk contains a point of $P$. The best previous work can maintain $P$ in a data structure of $O(n)$ space that supports $O(\log^2 n)$ amortized insertion time, $O(\log^4n)$ amortized deletion time, and $O(\log^2 n)$ query time. Our new data structure also uses $O(n)$ space but can support each update in $O(\log^{1+\epsilon} n)$ amortized time and support each query in $O(\log n)$ time."
2501.00207,"Given a set $P$ of $n$ points in the plane, its unit-disk graph $G(P)$ is a graph with $P$ as its vertex set such that two points of $P$ are connected by an edge if their (Euclidean) distance is at most $1$. We consider several classical problems on $G(P)$ in a special setting when points of $P$ are in convex position. These problems are all NP-hard in the general case. We present efficient algorithms for these problems under the convex position assumption. The considered problems include the following: finding a minimum weight dominating set in $G(P)$, the discrete $k$-center problem for $P$, finding a maximum weight independent set in $G(P)$, the dispersion problem for $P$, and several of their variations. For some of these problems, our algorithms improve the previously best results, while for others, our results provide first-known solutions."
2501.01901,"Standard sweep algorithms require an order of discrete points in Euclidean space, and rely on the property that, at a given point, all points in the halfspace below come earlier in this order. We are motivated by the problem of reconstructing a graph in $\mathbb{R}^d$ from vertex locations and degree information, which was addressed using standard sweep algorithms by Fasy et al. We generalize this to the reconstruction of general simplicial complexes. As our main ingredient, we introduce a generalized \emph{sweeping order} on $i$-simplices, maintaining the property that, at a given $i$-simplex $\sigma$, all $(i+1)$-dimensional cofaces of $\sigma$ in the halfspace below $\sigma$ have an $i$-dimensional face that appeared earlier in the order (""below"" with respect to some direction perpendicular to $\sigma$).We then go on to incorporate computing such sweeping orders to reconstruct an unknown simplicial complex $K$, starting with only its vertex locations, i.e., its $0$-skeleton. Specifically, once we have found the $i$-skeleton of $K$, we compute a sweeping order for the $i$-simplices, and use it to reconstruct the $(i+1)$-skeleton of $K$ by querying the \emph{indegree}, or the number of $(i+1)$-simplices incident to and below a given $i$-simplex. In addition to generalizing the algorithm by Fasy et al. to simplicial complexes, we improve upon the running time of their central subroutine of radially finding edges above a vertex."
2501.02195,"Given a set $ P $ of $n$ points and a set $ H $ of $n$ half-planes in the plane, we consider the problem of computing a smallest subset of points such that each half-plane contains at least one point of the subset. The previously best algorithm solves the problem in $O(n^3 \log n)$ time. It is also known that $\Omega(n \log n)$ is a lower bound for the problem under the algebraic decision tree model. In this paper, we present an $O(n \log n)$ time algorithm, which matches the lower bound and thus is optimal. Another virtue of the algorithm is that it is relatively simple."
2501.03834,"The Fréchet distance is a popular similarity measure that is well-understood for polygonal curves in $\mathbb{R}^d$: near-quadratic time algorithms exist, and conditional lower bounds suggest that these results cannot be improved significantly, even in one dimension and when approximating with a factor less than three. We consider the special case where the curves bound a simple polygon and distances are measured via geodesics inside this simple polygon. Here the conditional lower bounds do not apply; Efrat $et$ $al.$ (2002) were able to give a near-linear time $2$-approximation algorithm.In this paper, we significantly improve upon their result: we present a $(1+\varepsilon)$-approximation algorithm, for any $\varepsilon > 0$, that runs in $\mathcal{O}(\frac{1}{\varepsilon} (n+m \log n) \log nm \log \frac{1}{\varepsilon})$ time for a simple polygon bounded by two curves with $n$ and $m$ vertices, respectively. To do so, we show how to compute the reachability of specific groups of points in the free space at once, by interpreting the free space as one between separated one-dimensional curves. We solve this one-dimensional problem in near-linear time, generalizing a result by Bringmann and Künnemann (2015). Finally, we give a linear time exact algorithm if the two curves bound a convex polygon."
2501.05315,"In 1873, James C. Maxwell conjectured that the electric field generated by $n$ point charges in generic position has at most $(n-1)^2$ isolated zeroes. The first (non-optimal) upper bound was only obtained in 2007 by Gabrielov, Novikov and Shapiro, who also posed two additional interesting conjectures.In this article, we give the best upper bound known to date on the number of zeroes of the electric field, and construct a counterexample to a conjecture of Gabrielov, Novikov and Shapiro that the number of equilibria cannot exceed those of the distance function defined by the unit point charges.Finally, we note that it is quite possible that Maxwell's quadratic upper bound is not tight, so it is prudent to find smaller bounds. Hence, we also explore examples and construct configurations of charges achieving the highest ratios of the number of electric field zeroes by point charges found to this day."
2501.05529,"In this paper, we present a novel heuristic algorithm for the stable but NP-complete deformation-based edit distance on merge trees. Our key contribution is the introduction of a user-controlled look-ahead parameter that allows to trade off accuracy and computational cost. We achieve a fixed parameter tractable running time that is polynomial in the size of the input but exponential in the look-ahead value. This extension unlocks the potential of the deformation-based edit distance in handling saddle swaps, while maintaining feasible computation times. Experimental results demonstrate the computational efficiency and effectiveness of this approach in handling specific perturbations."
2501.06588,"We consider coresets for $k$-clustering problems, where the goal is to assign points to centers minimizing powers of distances. A popular example is the $k$-median objective $\sum_{p}\min_{c\in C}dist(p,C)$. Given a point set $P$, a coreset $\Omega$ is a small weighted subset that approximates the cost of $P$ for all candidate solutions $C$ up to a $(1\pm\varepsilon )$ multiplicative factor. In this paper, we give a sharp VC-dimension based analysis for coreset construction. As a consequence, we obtain improved $k$-median coreset bounds for the following metrics:Coresets of size $\tilde{O}\left(k\varepsilon^{-2}\right)$ for shortest path metrics in planar graphs, improving over the bounds $\tilde{O}\left(k\varepsilon^{-6}\right)$ by [Cohen-Addad, Saulpic, Schwiegelshohn, STOC'21] and $\tilde{O}\left(k^2\varepsilon^{-4}\right)$ by [Braverman, Jiang, Krauthgamer, Wu, SODA'21].Coresets of size $\tilde{O}\left(kd\ell\varepsilon^{-2}\log m\right)$ for clustering $d$-dimensional polygonal curves of length at most $m$ with curves of length at most $\ell$ with respect to Frechet metrics, improving over the bounds $\tilde{O}\left(k^3d\ell\varepsilon^{-3}\log m\right)$ by [Braverman, Cohen-Addad, Jiang, Krauthgamer, Schwiegelshohn, Toftrup, and Wu, FOCS'22] and $\tilde{O}\left(k^2d\ell\varepsilon^{-2}\log m \log |P|\right)$ by [Conradi, Kolbe, Psarros, Rohde, SoCG'24]."
2501.07617,"Simplicial partitions are a fundamental structure in computational geometry, as they form the basis of optimal data structures for range searching and several related problems. Current algorithms are built on very specific spatial partitioning tools tailored for certain geometric cases. This severely limits their applicability to general set systems. In this work, we propose a simple greedy heuristic for constructing simplicial partitions of any set system. We present a thorough empirical evaluation of its behavior on a variety of geometric and non-geometric set systems, showing that it performs well on most instances. Implementation of these algorithms is available on Github."
2501.07707,"Much prior work has been done on designing computational geometry algorithms that handle input degeneracies, data imprecision, and arithmetic round-off errors. We take a new approach, inspired by the noisy sorting literature, and study computational geometry algorithms subject to noisy Boolean primitive operations in which, e.g., the comparison ""is point q above line L?"" returns the wrong answer with some fixed probability. We propose a novel technique called path-guided pushdown random walks that generalizes the results of noisy sorting. We apply this technique to solve point-location, plane-sweep, convex hulls in 2D and 3D, dynamic 2D convex hulls, and Delaunay triangulations for noisy primitives in optimal time with high probability."
2501.10335,"We propose a modification of the As-Rigid-As-Possible (ARAP) mesh deformation energy with higher order smoothness, which overcomes a prominent limitation of the original ARAP formulation: spikes and lack of continuity at the manipulation handles. Our method avoids spikes even when using single-point positional constraints. Since no explicit rotations have to be specified, the user interaction can be realized through a simple click-and-drag interface, where points on the mesh can be selected and moved around while the rest of the mesh surface automatically deforms accordingly. Our method preserves the benefits of ARAP deformations: it is easy to implement and thus useful for practical applications, while its efficiency makes it usable in real-time, interactive scenarios on detailed models."
2501.10728,"Merge trees are a powerful tool from topological data analysis that is frequently used to analyze scalar fields. The similarity between two merge trees can be captured by an interleaving: a pair of maps between the trees that jointly preserve ancestor relations in the trees. Interleavings can have a complex structure; visualizing them requires a sense of (drawing) order which is not inherent in this purely topological concept. However, in practice it is often desirable to introduce additional geometric constraints, which leads to variants such as labeled or monotone interleavings. Monotone interleavings respect a given order on the leaves of the merge trees and hence have the potential to be visualized in a clear and comprehensive manner.In this paper, we introduce ParkView: a schematic, scalable encoding for monotone interleavings. ParkView captures both maps of the interleaving using an optimal decomposition of both trees into paths and corresponding branches. We prove several structural properties of monotone interleavings, which support a sparse visual encoding using active paths and hedges that can be linked using a maximum of 6 colors for merge trees of arbitrary size. We show how to compute an optimal path-branch decomposition in linear time and illustrate ParkView on a number of real-world datasets."
2501.12261,"There has been considerable recent interest in computing a diverse collection of solutions to a given optimization problem, both in the AI and theory communities. Given a classical optimization problem $\Pi$ (e.g., spanning tree, minimum cuts, maximum matching, minimum vertex cover) with input size $n$ and an integer $k\geq 1$, the goal is to generate a collection of $k$ maximally diverse solutions to $\Pi$. This diverse-X paradigm not only allows the user to generate very different solutions, but also helps make systems more secure and robust by handling uncertainty, and achieve energy efficiency.For problems $\Pi$ in P (such as spanning tree and minimum cut), there are efficient $\text{poly}(n,k)$ approximation algorithms available for the diverse variants [Hanaka et al. AAAI 2021, 2022, 2023, Gao et al. LATIN 2022, de Berg et al. ISAAC 2023]. In contrast, only FPT algorithms are known for NP-hard problems such as vertex covers and independent sets [Baste et al. IJCAI 2020, Eiben et al. SODA 2024, Misra et al. ISAAC 2024, Austrin et al. ICALP 2025], but in the worst case, these algorithms run in time $\exp((kn)^c)$ for some $c>0$. In this work, we address this gap and give $\text{poly}(n,k)$ or $f(k)\text{poly}(n)$ time approximation algorithms for diversification variants of several NP-hard problems such as knapsack, maximum weight independent sets (MWIS) and minimum vertex covers in planar graphs, geometric (rectangle) knapsack, enclosing points by polygon, and MWIS in unit-disk-graphs of points in convex position. Our results are achieved by developing a general framework and applying it to problems with textbook dynamic-programming algorithms to find one solution."
2501.12306,"A set of n segments in the plane may form a Euclidean TSP tour, a tree, or a matching, among others. Optimal TSP tours as well as minimum spanning trees and perfect matchings have no crossing segments, but several heuristics and approximation algorithms may produce solutions with crossings. If two segments cross, then we can reduce the total length with the following flip operation. We remove a pair of crossing segments, and insert a pair of non-crossing segments, while keeping the same vertex degrees. In this paper, we consider the number of flips performed under different assumptions, using a new unifying framework that applies to tours, trees, matchings, and other types of (multi)graphs. Within this framework, we prove several new bounds that are sensitive to whether some endpoints are in convex position or not."
2501.12814,"We study the problem of computing the Fréchet distance between two polygonal curves under transformations. First, we consider translations in the Euclidean plane. Given two curves $\pi$ and $\sigma$ of total complexity $n$ and a threshold $\delta \geq 0$, we present an $\tilde{\mathcal{O}}(n^{7 + \frac{1}{3}})$ time algorithm to determine whether there exists a translation $t \in \mathbb{R}^2$ such that the Fréchet distance between $\pi$ and $\sigma + t$ is at most $\delta$. This improves on the previous best result, which is an $\mathcal{O}(n^8)$ time algorithm.We then generalize this result to any class of rationally parameterized transformations, which includes translation, rotation, scaling, and arbitrary affine transformations. For a class $\mathcal T$ of rationally parametrized transformations with $k$ degrees of freedom, we show that one can determine whether there is a transformation $\tau \in \mathcal T$ such that the Fréchet distance between $\pi$ and $\tau(\sigma)$ is at most $\delta$ in $\tilde{\mathcal{O}}(n^{3k+\frac{4}{3}})$ time."
2501.12821,"The Fréchet distance is a computational mainstay for comparing polygonal curves. The Fréchet distance under translation, which is a translation invariant version, considers the similarity of two curves independent of their location in space. It is defined as the minimum Fréchet distance that arises from allowing arbitrary translations of the input curves. This problem and numerous variants of the Fréchet distance under some transformations have been studied, with more work concentrating on the discrete Fréchet distance, leaving a significant gap between the discrete and continuous versions of the Fréchet distance under transformations. Our contribution is twofold: First, we present an algorithm for the Fréchet distance under translation on 1-dimensional curves of complexity n with a running time of $\mathcal{O}(n^{8/3} log^3 n)$. To achieve this, we develop a novel framework for the problem for 1-dimensional curves, which also applies to other scenarios and leads to our second contribution. We present an algorithm with the same running time of $\mathcal{O}(n^{8/3} \log^3 n)$ for the Fréchet distance under scaling for 1-dimensional curves. For both algorithms we match the running times of the discrete case and improve the previously best known bounds of $\tilde{\mathcal{O}}(n^4)$. Our algorithms rely on technical insights but are conceptually simple, essentially reducing the continuous problem to the discrete case across different length scales."
2501.13201,"Collision detection is a critical functionality for robotics. The degree to which objects collide cannot be represented as a continuously differentiable function for any shapes other than spheres. This paper proposes a framework for handling collision detection between polyhedral shapes. We frame the signed distance between two polyhedral bodies as the optimal value of a convex optimization, and consider constraining the signed distance in a bilevel optimization problem. To avoid relying on specialized bilevel solvers, our method exploits the fact that the signed distance is the minimal point of a convex region related to the two bodies. Our method enumerates the values obtained at all extreme points of this region and lists them as constraints in the higher-level problem. We compare our formulation to existing methods in terms of reliability and speed when solved using the same mixed complementarity problem solver. We demonstrate that our approach more reliably solves difficult collision detection problems with multiple obstacles than other methods, and is faster than existing methods in some cases."
2501.13737,"Surface parametrization is a crucial part in various fields, having applications in computer graphic, medical imaging, scientific computing and computational engineering. The majority of surface parametrization approaches are performed on triangular meshes. On the contrary, the theories and methods of point cloud surface parametrization are less researched, despite its rising significance. In this work, we compute surface parametrization in an optimization approach using neural networks, with novel loss functions introduced without extrinsic information, together with theoretical analyses. Based on the theory, we develop an optimization algorithm to improve the parametrization quality. Using our methods, general open surfaces can be parametrized in either free-boundary manner or with arbitrary domain constraints. Landmark matching can also be enforced under our framework. Numerical experiments are conducted and presented, along with applications including surface reconstruction and boundary detection."
2502.0112,"Lipschitz decomposition is a useful tool in the design of efficient algorithms involving metric spaces. While many bounds are known for different families of finite metrics, the optimal parameters for $n$-point subsets of $\ell_p$, for $p > 2$, remained open, see e.g. [Naor, SODA 2017]. We make significant progress on this question and establish the bound $\beta=O(\log^{1-1/p} n)$. Building on prior work, we demonstrate applications of this result to two problems, high-dimensional geometric spanners and distance labeling schemes. In addition, we sharpen a related decomposition bound for $1<p<2$, due to Filtser and Neiman [Algorithmica 2022]."
2502.03309,"A planar orthogonal drawing {\Gamma} of a connected planar graph G is a geometric representation of G such that the vertices are drawn as distinct points of the plane, the edges are drawn as chains of horizontal and vertical segments, and no two edges intersect except at common end-points. A bend of {\Gamma} is a point of an edge where a horizontal and a vertical segment meet. Drawing {\Gamma} is bend-minimum if it has the minimum number of bends over all possible planar orthogonal drawings of G. Its curve complexity is the maximum number of bends per edge. In this paper we present a linear-time algorithm for the computation of planar orthogonal drawings of 3-graphs (i.e., graphs with vertex-degree at most three), that minimizes both the total number of bends and the curve complexity. The algorithm works in the so-called variable embedding setting, that is, it can choose among the exponentially many planar embeddings of the input graph. While the time complexity of minimizing the total number of bends of a planar orthogonal drawing of a 3-graph in the variable embedding settings is a long standing, widely studied, open question, the existence of an orthogonal drawing that is optimal both in the total number of bends and in the curve complexity was previously unknown. Our result combines several graph decomposition techniques, novel data-structures, and efficient approaches to re-rooting decomposition trees."
2502.03633,"In the preprocessing framework one is given a set of regions that one is allowed to preprocess to create some auxiliary structure such that when a realization of these regions is given, consisting of one point per region, this auxiliary structure can be used to reconstruct some desired output geometric structure more efficiently than would have been possible without preprocessing. Prior work showed that a set of $n$ unit disks of constant ply can be preprocessed in $O(n\log n)$ time such that the convex hull of any realization can be reconstructed in $O(n)$ time. (This prior work focused on triangulations and the convex hull was a byproduct.) In this work we show for the first time that we can reconstruct the convex hull in time proportional to the number of \emph{unstable} disks, which may be sublinear, and that such a running time is the best possible. Here a disk is called \emph{stable} if the combinatorial structure of the convex hull does not depend on the location of its realized point.The main tool by which we achieve our results is by using a supersequence as the auxiliary structure constructed in the preprocessing phase, that is we output a supersequence of the disks such that the convex hull of any realization is a subsequence. One advantage of using a supersequence as the auxiliary structure is that it allows us to decouple the preprocessing phase from the reconstruction phase in a stronger sense than was possible in previous work, resulting in two separate algorithmic problems which may be independent interest. Finally, in the process of obtaining our results for convex hulls, we solve the corresponding problem of creating such supersequences for intervals in one dimension, yielding corresponding results for that case."
2502.05712,"For many simulation codes, block-structured hex meshes remain preferred while their automatic generation is unsolved. We investigate the usage of a polycube-based approach. More specifically, we focus on the labeling stage, which consists in assigning each boundary facet to one of the 6 signed principal axis. Similar works are confronted with 2 challenges: over-constraining validity criteria, and the conflated processing of validity criteria with quality metrics. We tackle these obstacles with automatic routines based on semi-global labeling operators. Our approach is successfully tested on CAD models, which are of interest for many numerical simulation problems."
2502.09299,"We consider the problem of reconfiguring a two-dimensional connected grid arrangement of passive building blocks from a start configuration to a goal configuration, using a single active robot that can move on the tiles, remove individual tiles from a given location and physically move them to a new position by walking on the remaining configuration. The objective is to determine a schedule that minimizes the overall makespan, while keeping the tile configuration connected.We provide both negative and positive results. (1) We generalize the problem by introducing weighted movement costs, which can vary depending on whether tiles are carried or not, and prove that this variant is NP-hard. (2) We give a polynomial-time constant-factor approximation algorithm for the case of disjoint start and target bounding boxes, which additionally yields optimal carry distance for 2-scaled instances."
2502.10066,"Given a plane geometric graph $G$ on $n$ vertices, we want to augment it so that given parity constraints of the vertex degrees are met. In other words, given a subset $R$ of the vertices, we are interested in a plane geometric supergraph $G'$ such that exactly the vertices of $R$ have odd degree in $G'\setminus G$. We show that the question whether such a supergraph exists can be decided in polynomial time for two interesting cases. First, when the vertices are in convex position, we present a linear-time algorithm. Building on this insight, we solve the case when $G$ is a plane geometric path in $O(n \log n)$ time. This solves an open problem posed by Catana, Olaverri, Tejel, and Urrutia (Appl. Math. Comput. 2020)."
2502.10609,"This work develops a framework to create meshes with user-specified homology from potentially dirty geometry by coupling background grids, persistent homology, and a generalization of volume fractions. For a mesh with fixed grid size, the topology of the output mesh changes predictably and monotonically as its volume-fraction threshold decreases. Topological anti-aliasing methods are introduced to resolve pinch points and disconnected regions that are artifacts of user choice of grid size and orientation, making the output meshes suitable for downstream processes including analysis. The methodology is demonstrated on geographical, mechanical, and graphics models in 2D and 3D using a custom-made software called Tusqh. The work demonstrates that the proposed framework is viable for generating meshes on topologically invalid geometries and for automatic defeaturing of small geometric artifacts. Finally, the work shows that although subdividing the background grid frequently improves the topological and geometrical fidelity of the output mesh, there are simple 2D examples for which the topology does not converge under refinement for volume-fraction codes."
2502.13425,"The contributions of the paper span theoretical and implementational results. First, we prove that Kd-trees can be extended to spaces in which the distance is measured with an arbitrary Bregman divergence. Perhaps surprisingly, this shows that the triangle inequality is not necessary for correct pruning in Kd-trees. Second, we offer an efficient algorithm and C++ implementation for nearest neighbour search for decomposable Bregman divergences.The implementation supports the Kullback--Leibler divergence (relative entropy) which is a popular distance between probability vectors and is commonly used in statistics and machine learning. This is a step toward broadening the usage of computational geometry algorithms.Our benchmarks show that our implementation efficiently handles both exact and approximate nearest neighbour queries. Compared to a naive approach, we achieve two orders of magnitude speedup for practical scenarios in dimension up to 100. Our solution is simpler and more efficient than competing methods."
2502.1566,"Let $G$ be a complete edge-weighted graph on $n$ vertices. To each subset of vertices of $G$ assign the cost of the minimum spanning tree of the subset as its weight. Suppose that $n$ is a multiple of some fixed positive integer $k$. The $k$-matching problem is the problem of finding a partition of the vertices of $G$ into $k$-sets, that minimizes the sum of the weights of the $k$-sets. The case $k=3$ has been shown to be NP-hard [Johnsson et al.,1998]. In the Euclidean version, the vertices of $G$ are points in the plane and the weight of an edge is the Euclidean distance between its endpoints. We call this problem the Euclidean $k$-matching problem. We show that, for every fixed $k \ge 3$, the Euclidean $k$-matching is NP-hard. This resolves an open problem in the literature and provides the first theoretical justification for the use of known heuristic methods in the case $k=3$. We also show that the problem remains NP-hard if the trees are required to be paths."
2502.16305,"We introduce the following variant of the Gale-Berlekamp switching game. Let $P$ be a set of n noncollinear points in the plane, each of them having weight $+1$ or $-1$. At each step, we pick a line $\ell$ passing through at least two points of $P$, and switch the sign of every point $p \in P\cap\ell$. The objective is to maximize the total weight of the elements of $P$. We show that one can always achieve that this quantity is at least $n - o(n)$, as $n\rightarrow\infty$, and at least $n/3$, for every $n$. Moreover, these can be attained by a polynomial time algorithm."
2502.1631,"We present a GPU-native mesh adaptation procedure that incorporates a complex geometry represented with a triangle mesh within a primary Cartesian computational grid organized as a forest of octrees. A C++/CUDA program implements the procedure for execution on a single GPU as part of a new module with the AGAL framework, which was originally developed for GPU-native adaptive mesh refinement (AMR) and fluid flow simulation with the Lattice Boltzmann Method (LBM). Traditional LBM is limited to grids with regular prismatic cells with domain boundaries aligned with the cell faces. This work is a first step towards an implementation of the LBM that can simulate flow over irregular surfaces while retaining both adaptation of the mesh and the temporal integration routines entirely on the GPU. Geometries can be inputted as a text file (which generates primitive objects such as circles and spheres) or as an STL file (which can be generated by most 3D modeling software). The procedure is divided into three steps: 1) an import step where the geometry is loaded into either an index list arrangement or directly as a face-vertex coordinates list, 2) a spatial binning step where the faces are distributed to a set of bins with user-defined density, and 3) a near-wall refinement step where the cells of the computational grid detect adjacency to the faces stored in the appropriate bin to form the links between the geometry and the boundary nodes. We validate the implementation and assess its performance in terms of total execution time and speedup relative to a serial CPU implementation using a 2D circle and a 3D Stanford bunny."
2502.16621,"In the Segment Intersection Graph Representation Problem, we want to represent the vertices of a graph as straight line segments in the plane such that two segments cross if and only if there is an edge between the corresponding vertices. This problem is NP-hard (even $\exists\mathbb{R}$-complete [Schaefer, 2010]) in the general case [Kratochvíl & Neŝetril, 1992] and remains so if we restrict the segments to be axis-aligned, i.e., horizontal and vertical [Kratochvíl, 1994]. A long standing open question for the latter variant is its complexity when the order of segments along one axis (say the vertical order of horizontal segments) is already given [Kratochvíl & Neŝetril, 1992; Kratochvíl, 1994].We resolve this question by giving efficient solutions using two very different approaches that are interesting on their own. First, using a graph-drawing perspective, we relate the problem to a variant of the well-known Level Planarity problem, where vertices have to lie on pre-assigned horizontal levels. In our case, each level also carries consecutivity constraints on its vertices; this Level Planarity variant is known to have a quadratic solution.Second, we use an entirely combinatorial approach, and show that both problems can equivalently be formulated as a linear ordering problem subject to certain consecutivity constraints. While the complexity of such problems varies greatly, we show that in this case the constraints are well-structured in a way that allows a direct quadratic solution. Thus, we obtain three different-but-equivalent perspectives on this problem: the initial geometric one, one from planar graph drawing and a purely combinatorial one."
2502.17277,"We propose sublinear algorithms for probabilistic testing of the discrete and continuous Fréchet distance - a standard similarity measure for curves. We assume the algorithm is given access to the input curves via a query oracle: a query returns the set of vertices of the curve that lie within a radius $\delta$ of a specified vertex of the other curve. The goal is to use a small number of queries to determine with constant probability whether the two curves are similar (i.e., their discrete Fréchet distance is at most $\delta$) or they are ''$\varepsilon$-far'' (for $0 < \varepsilon < 2$) from being similar, i.e., more than an $\varepsilon$-fraction of the two curves must be ignored for them to become similar. We present two algorithms which are sublinear assuming that the curves are $t$-approximate shortest paths in the ambient metric space, for some $t\ll n$. The first algorithm uses $O(\frac{t}{\varepsilon}\log\frac{t}{\varepsilon})$ queries and is given the value of $t$ in advance. The second algorithm does not have explicit knowledge of the value of $t$ and therefore needs to gain implicit knowledge of the straightness of the input curves through its queries. We show that the discrete Fréchet distance can still be tested using roughly $O(\frac{t^3+t^2\log n}{\varepsilon})$ queries ignoring logarithmic factors in $t$. Our algorithms work in a matrix representation of the input and may be of independent interest to matrix testing. Our algorithms use a mild uniform sampling condition that constrains the edge lengths of the curves, similar to a polynomially bounded aspect ratio. Applied to testing the continuous Fréchet distance of $t$-straight curves, our algorithms can be used for $(1+\varepsilon')$-approximate testing using essentially the same bounds as stated above with an additional factor of poly$(\frac{1}{\varepsilon'})$."
2502.176,"Let $S$ be a set of $n$ points in $\mathbb{R}^d$, where $d \geq 2$ is a constant, and let $H_1,H_2,\ldots,H_{m+1}$ be a sequence of vertical hyperplanes that are sorted by their first coordinates, such that exactly $n/m$ points of $S$ are between any two successive hyperplanes. Let $|A(S,m)|$ be the number of different closest pairs in the ${{m+1} \choose 2}$ vertical slabs that are bounded by $H_i$ and $H_j$, over all $1 \leq i < j \leq m+1$. We prove tight bounds for the largest possible value of $|A(S,m)|$, over all point sets of size $n$, and for all values of $1 \leq m \leq n$.As a result of these bounds, we obtain, for any constant $\epsilon>0$, a data structure of size $O(n)$, such that for any vertical query slab $Q$, the closest pair in the set $Q \cap S$ can be reported in $O(n^{1/2+\epsilon})$ time. Prior to this work, no linear space data structure with sublinear query time was known."
2502.17704,"Given a zigzag filtration, we want to find its barcode representatives, i.e., a compatible choice of bases for the homology groups that diagonalize the linear maps in the zigzag. To achieve this, we convert the input zigzag to a levelset zigzag of a real-valued function. This function generates a Mayer-Vietoris pyramid of spaces, which generates an infinite strip of homology groups. We call the origins of indecomposable (diamond) summands of this strip their apexes and give an algorithm to find representative cycles in these apexes from ordinary persistence computation. The resulting representatives map back to the levelset zigzag and thus yield barcode representatives for the input zigzag. Our algorithm for lifting a $p$-dimensional cycle from ordinary persistence to an apex representative takes $O(p \cdot m \log m)$ time. From this we can recover zigzag representatives in time $O(\log m + C)$, where $C$ is the size of the output."
2502.18189,"We provide a spectrum of new theoretical insights and practical results for finding a Minimum Dilation Triangulation (MDT), a natural geometric optimization problem of considerable previous attention: Given a set $P$ of $n$ points in the plane, find a triangulation $T$, such that a shortest Euclidean path in $T$ between any pair of points increases by the smallest possible factor compared to their straight-line distance. No polynomial-time algorithm is known for the problem; moreover, evaluating the objective function involves computing the sum of (possibly many) square roots. On the other hand, the problem is not known to be NP-hard.(1) We provide practically robust methods and implementations for computing an MDT for benchmark sets with up to 30,000 points in reasonable time on commodity hardware, based on new geometric insights into the structure of optimal edge sets. Previous methods only achieved results for up to $200$ points, so we extend the range of optimally solvable instances by a factor of $150$.(2) We develop scalable techniques for accurately evaluating many shortest-path queries that arise as large-scale sums of square roots, allowing us to certify exact optimal solutions, with previous work relying on (possibly inaccurate) floating-point computations.(3) We resolve an open problem by establishing a lower bound of $1.44116$ on the dilation of the regular $84$-gon (and thus for arbitrary point sets), improving the previous worst-case lower bound of $1.4308$ and greatly reducing the remaining gap to the upper bound of $1.4482$ from the literature. In the process, we provide optimal solutions for regular $n$-gons up to $n = 100$."
2502.1839,"Recently, there has been interest in representing single graphs by multiple drawings; for example, using graph stories, storyplans, or uncrossed collections. In this paper, we apply this idea to orthogonal graph drawing. Due to the orthogonal drawing style, we focus on 4-graphs, that is, graphs of maximum degree 4. We restrict ourselves to plane graphs, that is, planar graphs whose embedding is fixed. Our goal is to represent any plane 4-graph $G$ by an unbent collection, that is, a collection of orthogonal drawings of $G$ that adhere to the embedding of $G$ and ensure that each edge of $G$ is drawn without bends in at least one of the drawings. We investigate two objectives. First, we consider minimizing the number of drawings in an unbent collection. We prove that every plane 4-graph can be represented by a collection with at most three drawings, which is tight. We also give necessary and sufficient conditions for a graph to admit an unbent collection of size $2$. Second, we consider minimizing the total number of bends over all drawings in an unbent collection. We show that this problem is NP-hard and give a 3-approximation algorithm. For the special case of plane triconnected cubic graphs, we show how to compute minimum-bend collections in linear time."
2502.19088,"In the field of computer graphics, conformal surface flattening has been widely studied for tasks such as texture mapping, geometry processing, and mesh generation. Typically, existing methods aim to flatten a given input geometry while preserving conformality as much as possible, meaning the result is only as conformal as possible. By contrast, this study focuses on surfaces that can be flattened conformally without singularities, making the process a coupled problem: the input (or target) surface must be recursively refined while its flattening is computed.Although the uniformization theorem or the Riemann mapping theorem guarantees the existence of a conformal flattening for any simply connected, orientable surface, those theorems permit singularities in the flattening. If singularities are not allowed, only a special class of surfaces can be conformally flattened-though many practical surfaces do fall into this class.To address this, we develop a NURBS-based approach in which both the input surface and its flattening are refined in tandem, ensuring mutual conformality. Because NURBS surfaces cannot represent singularities, the resulting pair of surfaces is naturally singularity-free. Our work is inspired by the form-finding method by [Miki and Mitchell 2022, 2024], which solves bilinear PDEs by iteratively refining two surfaces together. Building on their demonstration of the effectiveness of variable projection (VarPro), we adopt a similar strategy: VarPro alternates between a linear projection and a nonlinear iteration, leveraging a partially linear (separable) problem structure. However, since our conformal condition separates into two nonlinear subproblems, we introduce a nonlinear extension of VarPro. Although this significantly increases computational cost, the quality of the results is noteworthy."
2502.20215,"This paper presents a novel topology-aware dimensionality reduction approach aiming at accurately visualizing the cyclic patterns present in high dimensional data. To that end, we build on the Topological Autoencoders (TopoAE) formulation. First, we provide a novel theoretical analysis of its associated loss and show that a zero loss indeed induces identical persistence pairs (in high and low dimensions) for the $0$-dimensional persistent homology (PH$^0$) of the Rips filtration. We also provide a counter example showing that this property no longer holds for a naive extension of TopoAE to PH$^d$ for $d\ge 1$. Based on this observation, we introduce a novel generalization of TopoAE to $1$-dimensional persistent homology (PH$^1$), called TopoAE++, for the accurate generation of cycle-aware planar embeddings, addressing the above failure case. This generalization is based on the notion of cascade distortion, a new penalty term favoring an isometric embedding of the $2$-chains filling persistent $1$-cycles, hence resulting in more faithful geometrical reconstructions of the $1$-cycles in the plane. We further introduce a novel, fast algorithm for the exact computation of PH for Rips filtrations in the plane, yielding improved runtimes over previously documented topology-aware methods. Our method also achieves a better balance between the topological accuracy, as measured by the Wasserstein distance, and the visual preservation of the cycles in low dimensions. Our C++ implementation is available atthis https URL."
2502.20909,"Pseudoline arrangements are fundamental objects in discrete and computational geometry, and different works have tackled the problem of improving the known bounds on the number of simple arrangements of $n$ pseudolines over the past decades. The lower bound in particular has seen two successive improvements in recent years (Dumitrescu and Mandal in 2020 and Cortés Kühnast et al. in 2024). Here we focus on the upper bound, and show that for large enough $n$, there are at most $2^{0.6496n^2}$ different simple arrangements of $n$ pseudolines. This follows a series of incremental improvements starting with work by Knuth in 1992 showing a bound of roughly $2^{0.7925n^2},$ then a bound of $2^{0.6975n^2}$ by Felsner in 1997, and finally the previous best known bound of $2^{0.6572n^2}$ by Felsner and Valtr in 2011. The improved bound presented here follows from a simple argument to combine the approach of this latter work with the use of the Zone Theorem."
2503.01526,"Unit edge-length drawings, rectilinear drawings (where each edge is either a horizontal or a vertical segment), and rectangular face drawings are among the most studied subjects in Graph Drawing. However, most of the literature on these topics refers to planar graphs and planar drawings. In this paper we study drawings with all the above nice properties but that can have edge crossings; we call them Unit Edge length Rectilinear drawings with Rectangular Faces (UER-RF drawings). We consider crossings as dummy vertices and apply the unit edge-length convention to the edge segments connecting any two (real or dummy) vertices. Note that UER-RF drawings are grid drawings (vertices are placed at distinct integer coordinates), which is another classical requirement of graph visualizations. We present several efficient and easily implementable algorithms for recognizing graphs that admit UER-RF drawings and for constructing such drawings if they exist. We consider restrictions on the degree of the vertices or on the size of the faces. For each type of restriction, we consider both the general unconstrained setting and a setting in which either the external boundary of the drawing is fixed or the rotation system of the graph is fixed as part of the input."
2503.01573,"Harmonic maps are important in generating parameterizations for various domains, particularly in two and three dimensions. General extensions of two-dimensional harmonic parameterizations for volumetric parameterizations are known to fail in a variety of contexts, though more specialized volumetric parameterizations have been proposed. This work provides and contextualizes a counterexample to various proposed proofs that employ harmonic maps to sweep a parameterization from a base surface, $\Gamma_0$, to the entire domain of a geometry that is homeomorphic to $\Gamma_0\times[0,1]$ or $\Gamma_0\times S^1$. While this does not negate the potential value of such topological sweep parameterizations, it does clarify that these swept parameterizations come with no inherent guarantees of bijectivity, as they may in two dimensions."
2503.01626,"Subdivision methods such as quadtrees, octrees, and higher-dimensional orthrees are standard practice in different domains of computer science. We can use these methods to represent given geometries, such as curves, meshes, or surfaces. This representation is achieved by splitting some bounding voxel recursively while further splitting only sub-voxels that intersect with the given geometry. It is fairly known that subdivision methods are more efficient than traversing a fine-grained voxel grid. In this short note, we propose another outlook on analyzing the construction time complexity of orthrees to represent implicitly defined geometries that are fibers (preimages) of some function. This complexity is indeed asymptotically better than traversing dense voxel grids, under certain conditions, which we specify in the note. In fact, the complexity is output sensitive, and is closely related to the Hausdorff measure and Hausdorff dimension of the resulting geometry."
2503.01808,"Train timetables can be represented as event graphs, where correspond to a train passing through a location at a certain point in time. A visual representation of an event graph is important for many applications such as dispatching and (the development of) dispatching software. A common way to represent event graphs are time-space diagrams. In such a diagram, key locations are visualized on the y-axis and time on the x-axis of a coordinate system. A train's movement is then represented as a connected sequence of line segments in this coordinate system. This visualization allows for an easy detection of infrastructure conflicts and safety distance violations. However, time-space diagrams are usually used only to depict event graphs that are restricted to corridors, where an obvious ordering of the locations exists.In this paper, we consider the visualization of general event graphs in time-space diagrams, where the challenge is to find an ordering of the locations that produces readable drawings. We argue that this means to minimize the number of turns, i.e., the total number of changes in y-direction. To this end, we establish a connection between this problem and Maximum Betweenness. Then we develop a preprocessing strategy to reduce the instance size. We also propose a parameterized algorithm and integer linear programming formulations. We experimentally evaluate the preprocessing strategy and the integer programming formulations on a real-world dataset. Our best algorithm solves every instance in the dataset in less than a second. This suggests that turn-optimal time-space diagrams can be computed in real time."
2503.01979,"There are many structures, both classical and modern, involving point-sets and polygons whose deeper understanding can be facilitated through interactive visualizations. The Ipe extensible drawing editor, developed by Otfried Cheong, is a widely used software system for generating geometric figures. One of its features is the capability to extend its functionality through programs called Ipelets. In this media submission, we showcase a collection of new Ipelets that construct a variety of geometric based structures based on point sets and polygons. These include quadtrees, trapezoidal maps, beta skeletons, floating bodies of convex polygons, onion graphs, fractals (Sierpiński triangle and carpet), simple polygon triangulations, and random point sets in simple polygons. All of our Ipelets are programmed in Lua and are freely available."
2503.01988,"Metric spaces defined within convex polygons, such as the Thompson, Funk, reverse Funk, and Hilbert metrics, are subjects of recent exploration and study in computational geometry. This paper contributes an educational piece of software for understanding these unique geometries while also providing a tool to support their research. We provide dynamic software for manipulating the Funk, reverse Funk, and Thompson balls in convex polygonal domains. Additionally, we provide a visualization program for traversing the Hilbert polygonal geometry."
2503.0201,"We study the problem of determining coordinated motions, of minimum total length, for two arbitrary convex centrally-symmetric (CCS) robots in an otherwise obstacle-free plane. Using the total path length traced by the two robot centres as a measure of distance, we give an exact characterization of a (not necessarily unique) shortest collision-avoiding motion for all initial and goal configurations of the robots. The individual paths are composed of at most six convex pieces, and their total length can be expressed as a simple integral with a closed form solution depending only on the initial and goal configuration of the robots. The path pieces are either straight segments or segments of the boundary of the Minkowski sum of the two robots (circular arcs, in the special case of disc robots). Furthermore, the paths can be parameterized in such a way that (i) only one robot is moving at any given time (decoupled motion), or (ii) the orientation of the robot configuration changes monotonically."
2503.02439,"We investigate blob-trees, a new way of connecting a set of points, by a mixture of enclosing them by cycles (as in the convex hull) and connecting them by edges (as in a spanning tree). We show that a minimum-cost blob-tree for $n$ points can be computed in $O(n^3)$ time."
2503.02715,"We consider the $k$-center problem on the space of fixed-size point sets in the plane under the $L_{\infty}$-bottleneck distance. While this problem is motivated by persistence diagrams in topological data analysis, we illustrate it as a \emph{Restaurant Supply Problem}: given $n$ restaurant chains of $m$ stores each, we want to place supermarket chains, also of $m$ stores each, such that each restaurant chain can select one supermarket chain to supply all its stores, ensuring that each store is matched to a nearby supermarket. How many supermarket chains are required to supply all restaurants? We address this questions under the constraint that any two restaurant chains are close enough under the $L_{\infty}$-distance to be satisfied by a single supermarket chain. We provide both upper and lower bounds for this problem and investigate its computational complexity."
2503.02842,"Given a point set $\mathcal{P}$ and a plane perfect matching $\mathcal{M}$ on $\mathcal{P}$, a flip is an operation that replaces two edges of $\mathcal{M}$ such that another plane perfect matching on $\mathcal{P}$ is obtained. Given two plane perfect matchings on $\mathcal{P}$, we show that it is NP-hard to minimize the number of flips that are needed to transform one matching into the other."
2503.03435,"In the nearest neighbor problem, we are given a set $S$ of point sites that we want to store such that we can find the nearest neighbor of a (new) query point efficiently. In the dynamic version of the problem, the goal is to design a data structure that supports both efficient queries and updates, i.e. insertions and deletions in $S$. This problem has been widely studied in various settings, ranging from points in the plane to more general distance measures and even points within simple polygons. When the sites do not live in the plane but in some domain, another dynamic problem arises: what happens if not the sites, but the domain itself is subject to updates?Updating sites often results in local changes to the solution or data structure, while updating the domain may incur many global changes. For example, in the closest pair problem, inserting a point only requires us to check if this point is in the new closest pair, while updating the domain might change the distances between most pairs of points in our set. Presumably, this is the reason that this form of dynamization has received much less attention. Only some basic problems, such as shortest paths and ray shooting, have been studied in this setting.Here, we tackle the nearest neighbor problem in a dynamic simple polygon. We allow insertions into both the set of sites and the polygon. An insertion in the polygon is the addition of a line segment starting at the boundary of the polygon. We present a near-linear size --in both the number of sites and the complexity of the polygon-- data structure with sublinear update and query time. This is the first nearest neighbor data structure that allows for updates to the domain."
2503.03577,"We investigate saturated geometric drawings of graphs with geometric thickness $k$, where no edge can be added without increasing $k$. We establish lower and upper bounds on the number of edges in such drawings if the vertices lie in convex position. We also study the more restricted version where edges are precolored, and for $k=2$ the case for vertices in non-convex position."
2503.05007,"Indexing data is a fundamental problem in computer science. Recently, various papers apply machine learning to this problem.For a fixed integer $\varepsilon$, a \emph{learned index} is a function $h : \mathcal{U} \rightarrow [0, n]$ where $\forall q \in \mathcal{U}$, $h(q) \in [\text{rank}(q) - \varepsilon, \text{rank}(q) + \varepsilon]$. These works use machine learning to compute $h$. Then, they store $S$ in a sorted array $A$ and access $A[\lfloor h(q) \rfloor]$ to answer queries in $O(k + \varepsilon + \log |h|)$ time. Here, $k$ denotes the output size and $|h|$ the complexity of $h$. Ferragina and Vinciguerra (VLDB 2020) observe that creating a learned index is a geometric problem. They define the PGM index by restricting $h$ to a piecewise linear function and show a linear-time algorithm to compute a PGM index of approximate minimum complexity.Since indexing queries are decomposable, the PGM index may be made dynamic through the logarithmic method. When allowing deletions, range query times deteriorate to worst-case $O(N + \sum\limits_i^{\lceil \log n \rceil } (\varepsilon + \log |h_i|))$ time (where $N$ is the largest size of $S$ seen so far).This paper offers a combination of theoretical insights and experiments as we apply techniques from computational geometry to dynamically maintain an approximately minimum-complexity learned index $h : \mathcal{U} \rightarrow [0, n]$ with $O(\log^2 n)$ update time.We also prove that if we restrict $h$ to lie in a specific subclass of piecewise-linear functions, then we can combine $h$ and hash maps to support queries in $O(k + \varepsilon + \log |h|)$ time (at the cost of increasing $|h|$). We implement our algorithm and compare it to the existing implementation. Our empirical analysis shows that our solution supports more efficient range queries in the special case where the update sequence contains many deletions."
2503.05071,"We address the problem of object arrangement and scheduling for sequential 3D printing. Unlike the standard 3D printing, where all objects are printed slice by slice at once, in sequential 3D printing, objects are completed one after other. In the sequential case, it is necessary to ensure that the moving parts of the printer do not collide with previously printed objects. We look at the sequential printing problem from the perspective of combinatorial optimization. We propose to express the problem as a linear arithmetic formula, which is then solved using a solver for satisfiability modulo theories (SMT). However, we do not solve the formula expressing the problem of object arrangement and scheduling directly, but we have proposed a technique inspired by counterexample guided abstraction refinement (CEGAR), which turned out to be a key innovation to efficiency."
2503.05178,We address the problem of computing the minimum number of triangles to separate a set of blue points from a set of red points in $\mathbb{R}^2$. A set of triangles is a \emph{separator} of one color from the other if every point of that color is contained in some triangle and no triangle contains points of both colors. We consider several variants of the problem depending on whether the triangles are allowed to overlap or not and whether all points or just the blue points need to be contained in a triangle. We show that computing the minimum cardinality triangular separator of a set of blue points from a set of red points is NP-hard and further investigate worst case bounds on the minimum cardinality of triangular separators for a bichromatic set of $n$ points.
2503.05216,"Assume that you have lost your puppy on an embedded graph. You can walk around on the graph and the puppy will run towards you at infinite speed, always locally minimizing the distance to your current position. Is it always possible for you to reunite with the puppy? We show that if the embedded graph is an orthogonal straight-line embedding the answer is yes."
2503.06857,"Given a set of points in the plane, the \textsc{General Position Subset Selection} problem is that of finding a maximum-size subset of points in general position, i.e., with no three points collinear. The problem is known to be ${\rm NP}$-complete and ${\rm APX}$-hard, and the best approximation ratio known is $\Omega\left({\rm OPT}^{-1/2}\right) =\Omega(n^{-1/2})$. Here we obtain better approximations in three specials cases:(I) A constant factor approximation for the case where the input set consists of lattice points and is \emph{dense}, which means that the ratio between the maximum and the minimum distance in $P$ is of the order of $\Theta(\sqrt{n})$.(II) An $\Omega\left((\log{n})^{-1/2}\right)$-approximation for the case where the input set is the set of vertices of a \emph{generic} $n$-line arrangement, i.e., one with $\Omega(n^2)$ vertices. The scenario in (I) is a special case of that in (II).(III) An $\Omega\left((\log{n})^{-1/2}\right)$-approximation for the case where the input set has at most $O(\sqrt{n})$ points collinear and can be covered by $O(\sqrt{n})$ lines.Our approximations rely on probabilistic methods and results from incidence geometry."
2503.07361,"A dichotomous ordinal graph consists of an undirected graph with a partition of the edges into short and long edges. A geometric realization of a dichotomous ordinal graph $G$ in a metric space $X$ is a drawing of $G$ in $X$ in which every long edge is strictly longer than every short edge. We call a graph $G$ pandichotomous in $X$ if $G$ admits a geometric realization in $X$ for every partition of its edge set into short and long edges. We exhibit a very close relationship between the degeneracy of a graph $G$ and its pandichotomic Euclidean or spherical dimension, that is, the smallest dimension $k$ such that $G$ is pandichotomous in $\mathbb{R}^k$ or the sphere $\mathbb{S}^k$, respectively. First, every $d$-degenerate graph is pandichotomous in $\mathbb{R}^{d}$ and $\mathbb{S}^{d-1}$ and these bounds are tight for the sphere and for $\mathbb{R}^2$ and almost tight for $\mathbb{R}^d$, for $d\ge 3$. Second, every $n$-vertex graph that is pandichotomous in $\mathbb{R}^k$ has at most $\mu kn$ edges, for some absolute constant $\mu<7.23$. This shows that the pandichotomic Euclidean dimension of any graph is linearly tied to its degeneracy and in the special cases $k\in \{1,2\}$ resolves open problems posed by Alam, Kobourov, Pupyrev, and Toeniskoetter. Further, we characterize which complete bipartite graphs are pandichotomous in $\mathbb{R}^2$: These are exactly the $K_{m,n}$ with $m\le 3$ or $m=4$ and $n\le 6$. For general bipartite graphs, we can guarantee realizations in $\mathbb{R}^2$ if the short or the long subgraph is constrained: namely if the short subgraph is outerplanar or a subgraph of a rectangular grid, or if the long subgraph forms a caterpillar."
2503.07769,"We study the problem of computing the diameter and the mean distance of a continuous graph, i.e., a connected graph where all points along the edges, instead of only the vertices, must be taken into account. It is known that for continuous graphs with $m$ edges these values can be computed in roughly $O(m^2)$ time. In this paper, we use geometric techniques to obtain subquadratic time algorithms to compute the diameter and the mean distance of a continuous graph for two well-established classes of sparse graphs. We show that the diameter and the mean distance of a continuous graph of treewidth at most $k$ can be computed in $O(n\log^{O(k)} n)$ time, where $n$ is the number of vertices in the graph. We also show that computing the diameter and mean distance of a continuous planar graph with $n$ vertices and $F$ faces takes $O(n F \log n)$ time."
2503.08863,"We study three fundamental three-dimensional (3D) geometric packing problems: 3D (Geometric) Bin Packing (3D-BP), 3D Strip Packing (3D-SP), and Minimum Volume Bounding Box (3D-MVBB), where given a set of 3D (rectangular) cuboids, the goal is to find an axis-aligned nonoverlapping packing of all cuboids. In 3D-BP, we need to pack the given cuboids into the minimum number of unit cube bins. In 3D-SP, we need to pack them into a 3D cuboid with a unit square base and minimum height. Finally, in 3D-MVBB, the goal is to pack into a cuboid box of minimum volume.It is NP-hard to even decide whether a set of rectangles can be packed into a unit square bin -- giving an (absolute) approximation hardness of 2 for 3D-BP and 3D-SP. The previous best (absolute) approximation for all three problems is by Li and Cheng (SICOMP, 1990), who gave algorithms with approximation ratios of 13, $46/7$, and $46/7+\varepsilon$, respectively, for 3D-BP, 3D-SP, and 3D-MVBB. We provide improved approximation ratios of 6, 6, and $3+\varepsilon$, respectively, for the three problems, for any constant $\varepsilon > 0$.For 3D-BP, in the asymptotic regime, Bansal, Correa, Kenyon, and Sviridenko (Math.~Oper.~Res., 2006) showed that there is no asymptotic polynomial-time approximation scheme (APTAS) even when all items have the same height. Caprara (Math.~Oper.~Res., 2008) gave an asymptotic approximation ratio of $T_{\infty}^2 + \varepsilon\approx 2.86$, where $T_{\infty}$ is the well-known Harmonic constant in Bin Packing. We provide an algorithm with an improved asymptotic approximation ratio of $3 T_{\infty}/2 +\varepsilon \approx 2.54$. Further, we show that unlike 3D-BP (and 3D-SP), 3D-MVBB admits an APTAS."
2503.09115,"We prove a quasi-linear upper bound on the size of $K_{t,t}$-free polygon visibility graphs. For visibility graphs of star-shaped and monotone polygons we show a linear bound. In the more general setting of $n$ points on a simple closed curve and visibility pseudo-segments, we provide an $O(n \log n)$ upper bound and an $\Omega(n\alpha(n))$ lower bound."
2503.10786,"This paper introduces a Delaunay triangulation algorithm based on the external incremental method. Unlike traditional random incremental methods, this approach uses convex hull and points as basic operational units instead of triangles. Since each newly added point is outside the convex hull, there is no need to search for which triangle contains the point, simplifying the algorithm implementation. The time complexity for point sorting is $O(n\log n)$, while the collective complexity for upper/lower tangent searches is proven to be $O(n)$. For uniformly distributed point sets, empirical results demonstrate linear time $O(n)$ for full triangulation construction. The overall time complexity remains $O(n\log n)$. This paper details the algorithm's data structures, implementation details, correctness proof, and comparison with other methods."
2503.12746,"Let $\tau$ and $\sigma$ be two polygonal curves in $\mathbb{R}^d$ for any fixed $d$. Suppose that $\tau$ and $\sigma$ have $n$ and $m$ vertices, respectively, and $m\le n$. While conditional lower bounds prevent approximating the Fréchet distance between $\tau$ and $\sigma$ within a factor of 3 in strongly subquadratic time, the current best approximation algorithm attains a ratio of $n^c$ in strongly subquadratic time, for some constant $c\in(0,1)$. We present a randomized algorithm with running time $O(nm^{0.99}\log(n/\varepsilon))$ that approximates the Fréchet distance within a factor of $7+\varepsilon$, with a success probability at least $1-1/n^6$. We also adapt our techniques to develop a randomized algorithm that approximates the \emph{discrete} Fréchet distance within a factor of $7+\varepsilon$ in strongly subquadratic time. They are the first algorithms to approximate the Fréchet distance and the discrete Fréchet distance within constant factors in strongly subquadratic time."
2503.14115,"We cluster a set of trajectories T using subtrajectories of T. Clustering quality may be measured by the number of clusters, the number of vertices of T that are absent from the clustering, and by the Fréchet distance between subtrajectories in a cluster. A $\Delta$-cluster of T is a cluster ${\mathcal{P}}$ of subtrajectories of T with a centre $P \in {\mathcal{P}}$ with complexity $\ell$, where all subtrajectories in ${\mathcal{P}}$ have Fréchet distance at most $\Delta$ to $P$. Buchin, Buchin, Gudmundsson, Löffler and Luo present two $O(n^2 + n m \ell)$-time algorithms: SC($\max$, $\ell$, $\Delta$, T) computes a single $\Delta$-cluster where $P$ has at least $\ell$ vertices and maximises the cardinality $m$ of ${\mathcal{P}}$. SC($m$, $\max$, $\Delta$, T) computes a single $\Delta$-cluster where ${\mathcal{P}}$ has cardinality $m$ and maximises the complexity $\ell$ of $P$.We use such maximum-cardinality clusters in a greedy clustering algorithm. We provide an efficient implementation of SC($\max$, $\ell$, $\Delta$, T) and SC($m$, $\max$, $\Delta$, T) that significantly outperforms previous implementations. We use these functions as a subroutine in a greedy clustering algorithm, which performs well when compared to existing subtrajectory clustering algorithms on real-world data. Finally, we observe that, for fixed $\Delta$ and T, these two functions always output a point on the Pareto front of some bivariate function $\theta(\ell, m)$. We design a new algorithm PSC($\Delta$, T) that in $O( n^2 \log^4 n)$ time computes a $2$-approximation of this Pareto front. This yields a broader set of candidate clusters, with comparable quality. We show that using PSC($\Delta$, T) as a subroutine improves the clustering quality and performance even further."
2503.17372,"There are several notions of duality between lines and points. In this note, it is shown that all these can be studied in a unified way. Most interesting properties are independent of specific choices.It is also shown that either dual mapping can be its own inverse or it can preserve relative order (but not both).Generalisation to higher dimensions is also discussed. An elementary and very intuitive treatment of relationship between arrangements in $d+1$ dimensions and searching for $k$-nearest neighbour in $d$-dimensions is also given."
2503.18508,"Metric embedding is a powerful tool used extensively in mathematics and computer science. We devise a new method of using metric embeddings recursively, which turns out to be particularly effective in $\ell_p$ spaces, $p>2$, yielding state-of-the-art results for Lipschitz decomposition, for Nearest Neighbor Search, and for embedding into $\ell_2$. In a nutshell, our method composes metric embeddings by viewing them as reductions between problems, and thereby obtains a new reduction that is substantially more effective than the known reduction that employs a single embedding. We in fact apply this method recursively, oftentimes using double recursion, which further amplifies the gap from a single embedding."
2503.19093,"Distance geometry explores the properties of distance spaces that can be exactly represented as the pairwise Euclidean distances between points in $\mathbb{R}^d$ ($d \geq 1$), or equivalently, distance spaces that can be isometrically embedded in $\mathbb{R}^d$. In this work, we investigate whether a distance space can be isometrically embedded in $\mathbb{R}^d$ after applying a limited number of modifications. Specifically, we focus on two types of modifications: outlier deletion (removing points) and distance modification (adjusting distances between points). The central problem, Euclidean Embedding Editing (EEE), asks whether an input distance space on $n$ points can be transformed, using at most $k$ modifications, into a space that is isometrically embeddable in $\mathbb{R}^d$.We present several fixed-parameter tractable (FPT) and approximation algorithms for this problem. Our first result is an algorithm that solves EEE in time $(dk)^{\mathcal{O}(d+k)} + n^{\mathcal{O}(1)}$. The core subroutine of this algorithm, which is of independent interest, is a polynomial-time method for compressing the input distance space into an equivalent instance of EEE with $\mathcal{O}((dk)^2)$ points.For the special but important case of EEE where only outlier deletions are allowed, we improve the parameter dependence of the FPT algorithm and obtain a running time of $\min\{(d+3)^k, 2^{d+k}\} \cdot n^{\mathcal{O}(1)}$. Additionally, we provide an FPT-approximation algorithm for this problem, which outputs a set of at most $2 \cdot {\rm OPT}$ outliers in time $2^d \cdot n^{\mathcal{O}(1)}$. This 2-approximation algorithm improves upon the previous $(3+\varepsilon)$-approximation algorithm by Sidiropoulos, Wang, and Wang [SODA '17]. Furthermore, we complement our algorithms with hardness results motivating our choice of parameterizations."
2503.2044,"Given a set of $n$ nonoverlapping circular discs on a plane, we aim to determine possible positions of points (referred to as cameras) that could fully illuminate all the circular discs' boundaries. This work presents a geometric approach for determining feasible camera positions that would provide total illumination of all circular discs. The Laguerre Delaunay triangulation, coupled with the intersection of slabs formed by the boundaries of circular discs, is employed to form the region that satisfies the given conditions. The experiment is conducted using a set of randomly positioned circular discs on a plane. This study has the potential to address the issue of illumination in forests by utilizing a LiDAR camera to determine the possible number and placement of cameras that can effectively illuminate trees within a forest."
2503.21317,"The treatment of breast cancer using radiotherapy involves uncertainties regarding breast positioning. As the studies progress, more is known about the expected breast positioning errors, which are taken into account in the Planning Target Volume (PTV) in the form of the margin around the clinical target volume. However, little is known about the non-rigid deformations of the breast in the course of radiotherapy, which is a non-negligible factor to the treatment.
Purpose: Taking into account such inter-fractional breast deformations would help develop a promising future direction, such as patient-specific adjustable irradiation plannings.
Methods: In this study, we develop a geometric approach to analyze inter-fractional breast deformation throughout the radiotherapy treatment. Our data consists of 3D surface scans of patients acquired during radiotherapy sessions using a handheld scanner. We adapt functional map framework to compute inter-and intra-patient non-rigid correspondences, which are then used to analyze intra-patient changes and inter-patient variability.
Results: The qualitative shape collection analysis highlight deformations in the contralateral breast and armpit areas, along with positioning shifts on the head or abdominal regions. We also perform extrinsic analysis, where we align surface acquisitions of the treated breast with the CT-derived skin surface to assess displacements and volume changes in the treated area. On average, displacements within the treated breast exhibit amplitudes of 1-2 mm across sessions, with higher values observed at the time of the 25 th irradiation session. Volume changes, inferred from surface variations, reached up to 10%, with values ranging between 2% and 5% over the course of treatment.
Conclusions: We propose a comprehensive workflow for analyzing and modeling breast deformations during radiotherapy using surface acquisitions, incorporating a novel inter-collection shape matching approach to model shape variability within a i shared space across multiple patient shape collections. We validate our method using 3D surface data acquired from patients during External Beam Radiotherapy (EBRT) sessions, demonstrating its effectiveness.
The clinical trial data used in this paper is registered under thethis http URLID NCT03801850."
2503.21711,This paper focuses on computing the directional extremal boundary of a union of equal-radius circles. We introduce an efficient algorithm that accurately determines this boundary by analyzing the intersections and dominant relationships among the circles. The algorithm has time complexity of O(n log n).
2503.22449,"A hypergraph $H$ consists of a set $V$ of vertices and a set $E$ of hyperedges that are subsets of $V$. A $t$-tuple of $H$ is a subset of $t$ vertices of $V$. A $t$-tuple $k$-coloring of $H$ is a mapping of its $t$-tuples into $k$ colors. A coloring is called $(t,k,f)$-polychromatic if each hyperedge of $E$ that has at least $f$ vertices contains tuples of all the $k$ colors. Let $f_H(t,k)$ be the minimum $f$ such that $H$ has a $(t,k,f)$-polychromatic coloring. For a family of hypergraphs $\cal{H}$ let $f_{\cal{H}}(t,k)$ be the maximum $f_H(t,k)$ over all hypergraphs $H$ in $\cal{H}$. We present several bounds on $f_{\cal{H}}(t,k)$ for $t\ge 2$.- Let $\cal{H}$ be the family of hypergraphs $H$ that is obtained by taking any set $P$ of points in $\Re^2$, setting $V:=P$ and $E:=\{d\cap P\colon d\text{ is a disk in }\Re^2\}$. We prove that $f_\cal{H}(2,k)\le 3.7^k$, that is, the pairs of points (2-tuples) can be $k$-colored such that any disk containing at least $3.7^k$ points has pairs of all colors.- For the family $\mathcal{H}$ of shrinkable hypergraphs of VC-dimension at most $d$ we prove that $ f_\cal{H}(d{+}1,k) \leq c^k$ for some constant $c=c(d)$. We also prove that every hypergraph with $n$ vertices and with VC-dimension at most $d$ has a $(d{+}1)$-tuple $T$ of depth at least $\frac{n}{c}$, i.e., any hyperedge that contains $T$ also contains $\frac{n}{c}$ other vertices.- For the relationship between $t$-tuple coloring and vertex coloring in any hypergraph $H$ we establish the inequality $\frac{1}{e}\cdot tk^{\frac{1}{t}}\le f_H(t,k)\le f_H(1,tk^{\frac{1}{t}})$. For the special case of $k=2$, we prove that $t+1\le f_H(t,2)\le\max\{f_H(1,2), t+1\}$; this improves upon the previous best known upper bound.- We generalize some of our results to higher dimensions, other shapes, pseudo-disks, and also study the relationship between tuple coloring and epsilon nets."
2503.23025,"While there are software systems that simplify trajectory streams on the fly, few curve simplification algorithms with quality guarantees fit the streaming requirements. We present streaming algorithms for two such problems under the Fréchet distance $d_F$ in $\mathbb{R}^d$ for some constant $d \geq 2$.Consider a polygonal curve $\tau$ in $\mathbb{R}^d$ in a stream. We present a streaming algorithm that, for any $\varepsilon\in (0,1)$ and $\delta > 0$, produces a curve $\sigma$ such that $d_F(\sigma,\tau[v_1,v_i])\le (1+\varepsilon)\delta$ and $|\sigma|\le 2\,\mathrm{opt}-2$, where $\tau[v_1,v_i]$ is the prefix in the stream so far, and $\mathrm{opt} = \min\{|\sigma'|: d_F(\sigma',\tau[v_1,v_i])\le \delta\}$. Let $\alpha = 2(d-1){\lfloor d/2 \rfloor}^2 + d$. The working storage is $O(\varepsilon^{-\alpha})$. Each vertex is processed in $O(\varepsilon^{-\alpha}\log\frac{1}{\varepsilon})$ time for $d \in \{2,3\}$ and $O(\varepsilon^{-\alpha})$ time for $d \geq 4$ . Thus, the whole $\tau$ can be simplified in $O(\varepsilon^{-\alpha}|\tau|\log\frac{1}{\varepsilon})$ time. Ignoring polynomial factors in $1/\varepsilon$, this running time is a factor $|\tau|$ faster than the best static algorithm that offers the same guarantees.We present another streaming algorithm that, for any integer $k \geq 2$ and any $\varepsilon \in (0,\frac{1}{17})$, maintains a curve $\sigma$ such that $|\sigma| \leq 2k-2$ and $d_F(\sigma,\tau[v_1,v_i])\le (1+\varepsilon) \cdot \min\{d_F(\sigma',\tau[v_1,v_i]): |\sigma'| \leq k\}$, where $\tau[v_1,v_i]$ is the prefix in the stream so far. The working storage is $O((k\varepsilon^{-1}+\varepsilon^{-(\alpha+1)})\log \frac{1}{\varepsilon})$. Each vertex is processed in $O(k\varepsilon^{-(\alpha+1)}\log^2\frac{1}{\varepsilon})$ time for $d \in \{2,3\}$ and $O(k\varepsilon^{-(\alpha+1)}\log\frac{1}{\varepsilon})$ time for $d \geq 4$."
2504.00067,"Given $n>0$, let $S\subset [0,1]^2$ be a set of $n$ points, chosen uniformly at random. Let $R\cup B$ be a random partition, or coloring, of $S$ in which each point of $S$ is included in $R$ uniformly at random with probability $1/2$. Corujo et al.~(JOCO 2023) studied the random variable $M(n)$ equal to the number of points of $S$ that are covered by the rectangles of a maximum matching of $S$ using pairwise-disjoint rectangles. Each rectangle is axis-aligned and covers exactly two points of $S$ of the same color. They designed a deterministic algorithm to match points of $S$, and the algorithm was modeled as a discrete stochastic process over a finite set of states. After claiming that the stochastic process is a Markov chain, they proved that almost surely $M(n)\ge 0.83\,n$ for $n$ large enough. The issue is that such a process is not actually a Markov one, as we discuss in this note. We argue this issue, and correct it by obtaining the same result but considering that the stochastic process is not Markov, but satisfies some kind of first-order homogeneity property that allows us to compute its marginal distributions."
2504.02334,"We propose a novel method for determining the radius of a spherical surface based on the distances measured between points on this surface. We consider the most general case of determining the radius when the distances are measured with errors and the sphere has random deviations from its ideal shape. For the solution, we used the minimally necessary four points and an arbitrary N number of points. We provide a new closed form solution for the radius of the sphere through the matrix of pairwise distances. We also determine the standard deviation of the radius estimate caused by measurement errors and deviations of the sphere from its ideal shape. We found optimal configurations of points on the sphere that provide the minimum standard deviation of the radius estimate. This paper describes our solution and provides all the mathematical derivations. We share the implementation of our method as open source code atthis https URL."
2504.02611,"Imprecise measurements of a point set P = (p1, ..., pn) can be modelled by a family of regions F = (R1, ..., Rn), where each imprecise region Ri contains a unique point pi. A retrieval models an accurate measurement by replacing an imprecise region Ri with its corresponding point pi. We construct the convex hull of an imprecise point set in the plane, where regions in F may be retrieved at unit cost. The goal is to determine the cyclic ordering of the convex hull vertices of P as efficiently as possible. Here, efficiency is interpreted in two ways: (i) minimising the number of retrievals, and (ii) computing each retrieval location quickly.Prior works focused on only one of these two aspects: either minimising retrievals or optimising algorithmic runtime. Our contribution is the first to simultaneously achieve both. Let r(F, P) denote the minimal number of retrievals required by any algorithm to determine the convex hull of P for a given instance (F, P). For a family F of n constant-complexity polygons, our main result is a reconstruction algorithm that performs O(r(F, P)) retrievals in O(r(F, P) log^3 n) time.Compared to previous approaches that achieve optimal retrieval counts, we improve the runtime per retrieval by a exponential factor, from polynomial to polylogarithmic. Compared to near-linear time algorithms, we significantly reduce the number of retrievals used, and broaden the input families to include overlapping regions. We further extend our results to simple k-gons and to pairwise disjoint disks with radii in [1,k], where our runtime scales linearly with k."
2504.0265,"We present a SAT framework which allows to investigate properties of simple drawings of the complete graph $K_n$ using the power of AI. In contrast to classic imperative programming, where a program is operated step by step, our framework models mathematical questions as Boolean formulas which are then solved using modern SAT solvers. Our framework for simple drawings is based on a characterization via rotation systems and finite forbidden substructures. We showcase its universality by addressing various open problems, reproving previous computational results and deriving several new computational results. In particular, we test and progress on several unavoidable configurations such as variants of Rafla's conjecture on plane Hamiltonian cycles, Harborth's conjecture on empty triangles, and crossing families for general simple drawings as well as for various subclasses. Moreover, based our computational results we propose some new challenging conjectures."
2504.03558,"We present a fixed-parameter tractable (FPT) algorithm to find a shortest curve that encloses a set of k required objects in the plane while paying a penalty for enclosing unwanted objects.The input is a set of interior-disjoint simple polygons in the plane, where k of the polygons are required to be enclosed and the remaining optional polygons have non-negative penalties. The goal is to find a closed curve that is disjoint from the polygon interiors and encloses the k required polygons, while minimizing the length of the curve plus the penalties of the enclosed optional polygons. If the penalties are high, the output is a shortest curve that separates the required polygons from the others. The problem is NP-hard if k is not fixed, even in very special cases. The runtime of our algorithm is $O(3^kn^3)$, where n is the number of vertices of the input polygons.We extend the result to a graph version of the problem where the input is a connected plane graph with positive edge weights. There are k required faces; the remaining faces are optional and have non-negative penalties. The goal is to find a closed walk in the graph that encloses the k required faces, while minimizing the weight of the walk plus the penalties of the enclosed optional faces. We also consider an inverted version of the problem where the required objects must lie outside the curve. Our algorithms solve some other well-studied problems, such as geometric knapsack."
2504.0361,"In this work we introduce a triangular Delaunay mesh generator that can be trained using reinforcement learning to maximize a given mesh quality metric. Our mesh generator consists of a graph neural network that distributes and modifies vertices, and a standard Delaunay algorithm to triangulate the vertices. We explore various design choices and evaluate our mesh generator on various tasks including mesh generation, mesh improvement, and producing variable resolution meshes. The learned mesh generator outputs meshes that are comparable to those produced by Triangle and DistMesh, two popular Delaunay-based mesh generators."
2504.03865,"Mapper graphs are a widely used tool in topological data analysis and visualization. They can be viewed as discrete approximations of Reeb graphs, offering insight into the shape and connectivity of complex data. Given a high-dimensional point cloud $\mathbb{X}$ equipped with a function $f: \mathbb{X} \to \mathbb{R}$, a mapper graph provides a summary of the topological structure of $\mathbb{X}$ induced by $f$, where each node represents a local neighborhood, and edges connect nodes whose corresponding neighborhoods overlap. Our focus is the interleaving distance for mapper graphs, arising from a discretization of the version for Reeb graphs, which is NP-hard to compute. This distance quantifies the similarity between two mapper graphs by measuring the extent to which they must be ``stretched"" to become comparable. Recent work introduced a loss function that provides an upper bound on the interleaving distance for mapper graphs, which evaluates how far a given assignment is from being a true interleaving. Finding the loss is computationally tractable, offering a practical way to estimate the distance.In this paper, we employ a categorical formulation of mapper graphs and develop the first framework for computing the associated loss function. Since the quality of the bound depends on the chosen assignment, we optimize this loss function by formulating the problem of finding the best assignment as an integer linear programming problem. To evaluate the effectiveness of our optimization, we apply it to small mapper graphs where the interleaving distance is known, demonstrating that the optimized upper bound successfully matches the interleaving distance in these cases. Additionally, we conduct an experiment on the MPEG-7 dataset, computing the pairwise optimal loss on a collection of mapper graphs derived from images and leveraging the distance bound for image classification."
2504.04412,"We give an overview of the 2025 Computational Geometry Challenge targeting the problem Minimum Non-Obtuse Triangulation: Given a planar straight-line graph G in the plane, defined by a set of points in the plane (representing vertices) and a set of non-crossing line segments connecting them (representing edges); the objective is to find a feasible non-obtuse triangulation that uses a minimum number of Steiner points. If no triangulation without obtuse triangles is found, the secondary objective is to minimize the number of obtuse triangles in the triangulation."
2504.05098,"We describe a promising approach to efficiently morph spherical graphs, extending earlier approaches of Awartani and Henderson [Trans. AMS 1987] and Kobourov and Landis [JGAA 2006]. Specifically, we describe two methods to morph shortest-path triangulations of the sphere by moving their vertices along longitudes into the southern hemisphere; we call a triangulation sinkable if such a morph exists. Our first method generalizes a longitudinal shelling construction of Awartani and Henderson; a triangulation is sinkable if a specific orientation of its dual graph is acyclic. We describe a simple polynomial-time algorithm to find a longitudinally shellable rotation of a given spherical triangulation, if one exists; we also construct a spherical triangulation that has no longitudinally shellable rotation. Our second method is based on a linear-programming characterization of sinkability. By identifying its optimal basis, we show that this linear program can be solved in $O(n^{\omega/2})$ time, where $\omega$ is the matrix-multiplication exponent, assuming the underlying linear system is non-singular. Finally, we pose several conjectures and describe experimental results that support them."
2504.05861,"We present new results on $2$- and $3$-hop spanners for geometric intersection graphs. These include improved upper and lower bounds for $2$- and $3$-hop spanners for many geometric intersection graphs in $\mathbb{R}^d$. For example, we show that the intersection graph of $n$ balls in $\mathbb{R}^d$ admits a $2$-hop spanner of size $O^*\left(n^{\frac{3}{2}-\frac{1}{2(2\lfloor d/2\rfloor +1)}}\right)$ and the intersection graph of $n$ fat axis-parallel boxes in $\mathbb{R}^d$ admits a $2$-hop spanner of size $O(n \log^{d+1}n)$.Furthermore, we show that the intersection graph of general semi-algebraic objects in $\mathbb{R}^d$ admits a $3$-hop spanner of size $O^*\left(n^{\frac{3}{2}-\frac{1}{2(2D-1)}}\right)$, where $D$ is a parameter associated with the description complexity of the objects. For such families (or more specifically, for tetrahedra in $\mathbb{R}^3$), we provide a lower bound of $\Omega(n^{\frac{4}{3}})$. For $3$-hop and axis-parallel boxes in $\mathbb{R}^d$, we provide the upper bound $O(n \log ^{d-1}n)$ and lower bound $\Omega\left(n (\frac{\log n}{\log \log n})^{d-2}\right)$."
2504.06079,"For any given metric space, obtaining an offline optimal solution to the classical $k$-server problem can be reduced to solving a minimum-cost partial bipartite matching between two point sets $A$ and $B$ within that metric space.For $d$-dimensional $\ell_p$ metric space, we present an $\tilde{O}(\min\{nk, n^{2-\frac{1}{2d+1}}\log \Delta\}\cdot \Phi(n))$ time algorithm for solving this instance of minimum-cost partial bipartite matching; here, $\Delta$ represents the spread of the point set, and $\Phi(n)$ is the query/update time of a $d$-dimensional dynamic weighted nearest neighbor data structure. Our algorithm improves upon prior algorithms that require at least $\Omega(nk\Phi(n))$ time. The design of minimum-cost (partial) bipartite matching algorithms that make sub-quadratic queries to a weighted nearest-neighbor data structure, even for bounded spread instances, is a major open problem in computational geometry. We resolve this problem at least for the instances that are generated by the offline version of the $k$-server problem.Our algorithm employs a hierarchical partitioning approach, dividing the points of $A\cup B$ into rectangles. It maintains a minimum-cost partial matching where any point $b \in B$ is either matched to a point $a\in A$ or to the boundary of the rectangle it is located in. The algorithm involves iteratively merging pairs of rectangles by erasing the shared boundary between them and recomputing the minimum-cost partial matching. This continues until all boundaries are erased and we obtain the desired minimum-cost partial matching of $A$ and $B$. We exploit geometry in our analysis to show that each point participates in only $\tilde{O}(n^{1-\frac{1}{2d+1}}\log \Delta)$ number of augmenting paths, leading to a total execution time of $\tilde{O}(n^{2-\frac{1}{2d+1}}\Phi(n)\log \Delta)$."
2504.06376,"We introduce a quantum algorithm design paradigm called combine and conquer, which is a quantum version of the ""marriage-before-conquest"" technique of Kirkpatrick and Seidel. In a quantum combine-and-conquer algorithm, one performs the essential computation of the combine step of a quantum divide-and-conquer algorithm prior to the conquer step while avoiding recursion. This model is better suited for the quantum setting, due to its non-recursive nature. We show the utility of this approach by providing quantum algorithms for 2D maxima set and convex hull problems for sorted point sets running in $\tilde{O}(\sqrt{nh})$ time, w.h.p., where $h$ is the size of the output."
2504.06503,"Let G = (V, E) be a directed graph on n vertices where each vertex has out-degree k. We say that G is kNN-realizable in d-dimensional Euclidean space if there exists a point set P = {p1, p2, ..., pn} in R^d along with a one-to-one mapping phi: V -> P such that for any u, v in V, u is an out-neighbor of v in G if and only if phi(u) is one of the k nearest neighbors of phi(v); we call the map phi a ""kNN realization"" of G in R^d. The kNN realization problem, which aims to compute such a mapping in R^d, is known to be NP-hard already for d = 2 and k = 1 (Eades and Whitesides, Theoretical Computer Science, 1996), and to the best of our knowledge, has not been studied in dimension d = 1. The main results of this paper are the following: (1) For any fixed dimension d >= 2, we can efficiently compute an embedding realizing at least a (1 - epsilon) fraction of G's edges, or conclude that G is not kNN-realizable in R^d. (2) For d = 1, we can decide in O(kn) time whether G is kNN-realizable and, if so, compute a realization in O(n^{2.5} * polylog(n)) time."
2504.0696,"Given a set $S$ of $n$ colored sites, each $s\in S$ associated with a distance-to-site function $\delta_s \colon \mathbb{R}^2 \to \mathbb{R}$, we consider two distance-to-color functions for each color: one takes the minimum of $\delta_s$ for sites $s\in S$ in that color and the other takes the maximum. These two sets of distance functions induce two families of higher-order Voronoi diagrams for colors in the plane, namely, the minimal and maximal order-$k$ color Voronoi diagrams, which include various well-studied Voronoi diagrams as special cases. In this paper, we derive an exact upper bound $4k(n-k)-2n$ on the total number of vertices in both the minimal and maximal order-$k$ color diagrams for a wide class of distance functions $\delta_s$ that satisfy certain conditions, including the case of point sites $S$ under convex distance functions and the $L_p$ metric for any $1\leq p \leq\infty$. For the $L_1$ (or, $L_\infty$) metric, and other convex polygonal metrics, we show that the order-$k$ minimal diagram of point sites has $O(\min\{k(n-k), (n-k)^2\})$ complexity, while its maximal counterpart has $O(\min\{k(n-k), k^2\})$ complexity. To obtain these combinatorial results, we extend the Clarkson--Shor framework to colored objects, and demonstrate its application to several fundamental geometric structures, including higher-order color Voronoi diagrams, colored $j$-facets, and levels in the arrangements of piecewise linear/algebraic curves/surfaces. We also present an iterative approach to compute higher-order color Voronoi diagrams."
2504.07545,"Inspired by the classical fractional cascading technique, we introduce new techniques to speed up the following type of iterated search in 3D: The input is a graph $\mathbf{G}$ with bounded degree together with a set $H_v$ of 3D hyperplanes associated with every vertex of $v$ of $\mathbf{G}$. The goal is to store the input such that given a query point $q\in \mathbb{R}^3$ and a connected subgraph $\mathbf{H}\subset \mathbf{G}$, we can decide if $q$ is below or above the lower envelope of $H_v$ for every $v\in \mathbf{H}$. We show that using linear space, it is possible to answer queries in roughly $O(\log n + |\mathbf{H}|\sqrt{\log n})$ time which improves trivial bound of $O(|\mathbf{H}|\log n)$ obtained by using planar point location data structures. Our data structure can in fact answer more general queries (it combines with shallow cuttings) and it even works when $\mathbf{H}$ is given one vertex at a time. We show that this has a number of new applications and in particular, we give improved solutions to a set of natural data structure problems that up to our knowledge had not seen any improvements.We believe this is a very surprising result because obtaining similar results for the planar point location problem was known to be impossible."
2504.08847,"Lattice structures, distinguished by their customizable geometries at the microscale and outstanding mechanical performance, have found widespread application across various industries. One fundamental process in their design and manufacturing is constructing boundary representation (B-rep) models, which are essential for running advanced applications like simulation, optimization, and process planning. However, this construction process presents significant challenges due to the high complexity of lattice structures, particularly in generating nodal shapes where robustness and smoothness issues can arise from the complex intersections between struts. To address these challenges, this paper proposes a novel approach for lattice structure construction by cutting struts and filling void regions with subdivisional nodal shapes. Inspired by soap films, the method generates smooth, shape-preserving control meshes using Laplacian fairing and subdivides them through the point-normal Loop (PN-Loop) subdivision scheme to obtain subdivisional nodal shapes. The proposed method ensures robust model construction with reduced shape deviations, enhanced surface fairness, and smooth transitions between subdivisional nodal shapes and retained struts. The effectiveness of the method has been demonstrated by a series of examples and comparisons. The code will be open-sourced upon publication."
2504.09174,"Topological Data Analysis (TDA) combines computational topology and data science to extract and analyze intrinsic topological and geometric structures in data set in a metric space. While the persistent homology (PH), a widely used tool in TDA, which tracks the lifespan information of topological features through a filtration process, has shown its effectiveness in applications,it is inherently limited in homotopy invariants and overlooks finer geometric and combinatorial details. To bridge this gap, we introduce two novel commutative algebra-based frameworks which extend beyond homology by incorporating tools from computational commutative algebra : (1) \emph{the persistent ideals} derived from the decomposition of algebraic objects associated to simplicial complexes, like those in theory of edge ideals and Stanley--Reisner ideals, which will provide new commutative algebra-based barcodes and offer a richer characterization of topological and geometric structures in filtrations.(2)\emph{persistent chain complex of free modules} associated with traditional persistent simplicial complex by labelling each chain in the chain complex of the persistent simplicial complex with elements in a commutative ring, which will enable us to detect local information of the topology via some pure algebraic operations. \emph{Crucially, both of the two newly-established framework can recover topological information got from conventional PH and will give us more information.} Therefore, they provide new insights in computational topology, computational algebra and data science."
2504.09489,"We consider the problem of packing a large square with nonoverlapping unit squares. Let $W(x)$ be the minimum wasted area when a large square of side length $x$ is packed with unit squares. In Roth and Vaughan's paper that proves the lower bound $W(x) \notin o(x^{1/2})$, a good square is defined to be a square with inclination at most $10^{-10}$ with respect to the large square. In this article, we prove that in calculating the asymptotic growth of the wasted space, it suffices to only consider packings with only good squares. This allows the lower bound proof in Roth and Vaughan's paper to be simplified by not having to handle bad squares."
2504.09615,"We describe a framework that unifies the two types of polynomials introduced respectively by Bacher and Mouton and by Rutschmann and Wettstein to analyze the number of triangulations of point sets. Using this insight, we generalize the triangulation polynomials of chains to a wider class of near-edges, enabling efficient computation of the number of triangulations of certain families of point sets. We use the framework to try to improve the result in Rutschmann and Wettstein without success, suggesting that their result is close to optimal."
2504.09733,"Accurately estimating decision boundaries in black box systems is critical when ensuring safety, quality, and feasibility in real-world applications. However, existing methods iteratively refine boundary estimates by sampling in regions of uncertainty, without providing guarantees on the closeness to the decision boundary and also result in unnecessary exploration that is especially disadvantageous when evaluations are costly. This paper presents $\varepsilon$-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample efficient and function-agnostic algorithm that leverages the intermediate value theorem to estimate the location of the decision boundary of a black box binary classifier within a user-specified $\varepsilon$-neighborhood. To demonstrate applicability, a case study is presented of an electric grid stability problem with uncertain renewable power injection. Evaluations are conducted on three test functions, where it is seen that the EDGE algorithm demonstrates superior sample efficiency and better boundary approximation than adaptive sampling techniques and grid-based searches."
2504.10657,"The traveling salesman problem (TSP) famously asks for a shortest tour that a salesperson can take to visit a given set of cities in any order. In this paper, we ask how much faster $k \ge 2$ salespeople can visit the cities if they divide the task among themselves. We show that, in the two-dimensional Euclidean setting, two salespeople can always achieve a speedup of at least $\frac12 + \frac1\pi \approx 0.818$, for any given input, and there are inputs where they cannot do better. We also give (non-matching) upper and lower bounds for $k \geq 3$."
2504.10859,"The problem of finding a path between two points while avoiding obstacles is critical in robotic path planning. We focus on the feasibility problem: determining whether such a path exists. We model the robot as a query-specific rectangular object capable of moving parallel to its sides. The obstacles are axis-aligned, rectangular, and may overlap. Most previous works only consider nondisjoint rectangular objects and point-sized or statically sized robots. Our approach introduces a novel technique leveraging generalized Gabriel graphs and constructs a data structure to facilitate online queries regarding path feasibility with varying robot sizes in sublinear time. To efficiently handle feasibility queries, we propose an online algorithm utilizing sweep line to construct a generalized Gabriel graph under the $L_\infty$ norm, capturing key gap constraints between obstacles. We utilize a persistent disjoint-set union data structure to efficiently determine feasibility queries in $\mathcal{O}(\log n)$ time and $\mathcal{O}(n)$ total space."
2504.11203,"Vineyards are a common way to study persistence diagrams of a data set which is changing, as strong stability means that it is possible to pair points in ``nearby'' persistence diagrams, yielding a family of point sets which connect into curves when stacked. Recent work has also studied monodromy in the persistent homology transform, demonstrating some interesting connections between an input shape and monodromy in the persistent homology transform for 0-dimensional homology embedded in $\mathbb{R}^2$. In this work, we re-characterize monodromy in terms of periodicity of the associated vineyard of persistence diagrams.We construct a family of objects in any dimension which have non-trivial monodromy for $l$-persistence of any periodicity and for any $l$. More generally we prove that any knot or link can appear as a vineyard for a shape in $\mathbb{R}^d$, with $d\geq 3$. This shows an intriguing and, to the best of our knowledge, previously unknown connection between knots and persistence vineyards. In particular this shows that vineyards are topologically as rich as one could possibly hope."
2504.12022,"We present polynomial-time approximation schemes based on local search} technique for both geometric (discrete) independent set (\mdis) and geometric (discrete) dominating set (\mdds) problems, where the objects are arbitrary radii disks and arbitrary side length axis-parallel squares. Further, we show that the \mdds~problem is \apx-hard for various shapes in the plane. Finally, we prove that both \mdis~and \mdds~problems are \np-hard for unit disks intersecting a horizontal line and axis-parallel unit squares intersecting a straight line with slope $-1$."
2504.12948,"Efficiently solving the Shortest Vector Problem (SVP) in two-dimensional lattices holds practical significance in cryptography and computational geometry. While simpler than its high-dimensional counterpart, two-dimensional SVP motivates scalable solutions for high-dimensional lattices and benefits applications like sequence cipher cryptanalysis involving large integers. In this work, we first propose a novel definition of reduced bases and develop an efficient adaptive lattice reduction algorithm \textbf{CrossEuc} that strategically applies the Euclidean algorithm across dimensions. Building on this framework, we introduce \textbf{HVec}, a vectorized generalization of the Half-GCD algorithm originally defined for integers, which can efficiently halve the bit-length of two vectors and may have independent interest. By iteratively invoking \textbf{HVec}, our optimized algorithm \textbf{HVecSBP} achieves a reduced basis in $O(\log n M(n) )$ time for arbitrary input bases with bit-length $n$, where $M(n)$ denotes the cost of multiplying two $n$-bit integers. Compared to existing algorithms, our design is applicable to general forms of input lattices, eliminating the cost of pre-converting input bases to Hermite Normal Form (HNF). The comprehensive experimental results demonstrate that for the input lattice bases in HNF, the optimized algorithm \textbf{HVecSBP} achieves at least a $13.5\times$ efficiency improvement compared to existing methods. For general-form input lattice bases, converting them to HNF before applying \textbf{HVecSBP} offers only marginal advantages in extreme cases where the two basis vectors are nearly degenerate. However, as the linear dependency between input basis vectors decreases, directly employing \textbf{HVecSBP} yields increasingly significant efficiency gains, outperforming hybrid approaches that rely on prior \textbf{HNF} conversion."
2504.13704,"Let $P$ be a polygon with $k$ vertices. Let $R$ and $B$ be two simple, interior disjoint curves on the boundary of $P$, with $n$ and $m$ vertices. We show how to compute the Fréchet distance between $R$ and $B$ using the geodesic $L_1$-distance in $P$ in $\mathcal{O}(k \log nm + (n+m) (\log^2 nm \log k + \log^4 nm))$ time."
2504.14743,"The medial axis of a smoothly embedded surface in $\mathbb{R}^3$ consists of all points for which the Euclidean distance function on the surface has at least two minima. We generalize this notion to the mid-sphere axis, which consists of all points for which the Euclidean distance function has two interchanging saddles that swap their partners in the pairing by persistent homology. It offers a discrete-algebraic multi-scale approach to computing ridge-like structures on the surface. As a proof of concept, an algorithm that computes stair-case approximations of the mid-sphere axis is provided."
2504.15292,"In this paper, we study estimators for geometric optimization problems in the sublinear geometric model. In this model, we have oracle access to a point set with size $n$ in a discrete space $[\Delta]^d$, where queries can be made to an oracle that responds to orthogonal range counting requests. The query complexity of an optimization problem is measured by the number of oracle queries required to compute an estimator for the problem. We investigate two problems in this framework, the Euclidean Minimum Spanning Tree (MST) and Earth Mover Distance (EMD). For EMD, we show the existence of an estimator that approximates the cost of EMD with $O(\log \Delta)$-relative error and $O(\frac{n\Delta}{s^{1+1/d}})$-additive error using $O(s\polylog \Delta)$ range counting queries for any parameter $s$ with $1\leq s \leq n$. Moreover, we prove that this bound is tight. For MST, we demonstrate that the weight of MST can be estimated within a factor of $(1 \pm \eps)$ using $\tilde{O}(\sqrt{n})$ range counting queries."
2504.16642,"Geometric hitting set problems, in which we seek a smallest set of points that collectively hit a given set of ranges, are ubiquitous in computational geometry. Most often, the set is discrete and is given explicitly. We propose new variants of these problems, dealing with continuous families of convex polyhedra, and show that they capture decision versions of the two-level finite adaptability problem in robust optimization. We show that these problems can be solved in strongly polynomial time when the size of the hitting/covering set and the dimension of the polyhedra and the parameter space are constant. We also show that the hitting set problem can be solved in strongly quadratic time for one-parameter families of convex polyhedra in constant dimension. This leads to new tractability results for finite adaptability that are the first ones with so-called left-hand-side uncertainty, where the underlying problem is non-linear."
2504.16762,"Tilt models offer intuitive and clean definitions of complex systems in which particles are influenced by global control commands. Despite a wide range of applications, there has been almost no theoretical investigation into the associated issues of filling and draining geometric environments. This is partly because a globally controlled system (i.e., passive matter) exhibits highly complex behavior that cannot be locally restricted. Thus, there is a strong need for theoretical studies that investigate these models both (1) in terms of relative power to each other, and (2) from a complexity theory perspective. In this work, we provide (1) general tools for comparing and contrasting different models of global control, and (2) both complexity and algorithmic results on filling and draining."
2504.17289,"Given two points in the plane, and a set of ""obstacles"" given as curves through the plane with assigned weights, we consider the point-separation problem, which asks for the minimum-weight subset of the obstacles separating the two points. A few computational models for this problem have been previously studied. We give a unified approach to this problem in all models via a reduction to a particular shortest-path problem, and obtain improved running times in essentially all cases. In addition, we also give fine-grained lower bounds for many cases."
2504.17342,"The Fréchet distance is a distance measure between trajectories in $\Bbb{R}^d$ or walks in a graph $G$. Given constant-time shortest path queries, the Discrete Fréchet distance $D_G(P, Q)$ between two walks $P$ and $Q$ can be computed in $O(|P| \cdot |Q|)$ time using a dynamic program. Driemel, van der Hoog, and Rotenberg [SoCG'22] show that for weighted planar graphs this approach is likely tight, as there can be no strongly-subquadratic algorithm to compute a $1.01$-approximation of $D_G(P, Q)$ unless the Orthogonal Vector Hypothesis (OVH) fails.Such quadratic-time conditional lower bounds are common to many Fréchet distance variants. However, they can be circumvented by assuming that the input comes from some well-behaved class: There exist $(1+\varepsilon)$-approximations, both in weighted graphs and in $\Bbb{R}^d$, that take near-linear time for $c$-packed or $\kappa$-straight walks in the graph. In $\Bbb{R}^d$ there also exists a near-linear time algorithm to compute the Fréchet distance whenever all input edges are long compared to the distance.We consider computing the Fréchet distance in unweighted planar graphs. We show that there exist no strongly-subquadratic $1.25$-approximations of the discrete Fréchet distance between two disjoint simple paths in an unweighted planar graph in strongly subquadratic time, unless OVH fails. This improves the previous lower bound, both in terms of generality and approximation factor. We subsequently show that adding graph structure circumvents this lower bound: If the graph is a regular tiling with unit-weighted edges, then there exists an $\tilde{O}((|P| + |Q|)^{1.5})$-time algorithm to compute $D_G(P, Q)$. Our result has natural implications in the plane, as it allows us to define a new class of well-behaved curves that facilitate $(1+\varepsilon)$-approximations of their discrete Fréchet distance in subquadratic time."
2504.17381,"Many application areas collect unstructured trajectory data. In subtrajectory clustering, one is interested to find patterns in this data using a hybrid combination of segmentation and clustering. We analyze two variants of this problem based on the well-known \textsc{SetCover} and \textsc{CoverageMaximization} problems. In both variants the set system is induced by metric balls under the Fréchet distance centered at polygonal curves. Our algorithms focus on improving the running time of the update step of the generic greedy algorithm by means of a careful combination of sweeps through a candidate space. In the first variant, we are given a polygonal curve $P$ of complexity $n$, distance threshold $\Delta$ and complexity bound $\ell$ and the goal is to identify a minimum-size set of center curves $\mathcal{C}$, where each center curve is of complexity at most $\ell$ and every point $p$ on $P$ is covered. A point $p$ on $P$ is covered if it is part of a subtrajectory $\pi_p$ of $P$ such that there is a center $c\in\mathcal{C}$ whose Fréchet distance to $\pi_p$ is at most $\Delta$. We present an approximation algorithm for this problem with a running time of $O((n^2\ell + \sqrt{k_\Delta}n^{5/2})\log^2n)$, where $k_\Delta$ is the size of an optimal solution. The algorithm gives a bicriterial approximation guarantee that relaxes the Fréchet distance threshold by a constant factor and the size of the solution by a factor of $O(\log n)$. The second problem variant asks for the maximum fraction of the input curve $P$ that can be covered using $k$ center curves, where $k\leq n$ is a parameter to the algorithm. Here, we show that our techniques lead to an algorithm with a running time of $O((k+\ell)n^2\log^2 n)$ and similar approximation guarantees. Note that in both algorithms $k,k_\Delta\in O(n)$ and hence the running time is cubic, or better if $k\ll n$."
2504.17955,"We introduce and study the Marco Polo problem, which is a combinatorial approach to geometric localization. In this problem, we are told there are one or more points of interest (POIs) within distance $n$ of the origin that we wish to localize. Given a mobile search point, $\Delta$, that is initially at the origin, a localization algorithm is a strategy to move $\Delta$ to be within a distance of $1$ of a POI. In the combinatorial localization problem we study, the only tool we can use is reminiscent of the children's game, ""Marco Polo,"" in that $\Delta$ can issue a probe signal out a specified distance, $d$, and the search algorithm learns whether or not there is a POI within distance $d$ of $\Delta$. For example, we could imagine that POIs are one or more hikers lost in a forest and we need to design a search-and-rescue (SAR) strategy to find them using radio signal probes to a response device that hikers carry. Unlike other known localization algorithms, probe responses do not inform our search algorithm of the direction or distance to a POI. The optimization problem is to minimize the number of probes and/or POI responses, as well as possibly minimizing the distance traveled by $\Delta$. We describe a number of efficient combinatorial Marco Polo localization strategies and we analyze each one in terms of the size, $n$, of the search domain. Moreover, we derive strong bounds for the constant factors for the search costs for our algorithms, which in some cases involve computer-assisted proofs. We also show how to extend these strategies to find all POIs using a simple, memoryless search algorithm, traveling a distance that is $\mathcal{O}(\log{k})$-competitive with the optimal traveling salesperson (TSP) tour for $k$ POIs."
2504.18178,"We study the smallest intersecting and enclosing ball problems in Euclidean spaces for input objects that are compact and convex. They link and unify many problems in computational geometry and machine learning. We show that both problems can be modeled as zero-sum games, and propose an approximation algorithm for the former. Specifically, the algorithm produces the first results in high-dimensional spaces for various input objects such as convex polytopes, balls, ellipsoids, etc."
2504.18352,"Given two convex polygons $P$ and $Q$ with $n$ and $m$ edges, the maximum overlap problem is to find a translation of $P$ that maximizes the area of its intersection with $Q$. We give the first randomized algorithm for this problem with linear running time. Our result improves the previous two-and-a-half-decades-old algorithm by de Berg, Cheong, Devillers, van Kreveld, and Teillaud (1998), which ran in $O((n+m)\log(n+m))$ time, as well as multiple recent algorithms given for special cases of the problem."
2504.21329,"Reeb graphs are simple topological descriptors with applications in many areas like topological data analysis and computational geometry. Despite their prevalence, visualization of Reeb graphs has received less attention. In this paper, we bridge an essential gap in the literature by exploring the complexity of drawing Reeb graphs. Specifically, we demonstrate that Reeb graph crossing number minimization is NP-hard, both for straight-lined and curved edges. On the other hand, we identify specific classes of Reeb graphs, namely paths and caterpillars, for which crossing-free drawings exist. We also give an optimal algorithm for drawing cycle-shaped Reeb graphs with the least number of crossings and provide initial observations on the complexities of drawing multi-cycle Reeb graphs. We hope that this work establishes the foundation for an understanding of the graph drawing challenges inherent in Reeb graph visualization and paves the way for future work in this area."
2505.00706,"Efficient methods to determine the relative position of two conics are of great interest for applications in robotics, computer animation, CAGD, computational physics, and other areas. We present a method to obtain the relative position of a parabola or a hyperbola, and a coplanar ellipse, directly from the coefficients of their implicit equations, even if they are not given in canonical form, and avoiding the computation of the corresponding intersection points (and their characteristics)."
2505.01194,"For a planar point set $P$, its convex hull is the smallest convex polygon that encloses all points in $P$. The construction of the convex hull from an array $I_P$ containing $P$ is a fundamental problem in computational geometry. By sorting $I_P$ in lexicographical order, one can construct the convex hull of $P$ in $O(n \log n)$ time which is worst-case optimal. Standard worst-case analysis, however, has been criticized as overly coarse or pessimistic, and researchers search for more refined analyses.Universal analysis provides an even stronger guarantee. It fixes a point set $P$ and considers the maximum running time across all permutations $I_P$ of $P$. Afshani, Barbay, Chan [FOCS'07] prove that the convex hull construction algorithm by Kirkpatrick, McQueen, and Seidel is universally optimal. Their proof restricts the model of computation to any algebraic decision tree model where the test functions have at most constant degree and at most a constant number of arguments. They rely upon involved algebraic arguments to construct a lower bound for each point set $P$ that matches the universal running time of [SICOMP'86].We provide a different proof of universal optimality. Instead of restricting the computational model, we further specify the output. We require as output (1) the convex hull, and (2) for each internal point of $P$ a witness for it being internal. Our argument is shorter, perhaps simpler, and applicable in more general models of computation."
2505.0148,"In this article we describe a program -- called planar_draw -- to draw maps on oriented surfaces in the plane. The drawings are coded as tikzfiles that can easily be manipulated and used in latex documents. Next to plane maps -- a case for which already several programs exist -- the program allowsto draw maps of genus at least one inside a fundamental polygon or with non-contractible cycles displayed as disjoint cycles that have to be identified.Several options allow to tailor the output for individual needs -- e.g.\ by forcing some edges to be completely inside the fundamental polygon. Incombination with a program embedding graphs, the tool can also be used for graphs that do not already come with an embedding in an orientable surface."
2505.01554,"Nonograms are a popular type of puzzle, where an arrangement of curves in the plane (in the classic version, a rectangular grid) is given together with a series of hints, indicating which cells of the subdivision are to be colored. The colored cells yield an image. Curved nonograms use a curve arrangement rather than a grid, leading to a closer approximation of an arbitrary solution image. While there is a considerable amount of previous work on the natural question of the hardness of solving a classic nonogram, research on curved nonograms has so far focused on their creation, which is already highly non-trivial. We address this gap by providing algorithmic and hardness results for curved nonograms of varying complexity."
2505.02373,"Given an $x$-monotone polygonal chain $T$ with $n$ vertices, and an integer $k$, we consider the problem of finding the lowest horizontal line $L$ lying above $T$ with $k$ point guards lying on $L$, so that every point on the chain is \emph{visible} from some guard. A natural optimization is to minimize the $y$-coordinate of $L$. We present an algorithm for finding the optimal placements of $L$ and $k$ point guards for $T$ in $O(k^2\lambda_{k-1}(n)\log n)$ time for even numbers $k\ge 2$, and in $O(k^2\lambda_{k-2}(n)\log n)$ time for odd numbers $k \ge 3$, where $\lambda_{s}(n)$ is the length of the longest $(n,s)$-Davenport-Schinzel sequence. We also study a variant with an additional requirement that $T$ is partitioned into $k$ subchains, each subchain is paired with exactly one guard, and every point on a subchain is visible from its paired guard. When $L$ is fixed, we can place the minimum number of guards in $O(n)$ time. When the number $k$ of guards is fixed, we can find an optimal placement of $L$ with $k$ point guards lying on $L$ in $O(kn)$ time."
2505.02472,"We present algorithms to find the minimum radius sphere that intersects every trajectory in a set of $n$ trajectories composed of at most $k$ line segments each. When $k=1$, we can reduce the problem to the LP-type framework to achieve a linear time complexity. For $k \geq 4$ we provide a trajectory configuration with unbounded LP-type complexity, but also present an almost $O\left((nk)^2\log n\right)$ algorithm through the farthest line segment Voronoi diagrams. If we tolerate a relative approximation, we can reduce to time near-linear in $n$."
2505.04617,"Given two points $p, q \in \mathbb R^d$, we say that $p$ dominates $q$ and write $p \succ q$ if each coordinate of $p$ is larger than the corresponding coordinate of $q$. That is, if $p = (p^{(1)}, p^{(2)}, \ldots, p^{(d)})$ and $q = (q^{(1)}, q^{(2)}, \ldots, q^{(d)})$, $p \succ q$ if and only if $p^{(i)} > q^{(i)}$ for all $1 \le i \le d$.For example, $p$ and $q$ could represent various ratings for $2$ restaurants, based on different metrics like taste, affordability, ratings on different platforms, et cetera. $p \succ q$ then means that the first restaurant outperformed the second on each metric.Given a list of restaurants and their rating, we solve the problem of determining, for each restaurant, the closest restaurant to it that dominates it. We improve upon the algorithm under some assumptions towards the end."
2505.05639,"This paper introduces a method to synthesize a 3D tensor field within a constrained geometric domain represented as a tetrahedral mesh. Whereas previous techniques optimize for isotropic fields, we focus on anisotropic tensor fields that are smooth and aligned with the domain boundary or user guidance. The key ingredient of our method is a novel computational design framework, built on top of the symmetric orthogonally decomposable (odeco) tensor representation, to optimize the stretching ratios and orientations for each tensor in the domain. In contrast to past techniques designed only for isotropic tensors, we demonstrate the efficacy of our approach in generating smooth volumetric tensor fields with high anisotropy and shape conformity, especially for the domain with complex shapes. We apply these anisotropic tensor fields to various applications, such as anisotropic meshing, structural mechanics, and fabrication."
2505.0609,"We present a data-structure for orthogonal range searching for random points in the plane. The new data-structure uses (in expectation) $O\bigl(n \log n ( \log \log n)^2 \bigr)$ space, and answers emptiness queries in constant time. As a building block, we construct a data-structure of expected linear size, that can answer predecessor/rank queries, in constant time, for random numbers sampled uniformly from $[0,1]$.While the basic idea we use is known [Dev89], we believe our results are still interesting."
2505.06884,"The $c$-packedness property, proposed in 2010, is a geometric property that captures the spatial distribution of a set of edges. Despite the recent interest in $c$-packedness, its utility has so far been limited to Fréchet distance problems. An open problem is whether a wider variety of algorithmic and data structure problems can be solved efficiently under the $c$-packedness assumption, and more specifically, on $c$-packed graphs.In this paper, we prove two fundamental properties of $c$-packed graphs: that there exists a linear-size well-separated pair decomposition under the graph metric, and there exists a constant size balanced separator. We then apply these fundamental properties to obtain a small tree cover for the metric space and distance oracles under the shortest path metric. In particular, we obtain a tree cover of constant size, an exact distance oracle of near-linear size and an approximate distance oracle of linear size."
2505.07012,"Hand shadow art is a captivating art form, creatively using hand shadows to reproduce expressive shapes on the wall. In this work, we study an inverse problem: given a target shape, find the poses of left and right hands that together best produce a shadow resembling the input. This problem is nontrivial, since the design space of 3D hand poses is huge while being restrictive due to anatomical constraints. Also, we need to attend to the input's shape and crucial features, though the input is colorless and textureless. To meet these challenges, we design Hand-Shadow Poser, a three-stage pipeline, to decouple the anatomical constraints (by hand) and semantic constraints (by shadow shape): (i) a generative hand assignment module to explore diverse but reasonable left/right-hand shape hypotheses; (ii) a generalized hand-shadow alignment module to infer coarse hand poses with a similarity-driven strategy for selecting hypotheses; and (iii) a shadow-feature-aware refinement module to optimize the hand poses for physical plausibility and shadow feature preservation. Further, we design our pipeline to be trainable on generic public hand data, thus avoiding the need for any specialized training dataset. For method validation, we build a benchmark of 210 diverse shadow shapes of varying complexity and a comprehensive set of metrics, including a novel DINOv2-based evaluation metric. Through extensive comparisons with multiple baselines and user studies, our approach is demonstrated to effectively generate bimanual hand poses for a large variety of hand shapes for over 85% of the benchmark cases."
2505.07147,"We prove that, for any two polyhedral manifolds $\mathcal P, \mathcal Q$, there is a polyhedral manifold $\mathcal I$ such that $\mathcal P, \mathcal I$ share a common unfolding and $\mathcal I,\mathcal Q$ share a common unfolding. In other words, we can unfold $\mathcal P$, refold (glue) that unfolding into $\mathcal I$, unfold $\mathcal I$, and then refold into $\mathcal Q$. Furthermore, if $\mathcal P, \mathcal Q$ have no boundary and can be embedded in 3D (without self-intersection), then so does $\mathcal I$. These results generalize to $n$ given manifolds $\mathcal P_1, \mathcal P_2, \dots, \mathcal P_n$; they all have a common unfolding with the same intermediate manifold $\mathcal I$. Allowing more than two unfold/refold steps, we obtain stronger results for two special cases: for doubly covered convex planar polygons, we achieve that all intermediate polyhedra are planar; and for tree-shaped polycubes, we achieve that all intermediate polyhedra are tree-shaped polycubes."
2505.08093,"This paper presents a novel gradient-informed slicing method for functionally graded additive manufacturing (FGM) that overcomes the limitations of conventional toolpath planning approaches, which struggle to produce truly continuous gradients. By integrating multi-material gradients into the toolpath generation process, our method enables the fabrication of FGMs with complex gradients that vary seamlessly in any direction. We leverage OpenVCAD's implicit representation of geometry and material fields to directly extract iso-contours, enabling accurate, controlled gradient toolpaths. Two novel strategies are introduced to integrate these gradients into the toolpath planning process. The first strategy maintains traditional perimeter, skin, and infill structures subdivided by mixture ratios, with automated 'zippering' to mitigate stress concentrations. The second strategy fills iso-contoured regions densely, printing directly against gradients to eliminate purging and reduce waste. Both strategies accommodate gradually changing printing parameters, such as mixed filament ratios, toolhead switching, and variable nozzle temperatures for foaming materials. This capability allows for controlled variation of composition, density, and other properties within a single build, expanding the design space for functionally graded parts. Experimental results demonstrate the fabrication of high-quality FGMs with complex, multi-axis gradients, highlighting the versatility of our method. We showcase the successful implementation of both strategies on a range of geometries and material combinations, demonstrating the potential of our approach to produce intricate and functional FGMs. This work provides a robust, open-source, and automated framework for designing and fabricating advanced FGMs, accelerating research in multi-material additive manufacturing."
2505.08957,"For two d-dimensional point sets A, B of size up to n, the Chamfer distance from A to B is defined as CH(A,B) = \sum_{a \in A} \min_{b \in B} \|a-b\|. The Chamfer distance is a widely used measure for quantifying dissimilarity between sets of points, used in many machine learning and computer vision applications. A recent work of Bakshi et al, NeuriPS'23, gave the first near-linear time (1+eps)-approximate algorithm, with a running time of O(ndlog(n)/eps^2). In this paper we improve the running time further, to O(nd(loglog(n)+log(1/eps))/eps^2). When eps is a constant, this reduces the gap between the upper bound and the trivial Omega(dn) lower bound significantly, from O(log n) to O(loglog n)."
2505.09046,"The Hausdorff distance is a metric commonly used to compute the set similarity of geometric sets.For sets containing a total of $n$ points, the exact distance can be computed naïvely in $O(n^2)$ time.In this paper, we show how to preprocess point sets individually so that the Hausdorff distance of any pair can then be approximated in linear time.We assume that the metric is doubling.The preprocessing time for each set is $O(n\log \Delta)$ where $\Delta$ is the ratio of the largest to smallest pairwise distances of the input.In theory, this can be reduced to $O(n\log n)$ time using a much more complicated algorithm.We compute $(1+\varepsilon)$-approximate Hausdorff distance in $(2 + \frac{1}{\varepsilon})^{O(d)}n$ time in a metric space with doubling dimension $d$.The $k$-partial Hausdorff distance ignores $k$ outliers to increase stability.Additionally, we give a linear-time algorithm to compute directed $k$-partial Hausdorff distance for all values of $k$ at once with no change to the preprocessing."
2505.10004,"Many multi-variate time series obtained in the natural sciences and engineering possess a repetitive behavior, as for instance state-space trajectories of industrial machines in discrete automation. Recovering the times of recurrence from such a multi-variate time series is of a fundamental importance for many monitoring and control tasks. For a periodic time series this is equivalent to determining its period length. In this work we present a persistent homology framework to estimate recurrence times in multi-variate time series with different generalizations of cyclic behavior (periodic, repetitive, and recurring). To this end, we provide three specialized methods within our framework that are provably stable and validate them using real-world data, including a new benchmark dataset from an injection molding machine."
2505.1085,"Low-level clouds are ubiquitous in Earth's atmosphere, playing a crucial role in transporting heat, moisture, and momentum across the planet. Their evolution and interaction with other atmospheric components, such as aerosols, are essential to understanding the climate system and its sensitivity to anthropogenic influences. Advanced high-resolution geostationary satellites now resolve cloud systems with greater accuracy, establishing cloud tracking as a vital research area for studying their spatiotemporal dynamics. It enables disentangling advective and convective components driving cloud evolution. This, in turn, provides deeper insights into the structure and lifecycle of low-level cloud systems and the atmospheric processes they govern. In this paper, we propose a novel framework for tracking cloud systems using topology-driven techniques based on optimal transport. We first obtain a set of anchor points for the cloud systems based on the merge tree of the cloud optical depth field. We then apply topology-driven probabilistic feature tracking of these anchor points to guide the tracking of cloud systems. We demonstrate the utility of our framework by tracking clouds over the ocean and land to test for systematic differences in the two physically distinct settings. We further evaluate our framework through case studies and statistical analyses, comparing it against two leading cloud tracking tools and two topology-based general-purpose tracking tools. The results demonstrate that incorporating system-based tracking improves the ability to capture the evolution of low-level clouds. Our framework paves the way for detailed low-level cloud characterization studies using satellite data records."
2505.11317,"We present an approximation algorithm for computing the diameter of a point-set in $\Re^d$. The new algorithm is sensitive to the ``hardness'' of computing the diameter of the given input, and for most inputs it is able to compute the exact diameter extremely fast. The new algorithm is simple, robust, has good empirical performance, and can be implemented quickly. As such, it seems to be the algorithm of choice in practice for computing/approximating the diameter."
2505.13539,"We present EuLearn, the first surface datasets equitably representing a diversity of topological types. We designed our embedded surfaces of uniformly varying genera relying on random knots, thus allowing our surfaces to knot with themselves. EuLearn contributes new topological datasets of meshes, point clouds, and scalar fields in 3D. We aim to facilitate the training of machine learning systems that can discern topological features. We experimented with specific emblematic 3D neural network architectures, finding that their vanilla implementations perform poorly on genus classification. To enhance performance, we developed a novel, non-Euclidean, statistical sampling method adapted to graph and manifold data. We also introduce adjacency-informed adaptations of PointNet and Transformer architectures that rely on our non-Euclidean sampling strategy. Our results demonstrate that incorporating topological information into deep learning workflows significantly improves performance on these otherwise challenging EuLearn datasets."
2505.14417,"In the era of foundation models and Large Language Models (LLMs), Euclidean space is the de facto geometric setting of our machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. To that end, non-Euclidean learning is quickly gaining traction, particularly in web-related applications where complex relationships and structures are prevalent. Non-Euclidean spaces, such as hyperbolic, spherical, and mixed-curvature spaces, have been shown to provide more efficient and effective representations for data with intrinsic geometric properties, including web-related data like social network topology, query-document relationships, and user-item interactions. Integrating foundation models with non-Euclidean geometries has great potential to enhance their ability to capture and model the underlying structures, leading to better performance in search, recommendations, and content understanding. This workshop focuses on the intersection of Non-Euclidean Foundation Models and Geometric Learning (NEGEL), exploring its potential benefits, including the potential benefits for advancing web-related technologies, challenges, and future directions. Workshop page: [this https URL](this https URL)"
2505.15949,"{\em Partial domination problem} is a generalization of the {\em minimum dominating set problem} on graphs. Here, instead of dominating all the nodes, one asks to dominate at least a fraction of the nodes of the given graph by choosing a minimum number of nodes. For any real number $\alpha\in(0,1]$, $\alpha$-partial domination problem can be proved to be NP-complete for general graphs. In this paper, we define the {\em maximum dominating $k$-set} of a graph, which is polynomially transformable to the partial domination problem. The existence of a graph class for which the minimum dominating set problem is polynomial-time solvable, whereas the partial dominating set problem is NP-hard, is shown. We also propose polynomial-time algorithms for the maximum dominating $k$-set problem for the unit and arbitrary interval graphs. The problem can also be solved in polynomial time for the intersection graphs of a set of 2D objects intersected by a straight line, where each object is an axis-parallel unit square, as well as in the case where each object is a unit disk. Our technique also works for axis-parallel unit-height rectangle intersection graphs, where a straight line intersects all the rectangles. Finally, a parametrized algorithm for the maximum dominating $k$-set problem in a disk graph where the input disks are intersected by a straight line is proposed; here the parameter is the ratio of the diameters of the largest and smallest input disks."
2505.18014,"We study the \emph{geometric $k$-colored crossing number} of complete graphs $\overline{\overline{\text{cr}}}_k(K_n)$, which is the smallest number of monochromatic crossings in any $k$-edge colored straight-line drawing of $K_n$.We substantially improve asymptotic upper bounds on $\overline{\overline{\text{cr}}}_k(K_n)$ for $k=2,\ldots, 10$ by developing a procedure for general $k$ that derives $k$-edge colored drawings of $K_n$ for arbitrarily large $n$ from initial drawings with a low number of monochromatic crossings.We obtain the latter by heuristic search, employing a \textsc{MAX-$k$-CUT}-formulation of a subproblem in the process."
2505.21177,"Geometry is a fundamental branch of mathematics and plays a crucial role in evaluating the reasoning capabilities of multimodal large language models (MLLMs). However, existing multimodal mathematics benchmarks mainly focus on plane geometry and largely ignore solid geometry, which requires spatial reasoning and is more challenging than plane geometry. To address this critical gap, we introduce SolidGeo, the first large-scale benchmark specifically designed to evaluate the performance of MLLMs on mathematical reasoning tasks in solid geometry. SolidGeo consists of 3,113 real-world K-12 and competition-level problems, each paired with visual context and annotated with difficulty levels and fine-grained solid geometry categories. Our benchmark covers a wide range of 3D reasoning subjects such as projection, unfolding, spatial measurement, and spatial vector, offering a rigorous testbed for assessing solid geometry. Through extensive experiments, we observe that MLLMs encounter substantial challenges in solid geometry math tasks, with a considerable performance gap relative to human capabilities on SolidGeo. Moreover, we analyze the performance, inference efficiency and error patterns of various models, offering insights into the solid geometric mathematical reasoning capabilities of MLLMs. We hope SolidGeo serves as a catalyst for advancing MLLMs toward deeper geometric reasoning and spatial intelligence."
2505.23375,"We present the winning implementation of the Seventh Computational Geometry Challenge (CG:SHOP 2025). The task in this challenge was to find non-obtuse triangulations for given planar regions, respecting a given set of constraints consisting of extra vertices and edges that must be part of the triangulation. The goal was to minimize the number of introduced Steiner points. Our approach is to maintain a constrained Delaunay triangulation, for which we repeatedly remove, relocate, or add Steiner points. We use local search to choose the action that improves the triangulation the most, until the resulting triangulation is non-obtuse."
2506.00272,"We study the minimum membership geometric set cover, i.e., MMGSC problem [SoCG, 2023] in the continuous setting. In this problem, the input consists of a set $P$ of $n$ points in $\mathbb{R}^{2}$, and a geometric object $t$, the goal is to find a set $\mathcal{S}$ of translated copies of the geometric object $t$ that covers all the points in $P$ while minimizing $\mathsf{memb}(P, \mathcal{S})$, where $\mathsf{memb}(P, \mathcal{S})=\max_{p\in P}|\{s\in \mathcal{S}: p\in s\}|$.For unit squares, we present a simple $O(n\log n)$ time algorithm that outputs a $1$-membership cover. We show that the size of our solution is at most twice that of an optimal solution. We establish the NP-hardness on the problem of computing the minimum number of non-overlapping unit squares required to cover a given set of points. This algorithm also generalizes to fixed-sized hyperboxes in $d$-dimensional space, where an $1$-membership cover with size at most $2^{d-1}$ times the size of a minimum-sized $1$-membership cover is computed in $O(dn\log n)$ time. Additionally, we characterize a class of objects for which a $1$-membership cover always exists. For unit disks, we prove that a $2$-membership cover exists for any point set, and the size of the cover is at most $7$ times that of the optimal cover. For arbitrary convex polygons with $m$ vertices, we present an algorithm that outputs a $4$-membership cover in $O(n\log n + nm)$ time."
2506.01079,"We consider the problem of finding and enumerating polyominos that can be folded into multiple non-isomorphic boxes. While several computational approaches have been proposed, including SAT, randomized algorithms, and decision diagrams, none has been able to perform at scale. We argue that existing SAT encodings are hindered by the presence of global constraints (e.g., graph connectivity or acyclicity), which are generally hard to encode effectively and hard for solvers to reason about. In this work, we propose a new SAT-based approach that replaces these global constraints with simple local constraints that have substantially better propagation properties. Our approach dramatically improves the scalability of both computing and enumerating common box unfoldings: (i) while previous approaches could only find common unfoldings of two boxes up to area 88, ours easily scales beyond 150, and (ii) while previous approaches were only able to enumerate common unfoldings up to area 30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the smallest areas allowing a common unfolding of three boxes, thereby refuting a conjecture of Xu et al. (2017)."
2506.01323,"We initiate the study of computing diverse triangulations to a given polygon. Given a simple $n$-gon $P$, an integer $ k \geq 2 $, a quality measure $\sigma$ on the set of triangulations of $P$ and a factor $ \alpha \geq 1 $, we formulate the Diverse and Nice Triangulations (DNT) problem that asks to compute $k$ \emph{distinct} triangulations $T_1,\dots,T_k$ of $P$ such that a) their diversity, $\sum_{i < j} d(T_i,T_j) $, is as large as possible \emph{and} b) they are nice, i.e., $\sigma(T_i) \leq \alpha \sigma^* $ for all $1\leq i \leq k$. Here, $d$ denotes the symmetric difference of edge sets of two triangulations, and $\sigma^*$ denotes the best quality of triangulations of $P$, e.g., the minimum Euclidean length.As our main result, we provide a $\mathrm{poly}(n,k)$-time approximation algorithm for the DNT problem that returns a collection of $k$ distinct triangulations whose diversity is at least $1 - \Theta(1/k)$ of the optimal, and each triangulation satisfies the quality constraint. This is accomplished by studying \emph{bi-criteria triangulations} (BCT), which are triangulations that simultaneously optimize two criteria, a topic of independent interest. We complement our approximation algorithms by showing that the DNT problem and the BCT problem are NP-hard.Finally, for the version where diversity is defined as $\min_{i < j} d(T_i,T_j) $, we show a reduction from the problem of computing optimal Hamming codes, and provide an $n^{O(k)}$-time $\tfrac12$-approximation algorithm. This improves over the naive ${C_{n-2} \choose k} \approx 2^{O(nk)}$ time bound for enumerating all $k$-tuples among the triangulations of a simple $n$-gon, where $C_n$ denotes the $n$-th Catalan number."
2506.01726,"Many problems in Euclidean geometry, arising in computational design and fabrication, amount to a system of constraints, which is challenging to solve. We suggest a new general approach to the solution, which is to start with analogous problems in isotropic geometry. Isotropic geometry can be viewed as a structure-preserving simplification of Euclidean geometry. The solutions found in the isotropic case give insight and can initialize optimization algorithms to solve the original Euclidean problems. We illustrate this general approach with three examples: quad-mesh mechanisms, composite asymptotic-geodesic gridshells, and asymptotic gridshells with constant node angle."
2506.0396,"Let $\mathcal{A}$ be the subdivision of $\mathbb{R}^d$ induced by $m$ convex polyhedra having $n$ facets in total. We prove that $\mathcal{A}$ has combinatorial complexity $O(m^{\lceil d/2 \rceil} n^{\lfloor d/2 \rfloor})$ and that this bound is tight. The bound is mentioned several times in the literature, but no proof for arbitrary dimension has been published before."
2506.04086,"This paper addresses the problem of improving the query performance of the triangular expansion algorithm (TEA) for computing visibility regions by finding the most advantageous instance of the triangular mesh, the preprocessing structure. The TEA recursively traverses the mesh while keeping track of the visible region, the set of all points visible from a query point in a polygonal world. We show that the measured query time is approximately proportional to the number of triangle edge expansions during the mesh traversal. We propose a new type of triangular mesh that minimizes the expected number of expansions assuming the query points are drawn from a known probability distribution. We design a heuristic method to approximate the mesh and evaluate the approach on many challenging instances that resemble real-world environments. The proposed mesh improves the mean query times by 12-16% compared to the reference constrained Delaunay triangulation. The approach is suitable to boost offline applications that require computing millions of queries without addressing the preprocessing time. The implementation is publicly available to replicate our experiments and serve the community."
2506.04246,"For a graph $G$ spanning a metric space, the dilation of a pair of points is the ratio of their distance in the shortest path graph metric to their distance in the metric space. Given a graph $G$ and a budget $k$, a classic problem is to augment $G$ with $k$ additional edges to reduce the maximum dilation.In this note, we consider a variant of this problem where the goal is to reduce the average dilation for pairs of points in $G$. We provide an $O(k)$ approximation algorithm for this problem, matching the approximation ratio given by prior work for the maximum dilation variant."
2506.05156,"We consider the problem of computing $\ell$-page queue layouts, which are linear arrangements of vertices accompanied with an assignment of the edges to pages from one to $\ell$ that avoid the nesting of edges on any of the pages. Inspired by previous work in the extension of stack layouts, here we consider the setting of extending a partial $\ell$-page queue layout into a complete one and primarily analyze the problem through the refined lens of parameterized complexity. We obtain novel algorithms and lower bounds which provide a detailed picture of the problem's complexity under various measures of incompleteness, and identify surprising distinctions between queue and stack layouts in the extension setting."
2506.06477,"Let $ \Pi(n) $ be the largest number such that for every set $ S $ of $ n $ points in a polygon~$ P $, there always exist two points $ x, y \in S $, where every geodesic disk containing $ x $ and $ y $ contains $ \Pi(n) $ points of~$ S $. We establish upper and lower bounds for $ \Pi(n)$, and show that $ \left\lceil \frac{n}{5}\right\rceil+1 \leq \Pi(n) \leq \left\lceil \frac{n}{4} \right\rceil +1 $. We also show that there always exist two points $x, y\in S$ such that every geodesic disk with $x$ and $y$ on its boundary contains at least $ \frac{n}{3+\sqrt{5}} \approx \left\lceil \frac{n}{5.2} \right\rceil$ points both inside and outside the disk. For the special case where the points of $ S $ are restricted to be the vertices of a geodesically convex polygon we give a tight bound of $\left\lceil \frac{n}{3} \right\rceil + 1$. We provide the same tight bound when we only consider geodesic disks having $ x $ and $ y $ as diametral endpoints. We give upper and lower bounds of $\left\lceil \frac{n}{5} \right\rceil + 1 $ and $\frac{n}{6+\sqrt{26}} \approx \left\lceil \frac{n}{11.1} \right\rceil$, respectively, for the two-colored version of the problem. Finally, for the two-colored variant we show that there always exist two points $x, y\in S$ where $x$ and $y$ have different colors and every geodesic disk with $x$ and $y$ on its boundary contains at least $\lceil \frac{n}{11.3}\rceil+1$ points both inside and outside the disk."
2506.0717,"A rectangular dual of a plane graph $G$ is a contact representation of $G$ by interior-disjoint rectangles such that (i) no four rectangles share a point, and (ii) the union of all rectangles is a rectangle. In this paper, we study rectangular duals of graphs that are embedded in surfaces other than the plane. In particular, we fully characterize when a graph embedded on a cylinder admits a cylindrical rectangular dual. For graphs embedded on the flat torus, we can test whether the graph has a toroidal rectangular dual if we are additionally given a \textit{regular edge labeling}, i.e. a combinatorial description of rectangle adjacencies. Furthermore we can test whether there exists a toroidal rectangular dual that respects the embedding and that resides on a flat torus for which the sides are axis-aligned. Testing and constructing the rectangular dual, if applicable, can be done efficiently."
2506.07571,"We prove that the single-source shortest-path problem on disk graphs can be solved in $O(n\log n)$ time, and that it can be solved on intersection graphs of fat triangles in $O(n\log^2 n)$ time."
2506.08585,"We introduce a two-way connection between FO transductions (first-order logical transformations) of planar graphs, and a certain variant of fan-crossing (fan-planar) drawings of graphs which for bounded-degree graphs essentially reduces to being k-planar for fixed k. For graph classes, this connection allows to derive non-transducibility results from nonexistence of the said drawings and, conversely, from nonexistence of a transduction to derive nonexistence of the said drawings. For example, the class of 3D-grids is not k-planar for any fixed k. We hope that this connection will help to draw a path to a possible proof that not all toroidal graphs are transducible from planar graphs.Our characterization can be extended to any fixed surface instead of the plane. The result is based on a very recent characterization of weakly sparse FO transductions of classes of bounded expansion by [Gajarský, Gładkowski, Jedelský, Pilipczuk and Toruńczyk,arXiv:2505.15655]."
2506.09269,"We resolve a conjecture posed by Covella, Frati and Patrignani by proving the straight-line orthogonal drawing of the complete ternary tree with $n$ nodes satisfying the subtree separation property with smallest area has area $\Omega(n^{1.031})$. We also improve the upper bound of this area to $O(n^{1.032})$."
2506.09579,"Extracting high-fidelity mesh surfaces from Signed Distance Fields has become a fundamental operation in geometry processing. Despite significant progress over the past decades, key challenges remain namely, how to automatically capture the intricate geometric and topological structures encoded in the zero level set of SDFs. In this paper, we present a novel isosurface extraction algorithm that introduces two key innovations: 1. An incrementally constructed power diagram through the addition of sample points, which enables repeated updates to the extracted surface via its dual regular Delaunay tetrahedralization; and 2. An adaptive point insertion strategy that identifies regions exhibiting the greatest discrepancy between the current mesh and the underlying continuous surface. As the teaser figure shows, our framework progressively refines the extracted mesh with minimal computational cost until it sufficiently approximates the underlying surface. Experimental results demonstrate that our approach outperforms sofa methods, particularly for models with intricate geometric variations and complex topologies."
2506.11628,"We show that the following problem is undecidable: given two polygonal prototiles, determine whether the plane can be tiled with rotated and translated copies of them. This improves a result of Demaine and Langerman [SoCG 2025], who showed undecidability for three tiles.Along the way, we show that tiling with one prototile is undecidable if there can be edge-to-edge matching rules. This is the first result to show undecidability for monotiling with only local matching constraints."
2506.12625,"A Delaunay graph built on a planar point set has an edge between two vertices when there exists a disk with the two vertices on its boundary and no vertices in its interior. When the disk is replaced with an equilateral triangle, the resulting graph is known as a Triangle-Distance Delaunay Graph or TD-Delaunay for short. A generalized $\text{TD}_{\theta_1,\theta_2}$-Delaunay graph is a TD-Delaunay graph whose empty region is a scaled translate of a triangle with angles of $\theta_1,\theta_2,\theta_3:=\pi-\theta_1-\theta_2$ with $\theta_1\leq\theta_2\leq\theta_3$. We prove that $\frac{1}{\sin(\theta_1/2)}$ is a lower bound on the spanning ratio of these graphs which matches the best known upper bound (Lubiw & Mondal, J. Graph Algorithms Appl., 23(2):345-369). Then we provide an online local routing algorithm for $\text{TD}_{\theta_1,\theta_2}$-Delaunay graphs with a routing ratio that is optimal in the worst case. When $\theta_1=\theta_2=\frac{\pi}{3}$, our expressions for the spanning ratio and routing ratio evaluate to $2$ and $\frac{\sqrt{5}}{3}$, matching the known tight bounds for TD-Delaunay graphs."
2506.13191,"We study the colorful sum of radii problem, where the input is a point set $P$ partitioned into classes $P_1, P_2, \dots, P_\omega$, along with per-class outlier bounds $m_1, m_2, \dots, m_\omega$, summing to $m$. The goal is to select a subset $\mathcal{C} \subseteq P$ of $k$ centers and assign points to centers in $\mathcal{C}$, allowing up to $m_i$ unassigned points (outliers) from each class $P_i$, while minimizing the sum of cluster radii. The radius of a cluster is defined as the maximum distance from any point in the cluster to its center. The classical (non-colorful) version of the sum of radii problem is known to be NP-hard, even on weighted planar graphs. The colorful sum of radii is introduced by Chekuri et al. (2022), who provide an $O(\log \omega)$-approximation algorithm. In this paper, we present the first constant-factor approximation algorithms for the colorful sum of radii running in FPT (fixed-parameter tractable) time. Our contributions are twofold: We design an iterative covering algorithm that achieves a $(2+\varepsilon)$-approximation with running time exponential in both $k$ and $m$; We further develop a $(7+\varepsilon)$-approximation algorithm by leveraging a colorful $k$-center subroutine, improving the running time by removing the exponential dependency on $m$."
2506.16021,"The problem of locally routing on geometric networks using limited memory is extensively studied in computational geometry. We consider one particular graph, the ordered $\Theta$-graph, which is significantly harder to route on than the $\Theta$-graph, for which a number of routing algorithms are known. Currently, no local routing algorithm is known for the ordered $\Theta$-graph.We prove that, unfortunately, there does not exist a deterministic memoryless local routing algorithm that works on the ordered $\Theta$-graph. This motivates us to consider allowing a small amount of memory, and we present a deterministic $O(1)$-memory local routing algorithm that successfully routes from the source to the destination on the ordered $\Theta$-graph. We show that our local routing algorithm converges to the destination in $O(n)$ hops, where $n$ is the number of vertices. To the best of our knowledge, our algorithm is the first deterministic local routing algorithm that is guaranteed to reach the destination on the ordered $\Theta$-graph."
2506.16299,"Unoriented surface reconstruction is an important task in computer graphics and has extensive applications. Based on the compact support of wavelet and orthogonality properties, classic wavelet surface reconstruction achieves good and fast reconstruction. However, this method can only handle oriented points. Despite some improved attempts for unoriented points, such as iWSR, these methods perform poorly on sparse point clouds. To address these shortcomings, we propose a wavelet-based method to represent the mollified indicator function and complete both the orientation and surface reconstruction tasks. We use the modifying kernel function to smoothen out discontinuities on the surface, aligning with the continuity of the wavelet basis function. During the calculation of coefficient, we fully utilize the properties of the convolutional kernel function to shift the modifying computation onto wavelet basis to accelerate. In addition, we propose a novel method for constructing the divergence-free function field and using them to construct the additional homogeneous constraints to improve the effectiveness and stability. Extensive experiments demonstrate that our method achieves state-of-the-art performance in both orientation and reconstruction for sparse models. We align the matrix construction with the compact support property of wavelet basis functions to further accelerate our method, resulting in efficient performance on CPU. Our source codes will be released on GitHub."
2506.16979,"Given a set $P$ of $n$ weighted points and a set $H$ of $n$ half-planes in the plane, the hitting set problem is to compute a subset $P'$ of points from $P$ such that each half-plane contains at least one point from $P'$ and the total weight of the points in $P'$ is minimized. The previous best algorithm solves the problem in $O(n^{7/2}\log^2 n)$ time. In this paper, we present a new algorithm with runtime $O(n^{5/2}\log^2 n)$."
2506.17025,"With advances in technology, there has been growing interest in developing effective mapping methods for 3-dimensional objects in recent years. Volumetric parameterization for 3D solid manifolds plays an important role in processing 3D data. However, the conventional approaches cannot control the bijectivity and local geometric distortions of the result mappings due to the complex structure of the solid manifolds. Moreover, prior methods mainly focus on one property instead of balancing different properties during the mapping process. In this paper, we propose several novel methods for computing volumetric parameterizations for 3D simply-connected manifolds. Analogous to surface parameterization, our framework incorporates several models designed to preserve geometric structure, achieve density equalization, and optimally balance geometric and density distortions. With these methods, various 3D manifold parameterizations with different desired properties can be achieved. These methods are tested on different examples and manifold remeshing applications, demonstrating their effectiveness and accuracy."
2506.17507,"In the noisy primitives model, each primitive comparison performed by an algorithm, e.g., testing whether one value is greater than another, returns the incorrect answer with random, independent probability p < 1/2 and otherwise returns a correct answer. This model was first applied in the context of sorting and searching, and recent work by Eppstein, Goodrich, and Sridhar extends this model to sequential algorithms involving geometric primitives such as orientation and sidedness tests. However, their approaches appear to be inherently sequential; hence, in this paper, we study parallel computational geometry algorithms for 2D and 3D convex hulls in the noisy primitives model. We give the first optimal parallel algorithms in the noisy primitives model for 2D and 3D convex hulls in the CREW PRAM model. The main technical contribution of our work concerns our ability to detect and fix errors during intermediate steps of our algorithm using a generalization of the failure sweeping technique."
2506.18354,"In this paper, we study the problem of map matching with travel time constraints. Given a sequence of $k$ spatio-temporal measurements and an embedded path graph with travel time costs, the goal is to snap each measurement to a close-by location in the graph, such that consecutive locations can be reached from one another along the path within the timestamp difference of the respective measurements. This problem arises in public transit data processing as well as in map matching of movement trajectories to general graphs. We show that the classical approach for this problem, which relies on selecting a finite set of candidate locations in the graph for each measurement, cannot guarantee to find a consistent solution. We propose a new algorithm that can deal with an infinite set of candidate locations per measurement. We prove that our algorithm always detects a consistent map matching path (if one exists). Despite the enlarged candidate set, we also demonstrate that our algorithm has superior running time in theory and practice. For a path graph with $n$ nodes, we show that our algorithm runs in $\mathcal{O}(k^2 n \log {nk})$ and under mild assumptions in $\mathcal{O}(k n ^\lambda + n \log^3 n)$ for $\lambda \approx 0.695$. This is a significant improvement over the baseline, which runs in $\mathcal{O}(k n^2)$ and which might not even identify a correct solution. The performance of our algorithm hinges on an efficient segment-circle intersection data structure. We describe how to design and implement such a data structure for our application. In the experimental evaluation, we demonstrate the usefulness of our novel algorithm on a diverse set of generated measurements as well as GTFS data."
2506.18793,"Word cloud use is a popular text visualization technique that scales font sizes based on word frequencies within a defined spatial layout. However, traditional word clouds disregard semantic relationships between words, arranging them without considering their meanings. Semantic word clouds improved on this by positioning related words in proximity; however, still struggled with efficient space use and representing frequencies through font size variations, which can be misleading because of word length differences. This paper proposes StoryGem, a novel text visualization approach that addresses these limitations. StoryGem constructs a semantic word network from input text data, performs hierarchical clustering, and displays the results in a Voronoi treemap. Furthermore, this paper proposes an optimization problem to maximize the font size within the regions of a Voronoi treemap. In StoryGem, word frequencies map to area sizes rather than font sizes, allowing flexible text sizing that maximizes use of each region's space. This mitigates bias from varying word lengths affecting font size perception. StoryGem strikes a balance between a semantic organization and spatial efficiency, combining the strengths of word clouds and treemaps. Through hierarchical clustering of semantic word networks, it captures word semantics and relationships. The Voronoi treemap layout facilitates gapless visualization, with area sizes corresponding to frequencies for clearer representation. User study across diverse text datasets demonstrate StoryGem's potential as an effective technique for quickly grasping textual content and semantic structures."
2506.18818,"A set is star-shaped if there is a point in the set that can see every other point in the set in the sense that the line-segment connecting the points lies within the set. We show that testing whether a non-empty compact smooth region is star-shaped is $\forall\mathbb{R}$-complete. Since the obvious definition of star-shapedness has logical form $\exists\forall$, this is a somewhat surprising result, based on Krasnosel'ski\uı's theorem from convex geometry; we study several related complexity classifications in the real hierarchy based on other results from convex geometry."
2506.19991,"The Euler Characteristic Transform (ECT) is a robust method for shape classification. It takes an embedded shape and, for each direction, computes a piecewise constant function representing the Euler Characteristic of the shape's sublevel sets, which are defined by the height function in that direction. It has applications in TDA inverse problems, such as shape reconstruction, and is also employed with machine learning methodologies. In this paper, we define a distance between the ECTs of two distinct geometric embeddings of the same abstract simplicial complex and provide an upper bound for this distance. The Super Lifted Euler Characteristic Transform (SELECT), a related construction, extends the ECT to scalar fields defined on shapes. We establish a similar distance bound for SELECT, specifically when applied to fields defined on embedded simplicial complexes."
2506.20421,"A geometric graph is a drawing of a graph in the plane where the vertices are drawn as points in general position and the edges as straight-line segments connecting their endpoints. It is plane if it contains no crossing edges. We study plane cycles in geometric complete multipartite graphs. We prove that if a geometric complete multipartite graph contains a plane cycle of length $t$, with $t \geq 6$, it also contains a smaller plane cycle of length at least $\lfloor t/2\rfloor + 1$. We further give a characterization of geometric complete multipartite graphs that contain plane cycles with a color class appearing at least twice. For geometric drawings of $K_{n,n}$, we give a sufficient condition under which they have, for each $s \leq n$, a plane cycle of length 2s. We also provide an algorithm to decide whether a given geometric drawing of $K_{n,n}$ contains a plane Hamiltonian cycle in time $O(n \log n + nk^2) + O(k^{5k})$, where k is the number of vertices inside the convex hull of all vertices. Finally, we prove that it is NP-complete to decide if a subset of edges of a geometric complete bipartite graph H is contained in a plane Hamiltonian cycle in H."
2506.20508,"We address the problem of covering a target segment $\overline{uv}$ using a finite set of guards $\mathcal{S}$ placed on a source segment $\overline{xy}$ within a simple polygon $\mathcal{P}$, assuming weak visibility between the target and source. Without geometric constraints, $\mathcal{S}$ may be infinite, as shown by prior hardness results. To overcome this, we introduce the {\it line aspect ratio} (AR), defined as the ratio of the \emph{long width} (LW) to the \emph{short width} (SW) of $\mathcal{P}$. These widths are determined by parallel lines tangent to convex vertices outside $\mathcal{P}$ (LW) and reflex vertices inside $\mathcal{P}$ (SW), respectively.Under the assumption that AR is constant or polynomial in $n$ (the polygon's complexity), we prove that a finite guard set $\mathcal{S}$ always exists, with size bounded by $\mathcal{O}(\text{AR})$. This AR-based framework generalizes some previous assumptions, encompassing a broader class of polygons.Our result establishes a framework guaranteeing finite solutions for segment guarding under practical and intuitive geometric constraints."
2506.21307,"We investigate the Dispersive Art Gallery Problem with vertex guards and rectangular visibility ($r$-visibility) for a class of orthogonal polygons that reflect the properties of real-world floor plans: these office-like polygons consist of rectangular rooms and corridors. In the dispersive variant of the Art Gallery Problem, the objective is not to minimize the number of guards but to maximize the minimum geodesic $L_1$-distance between any two guards, called the dispersion distance.Our main contributions are as follows. We prove that determining whether a vertex guard set can achieve a dispersion distance of $4$ in office-like polygons is NP-complete, where vertices of the polygon are restricted to integer coordinates. Additionally, we present a simple worst-case optimal algorithm that guarantees a dispersion distance of $3$ in polynomial time. Our complexity result extends to polyominoes, resolving an open question posed by Rieck and Scheffer (CGTA 2024). When vertex coordinates are allowed to be rational, we establish analogous results, proving that achieving a dispersion distance of $2+\varepsilon$ is NP-hard for any $\varepsilon > 0$, while the classic Art Gallery Problem remains solvable in polynomial time for this class of polygons. Furthermore, we give a straightforward polynomial-time algorithm that computes worst-case optimal solutions with a dispersion distance of $2$.On the other hand, for the more restricted class of hole-free independent office-like polygons, we propose a dynamic programming approach that computes optimal solutions. Moreover, we demonstrate that the problem is practically tractable for arbitrary orthogonal polygons. To this end, we compare solvers based on SAT, CP, and MIP formulations. Notably, SAT solvers efficiently compute optimal solutions for randomly generated instances with up to $1600$ vertices in under $15$s."
2506.21926,"Given a set $P$ of $n$ points in the plane, the unit-disk graph $G(P)$ is a graph with $P$ as its vertex set such that two points of $P$ have an edge if their Euclidean distance is at most $1$. We consider the problem of computing a maximum clique in $G(P)$. The previously best algorithm for the problem runs in $O(n^{7/3+o(1)})$ time. We show that the problem can be solved in $O(n \log n + n K^{4/3+o(1)})$ time, where $K$ is the maximum clique size. The algorithm is faster than the previous one when $K=o(n)$. In addition, if $P$ is in convex position, we give a randomized algorithm that runs in $O(n^{15/7+o(1)})= O(n^{2.143})$ worst-case time and the algorithm can compute a maximum clique with high probability. For points in convex position, one special case we solve is when a point in the maximum clique is given; we present an $O(n^2\log n)$ time (deterministic) algorithm for this special case."
2506.22263,"Directed graphs arise in many applications where computing persistent homology helps to encode the shape and structure of the input information. However, there are only a few ways to turn the directed graph information into an undirected simplicial complex filtration required by the standard persistent homology framework. In this paper, we present a new filtration constructed from a directed graph, called the walk-length filtration. This filtration mirrors the behavior of small walks visiting certain collections of vertices in the directed graph. We show that, while the persistence is not stable under the usual $L_\infty$-style network distance, a generalized $L_1$-style distance is, indeed, stable. We further provide an algorithm for its computation, and investigate the behavior of this filtration in examples, including cycle networks and synthetic hippocampal networks with a focus on comparison to the often used Dowker filtration."
2507.00235,"In a connected simple graph G = (V(G),E(G)), each vertex is assigned a color from the set of colors C={1, 2,..., c}. The set of vertices V(G) is partitioned as V_1, V_2, ... ,V_c, where all vertices in V_j share the same color j. A subset S of V(G) is called Selective Subset if, for every vertex v in V(G), and if v is in V_j, at least one of its nearest neighbors in (S union (V(G)\ V_j)) has the same color as v. The Minimum Selective Subset (MSS) problem seeks to find a selective subset of minimum size. The problem was first introduced by Wilfong in 1991 for a set of points in the Euclidean plane, where two major problems, MCS (Minimum Consistent Subset) and MSS, were proposed.In graph algorithms, the only known result is that the MSS problem is NP-complete, as shown in 2018. Beyond this, no further progress has been made to date. In contrast, the MCS problem has been widely studied in various graph classes over the years. Therefore, in this work, we also extend the algorithmic study of MSS on various graph classes. We first present a log(n)-approximation algorithm for general graphs with n vertices and regardless of the number of colors. We also show that the problem remains NP-complete in planar graphs when restricted to just two colors.. Finally, we provide polynomial-time algorithms for computing optimal solutions in trees and unit interval graphs for any number of colors."
2507.01076,"The NP-complete mutual-visibility (MV) problem currently lacks empirical analysis on its practical behaviour despite theoretical studies. This paper addresses this gap by implementing and evaluating three distinct algorithms -- a direct random heuristic, a hypergraph-based approximation, and a genetic algorithm -- on diverse synthetic graph datasets, including those with analytically known $\mu(G)$ values and general graph models. Our results demonstrate that for smaller graphs, the algorithms consistently achieve MV set sizes aligning with theoretical bounds. However, for larger instances, achieved solution sizes notably diverge from theoretical limits; this, combined with the absence of tight bounds, complicates absolute quality assessment. Nevertheless, validation on known optimal graphs showed the Genetic Algorithm and other heuristics empirically performing best among tested methods."
2507.01171,"Reeb graphs are a fundamental structure for analyzing the topological and geometric properties of scalar fields. Comparing Reeb graphs is crucial for advancing research in this domain, yet existing metrics are often computationally prohibitive or fail to capture essential topological features effectively. In this paper, we explore the application of the Gromov-Wasserstein distance, a versatile metric for comparing metric measure spaces, to Reeb graphs. We propose a framework integrating a symmetric variant of the Reeb radius for robust geometric comparison, and a novel probabilistic weighting scheme based on Persistence Images derived from extended persistence diagrams to effectively incorporate topological significance. A key contribution of this work is the rigorous theoretical proof of the stability of our proposed Reeb Gromov-Wasserstein distance with respect to perturbations in the underlying scalar fields. This ensures that small changes in the input data lead to small changes in the computed distance between Reeb graphs, a critical property for reliable analysis. We demonstrate the advantages of our approach, including its enhanced ability to capture topological features and its proven stability, through comparisons with other alternatives on several datasets, showcasing its practical utility and theoretical soundness."
2507.01775,"In this paper, we present a deterministic variant of Chan's randomized partition tree [Discret. Comput. Geom., 2012]. This result leads to numerous applications. In particular, for $d$-dimensional simplex range counting (for any constant $d \ge 2$), we construct a data structure using $O(n)$ space and $O(n^{1+\epsilon})$ preprocessing time, such that each query can be answered in $o(n^{1-1/d})$ time (specifically, $O(n^{1-1/d} / \log^{\Omega(1)} n)$ time), thereby breaking an $\Omega(n^{1-1/d})$ lower bound known for the semigroup setting. Notably, our approach does not rely on any bit-packing techniques. We also obtain deterministic improvements for several other classical problems, including simplex range stabbing counting and reporting, segment intersection detection, counting and reporting, ray-shooting among segments, and more. Similar to Chan's original randomized partition tree, we expect that additional applications will emerge in the future, especially in situations where deterministic results are preferred."
2507.0194,"We consider the watchman route problem for multiple watchmen in staircase polygons, which are rectilinear $x$- and $y$-monotone polygons. For two watchmen, we propose an algorithm to find an optimal solution that takes quadratic time, improving on the cubic time of a trivial solution. For $m \geq 3$ watchmen, we explain where this approach fails, and present an approximation algorithm for the min-max criterion with only an additive error."
2507.0274,"We provide a linear time algorithm to determine the flip distance between two plane spanning paths on a point set in convex position. At the same time, we show that the happy edge property does not hold in this setting. This has to be seen in contrast to several results for reconfiguration problems where the absence of the happy edge property implies algorithmic hardness of the flip distance problem. Further, we show that our algorithm can be adapted for (1) compatible flips (2) local flips and (3) flips for plane spanning paths in simple polygons."
2507.0417,"A configuration of $n$ unit-cube-shaped \textit{modules} (or \textit{robots}) is a lattice-aligned placement of the $n$ modules so that their union is face-connected. The reconfiguration problem aims at finding a sequence of moves that reconfigures the modules from one given configuration to another. The sliding cube model (in which modules are allowed to slide over the face or edge of neighboring modules) is one of the most studied theoretical models for modular robots.In the sliding cubes model we can reconfigure between any two shapes in $O(n^2)$ moves ([Abel \textit{et al.} SoCG 2024]). If we are interested in a reconfiguration algorithm into a \textit{compact configuration}, the number of moves can be reduced to the sum of coordinates of the input configuration (a number that ranges from $\Omega(n^{4/3})$ to $O(n^2)$, [Kostitsyna \textit{et al.} SWAT 2024]). We introduce a new algorithm that combines both universal reconfiguration and an input-sensitive bound on the sum of coordinates of both configurations, with additional advantages, such as $O(1)$ amortized computation per move."
2507.04538,"We show that the max-min-angle polygon in a planar point set can be found in time $O(n\log n)$ and a max-min-solid-angle convex polyhedron in a three-dimensional point set can be found in time $O(n^2)$. We also study the maxmin-angle polygonal curve in 3d, which we show to be $\mathsf{NP}$-hard to find if repetitions are forbidden but can be found in near-cubic time if repeated vertices or line segments are allowed, by reducing the problem to finding a bottleneck cycle in a graph. We formalize a class of problems on which a decremental greedy algorithm can be guaranteed to find an optimal solution, generalizing our max-min-angle and bottleneck cycle algorithms, together with a known algorithm for graph degeneracy."
2507.04933,"We study the problem of computing a convex region with bounded area and diameter that contains the maximum number of points from a given point set $P$. We show that this problem can be solved in $O(n^6k)$ time and $O(n^3k)$ space, where $n$ is the size of $P$ and $k$ is the maximum number of points in the found region. We experimentally compare this new algorithm with an existing algorithm that does the same but without the diameter constraint, which runs in $O(n^3k)$ time. For the new algorithm, we use different diameters. We use both synthetic data and data from an application in cancer detection, which motivated our research."
2507.04948,"The network homology Hk-core decomposition proposed in this article is similar to the k-core decomposition based on node degrees of the network. The C. elegans neural network and the cat cortical network are used as examples to reveal the symmetry of the deep structures of such networks. First, based on the concept of neighborhood in mathematics, some new concepts are introduced, including such as node-neighbor subnetwork and Betti numbers of the neighbor subnetwork, among others. Then, the Betti numbers of the neighbor subnetwork of each node are computed, which are used to perform Hk-core decomposition of the network homology. The construction process is as follows: the initial network is referred to as the H0-core; the H1-core is obtained from the H0-core by deleting some nodes of certain properties; the H2-core is obtained from the H1-core by deleting some nodes or edges of certain properties; the H3-core is obtained from the H2-core by deleting some nodes of certain properties or by retaining the nodes of certain properties, and so on, which will be described in detail in the main text. Throughout the process, the index of node involved in deleting edge needs to be updated in every step. The Hk-core decomposition is easy to implement in parallel. It has a wide range of applications in many fields such as network science, data science, computational topology, and artificial intelligence. In this article, we also show how to use it to simplify homology calculation, e.g. for the C. elegans neural network, whereas the results of decomposition are the H1-core, the H2-core, and the H3-core. Thus, the simplexes consisting of four highest-order cavities in the H3-core subnetwork can also be directly obtained."
2507.04974,"We introduce the Polychromatic Traveling Salesman Problem (PCTSP), where the input is an edge weighted graph whose vertices are partitioned into $k$ equal-sized color classes, and the goal is to find a minimum-length Hamiltonian cycle that visits the classes in a fixed cyclic order. This generalizes the Bipartite TSP (when $k = 2$) and the classical TSP (when $k = n$). We give a polynomial-time $(3 - 2 * 10^{-36})$-approximation algorithm for metric PCTSP. Complementing this, we show that Euclidean PCTSP is APX-hard even in $R^2$, ruling out the existence of a PTAS unless P = NP."
2507.05569,"Given in the plane a set $S$ of $n$ points and a set of disks centered at these points, the disk graph $G(S)$ induced by these disks has vertex set $S$ and an edge between two vertices if their disks intersect. Note that the disks may have different radii. We consider the problem of computing shortest paths from a source point $s\in S$ to all vertices in $G(S)$ where the length of a path in $G(S)$ is defined as the number of edges in the path. The previously best algorithm solves the problem in $O(n\log^2 n)$ time. A lower bound of $\Omega(n\log n)$ is also known for this problem under the algebraic decision tree model. In this paper, we present an $O(n\log n)$ time algorithm, which matches the lower bound and thus is optimal. Another virtue of our algorithm is that it is quite simple."
2507.06212,The Mapper construction is one of the most widespread tools from Topological Data Analysis. There is an unfortunate trend as the construction has gained traction to use clustering methods with properties that end up distorting any analysis results from the construction. In this paper we will see a few ways in which widespread choices of clustering algorithms have arbitrarily large distortions of the features visible in the final Mapper complex.
2507.06477,"A covering path for a finite set $P$ of points in the plane is a polygonal path such that every point of $P$ lies on a segment of the path. The vertices of the path need not be at points of $P$. A covering path is plane if its segments do not cross each other. Let $\pi(n)$ be the minimum number such that every set of $n$ points in the plane admits a plane covering path with at most $\pi(n)$ segments. We prove that $\pi(n)\le \lceil6n/7\rceil$. This improves the previous best-known upper bound of $\lceil 21n/22\rceil$, due to Biniaz (SoCG 2023). Our proof is constructive and yields a simple $O(n\log n)$-time algorithm for computing a plane covering path."
2507.08356,"In analogy to flexible bipyramids, also known as Bricard octahedra, we study flexible couplings of two Bennett mechanisms. The resulting flexible bi-Bennett structures can be used as building blocks of flexible tubes with quadrilateral cross-section that possess skew faces. We present three 4-dimensional families of flexible arranged Bennett tubes and discuss some of their properties as well as their relation to flexible bipyramids and biprisms, respectively."
2507.08478,"The detection and classification of intersections between triangles are crucial tasks in a wide range of applications within Computer Graphics and Geometry Processing, including mesh Arrangements, mesh Booleans, and generic mesh processing and fixing tasks. Existing methods are hard-coded and deeply integrated into specific algorithms, and significant efforts are usually required to integrate them into new pipelines or to extend them to different numerical representations. This paper presents a versatile and exhaustive algorithm to identify and classify intersections between triangles with either floating points, rational numbers, or implicit representations. The proposed tool is implemented as a C++ templated and header-only code that is generic and easy to integrate into further algorithms requiring the triangle-triangle intersection detection step. The developed tool has been tested and compared with a state-of-the-art approach, and it is shared with the Geometry Processing community with an Open Source license."
2507.08724,"In this paper, we introduce a trajectory planning problem for a marsupial robotics system consisting of a ground robot, a drone, and a taut tether of bounded length connecting the two robots. This problem can be framed within the context of a pursuit-evasion game. Using a geometric modeling approach, we present an optimal algorithm to compute a minimum-link path for the pursuer (ground robot), given the known path of the evader (drone). Furthermore, we address and solve three related geometric optimization problems, leveraging the intrinsic connections between them."
2507.11212,"We study the problem of aggregating polygons by covering them with disjoint representative regions, thereby inducing a clustering of the polygons. Our objective is to minimize a weighted sum of the total area and the total perimeter of the regions. This problem has applications in cartographic map generalization and urban analytics. Here, the polygons represent building footprints and the clusters may represent urban areas. Previous approaches forced the boundaries of the regions to come from a fixed subdivision of the plane, which allows the optimal solution (restricted in this way) to be found from a minimum cut in a dual graph. It is natural to ask whether the problem can still be solved efficiently if this restriction is removed, allowing output regions to be bounded by arbitrary curves. We provide a positive answer in the form of a polynomial-time algorithm. Additionally, we fully characterize the optimal solutions by showing that their boundaries are composed of input polygon edges and circular arcs of constant radius. Since some applications favor straight edges, we also study two problem variants in which the output regions must be polygons, but are not restricted to have boundaries from a fixed subdivision. In the first variant, region vertices must lie on the boundaries of the input polygons. The second variant requires them to be vertices of the input polygons. We show that both variants can be approximated up to a constant factor in polynomial time by altering an optimal solution for the unrestricted problem. Our experimental evaluation on real-world building footprints demonstrates that these approximate solutions are visually similar to the optimal unrestricted ones and achieve near-optimal objective values."
2507.11406,"Heegaard splittings provide a natural representation of closed 3-manifolds by gluing handlebodies along a common surface. These splittings can be equivalently given by two finite sets of meridians lying in the surface, which define a Heegaard diagram. We present a data structure to effectively represent Heegaard diagrams as normal curves with respect to triangulations of a surface of complexity measured by the space required to express the normal coordinates' vectors in binary. This structure can be significantly more compressed than triangulations of 3-manifolds, given exponential gains for some families. Even with this succinct definition of complexity, we establish polynomial time algorithms for comparing and manipulating diagrams, performing stabilizations, detecting trivial stabilizations and reductions, and computing topological invariants of the underlying manifolds, such as their fundamental and first homology groups. We also contrast early implementations of our techniques with standard software programs for 3-manifolds, achieving better precision and faster algorithms for the average cases and exponential gains in speed for some particular presentations of the inputs."
2507.13096,"Tutte's celebrated barycentric embedding theorem describes a natural way to build straight-line embeddings (crossing-free drawings) of a (3-connected) planar graph: map the vertices of the outer face to the vertices of a convex polygon, and ensure that each remaining vertex is in convex position, namely, a barycenter with positive coefficients of its neighbors. Actually computing an embedding then boils down to solving a system of linear equations. A particularly appealing feature of this method is the flexibility given by the choice of the barycentric weights. Generalizations of Tutte's theorem to surfaces of nonpositive curvature are known, but due to their inherently continuous nature, they do not lead to an algorithm.In this paper, we propose a purely discrete analog of Tutte's theorem for surfaces (with or without boundary) of nonpositive curvature, based on the recently introduced notion of reducing triangulations. We prove a Tutte theorem in this setting: every drawing homotopic to an embedding such that each vertex is harmonious (a discrete analog of being in convex position) is a weak embedding (arbitrarily close to an embedding). We also provide a polynomial-time algorithm to make an input drawing harmonious without increasing the length of any edge, in a similar way as a drawing can be put in convex position without increasing the edge lengths."
2507.13641,"As an important metric for mesh quality evaluation, the isotropy property holds significant value for applications such as texture UV-mapping, physical simulation, and discrete geometric analysis. Classical isotropy remeshing methods adjust vertices and edge lengths, which exhibit certain limitations in terms of input data sensitivity, geometric consistency control, and convergence speed. In this paper, we propose an improved isotropy remeshing solution with inter-angle optimization during mesh editing to enhance shape control capability and accelerate convergence. The advantage of the solution lies in its ability to predict the impact of edge length adjustments on subsequent optimization by monitoring angle transformations. It avoids inefficient editing that may cause performance fluctuations, thereby improving efficiency. Experiments demonstrate that the proposed method effectively improves the overall efficiency of mesh optimization."
2507.15404,"Extracting a quad mesh from a grid preserving map is straightforward in theory, but typical inputs are not exactly grid preserving maps. Previous works can manage minor deviations from grid preserving maps, but without a clear specification of what is acceptable. This work clarifies how typical inputs differ from a grid preserving map, and shows how the differences with a grid preserving map can be reflected by a sequence of operations acting on a discrete structure. It opens research opportunities for the design of a robust quad extraction algorithm."
2507.1562,"Constructing cell developmental trajectories is a critical task in single-cell RNA sequencing (scRNA-seq) analysis, enabling the inference of potential cellular progression paths. However, current automated methods are limited to establishing cell developmental trajectories within individual samples, necessitating biologists to manually link cells across samples to construct complete cross-sample evolutionary trajectories that consider cellular spatial dynamics. This process demands substantial human effort due to the complex spatial correspondence between each pair of samples. To address this challenge, we first proposed a GNN-based model to predict cross-sample cell developmental trajectories. We then developed TrajLens, a visual analytics system that supports biologists in exploring and refining the cell developmental trajectories based on predicted links. Specifically, we designed the visualization that integrates features on cell distribution and developmental direction across multiple samples, providing an overview of the spatial evolutionary patterns of cell populations along trajectories. Additionally, we included contour maps superimposed on the original cell distribution data, enabling biologists to explore them intuitively. To demonstrate our system's performance, we conducted quantitative evaluations of our model with two case studies and expert interviews to validate its usefulness and effectiveness."
2507.16171,"This paper presents a novel algorithmic framework for the computational design, simulation, and fabrication of a hexagonal grid-based double-curvature structure with planar hexagonal panels. The journey begins with constructing a robust data structure through the meticulous subdivision of an equilateral triangle surface, forming a foundational triangular grid. This grid is the basis for a graph that encapsulates hexagons, laying the groundwork for simulating dynamic interactions and form-finding. The developed algorithm ensures a well-structured hexagonal grid data representation, and the experimental results showcase the successful implementation of the algorithm, leading to the fabrication of planar hexagons mirroring physics-generated mesh surfaces."
2507.16269,"The Freeze-Tag Problem (FTP) involves activating a set of initially asleep robots as quickly as possible, starting from a single awake robot. Once activated, a robot can assist in waking up other robots. Each active robot moves at unit speed. The objective is to minimize the makespan, i.e., the time required to activate the last robot. A key performance measure is the wake-up ratio, defined as the maximum time needed to activate any number of robots in any primary positions. This work focuses on the geometric (Euclidean) version of FTP in $\mathbb{R}^d$ under the $\ell_p$ norm, where the initial distance between each asleep robot and the single active robot is at most 1. For $(\mathbb{R}^2, \ell_2)$, we improve the previous upper bound of 4.62 ([7], CCCG 2024) to 4.31. Note that it is known that 3.82 is a lower bound for the wake-up ratio. In $\mathbb{R}^3$, we propose a new strategy that achieves a wake-up ratio of 12 for $(\mathbb{R}^3, \ell_1)$ and 12.76 for $(\mathbb{R}^3, \ell_2)$, improving upon the previous bounds of 13 and $13\sqrt{3}$, respectively, reported in [2]."
2507.18607,"Large language models (LLMs) produce high-dimensional embeddings that capture rich semantic and syntactic relationships between words, sentences, and concepts. Investigating the topological structures of LLM embedding spaces via mapper graphs enables us to understand their underlying structures. Specifically, a mapper graph summarizes the topological structure of the embedding space, where each node represents a topological neighborhood (containing a cluster of embeddings), and an edge connects two nodes if their corresponding neighborhoods overlap. However, manually exploring these embedding spaces to uncover encoded linguistic properties requires considerable human effort. To address this challenge, we introduce a framework for semi-automatic annotation of these embedding properties. To organize the exploration process, we first define a taxonomy of explorable elements within a mapper graph such as nodes, edges, paths, components, and trajectories. The annotation of these elements is executed through two types of customizable LLM-based agents that employ perturbation techniques for scalable and automated analysis. These agents help to explore and explain the characteristics of mapper elements and verify the robustness of the generated explanations. We instantiate the framework within a visual analytics workspace and demonstrate its effectiveness through case studies. In particular, we replicate findings from prior research on BERT's embedding properties across various layers of its architecture and provide further observations into the linguistic properties of topological neighborhoods."
2507.19284,"The Mumford-Shah (MS) model is an important technique for mesh segmentation. Many existing researches focus on piecewise constant MS mesh segmentation model with total variation regularization, which pursue the shortest length of boundaries. Different from previous efforts, in this article, we propose a novel piecewise smooth MS mesh segmentation model by utilizing the relaxed total generalized variation regularization (rTGV). The new model assumes that the feature function of a mesh can be approximated by the sum of piecewise constant function and asmooth function, and the rTGV regularization is able to characterize the high order discontinuity of the geometric structure. The newly introduced method is effective in segmenting meshes with irregular structures and getting the better boundaries rather than the shortest boundaries. We solve the new model by alternating minimization and alternating direction method of multipliers (ADMM). Our algorithm is discussed from several aspects, and comparisons with several state-of-art methods. Experimental results show that our method can yield competitive results when compared to other approaches. In addition, our results compare favorably to those of the several state-of-art techniques when evaluated on the Princeton Segmentation Benchmark. Furthermore, the quantitative errors and computational costs confirm the robustness and efficiency of the proposed method."
2507.20897,"In this thesis, a new approach for constructing subdivision algorithms for generalized quadratic and cubic B-spline subdivision for subdivision surfaces and volumes is presented. First, a catalog of quality criteria for these subdivision algorithms is developed, serving as a guideline for the construction process.The construction begins by generating the desired subdominant eigenvectors as the vertices of regular convex 3-polytopes for volumes using circle packings. Subsequently, these polytopes are utilized to construct a Colin-de-Verdiere-matrix for the generalized quadratic and a Colin-de-Verdiere-like matrix for the generalized cubic B-spline subdivision. These matrices are then adjusted using the matrix exponential to obtain subdivision matrices with the desired properties.All subdivision algorithms introduced in this paper empirically exhibit a subdominant eigenvalue of 1/2 with the desired algebraic and geometric multiplicity. For the quadratic case, this property can even be formally proven. Moreover, the corresponding eigenvectors form a convex polytope in the central region for the generalized quadratic B-spline subdivision algorithms, while for the generalized cubic B-spline subdivision algorithms, they represent the refinement of a convex polytope. Additionally, the constructed subdivision algorithms fulfill various other quality criteria, such as affine invariance and convex hull preservation and respecting all symmetries. Furthermore, it is demonstrated that the original Catmull-Clark algorithm is not suitable for generalization to volumetric subdivision and that the established subdivision algorithms [Baj+02] and [JM99] do not exhibit a suitable spectrum for several combinatorial configurations. Additionally, research approaches for the volumetric case are proposed, aiming to generalize from hexahedral to arbitrary structures."
2507.21322,"We study the problem of sweeping a pseudoline arrangement with $n$ $x$-monotone curves with a rope (an $x$-monotone curve that connects the points at infinity). The rope can move by flipping over a face of the arrangement, replacing parts of it from the lower to the upper chain of the face. Counting as length of the rope the number of edges, what rope-length can be needed in such a sweep? We show that all such arrangements can be swept with rope-length at most $2n-2$, and for some arrangements rope-length at least $7(n-2)/4+1$ is required. We also discuss some complexity issues around the problem of computing a sweep with the shortest rope-length."
2507.22293,"The (unweighted) point-separation} problem asks, given a pair of points $s$ and $t$ in the plane, and a set of candidate geometric objects, for the minimum-size subset of objects whose union blocks all paths from $s$ to $t$. Recent work has shown that the point-separation problem can be characterized as a type of shortest-path problem in a geometric intersection graph within a special lifted space. However, all known solutions to this problem essentially reduce to some form of APSP, and hence take at least quadratic time, even for special object types.In this work, we consider the unweighted form of the problem, for which we devise subquadratic approximation algorithms for many special cases of objects, including line segments and disks. In this paradigm, we are able to devise algorithms that are fundamentally different from the APSP-based approach. In particular, we will give Monte Carlo randomized additive $+1$ approximation algorithms running in $\widetilde{\mathcal{O}}(n^{\frac32})$ time for disks as well as axis-aligned line segments and rectangles, and $\widetilde{\mathcal{O}}(n^{\frac{11}6})$ time for line segments and constant-complexity convex polygons. We will also give deterministic multiplicative-additive approximation algorithms that, for any value $\varepsilon>0$, guarantee a solution of size $(1+\varepsilon)\text{OPT}+1$ while running in $\widetilde{\mathcal{O}}\left(\frac{n}{\varepsilon^2}\right)$ time for disks as well as axis-aligned line segments and rectangles, and $\widetilde{\mathcal{O}}\left(\frac{n^{4/3}}{\epsilon^2}\right)$ time for line segments and constant-complexity convex polygons."
2507.23105,"In this paper we consider the problem of approximating Euclidean distances by the infinite integer grid graph. Although the topology of the graph is fixed, we have control over the edge-weight assignment $w:E\to \mathbb{R}_{\ge 0}$, and hope to have grid distances be asymptotically isometric to Euclidean distances, that is, for all grid points $u,v$, $\mathrm{dist}_w(u,v) = (1\pm o(1))\|u-v\|_2$. We give three methods for solving this problem, each attractive in its own way.* Our first construction is based on an embedding of the recursive, non-periodic pinwheel tiling of Radin and Conway into the integer grid. Distances in the pinwheel graph are asymptotically isometric to Euclidean distances, but no explicit bound on the rate of convergence was known. We prove that the multiplicative distortion of the pinwheel graph is $(1+1/\Theta(\log^\xi \log D))$, where $D$ is the Euclidean distance and $\xi=\Theta(1)$. The pinwheel tiling approach is conceptually simple, but can be improved quantitatively.* Our second construction is based on a hierarchical arrangement of ""highways."" It is simple, achieving stretch $(1 + 1/\Theta(D^{1/9}))$, which converges doubly exponentially faster than the pinwheel tiling approach.* The first two methods are deterministic. An even simpler approach is to sample the edge weights independently from a common distribution $\mathscr{D}$. Whether there exists a distribution $\mathscr{D}^*$ that makes grid distances Euclidean, asymptotically and in expectation, is major open problem in the theory of first passage percolation. Previous experiments show that when $\mathscr{D}$ is a Fisher distribution, grid distances are within 1\% of Euclidean. We demonstrate experimentally that this level of accuracy can be achieved by a simple 2-point distribution that assigns weights 0.41 or 4.75 with probability 44\% and 56\%, respectively."
2508.00251,"Reconstructing models from unorganized point clouds presents a significant challenge, especially when the models consist of multiple components represented by their surface point clouds. Such models often involve point clouds with noise that represent multiple closed surfaces with shared regions, making their automatic identification and separation inherently complex. In this paper, we propose an automatic method that uses the topological understanding provided by persistent homology, along with representative 2-cycles of persistent homology groups, to effectively distinguish and separate each closed surface. Furthermore, we employ Loop subdivision and least squares progressive iterative approximation (LSPIA) techniques to generate high-quality final surfaces and achieve complete model reconstruction. Our method is robust to noise in the point cloud, making it suitable for reconstructing models from such data. Experimental results demonstrate the effectiveness of our approach and highlight its potential for practical applications."
2508.01733,"The problem of embedding a set of objects into a low-dimensional Euclidean space based on a matrix of pairwise dissimilarities is fundamental in data analysis, machine learning, and statistics. However, the assumptions of many standard analytical methods are violated when the input dissimilarities fail to satisfy metric or Euclidean axioms. We present the mathematical and statistical foundations of Topolow, a physics-inspired, gradient-free optimization framework for such embedding problems. Topolow is conceptually related to force-directed graph drawing algorithms but is fundamentally distinguished by its goal of quantitative metric reconstruction. It models objects as particles in a physical system, and its novel optimization scheme proceeds through sequential, stochastic pairwise interactions, which circumvents the need to compute a global gradient and provides robustness against convergence to local optima, especially for sparse data. Topolow maximizes the likelihood under a Laplace error model, robust to outliers and heterogeneous errors, and properly handles censored data. Crucially, Topolow does not require the input dissimilarities to be metric, making it a robust solution for embedding non-metric measurements into a valid Euclidean space, thereby enabling the use of standard analytical tools. We demonstrate the superior performance of Topolow compared to standard Multidimensional Scaling (MDS) methods in reconstructing the geometry of sparse and non-Euclidean data. This paper formalizes the algorithm, first introduced as Topolow in the context of antigenic mapping in (Arhami and Rohani, 2025) (open access), with emphasis on its metric embedding and mathematical properties for a broader audience. The general-purpose function Euclidify is available in the R package topolow."
2508.02352,"This paper introduces a novel stability measure for edit distances between merge trees of piecewise linear scalar fields. We apply the new measure to various metrics introduced recently in the field of scalar field comparison in scientific visualization. While previous stability measures are unable to capture the fine-grained hierarchy of the considered distances, we obtain a classification of stability that fits the efficiency of current implementations and quality of practical results. Our results induce several open questions regarding the lacking theoretical analysis of such practical distances."
2508.04603,"We show a new construction for square packing, and prove that it is more efficient than previous results."
2508.05099,"This paper proposes improvements to the physically-based surface triangulation method, bubble meshing. The method simulates physical bubbles to automatically generate mesh vertices, resulting in high-quality Delaunay triangles. Despite its flexibility in local mesh size control and the advantage of local re-meshing, bubble meshing is constrained by high computational costs and slow convergence on complex surfaces. The proposed approach employs conformal mapping to simplify surface bubble packing by flattening the surface onto a plane. Surface triangulation is induced from the planar mesh, avoiding direct bubble movement on the surface. Optimizing bubble quantity control and separating it from the relaxation process accelerates convergence, cutting computation time by over 70%. The enhanced method enables efficient triangulation of disk topology surfaces, supports local size control, curvature adaptation, and re-meshing of discrete surfaces. Keywords: Adaptive triangulation, Surface remeshing, Bubble meshing, Conformal parameterization, Algorithm efficiency"
2508.07119,"To what extent is it possible to visualize high-dimensional datasets in a two- or three-dimensional space? We reframe this question in terms of embedding $n$-vertex graphs (representing the neighborhood structure of the input points) into metric spaces of low doubling dimension $d$, in such a way that maintains the separation between neighbors and non-neighbors. This seemingly lax embedding requirement is surprisingly difficult to satisfy. Our investigation shows that an overwhelming fraction of graphs require $d = \Omega(\log n)$. Even when considering sparse regular graphs, the situation does not improve, as an overwhelming fraction of such graphs requires $d= \Omega(\log n / \log\log n)$. The landscape changes dramatically when embedding into normed spaces. In particular, all but a vanishing fraction of graphs demand $d=\Theta(n)$. Finally, we study the implications of these results for visualizing data with intrinsic cluster structure. We find that graphs produced from a planted partition model with $k$ clusters on $n$ points typically require $d=\Omega(\log n)$, even when the cluster structure is salient. These results challenge the aspiration that constant-dimensional visualizations can faithfully preserve neighborhood structure."
2508.07529,"Chorematic diagrams are highly reduced schematic maps of geospatial data and processes. They can visually summarize complex situations using only a few simple shapes (choremes) placed upon a simplified base map. Due to the extreme reduction of data in chorematic diagrams, they tend to be produced manually; few automated solutions exist. In this paper we consider the algorithmic problem of summarizing classed region maps, such as choropleth or land use maps, using a chorematic diagram with a single disk choreme. It is infeasible to solve this problem exactly for large maps. Hence, we propose several point sampling strategies and use algorithms for classed point sets to efficiently find the best disk that represents one of the classes. We implemented our algorithm and experimentally compared sampling strategies and densities. The results show that with the right sampling strategy, high-quality results can be obtained already with moderately sized point sets and within seconds of computation time."
2508.08341,"Addressing irregular cutting and packing (C&P) optimization problems poses two distinct challenges: the geometric challenge of determining whether or not an item can be placed feasibly at a certain position, and the optimization challenge of finding a good solution according to some objective function. Until now, those tackling such problems have had to address both challenges simultaneously, requiring two distinct sets of expertise and a lot of research & development effort. One way to lower this barrier is to decouple the two challenges. In this paper we introduce a powerful collision detection engine (CDE) for 2D irregular C&P problems which assumes full responsibility for the geometric challenge. The CDE (i) allows users to focus with full confidence on their optimization challenge by abstracting geometry away and (ii) enables independent advances to propagate to all optimization algorithms built atop it. We present a set of core principles and design philosophies to model a general and adaptable CDE focused on maximizing performance, accuracy and robustness. These principles are accompanied by a concrete open-source implementation called $\texttt{jagua-rs}$. This paper together with its implementation serves as a catalyst for future advances in irregular C&P problems by providing a solid foundation which can either be used as it currently exists or be further improved upon."
2508.08433,"Contour trees offer an abstract representation of the level set topology in scalar fields and are widely used in topological data analysis and visualization. However, applying contour trees to large-scale scientific datasets remains challenging due to scalability limitations. Recent developments in distributed hierarchical contour trees have addressed these challenges by enabling scalable computation across distributed systems. Building on these structures, advanced analytical tasks -- such as volumetric branch decomposition and contour extraction -- have been introduced to facilitate large-scale scientific analysis. Despite these advancements, such analytical tasks substantially increase memory usage, which hampers scalability. In this paper, we propose a pre-simplification strategy to significantly reduce the memory overhead associated with analytical tasks on distributed hierarchical contour trees. We demonstrate enhanced scalability through strong scaling experiments, constructing the largest known contour tree -- comprising over half a trillion nodes with complex topology -- in under 15 minutes on a dataset containing 550 billion elements."
2508.09638,"The sliding square model is a widely used abstraction for studying self-reconfigurable robotic systems, where modules are square-shaped robots that move by sliding or rotating over one another. In this paper, we propose a novel distributed algorithm that allows a group of modules to reconfigure into a rhombus shape, starting from an arbitrary side-connected configuration. It is connectivity-preserving and operates under minimal assumptions: one leader module, common chirality, constant memory per module, and visibility and communication restricted to immediate neighbors. Unlike prior work, which relaxes the original sliding square move-set, our approach uses the unmodified move-set, addressing the additional challenge of handling locked configurations. Our algorithm is sequential in nature and operates with a worst-case time complexity of $\mathcal{O}(n^2)$ rounds, which is optimal for sequential algorithms. To improve runtime, we introduce two parallel variants of the algorithm. Both rely on a spanning tree data structure, allowing modules to make decisions based on local connectivity. Our experimental results show a significant speedup for the first variant, and linear average runtime for the second variant, which is worst-case optimal for parallel algorithms."
2508.09734,"The contiguous art gallery problem was introduced at SoCG'25 in a merged paper that combined three simultaneous results, each achieving a polynomial-time algorithm for the problem. This problem is a variant of the classical art gallery problem, first introduced by Klee in 1973. In the contiguous art gallery problem, we are given a polygon P and asked to determine the minimum number of guards needed, where each guard is assigned a contiguous portion of the boundary of P that it can see, such that all assigned portions together cover the boundary of P. The classical art gallery problem is NP-hard and ER-complete, and the three independent works investigated whether this variant admits a polynomial-time solution. Each of these works indeed presented such a solution, with the fastest running in O(k n^5 log n) time, where n denotes the number of vertices of P and k is the size of a minimum guard set covering the boundary of P. We present a solution that is both considerably simpler and significantly faster, yielding a concise and almost entirely self-contained O(k n^2 log^2 n)-time algorithm."
2508.09909,"This SHREC 2025 track focuses on the recognition and segmentation of relief patterns embedded on the surface of a set of synthetically generated triangle meshes. We report the methods proposed by the participants, whose performance highlights the inherent complexity of solving the problem, which is still open. Then, we discuss the critical aspects of the proposed tasks, highlight the limitations of current techniques, and outline possible directions for future research. All resources and track details are available at the official track webpage:this https URL."
2508.10136,"We show that the recognition problem for penny graphs (contact graphs of unit disks in the plane) is $\exists\mathbb{R}$-complete, that is, computationally as hard as the existential theory of the reals, even if a combinatorial plane embedding of the graph is given. The exact complexity of the penny graph recognition problem has been a long-standing open problem.We lift the penny graph result to three dimensions and show that the recognition problem for marble graphs (contact graphs of unit balls in three dimensions) is $\exists\mathbb{R}$-complete.Finally, we show that rigidity of penny graphs is $\forall\mathbb{R}$-complete and look at grid embeddings of penny graphs that are trees."
2508.10537,"We study approximating the continuous Fréchet distance of two curves with complexity $n$ and $m$, under the assumption that only one of the two curves is $c$-packed. Driemel, Har{-}Peled and Wenk DCG'12 studied Fréchet distance approximations under the assumption that both curves are $c$-packed. In $\mathbb{R}^d$, they prove a $(1+\varepsilon)$-approximation in $\tilde{O}(d \, c\,\frac{n+m}{\varepsilon})$ time. Bringmann and Künnemann IJCGA'17 improved this to $\tilde{O}(c\,\frac{n + m }{\sqrt{\varepsilon}})$ time, which they showed is near-tight under SETH. Recently, Gudmundsson, Mai, and Wong ISAAC'24 studied our setting where only one of the curves is $c$-packed. They provide an involved $\tilde{O}( d \cdot (c+\varepsilon^{-1})(cn\varepsilon^{-2} + c^2m\varepsilon^{-7} + \varepsilon^{-2d-1}))$-time algorithm when the $c$-packed curve has $n$ vertices and the arbitrary curve has $m$, where $d$ is the dimension in Euclidean space. In this paper, we show a simple technique to compute a $(1+\varepsilon)$-approximation in $\mathbb{R}^d$ in time $O(d \cdot c\,\frac{n+m}{\varepsilon}\log\frac{n+m}{\varepsilon})$ when one of the curves is $c$-packed. Our approach is not only simpler than previous work, but also significantly improves the dependencies on $c$, $\varepsilon$, and $d$. Moreover, it almost matches the asymptotically tight bound for when both curves are $c$-packed. Our algorithm is robust in the sense that it does not require knowledge of $c$, nor information about which of the two input curves is $c$-packed."
2508.11507,"A {$t$-stretch tree cover} of a metric space $M = (X,\delta)$, for a parameter $t \ge 1$, is a collection of trees such that every pair of points has a $t$-stretch path in one of the trees. Tree covers provide an important sketching tool that has found various applications over the years. The celebrated {Dumbbell Theorem} by Arya et al. [STOC'95] states that any set of points in the Euclidean plane admits a $(1+\epsilon)$-stretch tree cover with $O_\epsilon(1)$ trees. This result extends to any (constant) dimension and was also generalized for arbitrary doubling metrics by Bartal et al. [ICALP'19].Although the number of trees provided by the Dumbbell Theorem is constant, this constant is not small, even for a stretch significantly larger than $1+\epsilon$. At the other extreme, any single tree on the vertices of a regular $n$-polygon must incur a stretch of $\Omega(n)$. Using known results of ultrametric embeddings, one can easily get a stretch of $\tilde{O}(\sqrt{n})$ using two trees. The question of whether a low stretch can be achieved using two trees has remained illusive, even in the Euclidean plane.In this work, we resolve this fundamental question in the affirmative by presenting a constant-stretch cover with a pair of trees, for any set of points in the Euclidean plane. Our main technical contribution is a {surprisingly simple} Steiner construction, for which we provide a {tight} stretch analysis of $\sqrt{26}$. The Steiner points can be easily pruned if one is willing to increase the stretch by a small constant. Moreover, we can bound the maximum degree of the construction by a constant.Our result thus provides a simple yet effective reduction tool -- for problems that concern approximate distances -- from the Euclidean plane to a pair of trees. To demonstrate the potential power of this tool, we present some applications [...]"
2508.11555,"Any $n$-point set in the $d$-dimensional Euclidean space $\mathbb{R}^d$, for $d = O(1)$, admits a $(1+\epsilon)$-spanner with $\tilde{O}(n \cdot \epsilon^{-d+1})$ edges and lightness $\tilde{O}(\epsilon^{-d})$, for any $\epsilon > 0$. (The {lightness} is a normalized notion of weight, where we divide the spanner weight by the MST weight. The $\tilde{O}$ and $\tilde{\Omega}$ notations hide $\texttt{polylog}(\epsilon^{-1})$ terms.) Moreover, this result is tight: For any $2 \le d = O(1)$, there exists an $n$-point set in $\mathbb{R}^d$, for which any $(1+\epsilon)$-spanner has $\tilde{\Omega}(n \cdot \epsilon^{-d+1})$ edges and lightness $\tilde{\Omega}(n \cdot \epsilon^{-d})$.The upper bounds for Euclidean spanners rely heavily on the spatial property of {cone partitioning} in $\mathbb{R}^d$, which does not seem to extend to the wider family of {doubling metrics}, i.e., metric spaces of constant {doubling dimension}. In doubling metrics, a {simple spanner construction from two decades ago, the {net-tree spanner}}, has $\tilde{O}(n \cdot \epsilon^{-d})$ edges, and it could be transformed into a spanner of lightness $\tilde{O}(n \cdot \epsilon^{-(d+1)})$ by pruning redundant edges. Despite a large body of work, it has remained an open question whether the superior Euclidean bounds of $\tilde{O}(n \cdot \epsilon^{-d+1})$ edges and lightness $\tilde{O}(\epsilon^{-d})$ could be achieved also in doubling metrics. We resolve this question in the negative by presenting a surprisingly simple and tight lower bound, which shows, in particular, that the net-tree spanner and its pruned version are both optimal."
2508.11633,"Symbolic and graphical tools, such as Mathematica, enable precise visualization and analysis of void spaces in sphere packings. In the cubic close packing (CCP, or face-centred cubic packing; FCC) arrangement these voids can be partitioned into repeating geometric units we term spherically truncated polyhedra - bodies analogous to plane-truncated polyhedra but bounded by both planar and spherical surfaces. These structures are relevant in geometric studies and applications such as modelling diffusion in porous media and biological tissues. This work examines the properties of these complementary bodies, deriving their surface area-to-volume ratios, which are significant in physical contexts; and we establish a result concerning the packing density of truncated tetrahedra and octahedra, demonstrating how they tile the interstitial space surrounding packed spheres. These findings contribute to a deeper understanding of classical packing problems and their geometrical complements."
2508.14339,"Many scientific and engineering problems are modelled by simulating scalar fields defined either on space-filling meshes (Eulerian) or as particles (Lagrangian). For analysis and visualization, topological primitives such as contour trees can be used, but these often need simplification to filter out small-scale features. For parcel-based convective cloud simulations, simplification of the contour tree requires a volumetric measure rather than persistence. Unlike for cubic meshes, volume cannot be approximated by counting regular vertices. Typically, this is addressed by resampling irregular data onto a uniform grid. Unfortunately, the spatial proximity of parcels requires a high sampling frequency, resulting in a massive increase in data size for processing. We therefore extend volume-based contour tree simplification to parcel-in-cell simulations with a graph adaptor in Viskores (VTK-m), using Delaunay tetrahedralization of the parcel centroids as input. Instead of relying on a volume approximation by counting regular vertices -- as was done for cubic meshes -- we adapt the 2D area splines reported by Bajaj et al.https://doi.org/10.1145/259081.259279, and Zhou et al.https://doi.org/10.1109/TVCG.2018.2796555. We implement this in Viskores (formerly called VTK-m) as prefix-sum style hypersweeps for parallel efficiency and show how it can be generalized to compute any integrable property. Finally, our results reveal that contour trees computed directly on the parcels are orders of magnitude faster than computing them on a resampled grid, while also arguably offering better quality segmentation, avoiding interpolation artifacts."
2508.14369,"The extended Gaussian family is the closure of the Gaussian family obtained by completing the Gaussian family with the counterpart elements induced by degenerate covariance or degenerate precision matrices, or a mix of both degeneracies. The parameter space of the extended Gaussian family forms a symmetric positive semi-definite matrix bicone, i.e. two partial symmetric positive semi-definite matrix cones joined at their bases. In this paper, we study the Hilbert geometry of such an open bounded convex symmetric positive-definite bicone. We report the closed-form formula for the corresponding Hilbert metric distance and study exhaustively its invariance properties. We also touch upon potential applications of this geometry for dealing with extended Gaussian distributions."
2508.14407,"This study presents a novel algorithm for identifying the set of extreme points that constitute the exact convex hull of a point set in high-dimensional Euclidean space. The proposed method iteratively solves a sequence of dynamically updated quadratic programming (QP) problems for each point and exploits their solutions to provide theoretical guarantees for exact convex hull identification. For a dataset of \( n \) points in an \( m \)-dimensional space, the algorithm achieves a dimension-independent worst-case time complexity of \( O(n^{p+2} \log(1/\epsilon)) \), where \( p \) depends on the choice of QP solver (e.g., \( p = 4 \) corresponds to the worst-case bound when using an interior-point method), and \( \epsilon \) denotes the target numerical precision (i.e., the optimality tolerance of the QP solver).The proposed method is applicable to spaces of arbitrary dimensionality and exhibits particular efficiency in high-dimensional settings, owing to its polynomial-time complexity, whereas existing exponential-time algorithms become computationally impractical."
2508.14429,"The computation of homology groups for evolving simplicial complexes often requires repeated reconstruction of boundary operators, resulting in prohibitive costs for large-scale or frequently updated data. This work introduces MMHM, a Morse-based Modular Homology Maintenance framework that preserves homological invariants under local complex modifications. An initial discrete Morse reduction produces a critical cell complex chain-homotopy equivalent to the input; subsequent edits trigger localized updates to the affected part of the reduced boundary operators over a chosen coefficient ring. By restricting recomputation to the affected critical cells and applying localized matrix reductions, the approach achieves significant amortized performance gains while guaranteeing homology preservation. A periodic recompression policy together with topology-aware gating and a column-oriented sparse boundary representation with a pivot-ownership map confines elimination to the affected columns and can bypass linear algebra when invariants are decidable combinatorially. The framework offers a drop-in upgrade for topology pipelines, turning costly rebuilds into fast, exact updates that track homology through local edits. Reframing dynamic homology as a locality-bounded maintenance task provides an exact alternative to global recomputation for evolving meshes and complexes."
2508.1482,"We study the rectilinear Marco Polo problem, which generalizes the Euclidean version of the Marco Polo problem for performing geometric localization to rectilinear search environments, such as in geometries motivated from urban settings, and to higher dimensions. In the rectilinear Marco Polo problem, there is at least one point of interest (POI) within distance $n$, in either the $L_1$ or $L_\infty$ metric, from the origin. Motivated from a search-and-rescue application, our goal is to move a search point, $\Delta$, from the origin to a location within distance $1$ of a POI. We periodically issue probes from $\Delta$ out a given distance (in either the $L_1$ or $L_\infty$ metric) and if a POI is within the specified distance of $\Delta$, then we learn this (but no other location information). Optimization goals are to minimize the number of probes and the distance traveled by $\Delta$. We describe a number of efficient search strategies for rectilinear Marco Polo problems and we analyze each one in terms of the size, $n$, of the search domain, as defined by the maximum distance to a POI."
2508.1552,"A flip in a plane spanning tree $T$ is the operation of removing one edge from $T$ and adding another edge such that the resulting structure is again a plane spanning tree. For trees on a set of points in convex position we study two classic types of constrained flips: (1)~Compatible flips are flips in which the removed and inserted edge do not cross each other. We relevantly improve the previous upper bound of $2n-O(\sqrt{n})$ on the diameter of the compatible flip graph to~$\frac{5n}{3}-O(1)$, by this matching the upper bound for unrestricted flips by Bjerkevik, Kleist, Ueckerdt, and Vogtenhuber [SODA~2025] up to an additive constant of $1$. We further show that no shortest compatible flip sequence removes an edge that is already in its target position. Using this so-called happy edge property, we derive a fixed-parameter tractable algorithm to compute the shortest compatible flip sequence between two given trees. (2)~Rotations are flips in which the removed and inserted edge share a common vertex. Besides showing that the happy edge property does not hold for rotations, we improve the previous upper bound of $2n-O(1)$ for the diameter of the rotation graph to~$\frac{7n}{4}-O(1)$."
2508.15557,"Graph drawings are commonly used to visualize relational data. User understanding and performance are linked to the quality of such drawings, which is measured by quality metrics. The tacit knowledge in the graph drawing community about these quality metrics is that they are not always able to accurately capture the quality of graph drawings. In particular, such metrics may rate drawings with very poor quality as very good. In this work we make this tacit knowledge explicit by showing that we can modify existing graph drawings into arbitrary target shapes while keeping one or more quality metrics almost identical. This supports the claim that more advanced quality metrics are needed to capture the 'goodness' of a graph drawing and that we cannot confidently rely on the value of a single (or several) certain quality metrics."
2508.16178,"In a simple drawing of a graph, any two edges intersect in at most one point (either a common endpoint or a proper crossing). A simple drawing is generalized twisted if it fulfills certain rather specific constraints on how the edges are drawn. An abstract rotation system of a graph assigns to each vertex a cyclic order of its incident edges. A realizable rotation system is one that admits a simple drawing such that at each vertex, the edges emanate in that cyclic order, and a generalized twisted rotation system can be realized as a generalized twisted drawing. Generalized twisted drawings have initially been introduced to obtain improved bounds on the size of plane substructures in any simple drawing of $K_n$. They have since gained independent interest due to their surprising properties. However, the definition of generalized twisted drawings is very geometric and drawing-specific.In this paper, we develop characterizations of generalized twisted drawings that enable a purely combinatorial view on these drawings and lead to efficient recognition algorithms. Concretely, we show that for any $n \geq 7$, an abstract rotation system of $K_n$ is generalized twisted if and only if all subrotation systems induced by five vertices are generalized twisted. This implies a drawing-independent and concise characterization of generalized twistedness. Besides, the result yields a simple $O(n^5)$-time algorithm to decide whether an abstract rotation system is generalized twisted and sheds new light on the structural features of simple drawings. We further develop a characterization via the rotations of a pair of vertices in a drawing, which we then use to derive an $O(n^2)$-time algorithm to decide whether a realizable rotation system is generalized twisted."
2508.1671,"We present more optimal solutions to the snowblower problem introduced inarXiv:cs/0603026. In particular, we present more optimal ways to clear lines and combs, which are shapes as described in the aforementioned paper that the original input is dissected into. We note that our methods to clear lines and combs reduce the number of steps needed to clear such shapes, while also acknowledging that they do not change the overall complexity."
2508.16875,"For a given metric space $(P,\phi)$, a tree cover of stretch $t$ is a collection of trees on $P$ such that edges $(x,y)$ of trees receive length $\phi(x,y)$, and such that for any pair of points $u,v\in P$ there is a tree $T$ in the collection such that the induced graph distance in $T$ between $u$ and $v$ is at most $t\phi(u,v).$ In this paper, we show that, for any set of points $P$ on the Euclidean plane, there is a tree cover consisting of two trees and with stretch $O(1).$ Although the problem in higher dimensions remains elusive, we manage to prove that for a slightly stronger variant of a tree cover problem we must have at least $(d+1)/2$ trees in any constant stretch tree cover in $\mathbb R^d$."
2508.17349,"In this paper, we give a polynomial-time algorithm for deciding whether an input bipartite graph admits a 2-layer fan-planar drawing, resolving an open problem posed in several papers since 2015."
2508.17496,"Convex hull data structures are fundamental in computational geometry. We study insertion-only data structures, supporting various containment and intersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex hulls can be constructed in linear time using classical algorithms such as Graham scan. We investigate a variety of methods tailored to the insertion-only setting. We explore a broad selection of trade-offs involving robustness, memory access patterns, and space usage, providing an extensive evaluation of both existing and novel techniques. Logarithmic-time methods rely on pointer-based tree structures, which suffer in practice due to poor memory locality. Motivated by this, we develop a vector-based solution inspired by Overmars' logarithmic method. Our structure has worse asymptotic bounds, supporting queries in $O(\log^2 n)$ time, but stores data in $O(\log n)$ contiguous vectors, greatly improving cache performance.Through empirical evaluation on real-world and synthetic data sets, we uncover surprising trends. Let $h$ denote the size of the convex hull. We show that a naïve $O(h)$ insertion-only algorithm based on Graham scan consistently outperforms both theoretical and practical state-of-the-art methods under realistic workloads, even on data sets with rather large convex hulls. While tree-based methods with $O(\log h)$ update times offer solid theoretical guarantees, they are never optimal in practice. In contrast, our vector-based logarithmic method, despite its theoretically inferior bounds, is highly competitive across all tested scenarios. It is optimal whenever the convex hull becomes large."
2508.17532,"We address the problem of computing a dynamic visualization of a geometric graph $G$ as a sequence of frames. Each frame shows only a portion of the graph but their union covers $G$ entirely. The two main requirements of our dynamic visualization are: $(i)$ guaranteeing drawing stability, so to preserve the user's mental map; $(ii)$ keeping the visual complexity of each frame low. To satisfy the first requirement, we never change the position of the vertices. Regarding the second requirement, we avoid edge crossings in each frame. More precisely, in the first frame we visualize a suitable subset of non-crossing edges; in each subsequent frame, exactly one new edge enters the visualization and all the edges that cross with it are deleted. We call such a sequence of frames a planar story of $G$. Our goal is to find a planar story whose minimum number of edges contemporarily displayed is maximized (i.e., a planar story that maximizes the minimum frame size). Besides studying our model from a theoretical point of view, we also design and experimentally compare different algorithms, both exact techniques and heuristics. These algorithms provide an array of alternative trade-offs between efficiency and effectiveness, also depending on the structure of the input graph."
2508.17549,"We prove that, for every plane graph $G$ and every smooth convex curve $C$ not on a single line, there exists a straight-line drawing of $G$ for which every face is crossed by $C$."
2508.18222,"I present a coordinate-free, symbolic framework for determining whether a given set of polygonal faces can form a closed, genus-zero polyhedral surface and for predicting how such a surface could be decomposed into internal tetrahedra. The method uses only discrete incidence variables, such as the number of internal tetrahedra $T$, internal gluing triangles $N_i$, and internal triangulation segments $S_i$, and applies combinatorial feasibility checks before any geometric embedding is attempted. For polyhedra in \emph{normal form}, I record exact incidence identities linking $V,E,F$ to a flatness parameter $S:=\sum_f(\tmop{deg} f-3)$, and I identify parity-sensitive effects in $E$, $F$, and $S$. The external identities and parity-sensitive bounds hold universally for genus-0 polyhedral graphs. For internal quantities, I prove exact relations $N_i=2T-V+2$ and $T-N_i+S_i=1$ (with $S_i$ taken to be the number of interior edges) and obtain restricted linear ranges for internally decomposed polyhedra with the minimal number of added internal edges. Consequently, I propose a symbolic workflow that yields rapid pre-checks for structural impossibility, reducing the need for costly geometric validation in computational geometry, graphics, and automated modeling."
2508.18457,"We study the problem of reconfiguring odd matchings, that is, matchings that cover all but a single vertex. Our reconfiguration operation is a so-called flip where the unmatched vertex of the first matching gets matched, while consequently another vertex becomes unmatched. We consider two distinct settings: the geometric setting, in which the vertices are points embedded in the plane and all occurring odd matchings are crossing-free, and a combinatorial setting, in which we consider odd matchings in general graphs.For the latter setting, we provide a complete polynomial time checkable characterization of graphs in which any two odd matchings can be reconfigured into each another. This complements the previously known result that the flip graph is always connected in the geometric setting [Aichholzer, Brötzner, Perz, and Schnider. Flips in odd matchings]. In the combinatorial setting, we prove that the diameter of the flip graph, if connected, is linear in the number of vertices. Furthermore, we establish that deciding whether there exists a flip sequence of length $k$ transforming one given matching into another is NP-complete in both the combinatorial and the geometric settings. To prove the latter, we introduce a framework that allows us to transform partial order types into general position with only polynomial overhead. Finally, we demonstrate that when parameterized by the flip distance $k$, the problem is fixed-parameter tractable (FPT) in the geometric setting when restricted to convex point sets."
2508.18535,"We study a question that lies at the intersection of classical research subjects in Topological Graph Theory and Graph Drawing: Computing a drawing of a graph with a prescribed number of crossings on a given set $S$ of points, while ensuring that its curve complexity (i.e., maximum number of bends per edge) is bounded by a constant. We focus on trees: Let $T$ be a tree, $\vartheta(T)$ be its thrackle number, and $\chi$ be any integer in the interval $[0,\vartheta(T)]$. In the tangling phase we compute a topological linear embedding of $T$ with $\vartheta(T)$ edge crossings and a constant number of spine traversals. In the untangling phase we remove edge crossings without increasing the spine traversals until we reach $\chi$ crossings. The computed linear embedding is used to construct a drawing of $T$ on $S$ with $\chi$ crossings and constant curve complexity. Our approach gives rise to an $O(n^2)$-time algorithm for general trees and an $O(n \log n)$-time algorithm for paths. We also adapt the approach to compute RAC drawings, i.e. drawings where the angles formed at edge crossings are $\frac{\pi}{2}$."
2509.0084,"We propose a novel method to generate a small set of ruled surfaces that do not collide with the input shape for linear hot-wire rough machining. Central to our technique is a new observation: the ruled surfaces constructed by vertical extrusion from planar smooth curves, which approach the input shape's outer contour lines while having no collisions, are capable of removing materials effectively during rough machining. Accordingly, we develop an iterative algorithm that alternates in each iteration between computing a viewpoint to determine an outer contour line and optimizing a smooth curve to approximate that contour line under the collision-free constraint. Specifically, a view selection approach based on genetic algorithm is used to optimize the viewpoint for removing materials as much as possible, and present an adaptive algorithm to find the constrained curves. The feasibility and practicability of our method are demonstrated through 10 physical examples. Compared with manual designs, our method obtains lower errors with the same number of cuts."
2509.01096,"Given two classes of graphs, $\mathcal{G}_1\subseteq \mathcal{G}_2$, and a $c$-connected graph $G\in \mathcal{G}_1$, we wish to augment $G$ with a smallest cardinality set of new edges $F$ to obtain a $k$-connected graph $G'=(V,E\cup F) \in \mathcal{G}_2$. In general, this is the $c\to k$ connectivity augmentation problem. Previous research considered variants where $\mathcal{G}_1=\mathcal{G}_2$ is the class of planar graphs, plane graphs, or planar straight-line graphs. In all three settings, we prove that the $c\to k$ augmentation problem is NP-complete when $2\leq c<k\leq 5$.However, the connectivity of the augmented graph $G'$ is at most $5$ if $\mathcal{G}_2$ is limited to planar graphs. We initiate the study of the $c\to k$ connectivity augmentation problem for arbitrary $k\in \mathbb{N}$, where $\mathcal{G}_1$ is the class of planar graphs, plane graphs, or planar straight-line graphs, and $\mathcal{G}_2$ is a beyond-planar class of graphs: $\ell$-planar, $\ell$-plane topological, or $\ell$-plane geometric graphs. We obtain tight bounds on the tradeoffs between the desired connectivity $k$ and the local crossing number $\ell$ of the augmented graph $G'$. We also show that our hardness results apply to this setting.The connectivity augmentation problem for triangulations is intimately related to edge flips; and the minimum augmentation problem to the flip distance between triangulations. We prove that it is NP-complete to find the minimum flip distance between a given triangulation and a 4-connected triangulation, settling an open problem posed in 2014, and present an EPTAS for this problem."
2509.01443,"We consider Steiner spanners in Euclidean and non-Euclidean geometries. In the Euclidean setting, a recent line of work initiated by Le and Solomon [FOCS'19] and further improved by Chang et al. [SoCG'24] obtained Steiner $(1+\varepsilon)$-spanners of size $O_d(\varepsilon^{(1-d)/2}\log(1/\varepsilon)n)$, nearly matching the lower bounds of Bhore and Tóth [SIDMA'22].We obtain Steiner $(1+\varepsilon)$-spanners of size $O_d(\varepsilon^{(1-d)/2}\log(1/\varepsilon)n)$ not only in $d$-dimensional Euclidean space, but also in $d$-dimensional spherical and hyperbolic space. For any fixed dimension $d$, the obtained edge count is optimal up to an $O(\log(1/\varepsilon))$ factor in each of these spaces. Unlike earlier constructions, our Steiner spanners are based on simple quadtrees, and they can be dynamically maintained, leading to efficient data structures for dynamic approximate nearest neighbours and bichromatic closest pair.In the hyperbolic setting, we also show that $2$-spanners in the hyperbolic plane must have $\Omega(n\log n)$ edges, and we obtain a $2$-spanner of size $O_d(n\log n)$ in $d$-dimensional hyperbolic space, matching our lower bound for any constant $d$. Finally, we give a Steiner spanner with additive error $\varepsilon$ in hyperbolic space with $O_d(\varepsilon^{(1-d)/2}\log(\alpha(n)/\varepsilon)n)$ edges, where $\alpha(n)$ is the inverse Ackermann function.Our techniques generalize to closed orientable surfaces of constant curvature as well as to some quotient spaces."
2509.0158,"A curve $\gamma$ that connects $s$ and $t$ has the increasing chord property if $|bc| \leq |ad|$ whenever $a,b,c,d$ lie in that order on $\gamma$. For planar curves, the length of such a curve is known to be at most $2\pi/3 \cdot |st|$. Here we examine the question in higher dimensions and from the algorithmic standpoint and show the following:(I) The length of any $s-t$ curve with increasing chords in $\mathbf{R}^d$ is at most $2 \cdot \left( e/2 \cdot (d+4) \right)^{d-1} \cdot |st|$ for every $d \geq 3$. This is the first bound in higher dimensions.(II) Given a polygonal chain $P=(p_1, p_2, \dots, p_n)$ in $\mathbf{R}^d$, where $d \geq 4$, $k =\lfloor d/2 \rfloor$, it can be tested whether it satisfies the increasing chord property in $O\left(n^{2-1/(k+1)} {\rm polylog} (n) \right)$ expected time. This is the first subquadratic algorithm in higher dimensions."
2509.03761,"Alluvial plots can be effective for visualization of multivariate data, but rely on ordering of alluvia that can be non-trivial to arrange. We formulate two optimization problems that formalize the challenge of ordering and coloring partitions in alluvial plots. While solving these optimization problems is challenging in general, we show that the NeighborNet algorithm from phylogenetics can be adapted to provide excellent results in typical use cases. Our methods are implemented in a freely available R package available on GitHub atthis https URL"
2509.04168,"A strengthened version of Harborth's well-known conjecture -- known as Kleber's conjecture -- states that every planar graph admits a planar straight-line drawing where every edge has integer length and each vertex is restricted to the integer grid. Positive results for Kleber's conjecture are known for planar 3-regular graphs, for planar graphs that have maximum degree 4, and for planar 3-trees. However, all but one of the existing results are existential and do not provide bounds on the required grid size. In this paper, we provide polynomial-time algorithms for computing crossing-free straight-line drawings of trees and cactus graphs with integer edge lengths and integer vertex position on polynomial-size integer grids."
2509.04211,"Suppose that a polygon $P$ is given as an array containing the vertices in counterclockwise order. We analyze how many vertices (including the index of each of these vertices) we need to know before we can bound $P$, i.e., report a bounded region $R$ in the plane such that $P\subset R$. We show that there exists polygons where $4\log_2 n+O(1)$ vertices are enough, while $\log_3n-o(\log n)$ must always be known. We thus answer the question up to a constant factor. This can be seen as an analysis of the shortest possible certificate or the best-case running time of any algorithm solving a variety of problems involving polygons, where a bound must be known in order to answer correctly. This includes various packing problems such as deciding whether a polygon can be contained inside another polygon."
2509.05997,"We revisit the notion of WSPD (i.e., well-separated pairs-decomposition), presenting a new construction of WSPD for any finite metric space, and show that it is asymptotically instance-optimal in size. Next, we describe a new WSPD construction for the weighted unit-distance metric in the plane, and show a bound $O( \varepsilon^{-2} n \log n)$ on its size, improving by a factor of $1/\varepsilon^2$ over previous work. The new construction is arguably simpler and more elegant.We point out that using WSPD, one can approximate, in near-linear time, the distortion of a bijection between two point sets in low dimensions. As a new application of WSPD, we show how to shortcut a polygonal curve such that its dilation is below a prespecified quantity. In particular, we show a near-linear time algorithm for computing a simple subcurve for a given polygonal curve in the plane so that the new subcurve has no self-intersection."
2509.06108,"We present a novel approach to graph drawing based on reinforcement learning for minimizing the global and the local crossing number, that is, the total number of edge crossings and the maximum number of crossings on any edge, respectively. In our framework, an agent learns how to move a vertex based on a given observation vector in order to optimize its position. The agent receives feedback in the form of local reward signals tied to crossing reduction. To generate an initial layout, we use a stress-based graph-drawing algorithm. We compare our method against force- and stress-based (baseline) algorithms as well as three established algorithms for global crossing minimization on a suite of benchmark graphs. The experiments show mixed results: our current algorithm is mainly competitive for the local crossing number. We see a potential for further development of the approach in the future."
2509.07906,"Given a periodic placement of copies of a tromino (either L or I), we prove co-RE-completeness (and hence undecidability) of deciding whether it can be completed to a plane tiling. By contrast, the problem becomes decidable if the initial placement is finite, or if the tile is a domino instead of a tromino (in any dimension). As a consequence, tiling a given periodic subset of the plane with a given tromino (L or I) is co-RE-complete.We also prove co-RE-completeness of tiling the entire plane with two polyominoes (one of which is disconnected and the other of which has constant size), and of tiling 3D space with two connected polycubes (one of which has constant size). If we restrict to tiling by translation only (no rotation), then we obtain co-RE-completeness with one more tile: two trominoes for a periodic subset of 2D, three polyominoes for the 2D plane, and three connected polycubes for 3D space.Along the way, we prove several new complexity and algorithmic results about periodic (infinite) graphs. Notably, we prove that Periodic Planar (1-in-)3SAT-3, 3DM, and Graph Orientation are co-RE-complete in 2D and PSPACE-complete in 1D; we extend basic results in graph drawing to 2D periodic graphs; and we give a polynomial-time algorithm for perfect matching in bipartite periodic graphs."
2509.09325,"Swept volume computation, the determination of regions occupied by moving objects, is essential in graphics, robotics, and manufacturing. Existing approaches either explicitly track surfaces, suffering from robustness issues under complex interactions, or employ implicit representations that trade off geometric fidelity and face optimization difficulties. We propose a novel inversion of motion perspective: rather than tracking object motion, we fix the object and trace spatial points backward in time, reducing complex trajectories to efficiently linearizable point motions. Based on this, we introduce a multi field tetrahedral framework that maintains multiple distance fileds per element, preserving fine geometric details at trajectory intersections where single field methods fail. Our method robustly computes swept volumes for diverse motions, including translations and screw motions, and enables practical applications in path planning and collision detection."
2509.09333,"Computing offsets of curves on parametric surfaces is a fundamental yet challenging operation in computer aided design and manufacturing. Traditional analytical approaches suffer from time-consuming geodesic distance queries and complex self intersection handling, while discrete methods often struggle with precision. In this paper, we propose a totally different algorithm paradigm. Our key insight is that by representing the source curve as a sequence of line segment primitives, the Voronoi decomposition constrained to the parametric surface enables localized offset computation. Specifically, the offsetting process can be efficiently traced by independently visiting the corresponding Voronoi cells. To address the challenge of computing the Voronoi decomposition on parametric surfaces, we introduce two key techniques. First, we employ intrinsic triangulation in the parameter space to accurately capture geodesic distances. Second, instead of directly computing the surface-constrained Voronoi decomposition, we decompose the triangulated parameter plane using a series of plane cutting operations. Experimental results demonstrate that our algorithm achieves superior accuracy and runtime performance compared to existing methods. We also present several practical applications enabled by our approach."
2509.09981,"We study the problem of partitioning a polygon into the minimum number of subpolygons using cuts in predetermined directions such that each resulting subpolygon satisfies a given width constraint. A polygon satisfies the unit-width constraint for a set of unit vectors if the length of the orthogonal projection of the polygon on a line parallel to a vector in the set is at most one. We analyze structural properties of the minimum partition numbers, focusing on monotonicity under polygon containment. We show that the minimum partition number of a simple polygon is at least that of any subpolygon, provided that the subpolygon satisfies a certain orientation-wise convexity with respect to the polygon. As a consequence, we prove a partition analogue of Bang's conjecture about coverings of convex regions in the plane: for any partition of a convex body in the plane, the sum of relative widths of all parts is at least one. For any convex polygon, there exists a direction along which an optimal partition is achieved by parallel cuts. Given such a direction, an optimal partition can be computed in linear time."
2509.12696,"Let $S$ be a set of $n$ points in the Euclidean plane and general position i.e., no three points are collinear. An \emph{at most $k$-out polygon of $S$} is a simple polygon such that each vertex is a point in $S$ and there are at most $k$ points outside the polygon. In this paper, we consider the problem of enumerating all the at most $k$-out polygon of $S$. We propose a new enumeration algorithm for the at most $k$-out polygons of a point set. Our algorithm enumerates all the at most $k$-out polygons in $\mathcal{O}(n^2 \log{n})$ delay, while the running time of an existing algorithm is $\mathcal{O}(n^3 \log{n})$ delay."
2509.13329,"2D nesting problems rank among the most challenging cutting and packing problems. Yet, despite their practical relevance, research over the past decade has seen remarkably little progress. One reasonable explanation could be that nesting problems are already solved to near optimality, leaving little room for improvement. However, as our paper demonstrates, we are not at the limit after all. This paper presents $\texttt{sparrow}$, an open-source heuristic approach to solving 2D irregular strip packing problems, along with ten new real-world instances for benchmarking. Our approach decomposes the optimization problem into a sequence of feasibility problems, where collisions between items are gradually resolved. $\texttt{sparrow}$ consistently outperforms the state of the art - in some cases by an unexpectedly wide margin. We are therefore convinced that the aforementioned stagnation is better explained by both a high barrier to entry and a widespread lack of reproducibility. By releasing $\texttt{sparrow}$'s source code, we directly address both issues. At the same time, we are confident there remains significant room for further algorithmic improvement. The ultimate aim of this paper is not only to take a single step forward, but to reboot the research culture in the domain and enable continued, reproducible progress."
2509.14357,"The Freeze-Tag Problem (FTP) is a scheduling problem with application in robot swarm activation and was introduced by Arkin et al. in 2002. This problem seeks an efficient way of activating a robot swarm starting with a single active robot. Activations occur through direct contact, and once a robot becomes active, it can move and help activate other robots.Although the problem has been shown to be NP-hard in the Euclidean plane $R^2$ under the $L_2$ distance, and in three-dimensional Euclidean space $R^3$ under any $L_p$ distance with $p \ge 1$, its complexity under the $L_1$ (Manhattan) distance in $R^2$ has remained an open question. In this paper, we settle this question by proving that FTP is strongly NP-hard in the Euclidean plane with $L_1$ distance."
2509.14709,"The presence of obstacles has a major impact on distance computation, motion planning, and visibility. While these problems are well studied in the plane, our understanding in three and higher dimensions is still limited. We investigate how different obstacle properties affect the induced geodesic metric in three-dimensional space. A finite metric space is said to be approximately realizable by a collection of obstacles if, for any $\varepsilon>0$, it can be embedded into the free space around the obstacles with geodesic distance and worst-case distortion $1+\varepsilon$. We focus on three key properties-convexity, disjointness, and fatness-and analyze how omitting each of them influences realizability.Our main result shows that if fatness is dropped, then every finite metric space can be realized with distortion $1+\varepsilon$ using convex, pairwise disjoint obstacles in $\mathbb{R}^3$, even if all obstacles are congruent equilateral triangles. Moreover, if we enforce fatness but drop convexity or disjointness, the same realizability still holds.Our results have important implications on the approximability of TSP with Obstacles, a natural variant of TSP introduced recently by Alkema et al. (ESA 2022). Specifically, we use the recent results of Banerjee et al. on TSP in doubling spaces (FOCS 2024) and of Chew et al. on distances among obstacles (Information Processing Letters 2002) to show that TSP with Obstacles admits a PTAS if the obstacles are convex, fat, and pairwise disjoint. If any of these three properties is dropped, then our results, combined with the APX-hardness of Metric TSP, demonstrate that TSP with Obstacles is APX-hard."
2509.15088,"Periodic point sets model all solid crystalline materials (crystals) whose atoms can be considered zero-sized points with or without atomic types. This paper addresses the fundamental problem of checking whether claimed crystals are novel, not noisy perturbations of known materials obtained by unrealistic atomic replacements. Such near-duplicates have skewed ground-truth because past comparisons relied on unstable cells and symmetries. The proposed Lipschitz continuity under noise is a new essential requirement for machine learning on any data objects that have ambiguous representations and live in continuous spaces. For periodic point sets under isometry (any distance-preserving transformation), we designed invariants that distinguish all known counter-examples to the completeness of past descriptors and detect thousands of (near-)duplicates in large high-profile databases of crystals within two days on a modest desktop computer."
2509.15687,"Merge trees are fundamental structures in topological data analysis. Interleaving distance is a widely accepted metric for comparing merge trees, with applications in visualization and scientific computing. While a greedy algorithm exists for finding the interleaving distance between labeled merge trees with overlapping labels, computing the interleaving distance between unlabeled trees or labeled trees with disjoint labels remains a significant challenge.In this work, we introduce a novel heuristic algorithm for approximating the interleaving distance between labeled merge trees with partial agreement and disagreement. Our method strategically assigns labels primarily to the leaves of the trees to infer structural correspondence. We also introduce an enhanced version of a previous algorithm that offers improved performance. Both algorithms run in polynomial time and provide practical, efficient alternatives for comparing merge trees, particularly in cases involving unlabeled or structurally diverse data. This work contributes a new direction for merge tree analysis and offers promising tools for real-world applications. We demonstrate this application on the simulation of time-varying electron density."
2509.16008,"We revisit the maximum range sum (MaxRS) problem: given a set $P$ of $n$ weighted points in $\mathbb{R}^d$ and a range $Q$ (typically axis-aligned $d$-box or $d$-ball), the goal is to place $Q$ to maximize the total weight of points in $P\cap Q$. We study three natural variations:(1) Dynamic MaxRS: The goal is to update the placement of a $d$-ball under point insertions and deletions. We give a randomized $(\frac{1}{2}-\epsilon)$-approximation with update time $O_\epsilon(\log n)$. The approximation factor holds with high probability. To the best of our knowledge, this is the first result on dynamic MaxRS.(2) Batched MaxRS: In $\mathbb{R}^1$, along with $P$ we are given $m$ intervals of varying lengths. We prove a conditional lower bound of $\Omega(mn)$ time (via conjectured $(\min,+)$-convolution hardness), showing the trivial $O(mn\log n)$ upper bound in $\mathbb{R}^2$ is essentially tight. We also establish a similar bound for a related problem of batched smallest $k$-enclosing interval.(3) Colored MaxRS: Each point has a color from $[m]$, and the goal is to place $Q$ to maximize the number of uniquely colored points in $P\cap Q$. Prior work only considered axis-aligned rectangles in $\mathbb{R}^2$. For $d$-balls, we give: (a) a randomized $(\frac{1}{2}-\epsilon)$-approximation in $O_\epsilon(n\log n)$ time (avoiding exponential dependence on $d$), and (b) in $\mathbb{R}^2$, a $(1-\epsilon)$-approximation in expected $O_\epsilon(n\log n)$ time. Both approximations hold with high probability.Our algorithms rely on two techniques of broader interest. The first yields $(\frac{1}{2}-\epsilon)$-approximations via a volume argument on $d$-balls and a randomized game. The second achieves $(1-\epsilon)$-approximations through an exact output-sensitive algorithm, which we speed up by random sampling on colors."
2509.17333,"We propose a novel graph visualization method leveraging random walk-based embeddings to replace costly graph-theoretical distance computations. Using word2vec-inspired embeddings, our approach captures both structural and semantic relationships efficiently. Instead of relying on exact shortest-path distances, we optimize layouts using cosine dissimilarities, significantly reducing computational overhead. Our framework integrates differentiable stress optimization with stochastic gradient descent (SGD), supporting multi-criteria layout objectives. Experimental results demonstrate that our method produces high-quality, semantically meaningful layouts while efficiently scaling to large graphs. Code available at:this https URL"
2509.17485,"In the paper ``Lower bounds on the number of crossing-free subgraphs of $K_N$'' (Computational Geometry 16 (2000), 211-221), it is shown that a double chain of $n$ points in the plane admits at least $\Omega(4.642126305^n)$ polygonizations, and it is claimed that it admits at most $O(5.61^n)$ polygonizations. In this note, we provide a proof of this last result. The proof is based on counting non-crossing path partitions for points in the plane in convex position, where a non-crossing path partition consists of a set of paths connecting the points such that no two edges cross and isolated points are allowed.We prove that a set of $n$ points in the plane in convex position admits $\mathcal{O}^*(5.610718614^{n})$ non-crossing path partitions and a double chain of $n$ points in the plane admits at least $\Omega(7.164102920^n)$ non-crossing path partitions. If isolated points are not allowed, we also show that there are $\mathcal{O}^*(4.610718614^n)$ non-crossing path partitions for $n$ points in the plane in convex position and at least $\Omega(6.164492582^n)$ non-crossing path partitions in a double chain of $n$ points in the plane. In addition, using a particular family of non-crossing path partitions for points in convex position, we provide an alternative proof for the result that a double chain of $n$ points admits at least $\Omega(4.642126305^n)$ polygonizations."
2509.19048,"This paper introduces a novel computational framework for modeling and analyzing the spatiotemporal shape variability of tree-like 4D structures whose shapes deform and evolve over time. Tree-like 3D objects, such as botanical trees and plants, deform and grow at different rates. In this process, they bend and stretch their branches and change their branching structure, making their spatiotemporal registration challenging. We address this problem within a Riemannian framework that represents tree-like 3D objects as points in a tree-shape space endowed with a proper elastic metric that quantifies branch bending, stretching, and topological changes. With this setting, a 4D tree-like object becomes a trajectory in the tree-shape space. Thus, the problem of modeling and analyzing the spatiotemporal variability in tree-like 4D objects reduces to the analysis of trajectories within this tree-shape space. However, performing spatiotemporal registration and subsequently computing geodesics and statistics in the nonlinear tree-shape space is inherently challenging, as these tasks rely on complex nonlinear optimizations. Our core contribution is the mapping of the tree-like 3D objects to the space of the Extended Square Root Velocity Field, where the complex elastic metric is reduced to the L2 metric. By solving spatial registration in the ESRVF space, analyzing tree-like 4D objects can be reformulated as the problem of analyzing elastic trajectories in the ESRVF space. Based on this formulation, we develop a comprehensive framework for analyzing the spatiotemporal dynamics of tree-like objects, including registration under large deformations and topological differences, geodesic computation, statistical summarization through mean trajectories and modes of variation, and the synthesis of new, random tree-like 4D shapes."
2509.20903,"Removing overlaps is a central task in domains such as scheduling, visibility, and map labelling. This task can be modelled using graphs, where overlap removals correspond to enforcing a certain sparsity constraint on the graph structure. We continue the study of the problem Geometric Graph Edit Distance, where the aim is to minimise the total cost of editing a geometric intersection graph to obtain a graph contained in a specific graph class. For us, the edit operation is the movement of objects, and the cost is the movement distance. We present an algorithm for rendering the intersection graph of a set of unit circular arcs (i)~edgeless, (ii)~acyclic, and (iii)~$k$-clique-free in $O(n\log n)$ time, where $n$ is the number of arcs. We also show that the problem remains strongly NP-hard on unweighted interval graphs, solving an open problem of [Honorato-Droguett et al., WADS 2025]. We complement this result by showing that the problem is strongly NP-hard on tuples of $d$-balls and $d$-cubes, for any $d\ge 2$. Finally, we present an XP algorithm (parameterised by the number of maximal cliques) for rendering the intersection graph of a set of weighted unit intervals edgeless."
2509.21681,"Most of the literature of computational geometry concerns geometric properties of sets of static points. M.J. Atallah introduced dynamic computational geometry, concerned with both momentary and long-term geometric properties of sets of moving point-objects. This area of research seems to have been dormant recently. The current paper examines new problems in dynamic computational geometry."
2509.22816,"The Mapper algorithm is a fundamental tool in exploratory topological data analysis for identifying connectivity and topological clustering in data. Derived from the nerve construction, Mapper graphs can contain additional information about clustering density when considering the higher-dimensional skeleta. To observe two-dimensional features, and capture one-dimensional topology, we construct 2-Mapper. A common issue in using Mapper algorithms is parameter choice. We develop tools to choose 2-Mapper parameters that reflect persistent Betti-1 information. Computationally, we study how cover choice affects 2-Mapper and analyze this through a computational Multiscale Mapper algorithm. We test our constructions on three-dimensional shape data, including the Klein bottle."
2509.23966,"$\renewcommand{\Re}{\mathbb{R}}$Given a set $P$ of $n$ points in $\Re^d$, and a parameter $\varepsilon \in (0,1)$, we present a new construction of a directed graph $G$, of size $O(n/\varepsilon^d)$, such that $(1+\varepsilon)$-ANN queries can be answered by performing a greedy walk on $G$, repeatedly moving to a neighbor that is (significantly) better than the current point. To the best of our knowledge, this is the first construction of a linear size with no dependency on the spread of the point set. The resulting query time, is $O( \varepsilon^{-d} \log \Psi)$, where $\Psi$ is the spread of $P$. The new construction is surprisingly simple and should be practical."
2510.01931,"In a connected simple graph G = (V(G),E(G)), each vertex is assigned one of c colors, where V(G) can be written as a union of a total of c subsets V_{1},...,V_{c} and V_{i} denotes the set of vertices of color i. A subset S of V(G) is called a selective subset if, for every i, every vertex v in V_{i} has at least one nearest neighbor in $S \cup (V(G) \setminus V_{i})$ that also lies in V_{i}. The Minimum Selective Subset (MSS) problem asks for a selective subset of minimum size.We show that the MSS problem is log-APX-hard on general graphs, even when c=2. As a consequence, the problem does not admit a polynomial-time approximation scheme (PTAS) unless P = NP. On the positive side, we present a PTAS for unit disk graphs, which works without requiring a geometric representation and applies for arbitrary c. We further prove that MSS remains NP-complete in unit disk graphs for arbitrary c. In addition, we show that the MSS problem is log-APX-hard on circle graphs, even when c=2."
2510.01939,"Avraham et al. [AFK+15] presented an alternative approach to parametric search, called \emph{bifurcation}, that performs faster under certain circumstances. Intuitively, when the underlying decider execution can be rolled back cheaply and the decider has a near-linear running time. For some problems, this leads to fast algorithms that beat the seemingly natural lower bound arising from distance selection.Bifurcation boils down to a tree exploration problem. You are given a binary (unfortunately implicit) tree of height $n$ and $k$ internal nodes with two children (all other internal nodes have a single child), and assume each node has an associated parameter value. These values are sorted in the inorder traversal of the tree. Assume there is (say) a node (not necessarily a leaf) that is the target node that the exploration needs to discover.The player starts from the root. At each step, the player can move to adjacent nodes to the current location (i.e., one of the children or the parent). Alternatively, the player can call an oracle on the current node, which returns either that it is the target (thus, mission accomplished!) or whether the target value is strictly smaller or larger than the current one.A naive algorithm explores the whole tree, in $O(n k)$ time, then performs $O(\log k n)$ calls to the oracle to find the desired leaf. Avraham \etal showed that this can be improved to $O(n \sqrt{k} )$ time, and $O( \sqrt{k} \log n)$ oracle calls.Here, we improve this to $O(n \sqrt{k} )$ time, with only $ O( \sqrt{k} + \log n)$ oracle calls. We also show matching lower bounds, under certain assumptions. We believe our interpretation of bifurcation as a tree exploration problem, and the associated algorithm, are of independent interest."
2510.02856,"Given a convex polytope $P$ defined with $n$ vertices in $\mathbb{R}^3$, this paper presents an algorithm to preprocess $P$ to compute routing tables at every vertex of $P$ so that a data packet can be routed on $P$ from any vertex of $P$ to any other vertex of $P$. At every vertex $v$ of $P$ along the routing path, until the packet reaches its destination, the next hop is determined using the routing tables at $v$ and the information stored in the packet header. In $O(n+\min(n^3, \frac{1}{\epsilon^7}))$ time, the preprocessing algorithm computes a routing table at every vertex of $P$ of amortized size $O(\min(n, \frac{1}{\epsilon^{3/2}}))$ bits. If the optimal shortest distance between $s$ and $t$ on $P$ is $d(s, t)$, then the routing path produced by this algorithm has length at most $\frac{8+\epsilon}{\sin{\theta_m}}(D+d(s,t))$. Here, $\epsilon \in (0, 1)$ is an input parameter, $D$ is the maximum length of the diagonal of any cell when $\partial P$ is partitioned into $\frac{1}{\epsilon^3}$ geodesic cells of equal size, and $\theta_m$ is half the minimum angle between any two neighbouring edges of $P$ on $\partial P$."
2510.0381,"I present a regression algorithm that provides a continuous, piecewise-smooth function approximating scattered data. It is based on composing and blending linear functions over Voronoi cells, and it scales to high dimensions. The algorithm infers Voronoi cells from seed vertices and constructs a linear function for the input data in and around each cell. As the algorithm does not explicitly compute the Voronoi diagram, it avoids the curse of dimensionality. An accuracy of around 98.2% on the MNIST dataset with 722,200 degrees of freedom (without data augmentation, convolution, or other geometric operators) demonstrates the applicability and scalability of the algorithm."
2510.04553,"We introduce a scalable witness-based persistent homology pipeline for full-brain MRI volumes that couples density-aware landmark selection with a GPU-ready witness filtration. Candidates are scored by a hybrid metric that balances geometric coverage against inverse kernel density, yielding landmark sets that shrink mean pairwise distances by 30-60% over random or density-only baselines while preserving topological features. Benchmarks on BrainWeb, IXI, and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX 4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha filtrations. The package is distributed on PyPI as whale-tda (installable via pip); source and issues are hosted atthis https URL. The release also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps, and ships with reproducibility-focused scripts and artifacts for drop-in use in medical imaging workflows."
2510.05896,"A fundamental problem in shape matching and geometric similarity is computing the maximum area overlap between two polygons under translation. For general simple polygons, the best-known algorithm runs in $O((nm)^2 \log(nm))$ time [Mount, Silverman, Wu 96], where $n$ and $m$ are the complexities of the input polygons. In a recent breakthrough, Chan and Hair gave a linear-time algorithm for the special case when both polygons are convex. A key challenge in computational geometry is to design improved algorithms for other natural classes of polygons. We address this by presenting an $O((nm)^{3/2} \log(nm))$-time algorithm for the case when both polygons are orthogonal. This is the first algorithm for polygon overlap on orthogonal polygons that is faster than the almost 30 years old algorithm for simple polygons.Complementing our algorithmic contribution, we provide $k$-SUM lower bounds for problems on simple polygons with only orthogonal and diagonal edges. First, we establish that there is no algorithm for polygon overlap with running time $O(\max(n^2,nm^2)^{1-\varepsilon})$, where $m\leq n$, unless the $k$-SUM hypothesis fails. This matches the running time of our algorithm when $n=m$. We use part of the above construction to also show a lower bound for the polygon containment problem, a popular special case of the overlap problem. Concretely, there is no algorithm for polygon containment with running time $O(n^{2-\varepsilon})$ under the $3$-SUM hypothesis, even when the polygon to be contained has $m=O(1)$ vertices. Our lower bound shows that polygon containment for these types of polygons (i.e., with diagonal edges) is strictly harder than for orthogonal polygons, and also strengthens the previously known lower bounds for polygon containment. Furthermore, our lower bounds show tightness of the algorithm of [Mount, Silverman, Wu 96] when $m=O(1)$."
2510.07955,"We present theory and practice for robust implementations of bivariate Jacobi set and Reeb space algorithms. Robustness is a fundamental topic in computational geometry that deals with the issues of numerical errors and degenerate cases in algorithm implementations. Computational topology already uses some robustness techniques for the development of scalar field algorithms, such as those for computing critical points, merge trees, contour trees, Reeb graphs, Morse-Smale complexes, and persistent homology. In most cases, robustness can be ensured with floating-point arithmetic, and degenerate cases can be resolved with a standard symbolic perturbation technique called Simulation of Simplicity. However, this becomes much more complex for topological data structures of multifields, such as Jacobi sets and Reeb spaces. The geometric predicates used in their computation require exact arithmetic and a more involved treatment of degenerate cases to ensure correctness. Neither of these challenges has been fully addressed in the literature so far. In this paper, we describe how exact arithmetic and symbolic perturbation schemes can be used to enable robust implementations of bivariate Jacobi set and Reeb space algorithms. In the process, we develop a method for automatically evaluating predicates that can be expressed as large symbolic polynomials, which are difficult to factor appropriately by hand, as is typically done in the computational geometry literature. We provide implementations of all proposed approaches and evaluate their efficiency."
2510.09328,"We study the problem of constructing Steiner Minimal Trees (SMTs) in hyperbolic space. Exact SMT computation is NP-hard, and existing hyperbolic heuristics such as HyperSteiner are deterministic and often get trapped in locally suboptimal configurations. We introduce Randomized HyperSteiner (RHS), a stochastic Delaunay triangulation heuristic that incorporates randomness into the expansion process and refines candidate trees via Riemannian gradient descent optimization. Experiments on synthetic data sets and a real-world single-cell transcriptomic data show that RHS outperforms Minimum Spanning Tree (MST), Neighbour Joining, and vanilla HyperSteiner (HS). In near-boundary configurations, RHS can achieve a 32% reduction in total length over HS, demonstrating its effectiveness and robustness in diverse data regimes."
2510.09417,"Finding the convex hull is a fundamental problem in computational geometry. Quickhull is a fast algorithm for finding convex hulls. In this paper, we present VQhull, a fast parallel implementation of Quickhull that exploits vector instructions, and coordinates CPU cores in a way that minimizes data movement. This implementation obtains a sequential runtime improvement of 1.6--16x, and a parallel runtime improvement of 1.5-11x compared to the state of the art on the Problem Based Benchmark Suite. VQhull achieves 85--100% of non-NUMA architectures' peak bandwidth, and 66--78% on our two-CPU NUMA system. This leaves little room for further improvements.A 4x speedup on 8 cores has a parallel efficiency of 50%. This suggests a waste of energy, but our measurements show a more complicated picture: energy usage may even be lower in parallel. Quickhull serves as a case study that runtime and energy consumption do not go hand in hand."
2510.09431,"Quickhull is an algorithm for computing the convex hull of points in a plane that performs well in practice, but has poor complexity on adversarial input. In this paper we show the same holds for the numerical stability of Quickhull."
2510.10233,"Comparing probability measures when their supports are related by an unknown rigid transformation is an important challenge in geometric data analysis, arising in shape matching and machine learning. Classical optimal transport (OT) distances, including Wasserstein and sliced Wasserstein, are sensitive to rotations and reflections, while Gromov-Wasserstein (GW) is invariant to isometries but computationally prohibitive for large datasets. We introduce \emph{Rigid-Invariant Sliced Wasserstein via Independent Embeddings} (RISWIE), a scalable pseudometric that combines the invariance of NP-hard approaches with the efficiency of projection-based OT. RISWIE utilizes data-adaptive bases and matches optimal signed permutations along axes according to distributional similarity to achieve rigid invariance with near-linear complexity in the sample size. We prove bounds relating RISWIE to GW in special cases and empirically demonstrate dimension-independent statistical stability. Our experiments on cellular imaging and 3D human meshes demonstrate that RISWIE outperforms GW in clustering tasks and discriminative capability while significantly reducing runtime."
2510.13372,"In this paper, we propose a new variational framework for 3D surface denoising over triangulated meshes, which is inspired by the success of semi-sparse regularization in image processing. Differing from the uniformly sampled image data, mesh surfaces are typically represented by irregular, non-uniform structures, which thus complicate the direct application of the standard formulation and pose challenges in both model design and numerical implementation. To bridge this gap, we first introduce the discrete approximations of higher-order differential operators over triangle meshes and then develop a semi-sparsity regularized minimization model for mesh denoising. This new model is efficiently solved by using a multi-block alternating direction method of multipliers (ADMM) and achieves high-quality simultaneous fitting performance -- preserving sharp features while promoting piecewise-polynomial smoothing surfaces. To verify its effectiveness, we also present a series of experimental results on both synthetic and real scanning data, showcasing the competitive and superior results compared to state-of-the-art methods, both visually and quantitatively."
2510.13978,"We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website:this https URL"
2510.16682,"We introduce ellipsoidal filtration, a novel method for persistent homology, and demonstrate its effectiveness in denoising recurrent signals. Unlike standard Rips filtrations, which use isotropic neighbourhoods and ignore the signal's direction of evolution, our approach constructs ellipsoids aligned with local gradients to capture trajectory flow. The death scale of the most persistent H_1 feature defines a data-driven neighbourhood for averaging. Experiments on synthetic signals show that our method achieves better noise reduction than both topological and moving-average filters, especially for low-amplitude components."
2510.17735,"One common use of persistent homology is to explore the shape of point clouds, where points are assumed to be sampled from a geometric object. We propose a novel filtration, called ellipsoidal filtration, which assumes that point clouds are sampled from a dynamic smooth flow. Instead of creating topologies from point clouds at increasing scales using isotropic balls (for example, Vietoris-Rips filtration), ellipsoidal filtration creates ellipsoids around points based on local flow variances, approximating the flow's manifold as the scale increases. We show that constructing ellipsoidal neighbourhoods improves the denoising of recurrent signals and the estimation of recurrence times, especially when the data contain bottlenecks. Choosing ellipsoids according to the maximum persistence of the H1 class provides a data-driven threshold for both denoising and recurrence-time estimation."
2510.17737,"We exactly settle the complexity of graph realization, graph rigidity, and graph global rigidity as applied to three types of graphs: ""globally noncrossing"" graphs, which avoid crossings in all of their configurations; matchstick graphs, with unit-length edges and where only noncrossing configurations are considered; and unrestricted graphs (crossings allowed) with unit edge lengths (or in the global rigidity case, edge lengths in $\{1,2\}$). We show that all nine of these questions are complete for the class $\exists\mathbb{R}$, defined by the Existential Theory of the Reals, or its complement $\forall\mathbb{R}$; in particular, each problem is (co)NP-hard.One of these nine results--that realization of unit-distance graphs is $\exists\mathbb{R}$-complete--was shown previously by Schaefer (2013), but the other eight are new. We strengthen several prior results. Matchstick graph realization was known to be NP-hard (Eades \& Wormald 1990, or Cabello et al.\ 2007), but its membership in NP remained open; we show it is complete for the (possibly) larger class $\exists\mathbb{R}$. Global rigidity of graphs with edge lengths in $\{1,2\}$ was known to be coNP-hard (Saxe 1979); we show it is $\forall\mathbb{R}$-complete.The majority of the paper is devoted to proving an analog of Kempe's Universality Theorem--informally, ""there is a linkage to sign your name""--for globally noncrossing linkages. In particular, we show that any polynomial curve $\phi(x,y)=0$ can be traced by a noncrossing linkage, settling an open problem from 2004. More generally, we show that the regions in the plane that may be traced by a noncrossing linkage are precisely the compact semialgebraic regions (plus the trivial case of the entire plane). Thus, no drawing power is lost by restricting to noncrossing linkages. We prove analogous results for matchstick linkages and unit-distance linkages as well."
2510.18197,"We study the problem of whether rectangular polyominoes with holes are cube-foldable, that is, whether they can be folded into a cube, if creases are only allowed along grid lines. It is known that holes of sufficient size guarantee that this is the case.Smaller holes which by themselves do not make a rectangular polyomino cube-foldable can sometimes be combined to create cube-foldable polyominoes. We investigate minimal sets of holes which guarantee cube-foldability. We show that if all holes are of the same type, the these minimal sets have size at most 4, and if we allow different types of holes, then there is no upper bound on the size."
2511.0296,"Recently, a natural variant of the Art Gallery problem, known as the \emph{Contiguous Art Gallery problem} was proposed. Given a simple polygon $P$, the goal is to partition its boundary $\partial P$ into the smallest number of contiguous segments such that each segment is completely visible from some point in $P$. Unlike the classical Art Gallery problem, which is NP-hard, this variant is polynomial-time solvable. At SoCG~2025, three independent works presented algorithms for this problem, each achieving a running time of $O(k n^5 \log n)$ (or $O(n^6\log n)$), where $k$ is the size of an optimal solution. Interestingly, these results were obtained using entirely different approaches, yet all led to roughly the same asymptotic complexity, suggesting that such a running time might be inherent to the problem.We show that this is not the case. In the real RAM-model, the prevalent model in computational geometry, we present an $O(n \log n)$-time algorithm, achieving an $O(k n^4)$ factor speed-up over the previous state-of-the-art. We also give a straightforward sorting-based lower bound by reducing from the set intersection problem. We thus show that the Contiguous Art Gallery problem is in $\Theta(n \log n)$."
2511.00692,"Given a set $P$ of $n$ points in $\mathbf{R}^d$, and a positive integer $k \leq n$, the $k$-dispersion problem is that of selecting $k$ of the given points so that the minimum inter-point distance among them is maximized (under Euclidean distances). Among others, we show the following:(I) Given a set $P$ of $n$ points in the plane, and a positive integer $k \geq 2$, the $k$-dispersion problem can be solved by an algorithm running in $O\left(n^{k-1} \log{n}\right)$ time. This extends an earlier result for $k=3$, due to Horiyama, Nakano, Saitoh, Suetsugu, Suzuki, Uehara, Uno, and Wasa (2021) to arbitrary $k$. In particular, it improves on previous running times for small $k$.(II) Given a set $P$ of $n$ points in $\mathbf{R}^3$, and a positive integer $k \geq 2$, the $k$-dispersion problem can be solved by an algorithm running in $O\left(n^{k-1} \log{n}\right)$ time, if $k$ is even; and $O\left(n^{k-1} \log^2{n}\right)$ time, if $k$ is odd. For $k \geq 4$, no combinatorial algorithm running in $o(n^k)$ time was known for this problem.(III) Let $P$ be a set of $n$ random points uniformly distributed in $[0,1]^2$. Then under suitable conditions, a $0.99$-approximation for $k$-dispersion can be computed in $O(n)$ time with high probability."
2511.01562,"The boundary-boundary art-gallery problem asks, given a polygon $P$ representing an art-gallery, for a minimal set of guards that can see the entire boundary of $P$ (the wall of the art gallery), where the guards must be placed on the boundary. We show that this art-gallery variant is in NP. In order to prove this, we develop a constraint-propagation procedure for continuous constraint satisfaction problems where each constraint involves at most 2 variables.The X-Y variant of the art-gallery problem is the one where the guards must lie in X and need to see all of Y. Each of X and Y can be either the vertices of the polygon, the boundary of the polygon, or the entire polygon, giving 9 different variants. Previously, it was known that X-vertex and vertex-Y variants are all NP-complete and that the point-point, point-boundary, and boundary-point variants are $\exists \mathbb{R}$-complete [Abrahamsen, Adamaszek, and Miltzow, JACM 2021][Stade, SoCG 2025]. However, the boundary-boundary variant was only known to lie somewhere between NP and $\exists \mathbb{R}$.The X-vertex and vertex-Y variants can be straightforwardly reduced to discrete set-cover instances. In contrast, we give example to show that a solution to an instance of the boundary-boundary art-gallery problem sometimes requires placing guards at irrational coordinates, so it unlikely that the problem can be easily discretized."
2511.02064,"Constructing an adaptive hexahedral tessellation to fit an input triangle boundary is a key challenge in grid-based methods. The conventional method first removes outside elements (RO) and then projects the axis-aligned boundary onto the input triangle boundary, which has no guarantee on improving the initial Intersection over Union (IoU) and Hausdorff distance ratio (HR, w.r.t bounding box diagonal). The proposed MCHex approach replaces RO with a Marching Cubes method MCHex. Given the same computational budget (benchmarked using an identical precomputed Signed Distance Field, which dominates the runtime), MCHex provides better boundary approximation (higher IoU and lower HR) while guaranteeing a lower, yet still positive, minimum scaled Jacobian (>0 vs. RO's >0.48)."
2511.03642,"This paper introduces a novel $k$-cell decomposition method for pursuit-evasion problems in polygonal environments, where a searcher is equipped with a $k$-modem: a device capable of seeing through up to $k$ walls. The proposed decomposition ensures that as the searcher moves within a cell, the structure of unseen regions (shadows) remains unchanged, thereby preventing any geometric events between or on invisible regions, that is, preventing the appearance, disappearance, merge, or split of shadow regions. The method extends existing work on $0$- and $2$-visibility by incorporating m-visibility polygons for all even $0 \le m \le k$, constructing partition lines that enable robust environment division. The correctness of the decomposition is proved via three theorems. The decomposition enables reliable path planning for intruder detection in simulated environments and opens new avenues for visibility-based robotic surveillance. The difficulty in constructing the cells of the decomposition consists in computing the $k$-visibility polygon from each vertex and finding the intersection points of the partition lines to create the cells."
2511.03909,"The weighted Euler characteristic transform (WECT) and Euler characteristic function (ECF) have proven to be useful tools in a variety of applications. However, current methods for computing these functions are neither optimized for speed nor do they scale to higher-dimensional settings. In this work, we present a vectorized framework for computing such topological transforms using tensor operations, which is highly optimized for GPU architectures and works in full generality across geometric simplicial complexes (or cubical complexes) of arbitrary dimension. Experimentally, the framework demonstrates significant speedups (up to $180 \times$) over existing methods when computing the WECT and ECF across a variety of image datasets. Computation of these transforms is implemented in a publicly available Python package called pyECT."
2511.05371,"The Planar Separator Theorem, which states that any planar graph $\mathcal{G}$ has a separator consisting of $O(\sqrt{n})$ nodes whose removal partitions $\mathcal{G}$ into components of size at most $\tfrac{2n}{3}$, is a widely used tool to obtain fast algorithms on planar graphs. Intersection graphs of disks, which generalize planar graphs, do not admit such separators. It has recently been shown that disk graphs do admit so-called clique-based separators that consist of $O(\sqrt{n})$ cliques. This result has been generalized to intersection graphs of various other types of disk-like objects. Unfortunately, segment intersection graphs do not admit small clique-based separators, because they can contain arbitrarily large bicliques. This is true even in the simple case of axis-aligned segments.In this paper we therefore introduce biclique-based separators (and, in particular, star-based separators), which are separators consisting of a small number of bicliques (or stars). We prove that any $c$-oriented set of $n$ segments in the plane, where $c$ is a constant, admits a star-based separator consisting of $O(\sqrt{n})$ stars. In fact, our result is more general, as it applies to any set of $n$ pseudo-segments that is partitioned into $c$ subsets such that the pseudo-segments in the same subset are pairwise disjoint. We extend our result to intersection graphs of $c$-oriented polygons. These results immediately lead to an almost-exact distance oracle for such intersection graphs, which has $O(n\sqrt{n})$ storage and $O(\sqrt{n})$ query time, and that can report the hop-distance between any two query nodes in the intersection graph with an additive error of at most 2. This is the first distance oracle for such types of intersection graphs that has subquadratic storage and sublinear query time and that only has an additive error."
2511.06114,"Automatic assembly of apictorial jigsaw puzzles presents a classic curve matching problem, fundamentally challenged by discrete and noisy contour data obtained from digitization. Conventional smoothing methods, which are required to process these data, often distort the curvature-based criteria used for matching and cause a loss of critical information. This paper proposes a method to overcome these issues, demonstrated on the automatic reconstruction of a 54-piece puzzle.We reconstruct each piece's contour using a novel corotational beam spline, which models the boundary as a flexible beam with compliant spring supports at the measured data points. A distinctive feature is the dynamic re-indexing of these points; as their calculated positions are refined, they are re-numbered based on their projection onto the computed contour.Another contribution is a method for determining spring compliance in proportion to the distance between the point projections. This approach uniquely ensures a uniform degree of smoothing for corresponding curves, making the matching process robust to variations in point density and dependent only on measurement accuracy. Practical computations and the successful automatic reconstruction of the puzzle demonstrate the proposed method's effectiveness."
