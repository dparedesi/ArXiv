## [2025Wk26]: From output to outcome: engineering AI with feedback

After a long arc of research dedicated to engineering self-improving systems, mandating efficiency, taming advanced reasoning, modeling AI's social contract, ensuring system integrity, and finally, creating a new era of rigorous evaluation, the field is now focused on the next logical step: closing the loop. The sustained push to make AI more capable and trustworthy has revealed that generating a plausible output is no longer sufficient. This week's research demonstrates a clear pivot toward engineering systems that actively learn from their outcomes, refine their processes through feedback, and ground their intelligence in verifiable interaction. The new imperative is to build AI that is not just a generator of information, but a participant in a continuous cycle of action, feedback, and improvement.

### Refining thought: feedback in the reasoning process

The quality of an AI's output is directly tied to the integrity of its internal reasoning. Recognizing this, researchers are moving beyond simply eliciting long chains of thought and are now engineering the reasoning process itself for greater accuracy and consistency through feedback. This involves creating frameworks that can verify intermediate steps and use those signals to guide the model. One new approach, Refinement-oriented Critique Optimization, trains a critic model to provide feedback that is specifically designed to guide an actor model toward better responses, creating a tight loop where critique quality is measured by its impact on refinement. [cite: 2506.22157] Similar verification principles are being applied to structured reasoning, where preference optimization techniques teach models how to more accurately convert natural language problems into consistent logical programs. [cite: 2506.18383]

This feedback is also becoming more granular. A novel framework for multimodal reasoning, Metis-RISE, uses a unique two-stage process where reinforcement learning first incentivizes the model to explore new reasoning paths, and then supervised fine-tuning injects expert knowledge to correct specific failures identified during exploration. [cite: 2506.13056] This dynamic interplay between exploration and correction is also seen in the development of DiffuCoder, a diffusion-based language model for code generation, which uses a new reinforcement learning method to efficiently explore a diverse search space of token generation orders. [cite: 2506.20639] These methods show that by making the reasoning process itself an object of optimization and feedback, models can learn not just to answer questions, but to think more reliably.

### From open-loop generation to closed-loop agency

As AI systems evolve into autonomous agents, their ability to learn from interaction with an environment becomes paramount. The focus is shifting from generating open-loop plans to creating closed-loop systems that can adapt based on feedback and experience. A key innovation in this area is enabling agents to learn from their own, often imperfect, trajectories. One framework allows a model to relabel its own unsuccessful attempts by having a large language model identify meaningful subtasks that were completed along the way, effectively turning failures into valuable, instruction-rich training data. [cite: 2506.20061]

This learning from interaction is also becoming more direct and human-centric. A new interactive learning system for robotics allows a multi-modal large language model to learn from natural dialogue with non-expert users, clarifying intent through a "chain of question" process and using a memory module to avoid repeating mistakes. [cite: 2506.22896] The connection between high-level reasoning and low-level execution is also being tightened. The FaSTA* framework combines the fast, high-level planning of an LLM with a slow, meticulous A* search for executing individual subtasks, and then uses inductive reasoning to distill successful low-level action sequences into reusable subroutines for future use. [cite: 2506.20911] These approaches are essential for building agents that are not just following a script but are genuinely learning from and adapting to their world.

### Learning from reality: when the world becomes the feedback signal

For AI to act effectively in a feedback loop, its perception of the world must be grounded in reality. The most powerful feedback comes not from synthetic rewards but from the physical and semantic constraints of the real world itself. This is driving new methods for multimodal generation and understanding that use real-world data as a natural source of corrective signal. In video generation, where maintaining coherence over time is a major challenge, the Real Data Preference Optimisation method reverse-samples real video sequences to automatically create preference pairs that teach a model about physically plausible motion, using reality as ground truth for what constitutes valid action. [cite: 2506.18655] The MVAA framework takes a similar approach, using the temporal structure of music as a feedback signal to guide video generation, automatically aligning motion with beat-aligned timestamps to ensure both rhythmic and visual consistency. [cite: 2506.18881]

This push for reality-grounded perception also applies to 3D scene understanding, where geometric and semantic consistency serve as implicit feedback signals. The PanSt3R framework provides a unified, end-to-end model that jointly predicts 3D geometry and multi-view panoptic segmentation in a single forward pass, using cross-view consistency as a natural constraint. [cite: 2506.21348] For interactive scenes, the HoliGS framework uses invertible Gaussian splatting networks to reconstruct large-scale, dynamic environments from long monocular videos, leveraging temporal consistency across frames as feedback to decouple static and dynamic elements. [cite: 2506.19291] These methods demonstrate that the structure of reality itself—its physical laws and semantic regularities—can serve as the ultimate feedback signal for learning more accurate world models.

### The security feedback loop: from static defense to active response

In a closed-loop system, security cannot be a static feature; it must be an active, adaptive process. As agents and models become more autonomous, they also present new attack surfaces, demanding a new generation of security frameworks that can detect and respond to threats in real time. RedCoder, for example, is a red-teaming agent designed to simulate multi-turn adversarial interactions with code generation models, automatically discovering and cataloging vulnerabilities that emerge in conversational contexts. [cite: 2506.22063] This provides a continuous stream of feedback for hardening these systems.

The nature of attacks is also becoming more sophisticated, moving beyond simple prompt manipulation. A novel backdoor attack framework called SPA operates in the feature space, aligning the features of a trigger with those of a target class to create a stealthy and persistent attack that is difficult to detect through traditional label-based methods. [cite: 2506.20931] In response, detection mechanisms are also becoming more dynamic. A new framework for detecting privacy breaches in LLMs, PrivacyXray, analyzes a model's internal states, such as semantic coherence and probabilistic certainty, to identify when it is outputting sensitive information without needing external validation datasets. [cite: 2506.19563] This move toward active, feedback-driven security is essential for building systems that can defend themselves in a constantly evolving threat landscape.

In conclusion, this week's research demonstrates a field that is engineering AI for a new level of operational maturity. By refining reasoning through iterative critique, researchers are making AI's thought processes more reliable. By designing closed-loop agents that learn from direct experience, they are enabling systems that improve through interaction. By treating reality itself as a feedback signal, they are grounding perception in the physical and semantic structure of the world. Finally, by creating an active security feedback loop, they are building systems that can adapt to evolving threats. This collective focus on feedback and refinement is about transforming AI from a system that produces static outputs to one that achieves dynamic outcomes—systems that don't just answer questions but learn from mistakes, don't just generate content but verify its correctness, and don't just act but adapt. This is the foundation for truly intelligent and trustworthy partners.