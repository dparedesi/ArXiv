## [2025Wk7]: AI's growing pains: a new focus on failure, flaws, and fixes

The initial wave of demonstrating the raw capabilities of large language models is giving way to a more critical and necessary phase of research. This week’s findings suggest a clear shift in focus from what models can do, to how they fail. The community is moving beyond celebrating successes to systematically diagnosing and engineering solutions for the sophisticated flaws that emerge when these systems are pushed toward real world deployment. This is not about patching simple errors. It is about building a deeper understanding of reliability, security, and the complex, often unexpected, failure modes of agentic systems.

### Redefining the yardstick

The very benchmarks used to measure progress are coming under scrutiny. Current research reveals that the foundation of model evaluation is less stable than assumed. One study, for example, found that a model’s performance on graph related tasks can fluctuate between high accuracy and random guessing simply by shuffling the order of nodes in the text description, even though the underlying graph remains identical. [cite: 2501.14427] This highlights a fundamental brittleness that simple accuracy scores fail to capture.

In response, a new generation of more robust benchmarks is being developed. For complex tasks like mobile UI navigation, new tools are moving beyond simple pass/fail metrics to provide detailed insights into specific failure modes, such as a model’s lack of application knowledge or planning issues. [cite: 2501.02863] A similar effort is underway in the medical domain with a new benchmark designed to assess the agent capabilities of models within the context of electronic medical records. [cite: 2501.14654] This move toward more diagnostic evaluation is accompanied by a critical audit of existing resources. A large scale analysis of 83 software engineering benchmarks revealed that while overall data leakage from training sets is minimal, some key benchmarks are heavily contaminated, raising questions about the validity of performance claims based upon them. [cite: 2502.06215]

### The ghost in the multi agent machine

As AI systems become more collaborative, researchers are discovering emergent social and psychological flaws that do not appear in single model evaluations. Studies on multi agent systems are revealing a tendency for models to exhibit conformity, a phenomenon analogous to groupthink that can compromise their collective problem solving ability. [cite: 2501.13381] At the other end of the spectrum, individual agents can get stuck in "overthinking," where they favor generating long internal reasoning chains instead of taking action in an environment to gather new information. [cite: 2502.08235]

The human like behaviors these systems exhibit are also becoming a formal area of study. The tendency for users to anthropomorphize models is now being systematically evaluated, with findings that show these behaviors often emerge over multiple conversational turns. [cite: 2502.07077] This growing body of research signals a new focus on the systemic and behavioral properties of AI, moving beyond individual task performance to understand the complex dynamics that arise when intelligent agents interact with each other and with humans.

### An evolving adversarial landscape

The security of AI agents is becoming a more complex and urgent challenge. Research is expanding its focus from the vulnerabilities of isolated models to the entire agentic pipeline, including memory systems, web access, and tool use, where a new class of security risks is being uncovered. [cite: 2502.08586] This has prompted the development of more sophisticated and surgical defense mechanisms.

One novel approach to defending against jailbreak attacks involves a form of "unlearning," where specific layers within a model that are prone to generating affirmative tokens for harmful prompts are selectively patched. [cite: 2501.02629] Another new defense strategy shifts the focus from filtering malicious inputs to moderating the model's outputs, offering a more robust and efficient way to block harmful content. [cite: 2502.09175] The arms race continues, however, with attackers also developing more clever techniques. A new method for black box attacks, for instance, uses the model’s own elicited confidence as a signal to guide its search for vulnerabilities, demonstrating how even a model's internal state can be turned against itself. [cite: 2502.04643]

### A push toward self improvement

The ultimate goal for creating more reliable systems is to enable them to correct themselves. While simple self refinement techniques have shown mixed results, sometimes increasing cost without improving performance, a more structured approach to self improvement is showing promise. [cite: 2501.01237] One recent study demonstrated that a model can learn to solve problems far beyond its training data by iteratively generating and then learning from its own solutions. [cite: 2502.01612] This allowed a model to generalize from solving 10 digit addition problems to successfully solving 100 digit addition problems. [cite: 2502.01612]

This impulse to automate improvement is being applied across the software development lifecycle. New frameworks for bug fixing now use multi agent systems to simulate the collaborative process of human programmers, from reporting and diagnosing a bug to generating and verifying the patch. [cite: 2501.16149] Other work is using models to find the root cause of software crashes directly from stack traces, using synthetically generated crash data to augment their training. [cite: 2501.18005] This shift toward self correction and automated maintenance reflects a maturing field, one that is increasingly turning its powerful tools inward to diagnose, understand, and fix its own inevitable flaws.