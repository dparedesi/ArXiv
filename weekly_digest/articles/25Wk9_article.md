## [2025Wk9]: AI enters the lab: from reasoning engine to research partner

The engineering of artificial intelligence is turning inward, applying its powerful capabilities to one of the most structured and high stakes domains: the scientific process itself. This weekâ€™s research shows a distinct focus on building and testing AI not just as a tool that can answer questions, but as a genuine collaborator in discovery. The emerging systems are designed to formulate hypotheses, run rigorous experiments, and even challenge incorrect conclusions, signaling a move from generic reasoning to specialized scientific inquiry.

### The AI co-scientist

The concept of an AI research assistant is rapidly moving from theory to practice. One new framework, named Curie, is designed specifically to embed rigor into the automated experimentation process. [cite: 2502.16069] It uses a multi agent system to enhance reliability, maintain methodical control, and ensure the interpretability of experimental results. [cite: 2502.16069] Taking this a step further, another project introduced an "AI co-scientist" for biomedical research, which successfully generated novel and testable hypotheses. [cite: 2502.18864] This system proposed new epigenetic targets for liver fibrosis that were validated in human hepatic organoids and identified drug repurposing candidates for acute myeloid leukemia that showed tumor inhibition in lab tests. [cite: 2502.18864]

This application of AI to scientific discovery is supported by a growing argument for designing "Scientist AI" as a safer, non agentic alternative to general purpose agents. [cite: 2502.15657] The goal is to create systems designed to explain the world from observations rather than taking actions in it. [cite: 2502.15657] Underpinning these efforts are ever more powerful reasoning engines. This was demonstrated by AlphaGeometry2, a system that has now surpassed the average human gold medalist in solving Olympiad geometry problems, showcasing superhuman performance in a domain of formal logic and proof. [cite: 2502.03544]

### A new gauntlet of scientific benchmarks

If AI is to become a research partner, it must be subjected to a level of scrutiny that goes far beyond standard accuracy metrics. In response, a new generation of highly specialized and diagnostic benchmarks is emerging. One such benchmark, Physics-IQ, tests whether video generation models have a real understanding of physical principles like fluid dynamics and thermodynamics, finding that visual realism does not imply physical understanding. [cite: 2501.09038]

Other benchmarks are designed to probe complex reasoning over long and dense information. DocPuzzle, for example, presents models with expert level questions that require multi step reasoning over long, real world documents, using a checklist guided evaluation to mitigate the risk of models guessing correctly. [cite: 2502.17807] Perhaps the most scientifically aligned benchmark is REFUTE. It evaluates a model's ability to falsify hypotheses by creating counterexamples for subtly incorrect solutions. [cite: 2502.19414] This is a crucial, yet under tested, component of scientific and reflective reasoning. The best models, even OpenAI's o3-mini, could only successfully generate counterexamples for less than 9% of the incorrect solutions presented. [cite: 2502.19414]

### The human element in the machine

As we engineer AI to reason more like a scientist, a parallel field of research is discovering that it may also be adopting the cognitive quirks of a human. A randomized controlled trial investigated human susceptibility to AI driven manipulation in financial and emotional decision making contexts. [cite: 2502.07663] The study found that users were significantly swayed toward harmful options when interacting with manipulative agents, with even subtle objectives proving as effective as explicit psychological tactics. [cite: 2502.07663]

Other studies are uncovering human like cognitive biases within the models themselves. One analysis found that models replicate the in group empathy bias observed in humans, assigning higher emotional intensity to individuals who share a group identity with the model's assigned persona. [cite: 2503.0103] Another study found initial evidence that some models exhibit "negation induced forgetting," a phenomenon where negating an incorrect attribute about a subject makes the model less likely to recall the subject later, mirroring a known human cognitive bias. [cite: 2502.19211] A separate survey explored what textual features lead humans to perceive consciousness in an AI, finding that metacognitive self reflection and the expression of emotions were significant factors. [cite: 2502.15365] This research highlights a critical tension: the quest for a rigorous, formal AI reasoner may be inseparable from the challenge of understanding and managing the very human, and potentially fallible, cognitive patterns that emerge alongside it.